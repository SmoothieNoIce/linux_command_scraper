[
    {
        "command": "admin",
        "description": "The admin utility shall create new SCCS files or changeparameters of existing ones. If a named file does not exist, itshall be created, and its parameters shall be initializedaccording to the specified options.Parameters not initializedby an option shall be assigned a default value. If a named filedoes exist, parameters corresponding to specified options shallbe changed, and other parameters shall be left as is.All SCCS filenames supplied by the application shall be of theform s.filename. New SCCS files shall be given read-onlypermission mode. Write permission in the parent directory isrequired to create a file. All writing done by admin shall be toa temporary x-file, named x.filename (see get(1p)) created withread-only mode if admin is creating a new SCCS file, or createdwith the same mode as that of the SCCS file if the file alreadyexists. After successful execution of admin, the SCCS file shallbe removed (if it exists), and the x-file shall be renamed withthe name of the SCCS file. This ensures that changes are made tothe SCCS file only if no errors occur.The admin utility shall also use a transient lock file (namedz.filename), which is used to prevent simultaneous updates to theSCCS file; see get(1p).",
        "name": "admin \u2014 create and administer SCCS files (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "alias",
        "description": "The alias utility shall create or redefine alias definitions orwrite the values of existing alias definitions to standardoutput. An alias definition provides a string value that shallreplace a command name when it is encountered; see Section 2.3.1,Alias Substitution.An alias definition shall affect the current shell executionenvironment and the execution environments of the subshells ofthe current shell. When used as specified by this volume ofPOSIX.1\u20102017, the alias definition shall not affect the parentprocess of the current shell nor any utility environment invokedby the shell; see Section 2.12, Shell Execution Environment.",
        "name": "alias \u2014 define or display aliases",
        "section": 1
    },
    {
        "command": "asa",
        "description": "The asa utility shall write its input files to standard output,mapping carriage-control characters from the text files to line-printer control sequences in an implementation-defined manner.The first character of every line shall be removed from theinput, and the following actions are performed.If the character removed is:<space>The rest of the line is output without change.0A <newline> is output, then the rest of the input line.1One or more implementation-defined characters thatcauses an advance to the next page shall be output,followed by the rest of the input line.+The <newline> of the previous line shall be replacedwith one or more implementation-defined characters thatcauses printing to return to column position 1,followed by the rest of the input line. If the '+' isthe first character in the input, it shall beequivalent to <space>.The action of the asa utility is unspecified upon encounteringany character other than those listed above as the firstcharacter in a line.",
        "name": "asa \u2014 interpret carriage-control characters",
        "section": 1
    },
    {
        "command": "at",
        "description": "The at utility shall read commands from standard input and groupthem together as an at-job, to be executed at a later time.The at-job shall be executed in a separate invocation of theshell, running in a separate process group with no controllingterminal, except that the environment variables, current workingdirectory, file creation mask, and other implementation-definedexecution-time attributes in effect when the at utility isexecuted shall be retained and used when the at-job is executed.When the at-job is submitted, the at_job_id and scheduled timeshall be written to standard error. The at_job_id is anidentifier that shall be a string consisting solely ofalphanumeric characters and the <period> character. The at_job_idshall be assigned by the system when the job is scheduled suchthat it uniquely identifies a particular job.User notification and the processing of the job's standard outputand standard error are described under the -m option.Users shall be permitted to use at if their name appears in thefile at.allow which is located in an implementation-defineddirectory.If that file does not exist, the file at.deny, whichis located in an implementation-defined directory, shall bechecked to determine whether the user shall be denied access toat.If neither file exists, only a process with appropriateprivileges shall be allowed to submit a job. If only at.denyexists and is empty, global usage shall be permitted. Theat.allow and at.deny files shall consist of one user name perline.",
        "name": "at \u2014 execute commands at a later time",
        "section": 1
    },
    {
        "command": "awk",
        "description": "The awk utility shall execute programs written in the awkprogramming language, which is specialized for textual datamanipulation. An awk program is a sequence of patterns andcorresponding actions. When input is read that matches a pattern,the action associated with that pattern is carried out.Input shall be interpreted as a sequence of records. By default,a record is a line, less its terminating <newline>, but this canbe changed by using the RS built-in variable. Each record ofinput shall be matched in turn against each pattern in theprogram. For each pattern matched, the associated action shall beexecuted.The awk utility shall interpret each input record as a sequenceof fields where, by default, a field is a string of non-<blank>non-<newline> characters. This default <blank> and <newline>field delimiter can be changed by using the FS built-in variableor the -F sepstring option. The awk utility shall denote thefirst field in a record $1, the second $2, and so on. The symbol$0 shall refer to the entire record; setting any other fieldcauses the re-evaluation of $0. Assigning to $0 shall reset thevalues of all other fields and the NF built-in variable.",
        "name": "awk \u2014 pattern scanning and processing language",
        "section": 1
    },
    {
        "command": "batch",
        "description": "The batch utility shall read commands from standard input andschedule them for execution in a batch queue. It shall be theequivalent of the command:at -q b -m nowwhere queue b is a special at queue, specifically for batch jobs.Batch jobs shall be submitted to the batch queue with no timeconstraints and shall be run by the system using algorithms,based on unspecified factors, that may vary with each invocationof batch.Users shall be permitted to use batch if their name appears inthe file at.allow which is located in an implementation-defineddirectory.If that file does not exist, the file at.deny, whichis located in an implementation-defined directory, shall bechecked to determine whether the user shall be denied access tobatch.If neither file exists, only a process with appropriateprivileges shall be allowed to submit a job. If only at.denyexists and is empty, global usage shall be permitted. Theat.allow and at.deny files shall consist of one user name perline.",
        "name": "batch \u2014 schedule commands to be executed in a batch queue",
        "section": 1
    },
    {
        "command": "bc",
        "description": "The bc utility shall implement an arbitrary precision calculator.It shall take input from any files given, then read from thestandard input. If the standard input and standard output to bcare attached to a terminal, the invocation of bc shall beconsidered to be interactive, causing behavioral constraintsdescribed in the following sections.",
        "name": "bc \u2014 arbitrary-precision arithmetic language",
        "section": 1
    },
    {
        "command": "bg",
        "description": "If job control is enabled (see the description of set -m), the bgutility shall resume suspended jobs from the current environment(see Section 2.12, Shell Execution Environment) by running themas background jobs. If the job specified by job_id is already arunning background job, the bg utility shall have no effect andshall exit successfully.Using bg to place a job into the background shall cause itsprocess ID to become ``known in the current shell executionenvironment'', as if it had been started as an asynchronous list;see Section 2.9.3.1, Examples.",
        "name": "bg \u2014 run jobs in the background",
        "section": 1
    },
    {
        "command": "break",
        "description": "If n is specified, the break utility shall exit from the nthenclosing for, while, or until loop. If n is not specified, breakshall behave as if n was specified as 1. Execution shall continuewith the command immediately following the exited loop. The valueof n is a positive decimal integer. If n is greater than thenumber of enclosing loops, the outermost enclosing loop shall beexited. If there is no enclosing loop, the behavior isunspecified.A loop shall enclose a break or continue command if the looplexically encloses the command. A loop lexically encloses a breakor continue command if the command is:*Executing in the same execution environment (see Section2.12, Shell Execution Environment) as the compound-list ofthe loop's do-group (see Section 2.10.2, Shell GrammarRules), and*Contained in a compound-list associated with the loop (eitherin the compound-list of the loop's do-group or, if the loopis a while or until loop, in the compound-list following thewhile or until reserved word), and*Not in the body of a function whose function definitioncommand (see Section 2.9.5, Function Definition Command) iscontained in a compound-list associated with the loop.If n is greater than the number of lexically enclosing loops andthere is a non-lexically enclosing loop in progress in the sameexecution environment as the break or continue command, it isunspecified whether that loop encloses the command.",
        "name": "break \u2014 exit from for, while, or until loop",
        "section": 1
    },
    {
        "command": "c99",
        "description": "The c99 utility is an interface to the standard C compilationsystem; it shall accept source code conforming to the ISO Cstandard. The system conceptually consists of a compiler and linkeditor. The input files referenced by pathname operands and -loption-arguments shall be compiled and linked to produce anexecutable file. (It is unspecified whether the linking occursentirely within the operation of c99; some implementations mayproduce objects that are not fully resolved until the file isexecuted.)If the -c option is specified, for all pathname operands of theform file.c, the files:$(basename pathname .c).oshall be created as the result of successful compilation. If the-c option is not specified, it is unspecified whether such .ofiles are created or deleted for the file.c operands.If there are no options that prevent link editing (such as -c or-E), and all input files compile and link without error, theresulting executable file shall be written according to the -ooutfile option (if present) or to the file a.out.The executable file shall be created as specified in Section1.1.1.4, File Read, Write, and Creation, except that the filepermission bits shall be set to: S_IRWXO | S_IRWXG | S_IRWXUand the bits specified by the umask of the process shall becleared.",
        "name": "c99 \u2014 compile standard C programs",
        "section": 1
    },
    {
        "command": "cd",
        "description": "The cd utility shall change the working directory of the currentshell execution environment (see Section 2.12, Shell ExecutionEnvironment) by executing the following steps in sequence. (Inthe following steps, the symbol curpath represents anintermediate value used to simplify the description of thealgorithm used by cd.There is no requirement that curpath bemade visible to the application.)1. If no directory operand is given and the HOME environmentvariable is empty or undefined, the default behavior isimplementation-defined and no further steps shall be taken.2. If no directory operand is given and the HOME environmentvariable is set to a non-empty value, the cd utility shallbehave as if the directory named in the HOME environmentvariable was specified as the directory operand.3. If the directory operand begins with a <slash> character, setcurpath to the operand and proceed to step 7.4. If the first component of the directory operand is dot ordot-dot, proceed to step 6.5. Starting with the first pathname in the <colon>-separatedpathnames of CDPATH (see the ENVIRONMENT VARIABLES section)if the pathname is non-null, test if the concatenation ofthat pathname, a <slash> character if that pathname did notend with a <slash> character, and the directory operand namesa directory. If the pathname is null, test if theconcatenation of dot, a <slash> character, and the operandnames a directory. In either case, if the resulting stringnames an existing directory, set curpath to that string andproceed to step 7. Otherwise, repeat this step with the nextpathname in CDPATH until all pathnames have been tested.6. Set curpath to the directory operand.7. If the -P option is in effect, proceed to step 10. If curpathdoes not begin with a <slash> character, set curpath to thestring formed by the concatenation of the value of PWD, a<slash> character if the value of PWD did not end with a<slash> character, and curpath.8. The curpath value shall then be converted to canonical formas follows, considering each component from beginning to end,in sequence:a. Dot components and any <slash> characters that separatethem from the next component shall be deleted.b. For each dot-dot component, if there is a precedingcomponent and it is neither root nor dot-dot, then:i.If the preceding component does not refer (in thecontext of pathname resolution with symbolic linksfollowed) to a directory, then the cd utility shalldisplay an appropriate error message and no furthersteps shall be taken.ii.The preceding component, all <slash> charactersseparating the preceding component from dot-dot,dot-dot, and all <slash> characters separating dot-dot from the following component (if any) shall bedeleted.c. An implementation may further simplify curpath byremoving any trailing <slash> characters that are notalso leading <slash> characters, replacing multiple non-leading consecutive <slash> characters with a single<slash>, and replacing three or more leading <slash>characters with a single <slash>.If, as a result ofthis canonicalization, the curpath variable is null, nofurther steps shall be taken.9. If curpath is longer than {PATH_MAX} bytes (including theterminating null) and the directory operand was not longerthan {PATH_MAX} bytes (including the terminating null), thencurpath shall be converted from an absolute pathname to anequivalent relative pathname if possible. This conversionshall always be considered possible if the value of PWD, witha trailing <slash> added if it does not already have one, isan initial substring of curpath.Whether or not it isconsidered possible under other circumstances is unspecified.Implementations may also apply this conversion if curpath isnot longer than {PATH_MAX} bytes or the directory operand waslonger than {PATH_MAX} bytes.10. The cd utility shall then perform actions equivalent to thechdir() function called with curpath as the path argument. Ifthese actions fail for any reason, the cd utility shalldisplay an appropriate error message and the remainder ofthis step shall not be executed. If the -P option is not ineffect, the PWD environment variable shall be set to thevalue that curpath had on entry to step 9 (i.e., beforeconversion to a relative pathname). If the -P option is ineffect, the PWD environment variable shall be set to thestring that would be output by pwd -P.If there isinsufficient permission on the new directory, or on anyparent of that directory, to determine the current workingdirectory, the value of the PWD environment variable isunspecified.If, during the execution of the above steps, the PWD environmentvariable is set, the OLDPWD environment variable shall also beset to the value of the old working directory (that is thecurrent working directory immediately prior to the call to cd).",
        "name": "cd \u2014 change the working directory",
        "section": 1
    },
    {
        "command": "cflow",
        "description": "The cflow utility shall analyze a collection of object files orassembler, C-language, lex, or yacc source files, and attempt tobuild a graph, written to standard output, charting the externalreferences.",
        "name": "cflow \u2014 generate a C-language flowgraph (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "colon",
        "description": "This utility shall only expand command arguments.It is usedwhen a command is needed, as in the then condition of an ifcommand, but nothing is to be done by the command.",
        "name": "colon \u2014 null utility",
        "section": 1
    },
    {
        "command": "command",
        "description": "The command utility shall cause the shell to treat the argumentsas a simple command, suppressing the shell function lookup thatis described in Section 2.9.1.1, Command Search and Execution,item 1b.If the command_name is the same as the name of one of the specialbuilt-in utilities, the special properties in the enumerated listat the beginning of Section 2.14, Special Built-In Utilitiesshall not occur. In every other respect, if command_name is notthe name of a function, the effect of command (with no options)shall be the same as omitting command.When the -v or -V option is used, the command utility shallprovide information concerning how a command name is interpretedby the shell.",
        "name": "command \u2014 execute a simple command",
        "section": 1
    },
    {
        "command": "compress",
        "description": "The compress utility shall attempt to reduce the size of thenamed files by using adaptive Lempel-Ziv coding algorithm.Note:Lempel-Ziv is US Patent 4464650, issued to WilliamEastman, Abraham Lempel, Jacob Ziv, Martin Cohn on August7th, 1984, and assigned to Sperry Corporation.Lempel-Ziv-Welch compression is covered by US Patent4558302, issued to Terry A. Welch on December 10th,1985, and assigned to Sperry Corporation.On systems not supporting adaptive Lempel-Ziv coding algorithm,the input files shall not be changed and an error value greaterthan two shall be returned. Except when the output is to thestandard output, each file shall be replaced by one with theextension .Z.If the invoking process has appropriateprivileges, the ownership, modes, access time, and modificationtime of the original file are preserved. If appending the .Z tothe filename would make the name exceed {NAME_MAX} bytes, thecommand shall fail. If no files are specified, the standard inputshall be compressed to the standard output.",
        "name": "compress \u2014 compress data",
        "section": 1
    },
    {
        "command": "continue",
        "description": "If n is specified, the continue utility shall return to the topof the nth enclosing for, while, or until loop. If n is notspecified, continue shall behave as if n was specified as 1.Returning to the top of the loop involves repeating the conditionlist of a while or until loop or performing the next assignmentof a for loop, and re-executing the loop if appropriate.The value of n is a positive decimal integer. If n is greaterthan the number of enclosing loops, the outermost enclosing loopshall be used. If there is no enclosing loop, the behavior isunspecified.The meaning of ``enclosing'' shall be as specified in thedescription of the break utility.",
        "name": "continue \u2014 continue for, while, or until loop",
        "section": 1
    },
    {
        "command": "ctags",
        "description": "The ctags utility shall be provided on systems that support thethe Software Development Utilities option, and either or both ofthe C-Language Development Utilities option and FORTRANDevelopment Utilities option. On other systems, it is optional.The ctags utility shall write a tagsfile or an index of objectsfrom C-language or FORTRAN source files specified by the pathnameoperands. The tagsfile shall list the locators of language-specific objects within the source files. A locator consists of aname, pathname, and either a search pattern or a line number thatcan be used in searching for the object definition. The objectsthat shall be recognized are specified in the EXTENDEDDESCRIPTION section.",
        "name": "ctags \u2014 create a tags file (DEVELOPMENT, FORTRAN)",
        "section": 1
    },
    {
        "command": "cxref",
        "description": "The cxref utility shall analyze a collection of C-language filesand attempt to build a cross-reference table. Information from#define lines shall be included in the symbol table. A sortedlisting shall be written to standard output of all symbols (auto,static, and global) in each file separately, or with the -coption, in combination. Each symbol shall contain an <asterisk>before the declaring reference.",
        "name": "cxref \u2014 generate a C-language program cross-reference table(DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "delta",
        "description": "The delta utility shall be used to permanently introduce into thenamed SCCS files changes that were made to the files retrieved byget (called the g-files, or generated files).",
        "name": "delta \u2014 make a delta (change) to an SCCS file (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "dot",
        "description": "The shell shall execute commands from the file in the currentenvironment.If file does not contain a <slash>, the shell shall use thesearch path specified by PATH to find the directory containingfile.Unlike normal command search, however, the file searchedfor by the dot utility need not be executable. If no readablefile is found, a non-interactive shell shall abort; aninteractive shell shall write a diagnostic message to standarderror, but this condition shall not be considered a syntax error.",
        "name": "dot \u2014 execute commands in the current environment",
        "section": 1
    },
    {
        "command": "ed",
        "description": "The ed utility is a line-oriented text editor that uses twomodes: command mode and input mode.In command mode the inputcharacters shall be interpreted as commands, and in input modethey shall be interpreted as text. See the EXTENDED DESCRIPTIONsection.If an operand is '-', the results are unspecified.",
        "name": "ed \u2014 edit text",
        "section": 1
    },
    {
        "command": "eval",
        "description": "The eval utility shall construct a command by concatenatingarguments together, separating each with a <space> character.The constructed command shall be read and executed by the shell.",
        "name": "eval \u2014 construct command by concatenating arguments",
        "section": 1
    },
    {
        "command": "ex",
        "description": "The ex utility is a line-oriented text editor. There are twoother modes of the editor\u2014open and visual\u2014in which screen-oriented editing is available. This is described more fully bythe ex open and visual commands and in vi(1p).If an operand is '-', the results are unspecified.This section uses the term edit buffer to describe the currentworking text. No specific implementation is implied by this term.All editing changes are performed on the edit buffer, and nochanges to it shall affect any file until an editor commandwrites the file.Certain terminals do not have all the capabilities necessary tosupport the complete ex definition, such as the full-screenediting commands (visual mode or open mode).When these commandscannot be supported on such terminals, this condition shall notproduce an error message such as ``not an editor command'' orreport a syntax error. The implementation may either accept thecommands and produce results on the screen that are the result ofan unsuccessful attempt to meet the requirements of this volumeof POSIX.1\u20102017 or report an error describing the terminal-related deficiency.",
        "name": "ex \u2014 text editor",
        "section": 1
    },
    {
        "command": "exec",
        "description": "The exec utility shall open, close, and/or copy file descriptorsas specified by any redirections as part of the command.If exec is specified without command or arguments, and any filedescriptors with numbers greater than 2 are opened withassociated redirection statements, it is unspecified whetherthose file descriptors remain open when the shell invokes anotherutility.Scripts concerned that child shells could misuse openfile descriptors can always close them explicitly, as shown inone of the following examples.If exec is specified with command, it shall replace the shellwith command without creating a new process. If arguments arespecified, they shall be arguments to command.Redirectionaffects the current shell execution environment.",
        "name": "exec \u2014 execute commands and open, close, or copy file descriptors",
        "section": 1
    },
    {
        "command": "exit",
        "description": "The exit utility shall cause the shell to exit from its currentexecution environment with the exit status specified by theunsigned decimal integer n.If the current execution environmentis a subshell environment, the shell shall exit from the subshellenvironment with the specified exit status and continue in theenvironment from which that subshell environment was invoked;otherwise, the shell utility shall terminate with the specifiedexit status. If n is specified, but its value is not between 0and 255 inclusively, the exit status is undefined.A trap on EXIT shall be executed before the shell terminates,except when the exit utility is invoked in that trap itself, inwhich case the shell shall exit immediately.",
        "name": "exit \u2014 cause the shell to exit",
        "section": 1
    },
    {
        "command": "export",
        "description": "The shell shall give the export attribute to the variablescorresponding to the specified names, which shall cause them tobe in the environment of subsequently executed commands. If thename of a variable is followed by =word, then the value of thatvariable shall be set to word.The export special built-in shall support the Base Definitionsvolume of POSIX.1\u20102017, Section 12.2, Utility Syntax Guidelines.When -p is specified, export shall write to the standard outputthe names and values of all exported variables, in the followingformat:\"export %s=%s\\n\", <name>, <value>if name is set, and:\"export %s\\n\", <name>if name is unset.The shell shall format the output, including the proper use ofquoting, so that it is suitable for reinput to the shell ascommands that achieve the same exporting results, except:1. Read-only variables with values cannot be reset.2. Variables that were unset at the time they were output neednot be reset to the unset state if a value is assigned to thevariable between the time the state was saved and the time atwhich the saved output is reinput to the shell.When no arguments are given, the results are unspecified.",
        "name": "export \u2014 set the export attribute for variables",
        "section": 1
    },
    {
        "command": "fc",
        "description": "The fc utility shall list, or shall edit and re-execute, commandspreviously entered to an interactive sh.The command history list shall reference commands by number. Thefirst number in the list is selected arbitrarily. Therelationship of a number to its command shall not change exceptwhen the user logs in and no other process is accessing the list,at which time the system may reset the numbering to start theoldest retained command at another number (usually 1). When thenumber reaches an implementation-defined upper limit, which shallbe no smaller than the value in HISTSIZE or 32767 (whichever isgreater), the shell may wrap the numbers, starting the nextcommand with a lower number (usually 1). However, despite thisoptional wrapping of numbers, fc shall maintain the time-orderingsequence of the commands. For example, if four commands insequence are given the numbers 32766, 32767, 1 (wrapped), and 2as they are executed, command 32767 is considered the commandprevious to 1, even though its number is higher.When commands are edited (when the -l option is not specified),the resulting lines shall be entered at the end of the historylist and then re-executed by sh.The fc command that caused theediting shall not be entered into the history list. If the editorreturns a non-zero exit status, this shall suppress the entryinto the history list and the command re-execution.Any commandline variable assignments or redirection operators used with fcshall affect both the fc command itself as well as the commandthat results; for example:fc -s -- -1 2>/dev/nullreinvokes the previous command, suppressing standard error forboth fc and the previous command.",
        "name": "fc \u2014 process the command history list",
        "section": 1
    },
    {
        "command": "fg",
        "description": "If job control is enabled (see the description of set -m), the fgutility shall move a background job from the current environment(see Section 2.12, Shell Execution Environment) into theforeground.Using fg to place a job into the foreground shall remove itsprocess ID from the list of those ``known in the current shellexecution environment''; see Section 2.9.3.1, Examples.",
        "name": "fg \u2014 run jobs in the foreground",
        "section": 1
    },
    {
        "command": "fort77",
        "description": "The fort77 utility is the interface to the FORTRAN compilationsystem; it shall accept the full FORTRAN-77 language defined bythe ANSI X3.9\u20101978 standard. The system conceptually consists ofa compiler and link editor. The files referenced by operands arecompiled and linked to produce an executable file. It isunspecified whether the linking occurs entirely within theoperation of fort77; some implementations may produce objectsthat are not fully resolved until the file is executed.If the -c option is present, for all pathname operands of theform file.f, the files:$(basename pathname.f).oshall be created or overwritten as the result of successfulcompilation. If the -c option is not specified, it is unspecifiedwhether such .o files are created or deleted for the file.foperands.If there are no options that prevent link editing (such as -c)and all operands compile and link without error, the resultingexecutable file shall be written into the file named by the -ooption (if present) or to the file a.out.The executable fileshall be created as specified in the System Interfaces volume ofPOSIX.1\u20102017, except that the file permissions shall be set to:S_IRWXO | S_IRWXG | S_IRWXUand that the bits specified by the umask of the process shall becleared.",
        "name": "fort77 \u2014 FORTRAN compiler (FORTRAN)",
        "section": 1
    },
    {
        "command": "gencat",
        "description": "The gencat utility shall merge the message text source filemsgfile into a formatted message catalog catfile.The filecatfile shall be created if it does not already exist. If catfiledoes exist, its messages shall be included in the new catfile.If set and message numbers collide, the new message text definedin msgfile shall replace the old message text currently containedin catfile.",
        "name": "gencat \u2014 generate a formatted message catalog",
        "section": 1
    },
    {
        "command": "get",
        "description": "The get utility shall generate a text file from each named SCCSfile according to the specifications given by its options.The generated text shall normally be written into a file calledthe g-file whose name is derived from the SCCS filename by simplyremoving the leading \"s.\".",
        "name": "get \u2014 get a version of an SCCS file (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "getconf",
        "description": "In the first synopsis form, the getconf utility shall write tothe standard output the value of the variable specified by thesystem_var operand.In the second synopsis form, the getconf utility shall write tothe standard output the value of the variable specified by thepath_var operand for the path specified by the pathname operand.The value of each configuration variable shall be determined asif it were obtained by calling the function from which it isdefined to be available by this volume of POSIX.1\u20102017 or by theSystem Interfaces volume of POSIX.1\u20102017 (see the OPERANDSsection). The value shall reflect conditions in the currentoperating environment.",
        "name": "getconf \u2014 get configuration values",
        "section": 1
    },
    {
        "command": "getopts",
        "description": "The getopts utility shall retrieve options and option-argumentsfrom a list of parameters. It shall support the Utility SyntaxGuidelines 3 to 10, inclusive, described in the Base Definitionsvolume of POSIX.1\u20102017, Section 12.2, Utility Syntax Guidelines.Each time it is invoked, the getopts utility shall place thevalue of the next option in the shell variable specified by thename operand and the index of the next argument to be processedin the shell variable OPTIND.Whenever the shell is invoked,OPTIND shall be initialized to 1.When the option requires an option-argument, the getopts utilityshall place it in the shell variable OPTARG.If no option wasfound, or if the option that was found does not have an option-argument, OPTARG shall be unset.If an option character not contained in the optstring operand isfound where an option character is expected, the shell variablespecified by name shall be set to the <question-mark> ('?')character. In this case, if the first character in optstring is a<colon> (':'), the shell variable OPTARG shall be set to theoption character found, but no output shall be written tostandard error; otherwise, the shell variable OPTARG shall beunset and a diagnostic message shall be written to standarderror. This condition shall be considered to be an error detectedin the way arguments were presented to the invoking application,but shall not be an error in getopts processing.If an option-argument is missing:*If the first character of optstring is a <colon>, the shellvariable specified by name shall be set to the <colon>character and the shell variable OPTARG shall be set to theoption character found.*Otherwise, the shell variable specified by name shall be setto the <question-mark> character, the shell variable OPTARGshall be unset, and a diagnostic message shall be written tostandard error. This condition shall be considered to be anerror detected in the way arguments were presented to theinvoking application, but shall not be an error in getoptsprocessing; a diagnostic message shall be written as stated,but the exit status shall be zero.When the end of options is encountered, the getopts utility shallexit with a return value greater than zero; the shell variableOPTIND shall be set to the index of the first operand, or thevalue \"$#\"+1 if there are no operands; the name variable shall beset to the <question-mark> character. Any of the following shallidentify the end of options: the first \"--\" argument that is notan option-argument, finding an argument that is not an option-argument and does not begin with a '-', or encountering an error.The shell variables OPTIND and OPTARG shall be local to thecaller of getopts and shall not be exported by default.The shell variable specified by the name operand, OPTIND, andOPTARG shall affect the current shell execution environment; seeSection 2.12, Shell Execution Environment.If the application sets OPTIND to the value 1, a new set ofparameters can be used: either the current positional parametersor new arg values. Any other attempt to invoke getopts multipletimes in a single shell execution environment with parameters(positional parameters or arg operands) that are not the same inall invocations, or with an OPTIND value modified to be a valueother than 1, produces unspecified results.",
        "name": "getopts \u2014 parse utility options",
        "section": 1
    },
    {
        "command": "hash",
        "description": "The hash utility shall affect the way the current shellenvironment remembers the locations of utilities found asdescribed in Section 2.9.1.1, Command Search and Execution.Depending on the arguments specified, it shall add utilitylocations to its list of remembered locations or it shall purgethe contents of the list. When no arguments are specified, itshall report on the contents of the list.Utilities provided as built-ins to the shell shall not bereported by hash.",
        "name": "hash \u2014 remember or report utility locations",
        "section": 1
    },
    {
        "command": "jobs",
        "description": "The jobs utility shall display the status of jobs that werestarted in the current shell environment; see Section 2.12, ShellExecution Environment.When jobs reports the termination status of a job, the shellshall remove its process ID from the list of those ``known in thecurrent shell execution environment''; see Section 2.9.3.1,Examples.",
        "name": "jobs \u2014 display status of jobs in the current session",
        "section": 1
    },
    {
        "command": "lex",
        "description": "The lex utility shall generate C programs to be used in lexicalprocessing of character input, and that can be used as aninterface to yacc.The C programs shall be generated from lexsource code and conform to the ISO C standard, without dependingon any undefined, unspecified, or implementation-definedbehavior, except in cases where the code is copied directly fromthe supplied source, or in cases that are documented by theimplementation. Usually, the lex utility shall write the programit generates to the file lex.yy.c; the state of this file isunspecified if lex exits with a non-zero exit status. See theEXTENDED DESCRIPTION section for a complete description of thelex input language.",
        "name": "lex \u2014 generate programs for lexical tasks (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "m4",
        "description": "The m4 utility is a macro processor that shall read one or moretext files, process them according to their included macrostatements, and write the results to standard output.",
        "name": "m4 \u2014 macro processor",
        "section": 1
    },
    {
        "command": "mailx",
        "description": "The mailx utility provides a message sending and receivingfacility. It has two major modes, selected by the options used:Send Mode and Receive Mode.On systems that do not support the User Portability Utilitiesoption, an application using mailx shall have the ability to sendmessages in an unspecified manner (Send Mode). Unless the firstcharacter of one or more lines is <tilde> ('~'), all charactersin the input message shall appear in the delivered message, butadditional characters may be inserted in the message before it isretrieved.On systems supporting the User Portability Utilities option,mail-receiving capabilities and other interactive features,Receive Mode, described below, also shall be enabled.Send ModeSend Mode can be used by applications or users to send messagesfrom the text in standard input.Receive ModeReceive Mode is more oriented towards interactive users. Mail canbe read and sent in this interactive mode.When reading mail, mailx provides commands to facilitate saving,deleting, and responding to messages. When sending mail, mailxallows editing, reviewing, and other modification of the messageas it is entered.Incoming mail shall be stored in one or more unspecifiedlocations for each user, collectively called the system mailboxfor that user. When mailx is invoked in Receive Mode, the systemmailbox shall be the default place to find new mail. As messagesare read, they shall be marked to be moved to a secondary filefor storage, unless specific action is taken. This secondary fileis called the mbox and is normally located in the directoryreferred to by the HOME environment variable (see MBOX in theENVIRONMENT VARIABLES section for a description of this file).Messages shall remain in this file until explicitly removed. Whenthe -f option is used to read mail messages from secondary files,messages shall be retained in those files unless specificallyremoved. All three of these locations\u2014system mailbox, mbox, andsecondary file\u2014are referred to in this section as simply``mailboxes'', unless more specific identification is required.",
        "name": "mailx \u2014 process messages",
        "section": 1
    },
    {
        "command": "pax",
        "description": "The pax utility shall read, write, and write lists of the membersof archive files and copy directory hierarchies. A variety ofarchive formats shall be supported; see the -x format option.The action to be taken depends on the presence of the -r and -woptions. The four combinations of -r and -w are referred to asthe four modes of operation: list, read, write, and copy modes,corresponding respectively to the four forms shown in theSYNOPSIS section.listIn list mode (when neither -r nor -w are specified),pax shall write the names of the members of the archivefile read from the standard input, with pathnamesmatching the specified patterns, to standard output. Ifa named file is of type directory, the file hierarchyrooted at that file shall be listed as well.readIn read mode (when -r is specified, but -w is not), paxshall extract the members of the archive file read fromthe standard input, with pathnames matching thespecified patterns. If an extracted file is of typedirectory, the file hierarchy rooted at that file shallbe extracted as well. The extracted files shall becreated performing pathname resolution with thedirectory in which pax was invoked as the currentworking directory.If an attempt is made to extract a directory when thedirectory already exists, this shall not be consideredan error. If an attempt is made to extract a FIFO whenthe FIFO already exists, this shall not be consideredan error.The ownership, access, and modification times, and filemode of the restored files are discussed under the -poption.writeIn write mode (when -w is specified, but -r is not),pax shall write the contents of the file operands tothe standard output in an archive format. If no fileoperands are specified, a list of files to copy, oneper line, shall be read from the standard input andeach entry in this list shall be processed as if it hadbeen a file operand on the command line. A file of typedirectory shall include all of the files in the filehierarchy rooted at the file.copyIn copy mode (when both -r and -w are specified), paxshall copy the file operands to the destinationdirectory.If no file operands are specified, a list of files tocopy, one per line, shall be read from the standardinput. A file of type directory shall include all ofthe files in the file hierarchy rooted at the file.The effect of the copy shall be as if the copied fileswere written to a pax format archive file and thensubsequently extracted, except that copying of socketsmay be supported even if archiving them in write modeis not supported, and that there may be hard linksbetween the original and the copied files. If thedestination directory is a subdirectory of one of thefiles to be copied, the results are unspecified. If thedestination directory is a file of a type not definedby the System Interfaces volume of POSIX.1\u20102017, theresults are implementation-defined; otherwise, it shallbe an error for the file named by the directory operandnot to exist, not be writable by the user, or not be afile of type directory.In read or copy modes, if intermediate directories are necessaryto extract an archive member, pax shall perform actionsequivalent to the mkdir() function defined in the SystemInterfaces volume of POSIX.1\u20102017, called with the followingarguments:*The intermediate directory used as the path argument*The value of the bitwise-inclusive OR of S_IRWXU, S_IRWXG,and S_IRWXO as the mode argumentIf any specified pattern or file operands are not matched by atleast one file or archive member, pax shall write a diagnosticmessage to standard error for each one that did not match andexit with a non-zero exit status.The archive formats described in the EXTENDED DESCRIPTION sectionshall be automatically detected on input. The default outputarchive format shall be implementation-defined.A single archive can span multiple files. The pax utility shalldetermine, in an implementation-defined manner, what file to reador write as the next file.If the selected archive format supports the specification oflinked files, it shall be an error if these files cannot belinked when the archive is extracted. For archive formats that donot store file contents with each name that causes a hard link,if the file that contains the data is not extracted during thispax session, either the data shall be restored from the originalfile, or a diagnostic message shall be displayed with the name ofa file that can be used to extract the data. In traversingdirectories, pax shall detect infinite loops; that is, entering apreviously visited directory that is an ancestor of the last filevisited. When it detects an infinite loop, pax shall write adiagnostic message to standard error and shall terminate.",
        "name": "pax \u2014 portable archive interchange",
        "section": 1
    },
    {
        "command": "prs",
        "description": "The prs utility shall write to standard output parts or all of anSCCS file in a user-supplied format.",
        "name": "prs \u2014 print an SCCS file (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "qalter",
        "description": "The attributes of a batch job are altered by a request to thebatch server that manages the batch job. The qalter utility is auser-accessible batch client that requests the alteration of theattributes of one or more batch jobs.The qalter utility shall alter the attributes of those batchjobs, and only those batch jobs, for which a batch job_identifieris presented to the utility.The qalter utility shall alter the attributes of batch jobs inthe order in which the batch job_identifiers are presented to theutility.If the qalter utility fails to process a batch job_identifiersuccessfully, the utility shall proceed to process the remainingbatch job_identifiers, if any.For each batch job_identifier for which the qalter utilitysucceeds, each attribute of the identified batch job shall bealtered as indicated by all the options presented to the utility.For each identified batch job for which the qalter utility fails,the utility shall not alter any attribute of the batch job.For each batch job that the qalter utility processes, the utilityshall not modify any attribute other than those required by theoptions and option-arguments presented to the utility.The qalter utility shall alter batch jobs by sending a Modify JobRequest to the batch server that manages each batch job. At thetime the qalter utility exits, it shall have modified the batchjob corresponding to each successfully processed batchjob_identifier.An attempt to alter the attributes of a batchjob in the RUNNING state is implementation-defined.",
        "name": "qalter \u2014 alter batch job",
        "section": 1
    },
    {
        "command": "qdel",
        "description": "A batch job is deleted by sending a request to the batch serverthat manages the batch job. A batch job that has been deleted isno longer subject to management by batch services.The qdel utility is a user-accessible client of batch servicesthat requests the deletion of one or more batch jobs.The qdel utility shall request a batch server to delete thosebatch jobs for which a batch job_identifier is presented to theutility.The qdel utility shall delete batch jobs in the order in whichtheir batch job_identifiers are presented to the utility.If the qdel utility fails to process any batch job_identifiersuccessfully, the utility shall proceed to process the remainingbatch job_identifiers, if any.The qdel utility shall delete each batch job by sending a DeleteJob Request to the batch server that manages the batch job.The qdel utility shall not exit until the batch job correspondingto each successfully processed batch job_identifier has beendeleted.",
        "name": "qdel \u2014 delete batch jobs",
        "section": 1
    },
    {
        "command": "qhold",
        "description": "A hold is placed on a batch job by a request to the batch serverthat manages the batch job. A batch job that has one or moreholds is not eligible for execution. The qhold utility is a user-accessible client of batch services that requests one or moretypes of hold to be placed on one or more batch jobs.The qhold utility shall place holds on those batch jobs for whicha batch job_identifier is presented to the utility.The qhold utility shall place holds on batch jobs in the order inwhich their batch job_identifiers are presented to the utility.If the qhold utility fails to process any batch job_identifiersuccessfully, the utility shall proceed to process the remainingbatch job_identifiers, if any.The qhold utility shall place holds on each batch job by sendinga Hold Job Request to the batch server that manages the batchjob.The qhold utility shall not exit until holds have been placed onthe batch job corresponding to each successfully processed batchjob_identifier.",
        "name": "qhold \u2014 hold batch jobs",
        "section": 1
    },
    {
        "command": "qmove",
        "description": "To move a batch job is to remove the batch job from the batchqueue in which it resides and instantiate the batch job inanother batch queue.A batch job is moved by a request to thebatch server that manages the batch job. The qmove utility is auser-accessible batch client that requests the movement of one ormore batch jobs.The qmove utility shall move those batch jobs, and only thosebatch jobs, for which a batch job_identifier is presented to theutility.The qmove utility shall move batch jobs in the order in which thecorresponding batch job_identifiers are presented to the utility.If the qmove utility fails to process a batch job_identifiersuccessfully, the utility shall proceed to process the remainingbatch job_identifiers, if any.The qmove utility shall move batch jobs by sending a Move JobRequest to the batch server that manages each batch job. Theqmove utility shall not exit before the batch jobs correspondingto all successfully processed batch job_identifiers have beenmoved.",
        "name": "qmove \u2014 move batch jobs",
        "section": 1
    },
    {
        "command": "qmsg",
        "description": "To send a message to a batch job is to request that a serverwrite a message string into one or more output files of the batchjob. A message is sent to a batch job by a request to the batchserver that manages the batch job. The qmsg utility is a user-accessible batch client that requests the sending of messages toone or more batch jobs.The qmsg utility shall write messages into the files of batchjobs by sending a Job Message Request to the batch server thatmanages the batch job. The qmsg utility shall not directly writethe message into the files of the batch job.The qmsg utility shall send a Job Message Request for those batchjobs, and only those batch jobs, for which a batch job_identifieris presented to the utility.The qmsg utility shall send Job Message Requests for batch jobsin the order in which their batch job_identifiers are presentedto the utility.If the qmsg utility fails to process any batch job_identifiersuccessfully, the utility shall proceed to process the remainingbatch job_identifiers, if any.The qmsg utility shall not exit before a Job Message Request hasbeen sent to the server that manages the batch job thatcorresponds to each successfully processed batch job_identifier.",
        "name": "qmsg \u2014 send message to batch jobs",
        "section": 1
    },
    {
        "command": "qrerun",
        "description": "To rerun a batch job is to terminate the session leader of thebatch job, delete any associated checkpoint files, and return thebatch job to the batch queued state. A batch job is rerun by arequest to the batch server that manages the batch job. Theqrerun utility is a user-accessible batch client that requeststhe rerunning of one or more batch jobs.The qrerun utility shall rerun those batch jobs for which a batchjob_identifier is presented to the utility.The qrerun utility shall rerun batch jobs in the order in whichtheir batch job_identifiers are presented to the utility.If the qrerun utility fails to process any batch job_identifiersuccessfully, the utility shall proceed to process the remainingbatch job_identifiers, if any.The qrerun utility shall rerun batch jobs by sending a Rerun JobRequest to the batch server that manages each batch job.For each successfully processed batch job_identifier, the qrerunutility shall have rerun the corresponding batch job at the timethe utility exits.",
        "name": "qrerun \u2014 rerun batch jobs",
        "section": 1
    },
    {
        "command": "qrls",
        "description": "A batch job might have one or more holds, which prevent the batchjob from executing. A batch job from which all the holds havebeen removed becomes eligible for execution and is said to havebeen released. A batch job hold is removed by sending a requestto the batch server that manages the batch job. The qrls utilityis a user-accessible client of batch services that requests holdsbe removed from one or more batch jobs.The qrls utility shall remove one or more holds from those batchjobs for which a batch job_identifier is presented to theutility.The qrls utility shall remove holds from batch jobs in the orderin which their batch job_identifiers are presented to theutility.If the qrls utility fails to process a batch job_identifiersuccessfully, the utility shall proceed to process the remainingbatch job_identifiers, if any.The qrls utility shall remove holds on each batch job by sendinga Release Job Request to the batch server that manages the batchjob.The qrls utility shall not exit until the holds have been removedfrom the batch job corresponding to each successfully processedbatch job_identifier.",
        "name": "qrls \u2014 release batch jobs",
        "section": 1
    },
    {
        "command": "qselect",
        "description": "To select a set of batch jobs is to return the batchjob_identifiers for each batch job that meets a list of selectioncriteria. A set of batch jobs is selected by a request to a batchserver. The qselect utility is a user-accessible batch clientthat requests the selection of batch jobs.Upon successful completion, the qselect utility shall havereturned a list of zero or more batch job_identifiers that meetthe criteria specified by the options and option-argumentspresented to the utility.The qselect utility shall select batch jobs by sending a SelectJobs Request to a batch server. The qselect utility shall notexit until the server replies to each request generated.For each option presented to the qselect utility, the utilityshall restrict the set of selected batch jobs as described in theOPTIONS section.The qselect utility shall not restrict selection of batch jobsexcept by authorization and as required by the options presentedto the utility.When an option is specified with a mandatory or optional opcomponent to the option-argument, then op shall specify arelation between the value of a certain batch job attribute andthe value component of the option-argument. If an op is allowableon an option, then the description of the option letter indicatesthe op as either mandatory or optional. Acceptable strings forthe op component, and the relation the string indicates, areshown in the following list:.eq.The value represented by the attribute of the batch jobis equal to the value represented by the option-argument..ge.The value represented by the attribute of the batch jobis greater than or equal to the value represented by theoption-argument..gt.The value represented by the attribute of the batch jobis greater than the value represented by the option-argument..lt.The value represented by the attribute of the batch jobis less than the value represented by the option-argument..le.The value represented by the attribute of the batch jobis less than or equal to the value represented by theoption-argument..ne.The value represented by the attribute of the batch jobis not equal to the value represented by the option-argument.",
        "name": "qselect \u2014 select batch jobs",
        "section": 1
    },
    {
        "command": "qsig",
        "description": "To signal a batch job is to send a signal to the session leaderof the batch job. A batch job is signaled by sending a request tothe batch server that manages the batch job. The qsig utility isa user-accessible batch client that requests the signaling of abatch job.The qsig utility shall signal those batch jobs for which a batchjob_identifier is presented to the utility. The qsig utilityshall not signal any batch jobs whose batch job_identifiers arenot presented to the utility.The qsig utility shall signal batch jobs in the order in whichthe corresponding batch job_identifiers are presented to theutility. If the qsig utility fails to process a batchjob_identifier successfully, the utility shall proceed to processthe remaining batch job_identifiers, if any.The qsig utility shall signal batch jobs by sending a Signal JobRequest to the batch server that manages the batch job.For each successfully processed batch job_identifier, the qsigutility shall have received a completion reply to each Signal JobRequest sent to a batch server at the time the utility exits.",
        "name": "qsig \u2014 signal batch jobs",
        "section": 1
    },
    {
        "command": "qstat",
        "description": "The status of a batch job, batch queue, or batch server isobtained by a request to the server. The qstat utility is a user-accessible batch client that requests the status of one or morebatch jobs, batch queues, or servers, and writes the statusinformation to standard output.For each successfully processed batch job_identifier, the qstatutility shall display information about the corresponding batchjob.For each successfully processed destination, the qstat utilityshall display information about the corresponding batch queue.For each successfully processed server name, the qstat utilityshall display information about the corresponding server.The qstat utility shall acquire batch job status information bysending a Job Status Request to a batch server. The qstat utilityshall acquire batch queue status information by sending a QueueStatus Request to a batch server. The qstat utility shall acquireserver status information by sending a Server Status Request to abatch server.",
        "name": "qstat \u2014 show status of batch jobs",
        "section": 1
    },
    {
        "command": "qsub",
        "description": "To submit a script is to create a batch job that executes thescript. A script is submitted by a request to a batch server. Theqsub utility is a user-accessible batch client that submits ascript.Upon successful completion, the qsub utility shall have created abatch job that will execute the submitted script.The qsub utility shall submit a script by sending a Queue JobRequest to a batch server.The qsub utility shall place the value of the followingenvironment variables in the Variable_List attribute of the batchjob: HOME, LANG, LOGNAME, PATH, MAIL, SHELL, and TZ.The name ofthe environment variable shall be the current name prefixed withthe string PBS_O_.Note:If the current value of the HOME variable in theenvironment space of the qsub utility is /aa/bb/cc, thenqsub shall place PBS_O_HOME=/aa/bb/cc in the Variable_Listattribute of the batch job.In addition to the variables described above, the qsub utilityshall add the following variables with the indicated values tothe variable list:PBS_O_WORKDIR The absolute path of the current working directoryof the qsub utility process.PBS_O_HOSTThe name of the host on which the qsub utility isrunning.",
        "name": "qsub \u2014 submit a script",
        "section": 1
    },
    {
        "command": "read",
        "description": "The read utility shall read a single logical line from standardinput into one or more shell variables.By default, unless the -r option is specified, <backslash> shallact as an escape character. An unescaped <backslash> shallpreserve the literal value of the following character, with theexception of a <newline>.If a <newline> follows the<backslash>, the read utility shall interpret this as linecontinuation. The <backslash> and <newline> shall be removedbefore splitting the input into fields. All other unescaped<backslash> characters shall be removed after splitting the inputinto fields.If standard input is a terminal device and the invoking shell isinteractive, read shall prompt for a continuation line when itreads an input line ending with a <backslash> <newline>, unlessthe -r option is specified.The terminating <newline> (if any) shall be removed from theinput and the results shall be split into fields as in the shellfor the results of parameter expansion (see Section 2.6.5, FieldSplitting); the first field shall be assigned to the firstvariable var, the second field to the second variable var, and soon. If there are fewer fields than there are var operands, theremaining vars shall be set to empty strings. If there are fewervars than fields, the last var shall be set to a value comprisingthe following elements:*The field that corresponds to the last var in the normalassignment sequence described above*The delimiter(s) that follow the field corresponding to thelast var*The remaining fields and their delimiters, with trailing IFSwhite space ignoredThe setting of variables specified by the var operands shallaffect the current shell execution environment; see Section 2.12,Shell Execution Environment.If it is called in a subshell orseparate utility execution environment, such as one of thefollowing:(read foo)nohup read ...find . -exec read ... \\;it shall not affect the shell variables in the caller'senvironment.",
        "name": "read \u2014 read from standard input into shell variables",
        "section": 1
    },
    {
        "command": "readonly",
        "description": "The variables whose names are specified shall be given thereadonly attribute. The values of variables with the readonlyattribute cannot be changed by subsequent assignment, nor canthose variables be unset by the unset utility. If the name of avariable is followed by =word, then the value of that variableshall be set to word.The readonly special built-in shall support the Base Definitionsvolume of POSIX.1\u20102017, Section 12.2, Utility Syntax Guidelines.When -p is specified, readonly writes to the standard output thenames and values of all read-only variables, in the followingformat:\"readonly %s=%s\\n\", <name>, <value>if name is set, and\"readonly %s\\n\", <name>if name is unset.The shell shall format the output, including the proper use ofquoting, so that it is suitable for reinput to the shell ascommands that achieve the same value and readonly attribute-setting results in a shell execution environment in which:1. Variables with values at the time they were output do nothave the readonly attribute set.2. Variables that were unset at the time they were output do nothave a value at the time at which the saved output is reinputto the shell.When no arguments are given, the results are unspecified.",
        "name": "readonly \u2014 set the readonly attribute for variables",
        "section": 1
    },
    {
        "command": "return",
        "description": "The return utility shall cause the shell to stop executing thecurrent function or dot script. If the shell is not currentlyexecuting a function or dot script, the results are unspecified.",
        "name": "return \u2014 return from a function or dot script",
        "section": 1
    },
    {
        "command": "rmdel",
        "description": "The rmdel utility shall remove the delta specified by the SIDfrom each named SCCS file. The delta to be removed shall be themost recent delta in its branch in the delta chain of each namedSCCS file. In addition, the application shall ensure that the SIDspecified is not that of a version being edited for the purposeof making a delta; that is, if a p-file (see get(1p)) exists forthe named SCCS file, the SID specified shall not appear in anyentry of the p-file.Removal of a delta shall be restricted to:1. The user who made the delta2. The owner of the SCCS file3. The owner of the directory containing the SCCS file",
        "name": "rmdel \u2014 remove a delta from an SCCS file (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "sact",
        "description": "The sact utility shall inform the user of any impending deltas toa named SCCS file by writing a list to standard output. Thissituation occurs when get -e has been executed previously withouta subsequent execution of delta, unget, or sccs unedit.",
        "name": "sact \u2014 print current SCCS file-editing activity (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "sccs",
        "description": "The sccs utility is a front end to the SCCS programs. It alsoincludes the capability to run set-user-id to another user toprovide additional protection.The sccs utility shall invoke the specified command with thespecified options and operands.By default, each of the operandsshall be modified by prefixing it with the string \"SCCS/s.\".The command can be the name of one of the SCCS utilities in thisvolume of POSIX.1\u20102017 (admin, delta, get, prs, rmdel, sact,unget, val, or what) or one of the pseudo-utilities listed in theEXTENDED DESCRIPTION section.",
        "name": "sccs \u2014 front end for the SCCS subsystem (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "set",
        "description": "If no options or arguments are specified, set shall write thenames and values of all shell variables in the collation sequenceof the current locale. Each name shall start on a separate line,using the format:\"%s=%s\\n\", <name>, <value>The value string shall be written with appropriate quoting; seethe description of shell quoting in Section 2.2, Quoting.Theoutput shall be suitable for reinput to the shell, setting orresetting, as far as possible, the variables that are currentlyset; read-only variables cannot be reset.When options are specified, they shall set or unset attributes ofthe shell, as described below. When arguments are specified, theycause positional parameters to be set or unset, as describedbelow. Setting or unsetting attributes and positional parametersare not necessarily related actions, but they can be combined ina single invocation of set.The set special built-in shall support the Base Definitionsvolume of POSIX.1\u20102017, Section 12.2, Utility Syntax Guidelinesexcept that options can be specified with either a leading<hyphen-minus> (meaning enable the option) or <plus-sign>(meaning disable it) unless otherwise specified.Implementations shall support the options in the following listin both their <hyphen-minus> and <plus-sign> forms. These optionscan also be specified as options to sh.-aWhen this option is on, the export attribute shall be setfor each variable to which an assignment is performed; seethe Base Definitions volume of POSIX.1\u20102017, Section 4.23,Variable Assignment.If the assignment precedes a utilityname in a command, the export attribute shall not persistin the current execution environment after the utilitycompletes, with the exception that preceding one of thespecial built-in utilities causes the export attribute topersist after the built-in has completed. If the assignmentdoes not precede a utility name in the command, or if theassignment is a result of the operation of the getopts orread utilities, the export attribute shall persist untilthe variable is unset.-bThis option shall be supported if the implementationsupports the User Portability Utilities option. It shallcause the shell to notify the user asynchronously ofbackground job completions. The following message iswritten to standard error:\"[%d]%c %s%s\\n\", <job-number>, <current>, <status>, <job-name>where the fields shall be as follows:<current>The character '+' identifies the job that wouldbe used as a default for the fg or bgutilities; this job can also be specified usingthe job_id \"%+\" or \"%%\".The character '-'identifies the job that would become thedefault if the current default job were toexit; this job can also be specified using thejob_id \"%-\".For other jobs, this field is a<space>.At most one job can be identifiedwith '+' and at most one job can be identifiedwith '-'.If there is any suspended job, thenthe current job shall be a suspended job. Ifthere are at least two suspended jobs, then theprevious job also shall be a suspended job.<job-number>A number that can be used to identify theprocess group to the wait, fg, bg, and killutilities. Using these utilities, the job canbe identified by prefixing the job number with'%'.<status>Unspecified.<job-name>Unspecified.When the shell notifies the user a job has been completed,it may remove the job's process ID from the list of thoseknown in the current shell execution environment; seeSection 2.9.3.1, Examples.Asynchronous notification shallnot be enabled by default.-C(Uppercase C.) Prevent existing files from beingoverwritten by the shell's '>' redirection operator (seeSection 2.7.2, Redirecting Output); the \">|\" redirectionoperator shall override this noclobber option for anindividual file.-eWhen this option is on, when any command fails (for any ofthe reasons listed in Section 2.8.1, Consequences of ShellErrors or by returning an exit status greater than zero),the shell immediately shall exit, as if by executing theexit special built-in utility with no arguments, with thefollowing exceptions:1. The failure of any individual command in a multi-command pipeline shall not cause the shell to exit.Only the failure of the pipeline itself shall beconsidered.2. The -e setting shall be ignored when executing thecompound list following the while, until, if, or elifreserved word, a pipeline beginning with the !reserved word, or any command of an AND-OR list otherthan the last.3. If the exit status of a compound command other than asubshell command was the result of a failure while -ewas being ignored, then -e shall not apply to thiscommand.This requirement applies to the shell environment and eachsubshell environment separately. For example, in:set -e; (false; echo one) | cat; echo twothe false command causes the subshell to exit withoutexecuting echo one; however, echo two is executed becausethe exit status of the pipeline (false; echo one) | cat iszero.-fThe shell shall disable pathname expansion.-hLocate and remember utilities invoked by functions as thosefunctions are defined (the utilities are normally locatedwhen the function is executed).-mThis option shall be supported if the implementationsupports the User Portability Utilities option. All jobsshall be run in their own process groups. Immediatelybefore the shell issues a prompt after completion of thebackground job, a message reporting the exit status of thebackground job shall be written to standard error. If aforeground job stops, the shell shall write a message tostandard error to that effect, formatted as described bythe jobs utility. In addition, if a job changes statusother than exiting (for example, if it stops for input oroutput or is stopped by a SIGSTOP signal), the shell shallwrite a similar message immediately prior to writing thenext prompt. This option is enabled by default forinteractive shells.-nThe shell shall read commands but does not execute them;this can be used to check for shell script syntax errors.An interactive shell may ignore this option.-oWrite the current settings of the options to standardoutput in an unspecified format.+oWrite the current option settings to standard output in aformat that is suitable for reinput to the shell ascommands that achieve the same options settings.-o optionThis option is supported if the system supports the UserPortability Utilities option. It shall set various options,many of which shall be equivalent to the single optionletters. The following values of option shall be supported:allexport Equivalent to -a.errexitEquivalent to -e.ignoreeof Prevent an interactive shell from exiting on end-of-file. This setting prevents accidental logoutswhen <control>\u2010D is entered. A user shallexplicitly exit to leave the interactive shell.monitorEquivalent to -m.This option is supported ifthe system supports the User PortabilityUtilities option.noclobber Equivalent to -C (uppercase C).noglobEquivalent to -f.noexecEquivalent to -n.nologPrevent the entry of function definitions intothe command history; see Command History List.notifyEquivalent to -b.nounsetEquivalent to -u.verboseEquivalent to -v.viAllow shell command line editing using the built-in vi editor. Enabling vi mode shall disable anyother command line editing mode provided as animplementation extension.It need not be possible to set vi mode on forcertain block-mode terminals.xtraceEquivalent to -x.-uWhen the shell tries to expand an unset parameter otherthan the '@' and '*' special parameters, it shall write amessage to standard error and the expansion shall fail withthe consequences specified in Section 2.8.1, Consequencesof Shell Errors.-vThe shell shall write its input to standard error as it isread.-xThe shell shall write to standard error a trace for eachcommand after it expands the command and before it executesit. It is unspecified whether the command that turnstracing off is traced.The default for all these options shall be off (unset) unlessstated otherwise in the description of the option or unless theshell was invoked with them on; see sh.The remaining arguments shall be assigned in order to thepositional parameters. The special parameter '#' shall be set toreflect the number of positional parameters. All positionalparameters shall be unset before any new values are assigned.If the first argument is '-', the results are unspecified.The special argument \"--\" immediately following the set commandname can be used to delimit the arguments if the first argumentbegins with '+' or '-', or to prevent inadvertent listing of allshell variables when there are no arguments. The command set --without argument shall unset all positional parameters and setthe special parameter '#' to zero.",
        "name": "set \u2014 set or unset options and positional parameters",
        "section": 1
    },
    {
        "command": "sh",
        "description": "The sh utility is a command language interpreter that shallexecute commands read from a command line string, the standardinput, or a specified file. The application shall ensure that thecommands to be executed are expressed in the language describedin Chapter 2, Shell Command Language.Pathname expansion shall not fail due to the size of a file.Shell input and output redirections have an implementation-defined offset maximum that is established in the open filedescription.",
        "name": "sh \u2014 shell, the standard command language interpreter",
        "section": 1
    },
    {
        "command": "shift",
        "description": "The positional parameters shall be shifted. Positional parameter1 shall be assigned the value of parameter (1+n), parameter 2shall be assigned the value of parameter (2+n), and so on. Theparameters represented by the numbers \"$#\" down to \"$#-n+1\" shallbe unset, and the parameter '#' is updated to reflect the newnumber of positional parameters.The value n shall be an unsigned decimal integer less than orequal to the value of the special parameter '#'.If n is notgiven, it shall be assumed to be 1. If n is 0, the positional andspecial parameters are not changed.",
        "name": "shift \u2014 shift positional parameters",
        "section": 1
    },
    {
        "command": "talk",
        "description": "The talk utility is a two-way, screen-oriented communicationprogram.When first invoked, talk shall send a message similar to:Message from <unspecified string>talk: connection requested by your_addresstalk: respond with: talk your_addressto the specified address.At this point, the recipient of themessage can reply by typing:talk your_addressOnce communication is established, the two parties can typesimultaneously, with their output displayed in separate regionsof the screen. Characters shall be processed as follows:*Typing the <alert> character shall alert the recipient'sterminal.*Typing <control>\u2010L shall cause the sender's screen regions tobe refreshed.*Typing the erase and kill characters shall affect thesender's terminal in the manner described by the termiosinterface in the Base Definitions volume of POSIX.1\u20102017,Chapter 11, General Terminal Interface.*Typing the interrupt or end-of-file characters shallterminate the local talk utility. Once the talk session hasbeen terminated on one side, the other side of the talksession shall be notified that the talk session has beenterminated and shall be able to do nothing except exit.*Typing characters from LC_CTYPE classifications print orspace shall cause those characters to be sent to therecipient's terminal.*When and only when the stty iexten local mode is enabled, theexistence and processing of additional special controlcharacters and multi-byte or single-byte functions shall beimplementation-defined.*Typing other non-printable characters shall causeimplementation-defined sequences of printable characters tobe sent to the recipient's terminal.Permission to be a recipient of a talk message can be denied orgranted by use of the mesg utility. However, a user's privilegemay further constrain the domain of accessibility of other users'terminals. The talk utility shall fail when the user lacksappropriate privileges to perform the requested action.Certain block-mode terminals do not have all the capabilitiesnecessary to support the simultaneous exchange of messagesrequired for talk.When this type of exchange cannot besupported on such terminals, the implementation may support anexchange with reduced levels of simultaneous interaction or itmay report an error describing the terminal-related deficiency.",
        "name": "talk \u2014 talk to another user",
        "section": 1
    },
    {
        "command": "times",
        "description": "The times utility shall write the accumulated user and systemtimes for the shell and for all of its child processes, in thefollowing POSIX locale format:\"%dm%fs %dm%fs\\n%dm%fs %dm%fs\\n\", <shell user minutes>,<shell user seconds>, <shell system minutes>,<shell system seconds>, <children user minutes>,<children user seconds>, <children system minutes>,<children system seconds>The four pairs of times shall correspond to the members of the<sys/times.h> tms structure (defined in the Base Definitionsvolume of POSIX.1\u20102017, Chapter 13, Headers) as returned bytimes(): tms_utime, tms_stime, tms_cutime, and tms_cstime,respectively.",
        "name": "times \u2014 write process times",
        "section": 1
    },
    {
        "command": "trap",
        "description": "If the first operand is an unsigned decimal integer, the shellshall treat all operands as conditions, and shall reset eachcondition to the default value. Otherwise, if there are operands,the first is treated as an action and the remaining asconditions.If action is '-', the shell shall reset each condition to thedefault value. If action is null (\"\"), the shell shall ignoreeach specified condition if it arises. Otherwise, the argumentaction shall be read and executed by the shell when one of thecorresponding conditions arises. The action of trap shalloverride a previous action (either default action or oneexplicitly set). The value of \"$?\" after the trap actioncompletes shall be the value it had before trap was invoked.The condition can be EXIT, 0 (equivalent to EXIT), or a signalspecified using a symbolic name, without the SIG prefix, aslisted in the tables of signal names in the <signal.h> headerdefined in the Base Definitions volume of POSIX.1\u20102017, Chapter13, Headers; for example, HUP, INT, QUIT, TERM. Implementationsmay permit names with the SIG prefix or ignore case in signalnames as an extension. Setting a trap for SIGKILL or SIGSTOPproduces undefined results.The environment in which the shell executes a trap on EXIT shallbe identical to the environment immediately after the lastcommand executed before the trap on EXIT was taken.Each time trap is invoked, the action argument shall be processedin a manner equivalent to:eval actionSignals that were ignored on entry to a non-interactive shellcannot be trapped or reset, although no error need be reportedwhen attempting to do so. An interactive shell may reset or catchsignals ignored on entry. Traps shall remain in place for a givenshell until explicitly changed with another trap command.When a subshell is entered, traps that are not being ignoredshall be set to the default actions, except in the case of acommand substitution containing only a single trap command, whenthe traps need not be altered. Implementations may check for thiscase using only lexical analysis; for example, if `trap` and $(trap -- ) do not alter the traps in the subshell, cases such asassigning var=trap and then using $($var) may still alter them.This does not imply that the trap command cannot be used withinthe subshell to set new traps.The trap command with no operands shall write to standard outputa list of commands associated with each condition. If the commandis executed in a subshell, the implementation does not performthe optional check described above for a command substitutioncontaining only a single trap command, and no trap commands withoperands have been executed since entry to the subshell, the listshall contain the commands that were associated with eachcondition immediately before the subshell environment wasentered.Otherwise, the list shall contain the commandscurrently associated with each condition. The format shall be:\"trap -- %s %s ...\\n\", <action>, <condition> ...The shell shall format the output, including the proper use ofquoting, so that it is suitable for reinput to the shell ascommands that achieve the same trapping results. For example:save_traps=$(trap)...eval \"$save_traps\"XSI-conformant systems also allow numeric signal numbers for theconditions corresponding to the following signal names:1SIGHUP2SIGINT3SIGQUIT6SIGABRT9SIGKILL14SIGALRM15SIGTERMThe trap special built-in shall conform to the Base Definitionsvolume of POSIX.1\u20102017, Section 12.2, Utility Syntax Guidelines.",
        "name": "trap \u2014 trap signals",
        "section": 1
    },
    {
        "command": "type",
        "description": "The type utility shall indicate how each argument would beinterpreted if used as a command name.",
        "name": "type \u2014 write a description of command type",
        "section": 1
    },
    {
        "command": "ulimit",
        "description": "The ulimit utility shall set or report the file-size writinglimit imposed on files written by the shell and its childprocesses (files of any size may be read). Only a process withappropriate privileges can increase the limit.",
        "name": "ulimit \u2014 set or report file size limit",
        "section": 1
    },
    {
        "command": "umask",
        "description": "The umask utility shall set the file mode creation mask of thecurrent shell execution environment (see Section 2.12, ShellExecution Environment) to the value specified by the maskoperand. This mask shall affect the initial value of the filepermission bits of subsequently created files. If umask is calledin a subshell or separate utility execution environment, such asone of the following:(umask 002)nohup umask ...find . -exec umask ... \\;it shall not affect the file mode creation mask of the caller'senvironment.If the mask operand is not specified, the umask utility shallwrite to standard output the value of the file mode creation maskof the invoking process.",
        "name": "umask \u2014 get or set the file mode creation mask",
        "section": 1
    },
    {
        "command": "unalias",
        "description": "The unalias utility shall remove the definition for each aliasname specified. See Section 2.3.1, Alias Substitution.Thealiases shall be removed from the current shell executionenvironment; see Section 2.12, Shell Execution Environment.",
        "name": "unalias \u2014 remove alias definitions",
        "section": 1
    },
    {
        "command": "uncompress",
        "description": "The uncompress utility shall restore files to their originalstate after they have been compressed using the compress utility.If no files are specified, the standard input shall beuncompressed to the standard output. If the invoking process hasappropriate privileges, the ownership, modes, access time, andmodification time of the original file shall be preserved.This utility shall support the uncompressing of any filesproduced by the compress utility on the same implementation. Forfiles produced by compress on other systems, uncompress supports9 to 14-bit compression (see compress(1p), -b); it isimplementation-defined whether values of -b greater than 14 aresupported.",
        "name": "uncompress \u2014 expand compressed data",
        "section": 1
    },
    {
        "command": "unget",
        "description": "The unget utility shall reverse the effect of a get -e done priorto creating the intended new delta.",
        "name": "unget \u2014 undo a previous get of an SCCS file (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "unset",
        "description": "Each variable or function specified by name shall be unset.If -v is specified, name refers to a variable name and the shellshall unset it and remove it from the environment. Read-onlyvariables cannot be unset.If -f is specified, name refers to a function and the shell shallunset the function definition.If neither -f nor -v is specified, name refers to a variable; ifa variable by that name does not exist, it is unspecified whethera function by that name, if any, shall be unset.Unsetting a variable or function that was not previously setshall not be considered an error and does not cause the shell toabort.The unset special built-in shall support the Base Definitionsvolume of POSIX.1\u20102017, Section 12.2, Utility Syntax Guidelines.Note that:VARIABLE=is not equivalent to an unset of VARIABLE; in the example,VARIABLE is set to \"\".Also, the variables that can be unsetshould not be misinterpreted to include the special parameters(see Section 2.5.2, Special Parameters).",
        "name": "unset \u2014 unset values and attributes of variables and functions",
        "section": 1
    },
    {
        "command": "uucp",
        "description": "The uucp utility shall copy files named by the source-fileargument to the destination-file argument. The files named can beon local or remote systems.The uucp utility cannot guarantee support for all characterencodings in all circumstances. For example, transmission datamay be restricted to 7 bits by the underlying network, 8-bit dataand filenames need not be portable to non-internationalizedsystems, and so on. Under these circumstances, it is recommendedthat only characters defined in the ISO/IEC 646:1991 standardInternational Reference Version (equivalent to ASCII) 7-bit rangeof characters be used, and that only characters defined in theportable filename character set be used for naming files. Theprotocol for transfer of files is unspecified by POSIX.1\u20102008.Typical implementations of this utility require a communicationsline configured to use the Base Definitions volume ofPOSIX.1\u20102017, Chapter 11, General Terminal Interface, but othercommunications means may be used. On systems where there are noavailable communications means (either temporarily orpermanently), this utility shall write an error messagedescribing the problem and exit with a non-zero exit status.",
        "name": "uucp \u2014 system-to-system copy",
        "section": 1
    },
    {
        "command": "uudecode",
        "description": "The uudecode utility shall read a file, or standard input if nofile is specified, that includes data created by the uuencodeutility. The uudecode utility shall scan the input file,searching for data compatible with one of the formats specifiedin uuencode, and attempt to create or overwrite the filedescribed by the data (or overridden by the -o option). Thepathname shall be contained in the data or specified by the -ooption. The file access permission bits and contents for the fileto be produced shall be contained in that data. The mode bits ofthe created file (other than standard output) shall be set fromthe file access permission bits contained in the data; that is,other attributes of the mode, including the file mode creationmask (see umask), shall not affect the file being produced. Ifeither of the op characters '+' and '-' (see chmod) are specifiedin symbolic mode, the initial mode on which those operations arebased is unspecified.If the pathname of the file resolves to an existing file and theuser does not have write permission on that file, uudecode shallterminate with an error. If the pathname of the file resolves toan existing file and the user has write permission on that file,the existing file shall be overwritten and, if possible, the modebits of the file (other than standard output) shall be set asdescribed above; if the mode bits cannot be set, uudecode shallnot treat this as an error.If the input data was produced by uuencode on a system with adifferent number of bits per byte than on the target system, theresults of uudecode are unspecified.",
        "name": "uudecode \u2014 decode a binary file",
        "section": 1
    },
    {
        "command": "uuencode",
        "description": "The uuencode utility shall write an encoded version of the namedinput file, or standard input if no file is specified, tostandard output. The output shall be encoded using one of thealgorithms described in the STDOUT section and shall include thefile access permission bits (in chmod octal or symbolic notation)of the input file and the decode_pathname, for re-creation of thefile on another system that conforms to this volume ofPOSIX.1\u20102017.",
        "name": "uuencode \u2014 encode a binary file",
        "section": 1
    },
    {
        "command": "uustat",
        "description": "The uustat utility shall display the status of, or cancel,previously specified uucp requests, or provide general status onuucp connections to other systems.When no options are given, uustat shall write to standard outputthe status of all uucp requests issued by the current user.Typical implementations of this utility require a communicationsline configured to use the Base Definitions volume ofPOSIX.1\u20102017, Chapter 11, General Terminal Interface, but othercommunications means may be used. On systems where there are noavailable communications means (either temporarily orpermanently), this utility shall write an error messagedescribing the problem and exit with a non-zero exit status.",
        "name": "uustat \u2014 uucp status enquiry and job control",
        "section": 1
    },
    {
        "command": "uux",
        "description": "The uux utility shall gather zero or more files from varioussystems, execute a shell pipeline (see Section 2.9, ShellCommands) on a specified system, and then send the standardoutput of the command to a file on a specified system. Only thefirst command of a pipeline can have a system-name!prefix. Allother commands in the pipeline shall be executed on the system ofthe first command.The following restrictions are applicable to the shell pipelineprocessed by uux:*In gathering files from different systems, pathname expansionshall not be performed by uux.Thus, a request such as:uux \"c99 remsys!~/*.c\"would attempt to copy the file named literally *.c to thelocal system.*The redirection operators \">>\", \"<<\", \">|\", and \">&\" shallnot be accepted. Any use of these redirection operators shallcause this utility to write an error message describing theproblem and exit with a non-zero exit status.*The reserved word !cannot be used at the head of thepipeline to modify the exit status.(See the command-stringoperand description below.)*Alias substitution shall not be performed.A filename can be specified as for uucp; it can be an absolutepathname, a pathname preceded by ~name (which is replaced by thecorresponding login directory), a pathname specified as ~/dest(dest is prefixed by the public directory called PUBDIR; theactual location of PUBDIR is implementation-defined), or a simplefilename (which is prefixed by uux with the current directory).See uucp(1p) for the details.The execution of commands on remote systems shall take place inan execution directory known to the uucp system. All filesrequired for the execution shall be put into this directoryunless they already reside on that machine. Therefore, theapplication shall ensure that non-local filenames (without pathor machine reference) are unique within the uux request.The uux utility shall attempt to get all files to the executionsystem. For files that are output files, the application shallensure that the filename is escaped using parentheses.The remote system shall notify the user by mail if the requestedcommand on the remote system was disallowed or the files were notaccessible. This notification can be turned off by the -n option.Typical implementations of this utility require a communicationsline configured to use the Base Definitions volume ofPOSIX.1\u20102017, Chapter 11, General Terminal Interface, but othercommunications means may be used. On systems where there are noavailable communications means (either temporarily orpermanently), this utility shall write an error messagedescribing the problem and exit with a non-zero exit status.The uux utility cannot guarantee support for all characterencodings in all circumstances. For example, transmission datamay be restricted to 7 bits by the underlying network, 8-bit dataand filenames need not be portable to non-internationalizedsystems, and so on. Under these circumstances, it is recommendedthat only characters defined in the ISO/IEC 646:1991 standardInternational Reference Version (equivalent to ASCII) 7-bit rangeof characters be used and that only characters defined in theportable filename character set be used for naming files.",
        "name": "uux \u2014 remote command execution",
        "section": 1
    },
    {
        "command": "val",
        "description": "The val utility shall determine whether the specified file is anSCCS file meeting the characteristics specified by the options.",
        "name": "val \u2014 validate SCCS files (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "vi",
        "description": "This utility shall be provided on systems that both support theUser Portability Utilities option and define the POSIX2_CHAR_TERMsymbol.On other systems it is optional.The vi (visual) utility is a screen-oriented text editor. Onlythe open and visual modes of the editor are described inPOSIX.1\u20102008; see the line editor ex for additional editingcapabilities used in vi.The user can switch back and forthbetween vi and ex and execute ex commands from within vi.This reference page uses the term edit buffer to describe thecurrent working text. No specific implementation is implied bythis term. All editing changes are performed on the edit buffer,and no changes to it shall affect any file until an editorcommand writes the file.When using vi, the terminal screen acts as a window into theediting buffer. Changes made to the editing buffer shall bereflected in the screen display; the position of the cursor onthe screen shall indicate the position within the editing buffer.Certain terminals do not have all the capabilities necessary tosupport the complete vi definition. When these commands cannot besupported on such terminals, this condition shall not produce anerror message such as ``not an editor command'' or report asyntax error. The implementation may either accept the commandsand produce results on the screen that are the result of anunsuccessful attempt to meet the requirements of this volume ofPOSIX.1\u20102017 or report an error describing the terminal-relateddeficiency.",
        "name": "vi \u2014 screen-oriented (visual) display editor",
        "section": 1
    },
    {
        "command": "wait",
        "description": "When an asynchronous list (see Section 2.9.3.1, Examples) isstarted by the shell, the process ID of the last command in eachelement of the asynchronous list shall become known in thecurrent shell execution environment; see Section 2.12, ShellExecution Environment.If the wait utility is invoked with no operands, it shall waituntil all process IDs known to the invoking shell have terminatedand exit with a zero exit status.If one or more pid operands are specified that represent knownprocess IDs, the wait utility shall wait until all of them haveterminated. If one or more pid operands are specified thatrepresent unknown process IDs, wait shall treat them as if theywere known process IDs that exited with exit status 127. The exitstatus returned by the wait utility shall be the exit status ofthe process requested by the last pid operand.The known process IDs are applicable only for invocations of waitin the current shell execution environment.",
        "name": "wait \u2014 await process completion",
        "section": 1
    },
    {
        "command": "what",
        "description": "The what utility shall search the given files for all occurrencesof the pattern that get (see get(1p)) substitutes for the %Z%keyword (\"@(#)\") and shall write to standard output what followsuntil the first occurrence of one of the following:\">newline\\NUL",
        "name": "what \u2014 identify SCCS files (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "write",
        "description": "The write utility shall read lines from the standard input andwrite them to the terminal of the specified user. When firstinvoked, it shall write the message:Message from sender-login-id (sending-terminal) [date]...to user_name.When it has successfully completed the connection,the sender's terminal shall be alerted twice to indicate thatwhat the sender is typing is being written to the recipient'sterminal.If the recipient wants to reply, this can be accomplished bytyping:write sender-login-id [sending-terminal]upon receipt of the initial message. Whenever a line of input asdelimited by an NL, EOF, or EOL special character (see the BaseDefinitions volume of POSIX.1\u20102017, Chapter 11, General TerminalInterface) is accumulated while in canonical input mode, theaccumulated data shall be written on the other user's terminal.Characters shall be processed as follows:*Typing <alert> shall write the <alert> character to therecipient's terminal.*Typing the erase and kill characters shall affect thesender's terminal in the manner described by the termiosinterface in the Base Definitions volume of POSIX.1\u20102017,Chapter 11, General Terminal Interface.*Typing the interrupt or end-of-file characters shall causewrite to write an appropriate message (\"EOT\\n\" in the POSIXlocale) to the recipient's terminal and exit.*Typing characters from LC_CTYPE classifications print orspace shall cause those characters to be sent to therecipient's terminal.*When and only when the stty iexten local mode is enabled, theexistence and processing of additional special controlcharacters and multi-byte or single-byte functions isimplementation-defined.*Typing other non-printable characters shall causeimplementation-defined sequences of printable characters tobe written to the recipient's terminal.To write to a user who is logged in more than once, the terminalargument can be used to indicate which terminal to write to;otherwise, the recipient's terminal is selected in animplementation-defined manner and an informational message iswritten to the sender's standard output, indicating whichterminal was chosen.Permission to be a recipient of a write message can be denied orgranted by use of the mesg utility. However, a user's privilegemay further constrain the domain of accessibility of other users'terminals. The write utility shall fail when the user lacksappropriate privileges to perform the requested action.",
        "name": "write \u2014 write to another user",
        "section": 1
    },
    {
        "command": "yacc",
        "description": "The yacc utility shall read a description of a context-freegrammar in grammar and write C source code, conforming to theISO C standard, to a code file, and optionally header informationinto a header file, in the current directory. The generatedsource code shall not depend on any undefined, unspecified, orimplementation-defined behavior, except in cases where it iscopied directly from the supplied grammar, or in cases that aredocumented by the implementation. The C code shall define afunction and related routines and macros for an automaton thatexecutes a parsing algorithm meeting the requirements inAlgorithms.The form and meaning of the grammar are described in the EXTENDEDDESCRIPTION section.The C source code and header file shall be produced in a formsuitable as input for the C compiler (see c99(1p)).",
        "name": "yacc \u2014 yet another compiler compiler (DEVELOPMENT)",
        "section": 1
    },
    {
        "command": "zcat",
        "description": "The zcat utility shall write to standard output the uncompressedform of files that have been compressed using the compressutility. It is the equivalent of uncompress -c.Input files arenot affected.",
        "name": "zcat \u2014 expand and concatenate data",
        "section": 1
    },
    {
        "command": "AS",
        "description": "GNU as is really a family of assemblers.If you use (or haveused) the GNU assembler on one architecture, you should find afairly similar environment when you use it on anotherarchitecture.Each version has much in common with the others,including object file formats, most assembler directives (oftencalled pseudo-ops) and assembler syntax.as is primarily intended to assemble the output of the GNU Ccompiler \"gcc\" for use by the linker \"ld\".Nevertheless, we'vetried to make as assemble correctly everything that otherassemblers for the same machine would assemble.Any exceptionsare documented explicitly.This doesn't mean as always uses thesame syntax as another assembler for the same architecture; forexample, we know of several incompatible versions of 680x0assembly language syntax.Each time you run as it assembles exactly one source program.The source program is made up of one or more files.(Thestandard input is also a file.)You give as a command line that has zero or more input filenames.The input files are read (from left file name to right).A command-line argument (in any position) that has no specialmeaning is taken to be an input file name.If you give as no file names it attempts to read one input filefrom the as standard input, which is normally your terminal.Youmay have to type ctl-D to tell as there is no more program toassemble.Use -- if you need to explicitly name the standard input file inyour command line.If the source is empty, as produces a small, empty object file.as may write warnings and error messages to the standard errorfile (usually your terminal).This should not happen whenacompiler runs as automatically.Warnings report an assumptionmade so that as could keep assembling a flawed program; errorsreport a grave problem that stops the assembly.If you are invoking as via the GNU C compiler, you can use the-Wa option to pass arguments through to the assembler.Theassembler arguments must be separated from each other (and the-Wa) by commas.For example:gcc -c -g -O -Wa,-alh,-L file.cThis passes two options to the assembler: -alh (emit a listing tostandard output with high-level and assembly source) and -L(retain local symbols in the symbol table).Usually you do not need to use this -Wa mechanism, since manycompiler command-line options are automatically passed to theassembler by the compiler.(You can call the GNU compiler driverwith the -v option to see precisely what options it passes toeach compilation pass, including the assembler.)",
        "name": "AS - the portable GNU assembler.",
        "section": 1
    },
    {
        "command": "CLEAR",
        "description": "@CLEAR@ clears your terminal's screen if this is possible,including the terminal's scrollback buffer (if the extended \u201cE3\u201dcapability is defined).@CLEAR@ looks in the environment for theterminal type given by the environment variable TERM, and then inthe terminfo database to determine how to clear the screen.@CLEAR@ writes to the standard output.You can redirect thestandard output to a file (which prevents @CLEAR@ from actuallyclearing the screen), and later cat the file to the screen,clearing it at that point.",
        "name": "@CLEAR@ - clear the terminal screen",
        "section": 1
    },
    {
        "command": "Firecfg",
        "description": "Firecfg is the desktop integration utility for Firejail sandbox.It allows the user to sandbox applications automatically byclicking on desktop manager icons and menus.The integration covers:- programs started in a terminal - typing \"firefox\" wouldbe enough to start a sandboxed Firefox browser- programs started by clicking on desktop manager menus -all major desktop managers are supported- programs started by clicking on file icons in filemanager - only Cinnamon, KDE, LXDE/LXQT, MATE and XFCEdesktop managers are supported in this momentTo set it up, run \"sudo firecfg\" after installing Firejailsoftware.The same command should also be run after installingnew programs. If the program is supported by Firejail, thesymbolic link in /usr/local/bin will be created. For a full listof programs supported by default run \"cat/etc/firejail/firecfg.config\".For user-driven manual integration, see DESKTOP INTEGRATIONsection in man 1 firejail.",
        "name": "Firecfg - Desktop integration utility for Firejail software.",
        "section": 1
    },
    {
        "command": "Firejail",
        "description": "Firejail is a SUID sandbox program that reduces the risk ofsecurity breaches by restricting the running environment ofuntrusted applications using Linux namespaces, seccomp-bpf andLinux capabilities.It allows a process and all its descendantsto have their own private view of the globally shared kernelresources, such as the network stack, process table, mount table.Firejail can work in a SELinux or AppArmor environment, and it isintegrated with Linux Control Groups.Written in C with virtually no dependencies, the software runs onany Linux computer with a 3.x kernel version or newer.It cansandbox any type of processes: servers, graphical applications,and even user login sessions.Firejail allows the user to manage application security usingsecurity profiles.Each profile defines a set of permissions fora specific application or group of applications. The softwareincludes security profiles for a number of more common Linuxprograms, such as Mozilla Firefox, Chromium, VLC, Transmissionetc.Firejail is currently implemented as an SUID binary, which meansthat if a malicious or compromised user account manages toexploit a bug in Firejail, that could ultimately lead to aprivilege escalation to root.To mitigate this, it isrecommended to only allow trusted users to run firejail (seefirejail-users(5) for details on how to achieve that).For moredetails on the security/usability tradeoffs of Firejail, see:#4601 \u27e8https://github.com/netblue30/firejail/discussions/4601\u27e9Alternative sandbox technologies like snap(https://snapcraft.io/) and flatpak (https://flatpak.org/) arenot supported. Snap and flatpak packages have their own nativemanagement tools and will not work when sandboxed with Firejail.",
        "name": "Firejail - Linux namespaces sandbox program",
        "section": 1
    },
    {
        "command": "Firemon",
        "description": "Firemon monitors programs started in a Firejail sandbox.Withouta PID specified, all processes started by Firejail are monitored.Descendants of these processes are also being monitored. OnGrsecurity systems only root user can run this program.",
        "name": "Firemon - Monitoring program for processes started in a Firejailsandbox.",
        "section": 1
    },
    {
        "command": "PCPCompat",
        "description": null,
        "name": "PCPCompat, pcp-collectl, pmmgr, pmwebd - backward-compatibilityin the Performance Co-Pilot (PCP)",
        "section": 1
    },
    {
        "command": "PCPIntro",
        "description": null,
        "name": "PCPIntro - introduction to the Performance Co-Pilot (PCP)",
        "section": 1
    },
    {
        "command": "RESET",
        "description": "tset - initializationThis program initializes terminals.First, @TSET@ retrieves the current terminal mode settings foryour terminal.It does this by successively testing\u2022the standard error,\u2022standard output,\u2022standard input and\u2022ultimately \u201c/dev/tty\u201dto obtain terminal settings.Having retrieved these settings,@TSET@ remembers which file descriptor to use when updatingsettings.Next, @TSET@ determines the type of terminal that you are using.This determination is done as follows, using the first terminaltype found.1. The terminal argument specified on the command line.2. The value of the TERM environmental variable.3. (BSD systems only.) The terminal type associated with thestandard error output device in the /etc/ttys file.(OnSystem-V-like UNIXes and systems using that convention, getty(1)does this job by setting TERM according to the type passed to itby /etc/inittab.)4. The default terminal type, \u201cunknown\u201d.If the terminal type was not specified on the command-line, the-m option mappings are then applied (see the section TERMINALTYPE MAPPING for more information).Then, if the terminal typebegins with a question mark (\u201c?\u201d), the user is prompted forconfirmation of the terminal type.An empty response confirmsthe type, or, another type can be entered to specify a new type.Once the terminal type has been determined, the terminaldescription for the terminal is retrieved.If no terminaldescription is found for the type, the user is prompted foranother terminal type.Once the terminal description is retrieved,\u2022if the \u201c-w\u201d option is enabled, @TSET@ may update theterminal's window size.If the window size cannot be obtained from the operatingsystem, but the terminal description (or environment, e.g.,LINES and COLUMNS variables specify this), use this to setthe operating system's notion of the window size.\u2022if the \u201c-c\u201d option is enabled, the backspace, interrupt andline kill characters (among many other things) are set\u2022unless the \u201c-I\u201d option is enabled, the terminal and tabinitialization strings are sent to the standard error output,and @TSET@ waits one second (in case a hardware reset wasissued).\u2022Finally, if the erase, interrupt and line kill charactershave changed, or are not set to their default values, theirvalues are displayed to the standard error output.reset - reinitializationWhen invoked as @RESET@, @TSET@ sets the terminal modes to \u201csane\u201dvalues:\u2022sets cooked and echo modes,\u2022turns off cbreak and raw modes,\u2022turns on newline translation and\u2022resets any unset special characters to their default valuesbefore doing the terminal initialization described above.Also,rather than using the terminal initialization strings, it usesthe terminal reset strings.The @RESET@ command is useful after a program dies leaving aterminal in an abnormal state:\u2022you may have to type<LF>@RESET@<LF>(the line-feed character is normally control-J) to get theterminal to work, as carriage-return may no longer work inthe abnormal state.\u2022Also, the terminal will often not echo the command.",
        "name": "@TSET@, @RESET@ - terminal initialization",
        "section": 1
    },
    {
        "command": "SSHFS",
        "description": "SSHFS allows you to mount a remote filesystem using SSH (moreprecisely, the SFTP subsystem). Most SSH servers support andenable this SFTP access by default, so SSHFS is very simple touse - there's nothing to do on the server-side.By default, file permissions are ignored by SSHFS. Any user thatcan access the filesystem will be able to perform any operationthat the remote server permits - based on the credentials thatwere used to connect to the server. If this is undesired, localpermission checking can be enabled with -o default_permissions.By default, only the mounting user will be able to access thefilesystem. Access for other users can be enabled by passing -oallow_other. In this case you most likely also want to use -odefault_permissions.It is recommended to run SSHFS as regular user (not as root).For this to work the mountpoint must be owned by the user.Ifusername is omitted SSHFS will use the local username. If thedirectory is omitted, SSHFS will mount the (remote) homedirectory.If you need to enter a password sshfs will ask for it(actually it just runs ssh which ask for the password if needed).",
        "name": "SSHFS - filesystem client based on SSH",
        "section": 1
    },
    {
        "command": "TABS",
        "description": "The @TABS@ program clears and sets tab-stops on the terminal.This uses the terminfo clear_all_tabs and set_tab capabilities.If either is absent, @TABS@ is unable to clear/set tab-stops.The terminal should be configured to use hard tabs, e.g.,stty tab0Like @CLEAR@(1), @TABS@ writes to the standard output.You canredirect the standard output to a file (which prevents @TABS@from actually changing the tabstops), and later cat the file tothe screen, setting tabstops at that point.These are hardware tabs, which cannot be queried rapidly byapplications running in the terminal, if at all.Curses andother full-screen applications may use hardware tabs inoptimizing their output to the terminal.If the hardwaretabstops differ from the information in the terminal database,the result is unpredictable.Before running curses programs, youshould either reset tab-stops to the standard intervaltabs -8or use the @RESET@ program, since the normal initializationsequences do not ensure that tab-stops are reset.",
        "name": "@TABS@ - set tabs on a terminal",
        "section": 1
    },
    {
        "command": "TPUT",
        "description": "The @TPUT@ utility uses the terminfo database to make the valuesof terminal-dependent capabilities and information available tothe shell (see sh(1)), to initialize or reset the terminal, orreturn the long name of the requested terminal type.The resultdepends upon the capability's type:string@TPUT@ writes the string to the standard output.Notrailing newline is supplied.integer@TPUT@ writes the decimal value to the standard output,with a trailing newline.boolean@TPUT@ simply sets the exit code (0 for TRUE if theterminal has the capability, 1 for FALSE if it does not),and writes nothing to the standard output.Before using a value returned on the standard output, theapplication should test the exit code (e.g., $?, see sh(1)) to besure it is 0.(See the EXIT CODES and DIAGNOSTICS sections.)For a complete list of capabilities and the capname associatedwith each, see terminfo(5).Options-Sallows more than one capability per invocation of @TPUT@.The capabilities must be passed to @TPUT@ from thestandard input instead of from the command line (seeexample).Only one capname is allowed per line.The -Soption changes the meaning of the 0 and 1 boolean andstring exit codes (see the EXIT CODES section).Because some capabilities may use string parameters ratherthan numbers, @TPUT@ uses a table and the presence ofparameters in its input to decide whether to usetparm(3X), and how to interpret the parameters.-Ttype indicates the type of terminal.Normally this option isunnecessary, because the default is taken from theenvironment variable TERM.If -T is specified, then theshell variables LINES and COLUMNS will also be ignored.-Vreports the version of ncurses which was used in thisprogram, and exits.-xdo not attempt to clear the terminal's scrollback bufferusing the extended \u201cE3\u201d capability.CommandsA few commands (init, reset and longname) are special; they aredefined by the @TPUT@ program.The others are the names ofcapabilities from the terminal database (see terminfo(5) for alist).Although init and reset resemble capability names, @TPUT@uses several capabilities to perform these special functions.capnameindicates the capability from the terminal database.If the capability is a string that takes parameters, thearguments following the capability will be used asparameters for the string.Most parameters are numbers.Only a few terminalcapabilities require string parameters; @TPUT@ uses atable to decide which to pass as strings.Normally @TPUT@uses tparm(3X) to perform the substitution.If noparameters are given for the capability, @TPUT@ writes thestring without performing the substitution.initIf the terminal database is present and an entry for theuser's terminal exists (see -Ttype, above), the followingwill occur:(1)first, @TPUT@ retrieves the current terminal modesettings for your terminal.It does this bysuccessively testing\u2022the standard error,\u2022standard output,\u2022standard input and\u2022ultimately \u201c/dev/tty\u201dto obtain terminal settings.Having retrieved thesesettings, @TPUT@ remembers which file descriptor touse when updating settings.(2)if the window size cannot be obtained from theoperating system, but the terminal description (orenvironment, e.g., LINES and COLUMNS variablesspecify this), update the operating system's notionof the window size.(3)the terminal modes will be updated:\u2022any delays (e.g., newline) specified in the entrywill be set in the tty driver,\u2022tabs expansion will be turned on or off accordingto the specification in the entry, and\u2022if tabs are not expanded, standard tabs will beset (every 8 spaces).(4)if present, the terminal's initialization stringswill be output as detailed in the terminfo(5) sectionon Tabs and Initialization,(5)output is flushed.If an entry does not contain the information needed forany of these activities, that activity will silently beskipped.resetThis is similar to init, with two differences:(1)before any other initialization, the terminal modeswill be reset to a \u201csane\u201d state:\u2022set cooked and echo modes,\u2022turn off cbreak and raw modes,\u2022turn on newline translation and\u2022reset any unset special characters to theirdefault values(2)Instead of putting out initialization strings, theterminal's reset strings will be output if present(rs1, rs2, rs3, rf).If the reset strings are notpresent, but initialization strings are, theinitialization strings will be output.Otherwise, reset acts identically to init.longnameIf the terminal database is present and an entry for theuser's terminal exists (see -Ttype above), then the longname of the terminal will be put out.The long name isthe last name in the first line of the terminal'sdescription in the terminfo database [see term(5)].Aliases@TPUT@ handles the clear, init and reset commands specially: itallows for the possibility that it is invoked by a link withthose names.If @TPUT@ is invoked by a link named reset, this has the sameeffect as @TPUT@ reset.The @TSET@(1) utility also treats a linknamed reset specially.Before ncurses 6.1, the two utilities were different from eachother:\u2022@TSET@ utility reset the terminal modes and specialcharacters (not done with @TPUT@).\u2022On the other hand, @TSET@'s repertoire of terminalcapabilities for resetting the terminal was more limited,i.e., only reset_1string, reset_2string and reset_file incontrast to the tab-stops and margins which are set by thisutility.\u2022The reset program is usually an alias for @TSET@, because ofthis difference with resetting terminal modes and specialcharacters.With the changes made for ncurses 6.1, the reset feature of thetwo programs is (mostly) the same.A few differences remain:\u2022The @TSET@ program waits one second when resetting, in caseit happens to be a hardware terminal.\u2022The two programs write the terminal initialization strings todifferent streams (i.e., the standard error for @TSET@ andthe standard output for @TPUT@).Note: although these programs write to different streams,redirecting their output to a file will capture only part oftheir actions.The changes to the terminal modes are notaffected by redirecting the output.If @TPUT@ is invoked by a link named init, this has the sameeffect as @TPUT@ init.Again, you are less likely to use thatlink because another program named init has a more well-established use.Terminal SizeBesides the special commands (e.g., clear), @TPUT@ treats certainterminfo capabilities specially: lines and cols.@TPUT@ callssetupterm(3X) to obtain the terminal size:\u2022first, it gets the size from the terminal database (whichgenerally is not provided for terminal emulators which do nothave a fixed window size)\u2022then it asks the operating system for the terminal's size(which generally works, unless connecting via a serial linewhich does not support NAWS: negotiations about window size).\u2022finally, it inspects the environment variables LINES andCOLUMNS which may override the terminal size.If the -T option is given @TPUT@ ignores the environmentvariables by calling use_tioctl(TRUE), relying upon the operatingsystem (or finally, the terminal database).",
        "name": "@TPUT@, reset - initialize a terminal or query terminfo database",
        "section": 1
    },
    {
        "command": "TSET",
        "description": "tset - initializationThis program initializes terminals.First, @TSET@ retrieves the current terminal mode settings foryour terminal.It does this by successively testing\u2022the standard error,\u2022standard output,\u2022standard input and\u2022ultimately \u201c/dev/tty\u201dto obtain terminal settings.Having retrieved these settings,@TSET@ remembers which file descriptor to use when updatingsettings.Next, @TSET@ determines the type of terminal that you are using.This determination is done as follows, using the first terminaltype found.1. The terminal argument specified on the command line.2. The value of the TERM environmental variable.3. (BSD systems only.) The terminal type associated with thestandard error output device in the /etc/ttys file.(OnSystem-V-like UNIXes and systems using that convention, getty(1)does this job by setting TERM according to the type passed to itby /etc/inittab.)4. The default terminal type, \u201cunknown\u201d.If the terminal type was not specified on the command-line, the-m option mappings are then applied (see the section TERMINALTYPE MAPPING for more information).Then, if the terminal typebegins with a question mark (\u201c?\u201d), the user is prompted forconfirmation of the terminal type.An empty response confirmsthe type, or, another type can be entered to specify a new type.Once the terminal type has been determined, the terminaldescription for the terminal is retrieved.If no terminaldescription is found for the type, the user is prompted foranother terminal type.Once the terminal description is retrieved,\u2022if the \u201c-w\u201d option is enabled, @TSET@ may update theterminal's window size.If the window size cannot be obtained from the operatingsystem, but the terminal description (or environment, e.g.,LINES and COLUMNS variables specify this), use this to setthe operating system's notion of the window size.\u2022if the \u201c-c\u201d option is enabled, the backspace, interrupt andline kill characters (among many other things) are set\u2022unless the \u201c-I\u201d option is enabled, the terminal and tabinitialization strings are sent to the standard error output,and @TSET@ waits one second (in case a hardware reset wasissued).\u2022Finally, if the erase, interrupt and line kill charactershave changed, or are not set to their default values, theirvalues are displayed to the standard error output.reset - reinitializationWhen invoked as @RESET@, @TSET@ sets the terminal modes to \u201csane\u201dvalues:\u2022sets cooked and echo modes,\u2022turns off cbreak and raw modes,\u2022turns on newline translation and\u2022resets any unset special characters to their default valuesbefore doing the terminal initialization described above.Also,rather than using the terminal initialization strings, it usesthe terminal reset strings.The @RESET@ command is useful after a program dies leaving aterminal in an abnormal state:\u2022you may have to type<LF>@RESET@<LF>(the line-feed character is normally control-J) to get theterminal to work, as carriage-return may no longer work inthe abnormal state.\u2022Also, the terminal will often not echo the command.",
        "name": "@TSET@, @RESET@ - terminal initialization",
        "section": 1
    },
    {
        "command": "Wget",
        "description": "GNU Wget is a free utility for non-interactive download of filesfrom the Web.It supports HTTP, HTTPS, and FTP protocols, aswell as retrieval through HTTP proxies.Wget is non-interactive, meaning that it can work in thebackground, while the user is not logged on.This allows you tostart a retrieval and disconnect from the system, letting Wgetfinish the work.By contrast, most of the Web browsers requireconstant user's presence, which can be a great hindrance whentransferring a lot of data.Wget can follow links in HTML, XHTML, and CSS pages, to createlocal versions of remote web sites, fully recreating thedirectory structure of the original site.This is sometimesreferred to as \"recursive downloading.\"While doing that, Wgetrespects the Robot Exclusion Standard (/robots.txt).Wget can beinstructed to convert the links in downloaded files to point atthe local files, for offline viewing.Wget has been designed for robustness over slow or unstablenetwork connections; if a download fails due to a networkproblem, it will keep retrying until the whole file has beenretrieved.If the server supports regetting, it will instructthe server to continue the download from where it left off.",
        "name": "Wget - The non-interactive network downloader.",
        "section": 1
    },
    {
        "command": "abicompat",
        "description": null,
        "name": "abicompat - check ABI compatibilityabicompat checks that an application that links against a givenshared library is still ABI compatible with a subsequent versionof that library.If the new version of the library introduces anABI incompatibility, then abicompat hints the user at whatexactly that incompatibility is.",
        "section": 1
    },
    {
        "command": "abidiff",
        "description": null,
        "name": "abidiff - compare ABIs of ELF filesabidiff compares the Application Binary Interfaces (ABI) of twoshared libraries in ELF format.It emits a meaningful reportdescribing the differences between the two ABIs.This tool can also compare the textual representations of the ABIof two ELF binaries (as emitted by abidw) or an ELF binaryagainst a textual representation of another ELF binary.For a comprehensive ABI change report between two input sharedlibraries that includes changes about function and variablesub-types, abidiff uses by default, debug information in DWARFformat, if present, otherwise it compares interfaces using debuginformation in CTF or BTF formats, if present. Finally, if nodebug info in these formats is found, it only considers ELFsymbols and report about their addition or removal.This tool uses the libabigail library to analyze the binary aswell as its associated debug information.Here is its generalmode of operation.When instructed to do so, a binary and its associated debuginformation is read and analyzed.To that effect, libabigailanalyzes by default the descriptions of the types reachable bythe interfaces (functions and variables) that are visible outsideof their translation unit.Once that analysis is done, anApplication Binary Interface Corpus is constructed by onlyconsidering the subset of types reachable from interfacesassociated to ELF symbols that are defined and exported by thebinary.It's that final ABI corpus which libabigail considers asrepresenting the ABI of the analyzed binary.Libabigail then has capabilities to generate textualrepresentations of ABI Corpora, compare them, analyze theirchanges and report about them.",
        "section": 1
    },
    {
        "command": "abidw",
        "description": null,
        "name": "abidw - serialize the ABI of an ELF fileabidw reads a shared library in ELF format and emits an XMLrepresentation of its ABI to standard output.The emittedrepresentation format, named ABIXML, includes all the globallydefined functions and variables, along with a completerepresentation of their types.It also includes a representationof the globally defined ELF symbols of the file.When given the --linux-tree option, this program can also handlea Linux kernel tree.That is, a directory tree that containsboth the vmlinux binary and Linux Kernel modules.It analysesthose Linux Kernel binaries and emits an XML representation ofthe interface between the kernel and its module, to standardoutput.In this case, we don't call it an ABI, but a KMI (KernelModule Interface).The emitted KMI includes all the globallydefined functions and variables, along with a completerepresentation of their types.To generate either ABI or KMI representation, by default abidwuses debug information in the DWARF format, if present, otherwiseit looks for debug information in CTF or BTF formats, if present.Finally, if no debug info in these formats is found, it onlyconsiders ELF symbols and report about their addition or removal.This tool uses the libabigail library to analyze the binary aswell as its associated debug information.Here is its generalmode of operation.When instructed to do so, a binary and its associated debuginformation is read and analyzed.To that effect, libabigailanalyzes by default the descriptions of the types reachable bythe interfaces (functions and variables) that are visible outsideof their translation unit.Once that analysis is done, anApplication Binary Interface Corpus is constructed by onlyconsidering the subset of types reachable from interfacesassociated to ELF symbols that are defined and exported by thebinary.It's that final ABI corpus which libabigail considers asrepresenting the ABI of the analyzed binary.Libabigail then has capabilities to generate textualrepresentations of ABI Corpora, compare them, analyze theirchanges and report about them.",
        "section": 1
    },
    {
        "command": "abilint",
        "description": null,
        "name": "abilint - validate an abigail ABI representationabilint parses the native XML representation of an ABI as emittedby abidw.Once it has parsed the XML representation of the ABI,abilint builds and in-memory model from it.It then tries tosave it back to an XML form, to standard output.If thatread-write operation succeeds chances are the input XML ABIrepresentation is meaningful.Note that the main intent of this tool to help debugging issuesin the underlying Libabigail library.Note also that abilint can also read an ELF input file, build thein-memory model for its ABI, and serialize that model back intoXML to standard output.In that case, the ELF input file must beaccompanied with its debug information in the DWARF format.",
        "section": 1
    },
    {
        "command": "abipkgdiff",
        "description": null,
        "name": "abipkgdiff - compare ABIs of ELF files in software packagesabipkgdiff compares the Application Binary Interfaces (ABI) ofthe ELF binaries contained in two software packages.Thesoftware package formats currently supported are Deb, RPM, tararchives (either compressed or not) and plain directories thatcontain binaries.For a comprehensive ABI change report that includes changes aboutfunction and variable sub-types, the two input packages must beaccompanied with their debug information packages that containdebug information either in DWARF, CTF or in BTF formats.Pleasenote however that some packages contain binaries that embed thedebug information directly in a section of said binaries.Inthose cases, obviously, no separate debug information package isneeded as the tool will find the debug information inside thebinaries.By default, abipkgdiff uses debug information in DWARF format, ifpresent, otherwise it compares binaries interfaces using debuginformation in CTF or in BTF formats, if present. Finally, if nodebug info in these formats is found, it only considers ELFsymbols and report about their addition or removal.This tool uses the libabigail library to analyze the binary aswell as its associated debug information.Here is its generalmode of operation.When instructed to do so, a binary and its associated debuginformation is read and analyzed.To that effect, libabigailanalyzes by default the descriptions of the types reachable bythe interfaces (functions and variables) that are visible outsideof their translation unit.Once that analysis is done, anApplication Binary Interface Corpus is constructed by onlyconsidering the subset of types reachable from interfacesassociated to ELF symbols that are defined and exported by thebinary.It's that final ABI corpus which libabigail considers asrepresenting the ABI of the analyzed binary.Libabigail then has capabilities to generate textualrepresentations of ABI Corpora, compare them, analyze theirchanges and report about them.",
        "section": 1
    },
    {
        "command": "ac",
        "description": "ac prints out a report of connect time (in hours) based on thelogins/logouts in the current wtmp file.A total is also printedout.The accounting file wtmp is maintained by init(8) and login(1).Neither ac nor login creates the wtmp if it doesn't exist, noaccounting is done.To begin accounting, create the file with alength of zero.NOTE:The wtmp file can get really big, really fast.You mightwant to trim it every once and a while.GNU ac works nearly the same UNIX ac, though it's a littlesmarter in several ways.You should therefore expect differencesin the output of GNU ac and the output of ac's on other systems.Use the command info accounting to get additional information.",
        "name": "ac -print statistics about users' connect time",
        "section": 1
    },
    {
        "command": "addftinfo",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "addr2line",
        "description": "addr2line translates addresses or symbol+offset into file namesand line numbers.Given an address or symbol+offset in anexecutable or an offset in a section of a relocatable object, ituses the debugging information to figure out which file name andline number are associated with it.The executable or relocatable object to use is specified with the-e option.The default is the file a.out.The section in therelocatable object to use is specified with the -j option.addr2line has two modes of operation.In the first, hexadecimal addresses or symbol+offset arespecified on the command line, and addr2line displays the filename and line number for each address.In the second, addr2line reads hexadecimal addresses orsymbol+offset from standard input, and prints the file name andline number for each address on standard output.In this mode,addr2line may be used in a pipe to convert dynamically chosenaddresses.The format of the output is FILENAME:LINENO.By default eachinput address generates one line of output.Two options can generate additional lines before eachFILENAME:LINENO line (in that order).If the -a option is used then a line with the input address isdisplayed.If the -f option is used, then a line with the FUNCTIONNAME isdisplayed.This is the name of the function containing theaddress.One option can generate additional lines after theFILENAME:LINENO line.If the -i option is used and the code at the given address ispresent there because of inlining by the compiler then additionallines are displayed afterwards.One or two extra lines (if the-f option is used) are displayed for each inlined function.Alternatively if the -p option is used then each input addressgenerates a single, long, output line containing the address, thefunction name, the file name and the line number.If the -ioption has also been used then any inlined functions will bedisplayed in the same manner, but on separate lines, and prefixedby the text (inlined by).If the file name or function name can not be determined,addr2line will print two question marks in their place.If theline number can not be determined, addr2line will print 0.When symbol+offset is used, +offset is optional, except when thesymbol is ambigious with a hex number. The resolved symbols canbe mangled or unmangled, except unmangled symbols with + are notallowed.",
        "name": "addr2line - convert addresses or symbol+offset into file namesand line numbers",
        "section": 1
    },
    {
        "command": "afmtodit",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "apropos",
        "description": "Each manual page has a short description available within it.apropos searches the descriptions for instances of keyword.keyword is usually a regular expression, as if (-r) was used, ormay contain wildcards (-w), or match the exact keyword (-e).Using these options, it may be necessary to quote the keyword orescape (\\) the special characters to stop the shell frominterpreting them.The standard matching rules allow matches to be made against thepage name and word boundaries in the description.The database searched by apropos is updated by the mandb program.Depending on your installation, this may be run by a periodiccron job, or may need to be run manually after new manual pageshave been installed.",
        "name": "apropos - search the manual page names and descriptions",
        "section": 1
    },
    {
        "command": "ar",
        "description": "The GNU ar program creates, modifies, and extracts from archives.An archive is a single file holding a collection of other filesin a structure that makes it possible to retrieve the originalindividual files (called members of the archive).The original files' contents, mode (permissions), timestamp,owner, and group are preserved in the archive, and can berestored on extraction.GNU ar can maintain archives whose members have names of anylength; however, depending on how ar is configured on yoursystem, a limit on member-name length may be imposed forcompatibility with archive formats maintained with other tools.If it exists, the limit is often 15 characters (typical offormats related to a.out) or 16 characters (typical of formatsrelated to coff).ar is considered a binary utility because archives of this sortare most often used as libraries holding commonly neededsubroutines.Since libraries often will depend on otherlibraries, ar can also record the dependencies of a library whenthe --record-libdeps option is specified.ar creates an index to the symbols defined in relocatable objectmodules in the archive when you specify the modifier s.Oncecreated, this index is updated in the archive whenever ar makes achange to its contents (save for the q update operation).Anarchive with such an index speeds up linking to the library, andallows routines in the library to call each other without regardto their placement in the archive.You may use nm -s or nm --print-armap to list this index table.If an archive lacks the table, another form of ar called ranlibcan be used to add just the table.GNU ar can optionally create a thin archive, which contains asymbol index and references to the original copies of the memberfiles of the archive.This is useful for building libraries foruse within a local build tree, where the relocatable objects areexpected to remain available, and copying the contents of eachobject would only waste time and space.An archive can either be thin or it can be normal.It cannot beboth at the same time.Once an archive is created its formatcannot be changed without first deleting it and then creating anew archive in its place.Thin archives are also flattened, so that adding one thin archiveto another thin archive does not nest it, as would happen with anormal archive.Instead the elements of the first archive areadded individually to the second archive.The paths to the elements of the archive are stored relative tothe archive itself.GNU ar is designed to be compatible with two differentfacilities.You can control its activity using command-lineoptions, like the different varieties of ar on Unix systems; or,if you specify the single command-line option -M, you can controlit with a script supplied via standard input, like the MRI\"librarian\" program.",
        "name": "ar - create, modify, and extract from archives",
        "section": 1
    },
    {
        "command": "arch",
        "description": "Print machine architecture.--help display this help and exit--versionoutput version information and exit",
        "name": "arch - print machine hardware name (same as uname -m)",
        "section": 1
    },
    {
        "command": "aria_chk",
        "description": "Describe, check and repair of Aria tables.Used without optionsall tables on the command will be checked for errorsGlobal options-#, --debug=...Output debug log. Often this is 'd:t:o,filename'.-H, --HELPPrint all argument options sorted alphabetically.-?, --helpPrint all options by groups--datadir=pathPath for control file (and logs if --logdir not used)--logdir=pathPath for log files--ignore-control-fileDon't open the control file. Only use this if you are surethe tables are not in use by another program!--require-control-fileAbort if we can't find/read the maria_log_control file-s, --silentOnly print errors.One can use two -s to make maria_chkvery silent.-t, --tmpdir=pathPath for temporary files. Multiple paths can be specified,separated by colon (:), they will be used in a round-robinfashion.-v, --verbosePrint more information. This can be used with--description and --check. Use many -v for more verbosity.-V, --versionPrint version and exit.-w, --waitWait if table is locked.Check options (check is the default action for aria_chk)-c, --checkCheck table for errors.-e, --extend-checkCheck the table VERY thoroughly.Only use this in extremecases as aria_chk should normally be able to find out ifthe table is ok even without this switch.-F, --fastCheck only tables that haven't been closed properly.-C, --check-only-changedCheck only tables that have changed since last check.-f, --forceRestart with '-r' if there are any errors in the table.States will be updated as with '--update-state'.-i, --informationPrint statistics information about table that is checked.-m, --medium-checkFaster than extend-check, but only finds 99.99% of allerrors.Should be good enough for most cases.-T, --read-onlyDon't mark table as checked.-U, --update-stateMark tables as crashed if any errors were found and cleanif check didn't find any errors but table was marked as'not clean' before. This allows one to get rid of warningslike 'table not properly closed'. If table was updated,update also the timestamp for when the check was made.This option is on by default!Use --skip-update-state todisable.--warning-for-wrong-transaction-idGive a warning if we find a transaction id in the tablethat is bigger than what exists in the control file. Use--skip-... to disable warningRecover (repair)/ options (When using '--recover' or '--safe-recover')-B, --backupMake a backup of the .MAD file as 'filename-time.BAK'.--correct-checksumCorrect checksum information for table.-D, --data-file-length=#Max length of data file (when recreating data file whenit's full).-e, --extend-checkTry to recover every possible row from the data fileNormally this will also find a lot of garbage rows; Don'tuse this option if you are not totally desperate.-f, --forceOverwrite old temporary files.-k, --keys-used=#Tell Aria to update only some specific keys. # is a bitmask of which keys to use. This can be used to get fasterinserts.--max-record-length=#Skip rows bigger than this if aria_chk can't allocatememory to hold it.-r, --recoverCan fix almost anything except unique keys that aren'tunique.-n, --sort-recoverForces recovering with sorting even if the temporary filewould be very big.-p, --parallel-recoverUses the same technique as '-r' and '-n', but creates allthe keys in parallel, in different threads.-o, --safe-recoverUses old recovery method; Slower than '-r' but can handlea couple of cases where '-r' reports that it can't fix thedata file.--transaction-logLog repair command to transaction log. This is needed ifone wants to use the aria_read_log to repeat the repair--character-sets-dir=...Directory where character sets are.--set-collation=nameChange the collation used by the index.-q, --quickFaster repair by not modifying the data file.One cangive a second '-q' to force aria_chk to modify theoriginal datafile in case of duplicate keys.NOTE: Tableswhere the data file is corrupted can't be fixed with thisoption.-u, --unpackUnpack file packed with ariapack.Other actions-a, --analyzeAnalyze distribution of keys. Will make some joins inMariaDB faster.You can check the calculated distributionby using '--description --verbose table_name'.--stats_method=nameSpecifies how index statistics collection code shouldtreat NULLs. Possible values of name are \"nulls_unequal\"(default for 4.1/5.0), \"nulls_equal\" (emulate 4.0), and\"nulls_ignored\".-d, --descriptionPrints some information about table.-A, --set-auto-increment[=value]Force auto_increment to start at this or higher value Ifno value is given, then sets the next auto_increment valueto the highest used value for the auto key + 1.-S, --sort-indexSort index blocks.This speeds up 'read-next' inapplications.-R, --sort-records=#Sort records according to an index.This makes your datamuch more localized and may speed up things (It may beVERY slow to do a sort the first time!).-b,--block-search=#Find a record, a block at given offset belongs to.-z,--zerofillFill empty space in data and index files with zeroes.This makes the data file movable between differentservers.--zerofill-keep-lsnLike --zerofill but does not zero out LSN of data/indexpages.Variables--page_buffer_size=#Size of page buffer. Used by --safe-repair--read_buffer_size=#Read buffer size for sequential reads during scanning--sort_buffer_size=#Size of sort buffer. Used by --recover--sort_key_blocks=#Internal buffer for sorting keys; Don't touch.--write_buffer_size=#Write buffer size for sequential writes during repairDefault options are read from the following files in the givenorder: /etc/my.cnf /etc/mysql/my.cnf ~/.my.cnfThe following groups are read: aria_chkThe following options may be given as the first argument:--print-defaultsPrint the program argument list and exit.--no-defaultsDon't read default options from any option file.--defaults-file=#Only read default options from the given file #.--defaults-extra-file=#Read this file after the global files are read.",
        "name": "aria_chk - Aria table-maintenance utility",
        "section": 1
    },
    {
        "command": "aria_dump_log",
        "description": "Dump content of Aria log pages.-#, --debug[=name]Output debug log. Often the argument is 'd:t:o,filename'.-f, --file=namePath to file which will be read-?, --helpDisplay this help and exit.-o, --offset=#Start reading log from this offset-n, --pages=#Number of pages to read-U, --unit-testUse unit test record table (for logs created by unittests-V, --versionPrint version and exit.Default options are read from the following files in the givenorder: /etc/my.cnf /etc/mysql/my.cnf ~/.my.cnfThe following groups are read: aria_dump_logThe following options may be given as the first argument:--print-defaultsPrint the program argument list and exit.--no-defaultsDon't read default options from any option file.--defaults-file=#Only read default options from the given file #.--defaults-extra-file=#Read this file after the global files are read.",
        "name": "aria_dump_log - Dump content of Aria log pages.",
        "section": 1
    },
    {
        "command": "aria_ftdump",
        "description": "Use: aria_ft_dump <table_name> <index_num>-?, -h, --helpDisplay help and exit.-c, --countCalculate per-word stats (counts and global weights).-d, --dumpDump index (incl. data offsets and word weights).-l, --lengthReport length distribution.-s, --statsReport global stats.-v, --verboseBe verbose.",
        "name": "aria_ftdump - display full-text index information",
        "section": 1
    },
    {
        "command": "aria_pack",
        "description": "Pack a Aria-table to take much less space.Keys are not updated,you must run aria_chk -rq on the index (.MAI) file afterwards toupdate the keys.You should give the .MAI file as the filenameargument.To unpack a packed table, run aria_chk -u on the table-b, --backupMake a backup of the table as table_name.OLD.--character-sets-dir=nameDirectory where character sets are.-#, --debug[=name]Output debug log. Often this is 'd:t:o,filename'.-f, --forceForce packing of table even if it gets bigger or iftempfile exists.-j, --join=nameJoin all given tables into 'new_table_name'. All tablesMUST have identical layouts.-?, --helpDisplay this help and exit.-s, --silentBe more silent.-T, --tmpdir=nameUse temporary directory to store temporary table.-t, --testDon't pack table, only test packing it.-v, --verboseWrite info about progress and packing result. Use many -vfor more verbosity!-V, --versionOutput version information and exit.-w, --waitWait and retry if table is in use.Default options are read from the following files in the givenorder: /etc/my.cnf /etc/mysql/my.cnf ~/.my.cnfThe following groups are read: ariapackThe following options may be given as the first argument:--print-defaultsPrint the program argument list and exit.--no-defaultsDon't read default options from any option file.--defaults-file=#Only read default options from the given file #.--defaults-extra-file=#Read this file after the global files are read.",
        "name": "aria_pack - generate compressed, read-only Aria tables",
        "section": 1
    },
    {
        "command": "aria_read_log",
        "description": "Display and apply log records from a Aria transaction log foundin the current directory (for now)Note: Aria is compiled without -DIDENTICAL_PAGES_AFTER_RECOVERYwhich means that the table files are not byte-to-byte identicalto files created during normal execution. This should be ok,except for test scripts that tries to compare files before andafter recovery.You need to use one of -d or -a-a, --applyApply log to tables: modifies tables! you should make abackup first!Displays a lot of information if not runwith --silent--character-sets-dir=nameDirectory where character sets are.-c, --checkif --display-only, check if record is fully readable (fordebugging)-#, --debug[=name]Output debug log. Often the argument is 'd:t:o,filename'.--force-crash=#Force crash after # recovery events-?, --helpDisplay this help and exit.-d, --display-onlydisplay brief info read from records' header-e, --end-lsn=#Stop applying at this lsn. If end-lsn is used, UNDO:s willnot be applied-h, --aria-log-dir-path=namePath to the directory where to store transactional log-P, --page-buffer-size=#The size of the buffer used for index blocks for Ariatables-o, --start-from-lsn=#Start reading log from this lsn-C, --start-from-checkpointStart applying from last checkpoint-s, --silentPrint less information during apply/undo phase-T, --tables-to-redo=nameList of tables separated with , that we should apply REDOon. Use this if you only want to recover some tables-t, --tmpdir=namePath for temporary files. Multiple paths can be specified,separated by colon (:)--translog-buffer-size=#The size of the buffer used for transaction log for Ariatables-u, --undoApply UNDO records to tables. (disable with--disable-undo) (Defaults to on; use --skip-undo todisable.)-v, --verbosePrint more information during apply/undo phase-V, --versionPrint version and exit.Default options are read from the following files in the givenorder: /etc/my.cnf /etc/mysql/my.cnf ~/.my.cnfThe following groups are read: aria_read_logThe following options may be given as the first argument:--print-defaultsPrint the program argument list and exit.--no-defaultsDon't read default options from any option file.--defaults-file=#Only read default options from the given file #.--defaults-extra-file=#Read this file after the global files are read.",
        "name": "aria_read_log - display Aria log file contents",
        "section": 1
    },
    {
        "command": "aria_s3_copy",
        "description": "Usage: aria_s3_copy --aws-access-key=# --aws-secret-access-key=#--aws-region=# --op=(from_s3 | to_s3 | delete_from_s3) [OPTIONS]tables[.MAI]-?, -h, --helpDisplay help and exit.-k, --s3-access-key=nameAWS access key ID-r, -s3-region=nameAWS region-K, -s3-secret-key=nameAWS secret access key ID-b, -s3-bucket=nameAWS prefix for tables-h, -s3-host-name=nameHost name to S3 provider-c, -compressUse compression-o, -op=nameOperation to execute. One of 'from_s3', 'to_s3' or'delete_from_s3'-d, -database=nameDatabase for copied table (second prefix). If not given,the directory of the table file is used-B, -s3-block-size=#Block size for data/index blocks in s3-L, -s3-protocol-version=nameProtocol used to communication with S3. One of \"Auto\",\"Amazon\" or \"Original\".-f, -forceForce copy even if target exists-V, -versionPrint version and exit.--s3-debugOutput debug log from marias3 to stdout-v, --verboseBe verbose.For more information, please refer to the MariaDB Knowledge Basepage https://mariadb.com/kb/en/aria_s3_copy/",
        "name": "aria_s3_copy - Copy an Aria table to and from s3",
        "section": 1
    },
    {
        "command": "as",
        "description": "GNU as is really a family of assemblers.If you use (or haveused) the GNU assembler on one architecture, you should find afairly similar environment when you use it on anotherarchitecture.Each version has much in common with the others,including object file formats, most assembler directives (oftencalled pseudo-ops) and assembler syntax.as is primarily intended to assemble the output of the GNU Ccompiler \"gcc\" for use by the linker \"ld\".Nevertheless, we'vetried to make as assemble correctly everything that otherassemblers for the same machine would assemble.Any exceptionsare documented explicitly.This doesn't mean as always uses thesame syntax as another assembler for the same architecture; forexample, we know of several incompatible versions of 680x0assembly language syntax.Each time you run as it assembles exactly one source program.The source program is made up of one or more files.(Thestandard input is also a file.)You give as a command line that has zero or more input filenames.The input files are read (from left file name to right).A command-line argument (in any position) that has no specialmeaning is taken to be an input file name.If you give as no file names it attempts to read one input filefrom the as standard input, which is normally your terminal.Youmay have to type ctl-D to tell as there is no more program toassemble.Use -- if you need to explicitly name the standard input file inyour command line.If the source is empty, as produces a small, empty object file.as may write warnings and error messages to the standard errorfile (usually your terminal).This should not happen whenacompiler runs as automatically.Warnings report an assumptionmade so that as could keep assembling a flawed program; errorsreport a grave problem that stops the assembly.If you are invoking as via the GNU C compiler, you can use the-Wa option to pass arguments through to the assembler.Theassembler arguments must be separated from each other (and the-Wa) by commas.For example:gcc -c -g -O -Wa,-alh,-L file.cThis passes two options to the assembler: -alh (emit a listing tostandard output with high-level and assembly source) and -L(retain local symbols in the symbol table).Usually you do not need to use this -Wa mechanism, since manycompiler command-line options are automatically passed to theassembler by the compiler.(You can call the GNU compiler driverwith the -v option to see precisely what options it passes toeach compilation pass, including the assembler.)",
        "name": "AS - the portable GNU assembler.",
        "section": 1
    },
    {
        "command": "ascii-xfr",
        "description": "Ascii-xfr Transfers files in ASCII mode. This means no flowcontrol, no checksumming and no file-name negotiation. It shouldonly be used if the remote system doesn't understand anythingelse.The ASCII protocol transfers files line-by-line. The EOL (End-Of-Line) character is transmitted as CRLF. When receiving, the CRcharacter is stripped from the incoming file.The Control-Z(ASCII 26) character signals End-Of-File, if option -e isspecified (unless you change it to Control-D (ASCII 4) with -d).Ascii-xfr reads from stdin when receiving, and sends data onstdout when sending. Some form of input or output redirection tothe modem device is thus needed when downloading or uploading,respectively.",
        "name": "ascii-xfr - upload/download files using the ASCII protocol",
        "section": 1
    },
    {
        "command": "attr",
        "description": "The attr utility allows the manipulation of extended attributesassociated with filesystem objects from within shell scripts.There are four main operations that attr can perform:GETThe -g attrname option tells attr to search the namedobject and print (to stdout) the value associated withthat attribute name.With the -q flag, stdout will beexactly and only the value of the attribute, suitable forstorage directly into a file or processing via a pipedcommand.LISTThe -l option tells attr to list the names of all theattributes that are associated with the object, and thenumber of bytes in the value of each of those attributes.With the -q flag, stdout will be a simple list of only theattribute names, one per line, suitable for input into ascript.REMOVE The -r attrname option tells attr to remove an attributewith the given name from the object if the attributeexists.There is no output on successful completion.SET/CREATEThe -s attrname option tells attr to set the namedattribute of the object to the value read from stdin.Ifan attribute with that name already exists, its value willbe replaced with this one.If an attribute with that namedoes not already exist, one will be created with thisvalue.With the -V attrvalue flag, the attribute will beset to have a value of attrvalue and stdin will not beread.With the -q flag, stdout will not be used.Withoutthe -q flag, a message showing the attribute name and theentire value will be printed.When the -L option is given and the named object is a symboliclink, operate on the attributes of the object referenced by thesymbolic link.Without this option, operate on the attributes ofthe symbolic link itself.When the -R option is given and the process has appropriateprivileges, operate in the root attribute namespace rather thatthe USER attribute namespace.The -S option is similar, except it specifies use of the securityattribute namespace.When the -q option is given attr will try to keep quiet.It willoutput error messages (to stderr) but will not print statusmessages (to stdout).",
        "name": "attr - extended attributes on XFS filesystem objects",
        "section": 1
    },
    {
        "command": "audit2allow",
        "description": "This utility scans the logs for messages logged when the systemdenied permission for operations, and generates a snippet ofpolicy rules which, if loaded into policy, might have allowedthose operations to succeed. However, this utility only generatesType Enforcement (TE) allow rules.Certain permission denialsmay require other kinds of policy changes, e.g. adding anattribute to a type declaration to satisfy an existingconstraint, adding a role allow rule, or modifying a constraint.The audit2why(8) utility may be used to diagnose the reason whenit is unclear.Care must be exercised while acting on the output of this utilityto ensure that the operations being permitted do not pose asecurity threat. Often it is better to define new domains and/ortypes, or make other structural changes to narrowly allow anoptimal set of operations to succeed, as opposed to blindlyimplementing the sometimes broad changes recommended by thisutility.Certain permission denials are not fatal to theapplication, in which case it may be preferable to simplysuppress logging of the denial via a 'dontaudit' rule rather thanan 'allow' rule.",
        "name": "audit2allow - generate SELinux policy allow/dontaudit rules fromlogs of denied operationsaudit2why - translates SELinux audit messages into a descriptionof why the access was denied (audit2allow -w)",
        "section": 1
    },
    {
        "command": "audit2why",
        "description": "This utility scans the logs for messages logged when the systemdenied permission for operations, and generates a snippet ofpolicy rules which, if loaded into policy, might have allowedthose operations to succeed. However, this utility only generatesType Enforcement (TE) allow rules.Certain permission denialsmay require other kinds of policy changes, e.g. adding anattribute to a type declaration to satisfy an existingconstraint, adding a role allow rule, or modifying a constraint.The audit2why(8) utility may be used to diagnose the reason whenit is unclear.Care must be exercised while acting on the output of this utilityto ensure that the operations being permitted do not pose asecurity threat. Often it is better to define new domains and/ortypes, or make other structural changes to narrowly allow anoptimal set of operations to succeed, as opposed to blindlyimplementing the sometimes broad changes recommended by thisutility.Certain permission denials are not fatal to theapplication, in which case it may be preferable to simplysuppress logging of the denial via a 'dontaudit' rule rather thanan 'allow' rule.",
        "name": "audit2allow - generate SELinux policy allow/dontaudit rules fromlogs of denied operationsaudit2why - translates SELinux audit messages into a descriptionof why the access was denied (audit2allow -w)",
        "section": 1
    },
    {
        "command": "autofsd-probe",
        "description": "autofsd-probe will check the status of the autofsd(1) daemon onthe specified host.Unless directed to another host by the -h option, autofsd-probewill contact the AutoFS daemon on the local host.The AutoFS file system is built on the Remote Procedure Call(RPC(3)) library routines.The -t option allows the totaltimeout and retry timeout intervals to be set for all remoteprocedure call operations used with autofsd-probe.This optionaccepts an interval argument in the form described in thePCPIntro(1) manual page.autofsd-probe is typically used in an automated fashion fromwithin pmdashping(1) and in conjunction with pmie(1), formonitoring response time and service failure.By default autofsd-probe will not produce any output, unlessthere is an error in which case a diagnostic message will bedisplayed and the exit status will indicate the reason forfailure.",
        "name": "autofsd-probe - probe AutoFS mount/unmount daemon",
        "section": 1
    },
    {
        "command": "autopoint",
        "description": "Copies standard gettext infrastructure files into a sourcepackage.",
        "name": "autopoint - copies standard gettext infrastructure",
        "section": 1
    },
    {
        "command": "b2sum",
        "description": "Print or check BLAKE2b (512-bit) checksums.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-b, --binaryread in binary mode-c, --checkread checksums from the FILEs and check them-l, --length=BITSdigest length in bits; must not exceed the max for theblake2 algorithm and must be a multiple of 8--tagcreate a BSD-style checksum-t, --textread in text mode (default)-z, --zeroend each output line with NUL, not newline, and disablefile name escapingThe following five options are useful only when verifying checksums:--ignore-missingdon't fail or report status for missing files--quietdon't print OK for each successfully verified file--statusdon't output anything, status code shows success--strictexit non-zero for improperly formatted checksum lines-w, --warnwarn about improperly formatted checksum lines--help display this help and exit--versionoutput version information and exitThe sums are computed as described in RFC 7693.When checking,the input should be a former output of this program.The defaultmode is to print a line with: checksum, a space, a characterindicating input mode ('*' for binary, ' ' for text or wherebinary is insignificant), and name for each FILE.Note: There is no difference between binary mode and text mode onGNU systems.",
        "name": "b2sum - compute and check BLAKE2 message digest",
        "section": 1
    },
    {
        "command": "babeltrace2",
        "description": "babeltrace2 is an open-source trace converter and processorcommand-line program. The tool can open one or more traces andconvert between multiple formats, possibly with one or morefilters in the conversion path, and perform other operationsdepending on the command CMD (see \u201cCOMMANDS\u201d).NoteYou might be looking for the babeltrace2-convert(1) command\u2019smanual page; the convert command is the default command ofbabeltrace2 and is backward compatible with babeltrace(1).See \u201cEXAMPLES\u201d for convert command examples.See babeltrace2-intro(7) to learn more about the Babeltrace 2project and its core concepts.Most of the babeltrace2 commands load Babeltrace 2 plugins toperform their operation. The search path for Babeltrace 2 pluginsis, in this order:1. The colon-separated (or semicolon, on Windows) list ofdirectories in the BABELTRACE_PLUGIN_PATH environmentvariable.2. The colon-separated (or semicolon, on Windows) list ofdirectories in the --plugin-path option.3. $HOME/.local/lib/babeltrace2/plugins4. /usr/local/lib/babeltrace2/pluginsYou can use the babeltrace2-list-plugins(1) command todynamically list the available plugins and what they offer. See\u201cPROJECT\u2019S PLUGINS\u201d for a list of plugins shipped withBabeltrace 2.",
        "name": "babeltrace2 - Convert or process one or more traces, and more",
        "section": 1
    },
    {
        "command": "babeltrace2-convert",
        "description": "The convert command converts one or more traces to a givenformat, possibly with filters in the conversion path.See babeltrace2-intro(7) to learn more about the Babeltrace 2project and its core concepts.Noteconvert is the default babeltrace2(1) command: you generallydon\u2019t need to specify its name. The following commands areequivalent if the ... part does not start with anotherbabeltrace2(1) command\u2019s name, like run or list-plugins:$ babeltrace2 convert ...$ babeltrace2 ...If you need to make sure that you are executing the convertcommand, use babeltrace2 convert explicitly.More specifically, the convert command creates a conversiongraph.A conversion graph is a specialized trace processing graphfocused on the conversion of one or more traces to anotherformat, possibly filtering or modifying their events and othermessages in the process. A conversion graph is a linear chain ofcomponents once the source streams are merged:+----------+| source 1 @-.+----------+ ||+-------++----------+ '->@|+---------++------------+| source 2 @--->@ muxer @--->@ trimmer @--->@ debug-info @-.+----------+ .->@|+---------++------------+ ||+-------+|+----------+ |.----------------------------------------'|...@-'|+---------------++------++----------+'->@ other filters |--->@ sink |+---------------++------+Note that the trimmer, debugging information, and other filtersare optional. See \u201cCreate implicit components from options\u201d tolearn how to enable them.If you need another trace processing graph layout, use the moreflexible babeltrace2-run(1) command.Like with the babeltrace2-run(1) command, you can createcomponents explicitly with the --component option (see \u201cCreateexplicit components\u201d). You can also use one of the many specificconvert command options (see \u201cCreate implicit components fromoptions\u201d) and non-option arguments (see \u201cCreate implicitcomponents from non-option arguments\u201d) to create implicitcomponents.An implicit component is a component which is created and addedto the conversion graph without an explicit instantiation throughthe --component option. An implicit component is easier to createthan an explicit component: this is why the convert commandexists, as you can also create and run a conversion graph withthe generic babeltrace2-run(1) command.For example, you can specify one or more CTF trace path asnon-option arguments to pretty-print the merged events to thestandard output:$ babeltrace2 /path/to/trace /path/to/other/traceThis is the equivalent of creating and connecting together:\u2022One source.ctf.fs components with its inputs initializationparameter set to /path/to/trace.\u2022One source.ctf.fs components with its inputs initializationparameter set to /path/to/other/trace.\u2022A filter.utils.muxer component.\u2022A sink.text.pretty component.This creates the following conversion graph:+------------++-----------------++------------------+| src.ctf.fs || flt.utils.muxer || sink.text.pretty ||[ctf-fs]||[muxer]||[pretty]||||||||stream0 @--->@ in0out @--->@ in||stream1 @--->@ in1|+------------------+|stream2 @--->@ in2||stream3 @--->@ in3|+------------+||||+------------+||| src.ctf.fs |||| [ctf-fs-2] ||||||||stream0 @--->@ in4||stream1 @--->@ in5|+------------+@ in6|+-----------------+It is equivalent to the following babeltrace2-run(1) commandline:$ babeltrace2 run --component=ctf-fs:src.ctf.fs \\--params='inputs=[\"/path/to/trace\"] \\--component=ctf-fs-2:src.ctf.fs \\--params='inputs=[\"/path/to/other/trace\"] \\--component=muxer:filter.utils.muxer \\--component=pretty:sink.text.pretty \\--connect=ctf*:muxer --connect=muxer:prettyYou can use the --run-args option to make the convert commandprint its equivalent run command arguments instead of creatingand running the conversion graph. The printed arguments areescaped for shells, which means you can use them as is on thecommand line and possibly add more options to the run command:$ babeltrace2 run $(babeltrace2 --run-args /path/to/trace) ...The --run-args-0 option is like the --run-args option, but theprinted arguments are NOT escaped and they are separated by anull character instead of a space. This is useful if theresulting arguments are not the direct input of a shell, forexample if passed to xargs -0.See \u201cEXAMPLES\u201d for usage examples.Create explicit componentsTo explicitly create a component, use the --component option.This option specifies:\u2022Optional: The name of the component.\u2022The type of the component class to instantiate: source,filter, or sink.\u2022The name of the plugin in which to find the component classto instantiate.\u2022The name of the component class to instantiate.You can use the --component option multiple times to createmultiple components. You can instantiate the same component classmultiple times as different component instances.Immediately following a --component option on the command line,the created component is known as the current component (untilthe next --component option or non-option argument).The following command-line options apply to the currentcomponent:--log-level=LVLSet the log level of the current component to LVL.--params=PARAMSAdd PARAMS to the initialization parameters of the currentcomponent.If PARAMS contains a key which exists in the currentcomponent\u2019s initialization parameters, replace the parameter.See \u201cEXAMPLES\u201d for usage examples.Create implicit components from non-option argumentsWhen you specify a non-option argument to the convert command, ittries to find one or more components which can handle thisargument.For example, with this command line:$ babeltrace2 /path/to/traceIf /path/to/trace is a CTF trace directory, then the convertcommand creates a source.ctf.fs component to handle this specifictrace.This automatic source component discovery mechanism is possiblethanks to component classes which support the babeltrace.support-info query object (seebabeltrace2-query-babeltrace.support-info(7)).The non-option argument can be a directory. If no component canhandle that specific directory, then the convert commandtraverses that directory and recursively tries to find compatiblecomponents for each file and subdirectory. This means that asingle non-option argument can lead to the creation of manyimplicit components.The following command-line options apply to ALL the implicitcomponents created from the last non-option argument:--log-level=LVLSet the log level of those implicit components to LVL.--params=PARAMSAdd PARAMS to the initialization parameters of those implicitcomponents.For a given implicit component, if PARAMS contains a keywhich exists in this component\u2019s initialization parameters,replace the parameter.Note that it\u2019s also possible for two non-option arguments tocause the creation of a single implicit component. For example,if you specify:$ babeltrace2 /path/to/chunk1 /path/to/chunk2where /path/to/chunk1 and /path/to/chunk2 are paths to chunks ofthe same logical CTF trace, then the convert command creates asingle source.ctf.fs component which receives both paths atinitialization time. When this happens, any --log-level or--params option that you specify to one of them applies to thesingle implicit component. For example:$ babeltrace2 /path/to/chunk1 --params=clock-class-offset-s=450 \\/path/to/chunk2 --params=clock-class-offset-ns=98 \\--log-level=INFOHere, the single implicit component gets both clock-class-offset-s and clock-class-offset-ns initialization parameters, as well asthe INFO log level.For backward compatibility with the babeltrace(1) program, theconvert command ignores any non-option argument which does notcause the creation of any component. In that case, it emits awarning log statement and continues.Create implicit components from optionsThere are many ways to create implicit components from optionswith the convert command:\u2022To create an implicit filter.utils.trimmer component (streamtrimmer), specify the --begin, --end, or --timerange option.Examples:$ babeltrace2 /path/to/trace --begin=22:14:38 --end=22:15:07$ babeltrace2 /path/to/trace --timerange=22:14:38,22:15:07$ babeltrace2 /path/to/trace --end=12:31:04.882928015\u2022To create an implicit filter.lttng-utils.debug-info (adddebugging information to compatible LTTng events), specifyany of the --debug-info, --debug-info-dir, --debug-info-full-path, or --debug-info-target-prefix options.Examples:$ babeltrace2 /path/to/trace --debug-info$ babeltrace2 /path/to/trace \\--debug-info-target-prefix=/tmp/tgt-root$ babeltrace2 /path/to/trace --debug-info-full-path\u2022To create an implicit sink.text.pretty component(pretty-printing text output to the standard output or to afile), specify no other sink components, explicit orimplicit.The implicit sink.text.pretty component exists by default. Ifany other explicit or implicit sink component exists, theconvert command does not automatically create the implicitsink.text.pretty component.The --clock-cycles, --clock-date, --clock-gmt, --clock-seconds, --color, --fields, --names, and --no-delta optionsall apply to the implicit sink.text.pretty component.The --output option without --output-format=ctf makes theimplicit sink.text.pretty component write its content to afile, except the warnings for backward compatibility with thebabeltrace(1) program.Examples:$ babeltrace2 /path/to/trace$ babeltrace2 /path/to/trace --no-delta$ babeltrace2 /path/to/trace --output=/tmp/pretty-out\u2022To create an implicit sink.utils.dummy component (no output),specify the --output-format=dummy option.Example:$ babeltrace2 /path/to/trace --output-format=dummy\u2022To create an implicit sink.ctf.fs component (CTF traceswritten to the file system), specify the --output-format=ctfand the --output=DIR (base output directory) options.Example:$ babeltrace2 /path/to/input/trace --output-format=ctf \\--output=my-tracesYou can combine multiple methods to create multiple implicitcomponents. For example, you can trim an LTTng (CTF) trace, adddebugging information to it, and write it as another CTF trace:$ babeltrace2 /path/to/input/trace --timerange=22:14:38,22:15:07 \\--debug-info --output-format=ctf --output=out-dirThe equivalent babeltrace2-run(1) command of this convert commandis:$ babeltrace2 run --component=auto-disc-source-ctf-fs:source.ctf.fs \\--params='inputs=[\"/path/to/input/trace\"]' \\--component=sink-ctf-fs:sink.ctf.fs \\--params='path=\"out-dir\"' \\--component=muxer:filter.utils.muxer \\--component=trimmer:filter.utils.trimmer \\--params='begin=\"22:14:38\"' \\--params='end=\"22:15:07\"' \\--component=debug-info:filter.lttng-utils.debug-info \\--connect=auto-disc-source-ctf-fs:muxer \\--connect=muxer:trimmer \\--connect=trimmer:debug-info \\--connect=debug-info:sink-ctf-fsThe order of the implicit component options documented in thissubsection is not significant.See \u201cEXAMPLES\u201d for more examples.",
        "name": "babeltrace2-convert - Convert one or more traces to a givenformat",
        "section": 1
    },
    {
        "command": "babeltrace2-help",
        "description": "The help command prints the details and help text of either theBabeltrace 2 plugin named PLUGIN-NAME or the specific componentclass of type COMP-CLS-TYPE named COMP-CLS-NAME found in theplugin named PLUGIN-NAME.See babeltrace2-intro(7) to learn more about the Babeltrace 2project and its core concepts.The available values for COMP-CLS-TYPE are:source, srcSource component class.filter, fltFilter component class.sinkSink component class.See \u201cEXAMPLES\u201d for usage examples.",
        "name": "babeltrace2-help - Get help for a Babeltrace 2 plugin orcomponent class",
        "section": 1
    },
    {
        "command": "babeltrace2-list-plugins",
        "description": "The list-plugins command prints a list of available Babeltrace 2plugins along with their component classes and their properties.See babeltrace2-intro(7) to learn more about the Babeltrace 2project and its core concepts.",
        "name": "babeltrace2-list-plugins - List Babeltrace 2 plugins and theirproperties",
        "section": 1
    },
    {
        "command": "babeltrace2-query",
        "description": "The query command queries the object named OBJECT from thecomponent class named COMP-CLS-NAME of the type COMP-CLS-TYPEfound in the Babeltrace 2 plugin named PLUGIN-NAME and prints theresults.See babeltrace2-intro(7) to learn more about the Babeltrace 2project and its core concepts.The available values for COMP-CLS-TYPE are:source, srcSource component class.filter, fltFilter component class.sinkSink component class.The exact object names and the parameters that a given componentclass expects are described in its own documentation.babeltrace2-help(1) can generally provide this information.You can use the --params option to pass parameters to thecomponent class\u2019s query operation.The output of the query command can look like YAML (see<https://yaml.org/>), but it\u2019s not guaranteed to beYAML-compliant.See \u201cEXAMPLES\u201d for usage examples.",
        "name": "babeltrace2-query - Query an object from a Babeltrace 2 componentclass",
        "section": 1
    },
    {
        "command": "babeltrace2-run",
        "description": "The run command creates a Babeltrace 2 trace processing graph andruns it.See babeltrace2-intro(7) to learn more about the Babeltrace 2project and its core concepts.The run command dynamically loads Babeltrace 2 plugins whichsupply component classes. With the run command, you specify whichcomponent classes to instantiate as components and how to connectthem.The steps to write a babeltrace2 run command line are:1. Specify which component classes to instantiate as componentswith many --component options and how to configure them.This is the COMPONENTS part of the synopsis. See \u201cCreatecomponents\u201d to learn more.2. Specify how to connect components together with one or more--connect options.See \u201cConnect components\u201d to learn more.NoteThe babeltrace2-convert(1) command is a specialization of therun command for the very common case of converting one ormore traces: it generates a run command line and executes it.You can use its --run-args or --run-args-0 option to make itprint the equivalent run command line instead.Create componentsTo create a component, use the --component option. This optionspecifies:\u2022The name of the component, unique amongst all the componentnames of the trace processing graph.\u2022The type of the component class to instantiate: source,filter, or sink.\u2022The name of the plugin in which to find the component classto instantiate.\u2022The name of the component class to instantiate.Use the --component option multiple times to create multiplecomponents. You can instantiate the same component class multipletimes as different components.At any point in the command line, the --base-params sets thecurrent base initialization parameters and the --reset-base-params resets them. When you specify a --component option, itsinitial initialization parameters are a copy of the current baseinitialization parameters.Immediately following a --component option on the command line,the created component is known as the current component (untilthe next --component option).The --params=PARAMS option adds parameters to the currentcomponent\u2019s initialization parameters. If PARAMS contains a keywhich exists in the current component\u2019s initializationparameters, this parameter is replaced.Connect componentsThe components which you create from component classes with the--component option (see \u201cCreate components\u201d) add input and outputports depending on their type. An output port is from wheremessages, like trace events, are sent. An input port is wheremessages are received. For a given component, each port has aunique name.The purpose of the run command is to create a trace processinggraph, that is, to know which component ports to connecttogether. The command achieves this with the help of theconnection rules that you provide with one or more--connect=CONN-RULE options.The format of CONN-RULE is:UP-COMP-PAT[.UP-PORT-PAT]:DOWN-COMP-PAT[.DOWN-PORT-PAT]UP-COMP-PATUpstream component name pattern.UP-PORT-PATUpstream (output) port name pattern.DOWN-COMP-PATDownstream component name pattern.DOWN-PORT-PATDownstream (input) port name pattern.When a source or filter component adds a new output port withinthe processing graph, the run command does the following to findan input port to connect it to:For each connection rule (--connect options, in order):If the output port's component's name matches UP-COMP-PAT and theoutput port's name matches UP-PORT-PAT:For each component COMP in the trace processing graph:If the name of COMP matches DOWN-COMP-PAT:Select the first input port of COMP of which the name matchesDOWN-PORT-PAT, or fail with no match.No possible connection: fail with no match.UP-COMP-PAT, UP-PORT-PAT, DOWN-COMP-PAT, and DOWN-PORT-PAT areglobbing patterns where only the wildcard character, *, isspecial: it matches zero or more characters. You must escape the*, ?, [, ., :, and \\ characters with \\.When you do not specify UP-PORT-PAT or DOWN-PORT-PAT, they areequivalent to *.You can leverage this connection mechanism to specify fallbackswith a careful use of wildcards, as the order of the --connectoptions on the command line is significant. For example:--connect='A.out*:B.in*' --connect=A:B --connect='*:C'With those connection rules, the run command connects:\u2022Any output port of which the name starts with out ofcomponent A to the first input port of which the name startswith in of component B.\u2022Any other output port of component A to the first availableinput port of component B.\u2022Any other output port (of any component except A) to thefirst available input port of component C.The run command fails when it cannot find an input port to whichto connect a given output port using the provided connectionrules.See \u201cEXAMPLES\u201d for more examples.",
        "name": "babeltrace2-run - Create a Babeltrace 2 trace processing graphand run it",
        "section": 1
    },
    {
        "command": "base32",
        "description": "Base32 encode or decode FILE, or standard input, to standardoutput.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-d, --decodedecode data-i, --ignore-garbagewhen decoding, ignore non-alphabet characters-w, --wrap=COLSwrap encoded lines after COLS character (default 76).Use0 to disable line wrapping--help display this help and exit--versionoutput version information and exitThe data are encoded as described for the base32 alphabet in RFC4648.When decoding, the input may contain newlines in additionto the bytes of the formal base32 alphabet.Use --ignore-garbageto attempt to recover from any other non-alphabet bytes in theencoded stream.",
        "name": "base32 - base32 encode/decode data and print to standard output",
        "section": 1
    },
    {
        "command": "base64",
        "description": "Base64 encode or decode FILE, or standard input, to standardoutput.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-d, --decodedecode data-i, --ignore-garbagewhen decoding, ignore non-alphabet characters-w, --wrap=COLSwrap encoded lines after COLS character (default 76).Use0 to disable line wrapping--help display this help and exit--versionoutput version information and exitThe data are encoded as described for the base64 alphabet in RFC4648.When decoding, the input may contain newlines in additionto the bytes of the formal base64 alphabet.Use --ignore-garbageto attempt to recover from any other non-alphabet bytes in theencoded stream.",
        "name": "base64 - base64 encode/decode data and print to standard output",
        "section": 1
    },
    {
        "command": "basename",
        "description": "Print NAME with any leading directory components removed.Ifspecified, also remove a trailing SUFFIX.Mandatory arguments to long options are mandatory for shortoptions too.-a, --multiplesupport multiple arguments and treat each as a NAME-s, --suffix=SUFFIXremove a trailing SUFFIX; implies -a-z, --zeroend each output line with NUL, not newline--help display this help and exit--versionoutput version information and exit",
        "name": "basename - strip directory and suffix from filenames",
        "section": 1
    },
    {
        "command": "basenc",
        "description": "basenc encode or decode FILE, or standard input, to standardoutput.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.--base64same as 'base64' program (RFC4648 section 4)--base64urlfile- and url-safe base64 (RFC4648 section 5)--base32same as 'base32' program (RFC4648 section 6)--base32hexextended hex alphabet base32 (RFC4648 section 7)--base16hex encoding (RFC4648 section 8)--base2msbfbit string with most significant bit (msb) first--base2lsbfbit string with least significant bit (lsb) first-d, --decodedecode data-i, --ignore-garbagewhen decoding, ignore non-alphabet characters-w, --wrap=COLSwrap encoded lines after COLS character (default 76).Use0 to disable line wrapping--z85ascii85-like encoding (ZeroMQ spec:32/Z85); when encoding,input length must be a multiple of 4; when decoding, inputlength must be a multiple of 5--help display this help and exit--versionoutput version information and exitWhen decoding, the input may contain newlines in addition to thebytes of the formal alphabet.Use --ignore-garbage to attempt torecover from any other non-alphabet bytes in the encoded stream.",
        "name": "basenc - Encode/decode data and print to standard output",
        "section": 1
    },
    {
        "command": "bash",
        "description": "Bash is an sh-compatible command language interpreter thatexecutes commands read from the standard input or from a file.Bash also incorporates useful features from the Korn and C shells(ksh and csh).Bash is intended to be a conformant implementation of the Shelland Utilities portion of the IEEE POSIX specification (IEEEStandard 1003.1).Bash can be configured to be POSIX-conformantby default.",
        "name": "bash - GNU Bourne-Again SHell",
        "section": 1
    },
    {
        "command": "blkparse",
        "description": "The blkparse utility will attempt to combine streams of eventsfor various devices on various CPUs, and produce a formattedoutput of the event information.Specifically, it will take the(machine-readable) output of the blktrace utility and convert itto a nicely formatted and human-readable form.As with blktrace, some details concerning blkparse will help inunderstanding the command line options presented below.- By default, blkparse expects to run in a post-processing mode;one where the trace events have been saved by a previous run ofblktrace, and blkparse is combining event streams and dumpingformatted data.blkparse may be run in a live manner concurrently with blktraceby specifying -i - to blkparse, and combining it with the liveoption for blktrace.An example would be:% blktrace -d /dev/sda -o - | blkparse -i -- You can set how many blkparse batches event reads via the -boption, the default is to handle events in batches of 512.- If you have saved event traces in blktrace with differentoutput names (via the -o option to blktrace), you must specifythe same input name via the -i option.- The format of the output data can be controlled via the -f or-F options -- see OUTPUT DESCRIPTION AND FORMATTING fordetails.By default, blkparse sends formatted data to standard output.This may be changed via the -o option, or text output can bedisabled via the -O option. A merged binary stream can beproduced using the -d option.",
        "name": "blkparse - produce formatted output of event streams of blockdevices",
        "section": 1
    },
    {
        "command": "blkrawverify",
        "description": "The blkrawverify utility can be used to verify data retrieved viablktrace. It will check for valid event formats, forwardprogressing sequence numbers and time stamps, also doesreasonable checks for other potential issues within individualevents.Errors found will be tracked in <dev>.verify.out.",
        "name": "blkrawverify - verifies an output file produced by blkparse",
        "section": 1
    },
    {
        "command": "bno_plot",
        "description": "bno_plot is a visualization tool for the block layer IO tracingtool called blktrace(8).As noted in its documentation, blktraceis a block layer IO tracing mechanism which provides detailedinformation about request queue operations up to user space.bno_plot utilizes gnuplot to generate a 3D plot of the blocknumber output from btt.If no <files> are specified, it willutilize all files generated after btt was run with -B blknos(meaning: all files of the form blknos*[rw].dat).The -K option forces bno_plot to put the keys below the graph.If it is not specified, all keys for input files are put in theupper right corner of the graph. If the number of devices exceed10, then bno_plot will automatically push the keys under thegraph.To use this utility, the gnuplot package needs to be installed.To exit the plotter, enter 'quit' or ^D at the 'gnuplot> 'prompt.",
        "name": "bno_plot - generate interactive 3D plot of IO blocks and sizes",
        "section": 1
    },
    {
        "command": "btt",
        "description": "btt is a post-processing tool for the block layer IO tracing toolcalled blktrace(8).As noted in its documentation, blktrace is ablock layer IO tracing mechanism which provides detailedinformation about request queue operations up to user space.btt will take in binary dump data from blkparse, and analyse theevents, producing a series of output from the analysis. It willalso build .dat files containing \"range data\" -- showing thingslike Q activity (periods of time while Q events are beingproduced), C activity (likewise for command completions), andetc.Included with the distribution is a simple 3D plotting utility,bno_plot, which can plot the block numbers btt outputs if the -Boption is specified. The display will display each IO generated,with the time (seconds) along the X-axis, the block number(start) along the Y-axis and the number of blocks transferred inthe IO represented along the Z-axis.",
        "name": "btt - analyse block i/o traces produces by blktrace",
        "section": 1
    },
    {
        "command": "busctl",
        "description": "busctl may be used to introspect and monitor the D-Bus bus.",
        "name": "busctl - Introspect the bus",
        "section": 1
    },
    {
        "command": "c++filt",
        "description": "The C++ and Java languages provide function overloading, whichmeans that you can write many functions with the same name,providing that each function takes parameters of different types.In order to be able to distinguish these similarly namedfunctions C++ and Java encode them into a low-level assemblername which uniquely identifies each different version.Thisprocess is known as mangling. The c++filt [1] program does theinverse mapping: it decodes (demangles) low-level names intouser-level names so that they can be read.Every alphanumeric word (consisting of letters, digits,underscores, dollars, or periods) seen in the input is apotential mangled name.If the name decodes into a C++ name, theC++ name replaces the low-level name in the output, otherwise theoriginal word is output.In this way you can pass an entireassembler source file, containing mangled names, through c++filtand see the same source file containing demangled names.You can also use c++filt to decipher individual symbols bypassing them on the command line:c++filt <symbol>If no symbol arguments are given, c++filt reads symbol names fromthe standard input instead.All the results are printed on thestandard output.The difference between reading names from thecommand line versus reading names from the standard input is thatcommand-line arguments are expected to be just mangled names andno checking is performed to separate them from surrounding text.Thus for example:c++filt -n _Z1fvwill work and demangle the name to \"f()\" whereas:c++filt -n _Z1fv,will not work.(Note the extra comma at the end of the mangledname which makes it invalid).This command however will work:echo _Z1fv, | c++filt -nand will display \"f(),\", i.e., the demangled name followed by atrailing comma.This behaviour is because when the names areread from the standard input it is expected that they might bepart of an assembler source file where there might be extra,extraneous characters trailing after a mangled name.Forexample:.type_Z1fv, @function",
        "name": "c++filt - demangle C++ and Java symbols",
        "section": 1
    },
    {
        "command": "cal",
        "description": "cal displays a simple calendar. If no arguments are specified,the current month is displayed.The month may be specified as a number (1-12), as a month name oras an abbreviated month name according to the current locales.Two different calendar systems are used, Gregorian and Julian.These are nearly identical systems with Gregorian making a smalladjustment to the frequency of leap years; this facilitatesimproved synchronization with solar events like the equinoxes.The Gregorian calendar reform was introduced in 1582, but itsadoption continued up to 1923. By default cal uses the adoptiondate of 3 Sept 1752. From that date forward the Gregoriancalendar is displayed; previous dates use the Julian calendarsystem. 11 days were removed at the time of adoption to bring thecalendar in sync with solar events. So Sept 1752 has a mix ofJulian and Gregorian dates by which the 2nd is followed by the14th (the 3rd through the 13th are absent).Optionally, either the proleptic Gregorian calendar or the Juliancalendar may be used exclusively. See --reform below.",
        "name": "cal - display a calendar",
        "section": 1
    },
    {
        "command": "callgrind_annotate",
        "description": "callgrind_annotate takes an output file produced by the Valgrindtool Callgrind and prints the information in an easy-to-readform.",
        "name": "callgrind_annotate - post-processing tool for the Callgrind",
        "section": 1
    },
    {
        "command": "callgrind_control",
        "description": "callgrind_control controls programs being run by the Valgrindtool Callgrind. When a pid/program name argument is notspecified, all applications currently being run by Callgrind onthis system will be used for actions given by the specifiedoption(s). The default action is to give some brief informationabout the applications being run by Callgrind.",
        "name": "callgrind_control - observe and control programs being run byCallgrind",
        "section": 1
    },
    {
        "command": "cancel",
        "description": "The cancel command cancels print jobs.If no destination or idis specified, the currently printing job on the defaultdestination is canceled.",
        "name": "cancel - cancel jobs",
        "section": 1
    },
    {
        "command": "capsh",
        "description": "Linux capability support and use can be explored and constrainedwith this tool. This tool provides a handy wrapper for certaintypes of capability testing and environment creation. It alsoprovides some debugging features useful for summarizingcapability state.",
        "name": "capsh - capability shell wrapper",
        "section": 1
    },
    {
        "command": "cat",
        "description": "Concatenate FILE(s) to standard output.With no FILE, or when FILE is -, read standard input.-A, --show-allequivalent to -vET-b, --number-nonblanknumber nonempty output lines, overrides -n-eequivalent to -vE-E, --show-endsdisplay $ at end of each line-n, --numbernumber all output lines-s, --squeeze-blanksuppress repeated empty output lines-tequivalent to -vT-T, --show-tabsdisplay TAB characters as ^I-u(ignored)-v, --show-nonprintinguse ^ and M- notation, except for LFD and TAB--help display this help and exit--versionoutput version information and exit",
        "name": "cat - concatenate files and print on the standard output",
        "section": 1
    },
    {
        "command": "cdrwtool",
        "description": "The cdwrtool command can perform certain actions on a CD-R, CD-RW, or DVD-R device. Mainly these are blanking the media,formatting it for use with the packet-cd device,and applying anUDF filesystem.The most common usage is probably the `quick setup' option:cdrwtool -d device -qwhich will blank the disc, format it as one large track, andwrite the UDF filesystem structures.Other options get and set various parameters of how the device isset up, and provide for different offsets, modes and settingsfrom the defaults.The usefulness of most of the options is not explained.",
        "name": "cdrwtool - perform various actions on a CD-R, CD-RW, and DVD-R",
        "section": 1
    },
    {
        "command": "certtool",
        "description": "Tool to parse and generate X.509 certificates, requests andprivate keys.It can be used interactively or non interactivelyby specifying the template command line option.The tool accepts files or supported URIs via the --infile option.In case PIN is required for URI access you can provide it usingthe environment variables GNUTLS_PIN and GNUTLS_SO_PIN.",
        "name": "certtool - GnuTLS certificate tool",
        "section": 1
    },
    {
        "command": "cg_annotate",
        "description": "cg_annotate takes one or more Cachegrind output files and printsdata about the profiled program in an easy-to-read form.",
        "name": "cg_annotate - post-processing tool for Cachegrind",
        "section": 1
    },
    {
        "command": "cg_diff",
        "description": "cg_diff diffs two Cachegrind output files into a singleCachegrind output file. It is deprecated because cg_annotate cannow do much the same thing, but better.",
        "name": "cg_diff - (deprecated) diffs two Cachegrind output files",
        "section": 1
    },
    {
        "command": "cg_merge",
        "description": "cg_merge sums together multiple Cachegrind output files into asingle Cachegrind output file. It is deprecated becausecg_annotate can now do much the same thing, but better.",
        "name": "cg_merge - (deprecated) merges multiple Cachegrind output filesinto one",
        "section": 1
    },
    {
        "command": "cgcc",
        "description": "cgcc provides a wrapper around a C compiler (cc by default) whichalso invokes the Sparse static analysis tool.cgcc accepts all Sparse command-line options, such as warningoptions, and passes all other options through to the compiler.By providing the same interface as the C compiler, cgcc allowsprojects to run Sparse as part of their build without modifyingtheir build system, by using cgcc as the compiler.For manyprojects, setting CC=cgcc on the make command-line will work.",
        "name": "cgcc - Compiler wrapper to run Sparse after compiling",
        "section": 1
    },
    {
        "command": "chacl",
        "description": "chacl is an IRIX-compatibility command, and is maintained forthose users who are familiar with its use from either XFS orIRIX.Refer to the SEE ALSO section below for a description oftools which conform more closely to the (withdrawn draft) POSIX1003.1e standard which describes Access Control Lists (ACLs).chacl changes the ACL(s) for a file or directory.The ACL(s)specified are applied to each file in the pathname arguments.Each ACL is a string which is interpreted using theacl_from_text(3) routine.These strings are made up of commaseparated clauses each of which is of the form, tag:name:perm.Where tag can be:\"user\" (or \"u\")indicating that the entry is a \"user\" ACL entry.\"group\" (or \"g\")indicating that the entry is a \"group\" ACL entry.\"other\" (or \"o\")indicating that the entry is an \"other\" ACL entry.\"mask\" (or \"m\")indicating that the entry is a \"mask\" ACL entry.name is a string which is the user or group name for the ACLentry.A null name in a user or group ACL entry indicates thefile's owner or file's group.perm is the string \"rwx\" whereeach of the entries may be replaced by a \"-\" indicating no accessof that type, e.g. \"r-x\", \"--x\", \"---\".",
        "name": "chacl - change the access control list of a file or directory",
        "section": 1
    },
    {
        "command": "chage",
        "description": "The chage command changes the number of days between passwordchanges and the date of the last password change. Thisinformation is used by the system to determine when a user mustchange their password.",
        "name": "chage - change user password expiry information",
        "section": 1
    },
    {
        "command": "chattr",
        "description": "chattr changes the file attributes on a Linux file system.The format of a symbolic mode is +-=[aAcCdDeFijmPsStTux].The operator '+' causes the selected attributes to be added tothe existing attributes of the files; '-' causes them to beremoved; and '=' causes them to be the only attributes that thefiles have.The letters 'aAcCdDeFijmPsStTux' select the new attributes forthe files: append only (a), no atime updates (A), compressed (c),no copy on write (C), no dump (d), synchronous directory updates(D), extent format (e), case-insensitive directory lookups (F),immutable (i), data journaling (j), don't compress (m), projecthierarchy (P), secure deletion (s), synchronous updates (S), notail-merging (t), top of directory hierarchy (T), undeletable(u), and direct access for files (x).The following attributes are read-only, and may be listed bylsattr(1) but not modified by chattr: encrypted (E), indexeddirectory (I), inline data (N), and verity (V).Not all flags are supported or utilized by all file systems;refer to file system-specific man pages such as btrfs(5),ext4(5), mkfs.f2fs(8), and xfs(5) for more file system-specificdetails.",
        "name": "chattr - change file attributes on a Linux file system",
        "section": 1
    },
    {
        "command": "chcon",
        "description": "Change the SELinux security context of each FILE to CONTEXT.With --reference, change the security context of each FILE tothat of RFILE.Mandatory arguments to long options are mandatory for shortoptions too.--dereferenceaffect the referent of each symbolic link (this is thedefault), rather than the symbolic link itself-h, --no-dereferenceaffect symbolic links instead of any referenced file-u, --user=USERset user USER in the target security context-r, --role=ROLEset role ROLE in the target security context-t, --type=TYPEset type TYPE in the target security context-l, --range=RANGEset range RANGE in the target security context--no-preserve-rootdo not treat '/' specially (the default)--preserve-rootfail to operate recursively on '/'--reference=RFILEuse RFILE's security context rather than specifying aCONTEXT value-R, --recursiveoperate on files and directories recursively-v, --verboseoutput a diagnostic for every file processedThe following options modify how a hierarchy is traversed whenthe -R option is also specified.If more than one is specified,only the final one takes effect.-Hif a command line argument is a symbolic link to adirectory, traverse it-Ltraverse every symbolic link to a directory encountered-Pdo not traverse any symbolic links (default)--help display this help and exit--versionoutput version information and exit",
        "name": "chcon - change file security context",
        "section": 1
    },
    {
        "command": "chem",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "chfn",
        "description": "chfn is used to change your finger information. This informationis stored in the /etc/passwd file, and is displayed by the fingerprogram. The Linux finger command will display four pieces ofinformation that can be changed by chfn: your real name, yourwork room and phone, and your home phone.Any of the four pieces of information can be specified on thecommand line. If no information is given on the command line,chfn enters interactive mode.In interactive mode, chfn will prompt for each field. At aprompt, you can enter the new information, or just press returnto leave the field unchanged. Enter the keyword \"none\" to makethe field blank.chfn supports non-local entries (kerberos, LDAP, etc.) if linkedwith libuser, otherwise use ypchfn(1), lchfn(1) or any otherimplementation for non-local entries.",
        "name": "chfn - change your finger information",
        "section": 1
    },
    {
        "command": "chgrp",
        "description": "Change the group of each FILE to GROUP.With --reference, changethe group of each FILE to that of RFILE.-c, --changeslike verbose but report only when a change is made-f, --silent, --quietsuppress most error messages-v, --verboseoutput a diagnostic for every file processed--dereferenceaffect the referent of each symbolic link (this is thedefault), rather than the symbolic link itself-h, --no-dereferenceaffect symbolic links instead of any referenced file(useful only on systems that can change the ownership of asymlink)--no-preserve-rootdo not treat '/' specially (the default)--preserve-rootfail to operate recursively on '/'--reference=RFILEuse RFILE's group rather than specifying a GROUP.RFILEis always dereferenced if a symbolic link.-R, --recursiveoperate on files and directories recursivelyThe following options modify how a hierarchy is traversed whenthe -R option is also specified.If more than one is specified,only the final one takes effect.-Hif a command line argument is a symbolic link to adirectory, traverse it-Ltraverse every symbolic link to a directory encountered-Pdo not traverse any symbolic links (default)--help display this help and exit--versionoutput version information and exit",
        "name": "chgrp - change group ownership",
        "section": 1
    },
    {
        "command": "chkhelp",
        "description": "chkhelp checks the consistency of Performance Co-Pilot help textfiles generated by newhelp(1) and used by Performance MetricDomain Agents (PMDAs).The checking involves scanning the files,and optionally displaying selected entries.The files helpfile.dir and helpfile.pag are created bynewhelp(1), and are assumed to already exist.Without any options or metricname arguments, chkhelp silentlyverifies the structural integrity of the help files.If any metricname arguments are specified, then the help entriesfor only the corresponding metrics will be processed.If no metricname arguments are specified, then at least one ofthe options -i or -p must be given.The -i option causes entriesfor all instance domains to be processed (ignoring entries forperformance metrics).The -p option causes entries for allmetrics to be displayed (ignoring entries for instance domains).When metric entries are to be processed (via either themetricname arguments or the -p option or the -i option), the -Oand -H options request the display of the one-line and verbosehelp text respectively.The default is -O.Normally chkhelp operates on the default Performance Metrics NameSpace (PMNS), however if the -n option is specified analternative namespace is loaded from the file pmnsfile.The -e option provides an existence check where all of thespecified metrics from the PMNS (note, not from helpfile) arescanned, and only the names of the metrics for which no help textexists are reported.The -e option is mutually exclusive withthe -i and/or -p options.",
        "name": "chkhelp - check performance metrics help text files",
        "section": 1
    },
    {
        "command": "chmod",
        "description": "This manual page documents the GNU version of chmod.chmodchanges the file mode bits of each given file according to mode,which can be either a symbolic representation of changes to make,or an octal number representing the bit pattern for the new modebits.The format of a symbolic mode is [ugoa...][[-+=][perms...]...],where perms is either zero or more letters from the set rwxXst,or a single letter from the set ugo.Multiple symbolic modes canbe given, separated by commas.A combination of the letters ugoa controls which users' access tothe file will be changed: the user who owns it (u), other usersin the file's group (g), other users not in the file's group (o),or all users (a).If none of these are given, the effect is asif (a) were given, but bits that are set in the umask are notaffected.The operator + causes the selected file mode bits to be added tothe existing file mode bits of each file; - causes them to beremoved; and = causes them to be added and causes unmentionedbits to be removed except that a directory's unmentioned set userand group ID bits are not affected.The letters rwxXst select file mode bits for the affected users:read (r), write (w), execute (or search for directories) (x),execute/search only if the file is a directory or already hasexecute permission for some user (X), set user or group ID onexecution (s), restricted deletion flag or sticky bit (t).Instead of one or more of these letters, you can specify exactlyone of the letters ugo: the permissions granted to the user whoowns the file (u), the permissions granted to other users who aremembers of the file's group (g), and the permissions granted tousers that are in neither of the two preceding categories (o).A numeric mode is from one to four octal digits (0-7), derived byadding up the bits with values 4, 2, and 1.Omitted digits areassumed to be leading zeros.The first digit selects the setuser ID (4) and set group ID (2) and restricted deletion orsticky (1) attributes.The second digit selects permissions forthe user who owns the file: read (4), write (2), and execute (1);the third selects permissions for other users in the file'sgroup, with the same values; and the fourth for other users notin the file's group, with the same values.chmod never changes the permissions of symbolic links; the chmodsystem call cannot change their permissions.This is not aproblem since the permissions of symbolic links are never used.However, for each symbolic link listed on the command line, chmodchanges the permissions of the pointed-to file.In contrast,chmod ignores symbolic links encountered during recursivedirectory traversals.",
        "name": "chmod - change file mode bits",
        "section": 1
    },
    {
        "command": "choom",
        "description": "The choom command displays and adjusts Out-Of-Memory killer scoresetting.",
        "name": "choom - display and adjust OOM-killer score.choom -p PIDchoom -p PID -n numberchoom -n number [--] command [argument ...]",
        "section": 1
    },
    {
        "command": "chown",
        "description": "This manual page documents the GNU version of chown.chownchanges the user and/or group ownership of each given file.Ifonly an owner (a user name or numeric user ID) is given, thatuser is made the owner of each given file, and the files' groupis not changed.If the owner is followed by a colon and a groupname (or numeric group ID), with no spaces between them, thegroup ownership of the files is changed as well.If a colon butno group name follows the user name, that user is made the ownerof the files and the group of the files is changed to that user'slogin group.If the colon and group are given, but the owner isomitted, only the group of the files is changed; in this case,chown performs the same function as chgrp.If only a colon isgiven, or if the entire operand is empty, neither the owner northe group is changed.",
        "name": "chown - change file owner and group",
        "section": 1
    },
    {
        "command": "chroot",
        "description": "Run COMMAND with root directory set to NEWROOT.--groups=G_LISTspecify supplementary groups as g1,g2,..,gN--userspec=USER:GROUPspecify user and group (ID or name) to use--skip-chdirdo not change working directory to '/'--help display this help and exit--versionoutput version information and exitIf no command is given, run '\"$SHELL\" -i' (default: '/bin/sh-i').Exit status:125if the chroot command itself fails126if COMMAND is found but cannot be invoked127if COMMAND cannot be found-the exit status of COMMAND otherwise",
        "name": "chroot - run command or interactive shell with special rootdirectory",
        "section": 1
    },
    {
        "command": "chrt",
        "description": "chrt sets or retrieves the real-time scheduling attributes of anexisting PID, or runs command with the given attributes.",
        "name": "chrt - manipulate the real-time attributes of a process",
        "section": 1
    },
    {
        "command": "chsh",
        "description": "chsh is used to change your login shell. If a shell is not givenon the command line, chsh prompts for one.chsh supports non-local entries (kerberos, LDAP, etc.) if linkedwith libuser, otherwise use ypchsh(1), lchsh(1) or any otherimplementation for non-local entries.",
        "name": "chsh - change your login shell",
        "section": 1
    },
    {
        "command": "chvt",
        "description": "The command chvt N makes /dev/ttyN the foreground terminal.(Thecorresponding screen is created if it did not exist yet.To getrid of unused VTs, use deallocvt(1).)The key combination(Ctrl-)LeftAlt-FN (with N in the range 1-12) usually has asimilar effect.",
        "name": "chvt - change foreground virtual terminal",
        "section": 1
    },
    {
        "command": "cifsiostat",
        "description": "The cifsiostat command displays statistics about read and writeoperations on CIFS filesystems.The interval parameter specifies the amount of time in secondsbetween each report. The first report contains statistics for thetime since system startup (boot). Each subsequent report containsstatistics collected during the interval since the previousreport.A report consists of a CIFS header row followed by aline of statistics for each CIFS filesystem that is mounted.Thecount parameter can be specified in conjunction with the intervalparameter. If the count parameter is specified, the value ofcount determines the number of reports generated at intervalseconds apart. If the interval parameter is specified without thecount parameter, the cifsiostat command generates reportscontinuously.",
        "name": "cifsiostat - Report CIFS statistics.",
        "section": 1
    },
    {
        "command": "cksum",
        "description": "Print or verify checksums.By default use the 32 bit CRCalgorithm.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-a, --algorithm=TYPEselect the digest type to use.See DIGEST below.-b, --base64emit base64-encoded digests, not hexadecimal-c, --checkread checksums from the FILEs and check them-l, --length=BITSdigest length in bits; must not exceed the max for theblake2 algorithm and must be a multiple of 8--rawemit a raw binary digest, not hexadecimal--tagcreate a BSD-style checksum (the default)--untaggedcreate a reversed style checksum, without digest type-z, --zeroend each output line with NUL, not newline, and disablefile name escapingThe following five options are useful only when verifying checksums:--ignore-missingdon't fail or report status for missing files--quietdon't print OK for each successfully verified file--statusdon't output anything, status code shows success--strictexit non-zero for improperly formatted checksum lines-w, --warnwarn about improperly formatted checksum lines--debugindicate which implementation used--help display this help and exit--versionoutput version information and exitDIGEST determines the digest algorithm and default output format:sysv(equivalent to sum -s)bsd(equivalent to sum -r)crc(equivalent to cksum)md5(equivalent to md5sum)sha1(equivalent to sha1sum)sha224 (equivalent to sha224sum)sha256 (equivalent to sha256sum)sha384 (equivalent to sha384sum)sha512 (equivalent to sha512sum)blake2b(equivalent to b2sum)sm3(only available through cksum)When checking, the input should be a former output of thisprogram, or equivalent standalone program.",
        "name": "cksum - compute and verify file checksums",
        "section": 1
    },
    {
        "command": "clear",
        "description": "@CLEAR@ clears your terminal's screen if this is possible,including the terminal's scrollback buffer (if the extended \u201cE3\u201dcapability is defined).@CLEAR@ looks in the environment for theterminal type given by the environment variable TERM, and then inthe terminfo database to determine how to clear the screen.@CLEAR@ writes to the standard output.You can redirect thestandard output to a file (which prevents @CLEAR@ from actuallyclearing the screen), and later cat the file to the screen,clearing it at that point.",
        "name": "@CLEAR@ - clear the terminal screen",
        "section": 1
    },
    {
        "command": "cmp",
        "description": "Compare two files byte by byte.The optional SKIP1 and SKIP2 specify the number of bytes to skipat the beginning of each file (zero by default).Mandatory arguments to long options are mandatory for shortoptions too.-b, --print-bytesprint differing bytes-i, --ignore-initial=SKIPskip first SKIP bytes of both inputs-i, --ignore-initial=SKIP1:SKIP2skip first SKIP1 bytes of FILE1 and first SKIP2 bytes ofFILE2-l, --verboseoutput byte numbers and differing byte values-n, --bytes=LIMITcompare at most LIMIT bytes-s, --quiet, --silentsuppress all normal output--help display this help and exit-v, --versionoutput version information and exitSKIP values may be followed by the following multiplicativesuffixes: kB 1000, K 1024, MB 1,000,000, M 1,048,576, GB1,000,000,000, G 1,073,741,824, and so on for T, P, E, Z, Y.If a FILE is '-' or missing, read standard input.Exit status is0 if inputs are the same, 1 if different, 2 if trouble.",
        "name": "cmp - compare two files byte by byte",
        "section": 1
    },
    {
        "command": "cmtime",
        "description": "Determines min and max times for various \"steps\" in RDMA CMconnection setup and teardown between a client and serverapplication.\"Steps\" that are timed are: create id, bind address, resolveaddress, resolve route, create qp, connect, disconnect, anddestroy.",
        "name": "cmtime - RDMA CM connection steps timing test.",
        "section": 1
    },
    {
        "command": "col",
        "description": "col filters out reverse (and half-reverse) line feeds so theoutput is in the correct order, with only forward andhalf-forward line feeds. It also replaces any whitespacecharacters with tabs where possible. This can be useful inprocessing the output of nroff(1) and tbl(1).col reads from standard input and writes to standard output.",
        "name": "col - filter reverse line feeds from input",
        "section": 1
    },
    {
        "command": "colcrt",
        "description": "colcrt provides virtual half-line and reverse line feed sequencesfor terminals without such capability, and on which overstrikingis destructive. Half-line characters and underlining (changed todashing `-') are placed on new lines in between the normal outputlines.",
        "name": "colcrt - filter nroff output for CRT previewing",
        "section": 1
    },
    {
        "command": "collectl2pcp",
        "description": "collectl2pcp reads raw collectl(1) data from each file andcreates a new PCP archive with basename archive.Each input filemay be gzipped (with .gz suffix).The PCP archive and at leastone input file are required arguments.",
        "name": "collectl2pcp - import collectl data to a PCP archive",
        "section": 1
    },
    {
        "command": "colrm",
        "description": "colrm removes selected columns from a file. Input is taken fromstandard input. Output is sent to standard output.If called with one parameter the columns of each line will beremoved starting with the specified first column. If called withtwo parameters the columns from the first column to the lastcolumn will be removed.Column numbering starts with column 1.",
        "name": "colrm - remove columns from a file",
        "section": 1
    },
    {
        "command": "column",
        "description": "The column utility formats its input into multiple columns. Theutil support three modes:columns are filled before rowsThis is the default mode (required by backwardcompatibility).rows are filled before columnsThis mode is enabled by option -x, --fillrowstableDetermine the number of columns the input contains and createa table. This mode is enabled by option -t, --table andcolumns formatting is possible to modify by --table-*options. Use this mode if not sure. The output is aligned tothe terminal width in interactive mode and the 80 columns innon-interactive mode (see --output-width for more details).Input is taken from file, or otherwise from standard input. Emptylines are ignored and all invalid multibyte sequences are encodedby x<hex> convention.",
        "name": "column - columnate lists",
        "section": 1
    },
    {
        "command": "comm",
        "description": "Compare sorted files FILE1 and FILE2 line by line.When FILE1 or FILE2 (not both) is -, read standard input.With no options, produce three-column output.Column onecontains lines unique to FILE1, column two contains lines uniqueto FILE2, and column three contains lines common to both files.-1suppress column 1 (lines unique to FILE1)-2suppress column 2 (lines unique to FILE2)-3suppress column 3 (lines that appear in both files)--check-ordercheck that the input is correctly sorted, even if allinput lines are pairable--nocheck-orderdo not check that the input is correctly sorted--output-delimiter=STRseparate columns with STR--totaloutput a summary-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exitNote, comparisons honor the rules specified by 'LC_COLLATE'.",
        "name": "comm - compare two sorted files line by line",
        "section": 1
    },
    {
        "command": "comp_err",
        "description": "comp_err creates the errmsg.sys file that is used by mysqld todetermine the error messages to display for different errorcodes.comp_err normally is run automatically when MariaDB isbuilt. It compiles the errmsg.sys file from the plaintext filelocated at sql/share/errmsg.txt in MariaDB source distributions.comp_err also generates mysqld_error.h, mysqld_ername.h, andsql_state.h header files.For more information about how error messages are defined, seethe MySQL Internals Manual.Invoke comp_err like this:shell> comp_err [options]comp_err supports the following options.\u2022--help, -?Display a help message and exit.\u2022--charset=path, -C pathThe character set directory. The default is../sql/share/charsets.\u2022--debug=debug_options, -# debug_optionsWrite a debugging log. A typical debug_options string is\u00b4d:t:O,file_name\u00b4. The default is\u00b4d:t:O,/tmp/comp_err.trace\u00b4.\u2022--debug-info, -TPrint some debugging information when the program exits.\u2022--header_file=file_name, -H file_nameThe name of the error header file. The default ismysqld_error.h.\u2022--in_file=file_name, -F file_nameThe name of the input file. The default is../sql/share/errmsg.txt.\u2022--name_file=file_name, -N file_nameThe name of the error name file. The default ismysqld_ername.h.\u2022--out_dir=path, -D pathThe name of the output base directory. The default is../sql/share/.\u2022--out_file=file_name, -O file_nameThe name of the output file. The default is errmsg.sys.\u2022--statefile=file_name, -S file_nameThe name for the SQLSTATE header file. The default issql_state.h.\u2022--version, -VDisplay version information and exit.",
        "name": "comp_err - compile MariaDB error message file",
        "section": 1
    },
    {
        "command": "coredumpctl",
        "description": "coredumpctl is a tool that can be used to retrieve and processcore dumps and metadata which were saved by systemd-coredump(8).",
        "name": "coredumpctl - Retrieve and process saved core dumps and metadata",
        "section": 1
    },
    {
        "command": "coreutils",
        "description": "Execute the PROGRAM_NAME built-in program with the givenPARAMETERS.--help display this help and exit--versionoutput version information and exitUse: 'coreutils --coreutils-prog=PROGRAM_NAME --help' forindividual program help.",
        "name": "coreutils - single binary for coreutils programs",
        "section": 1
    },
    {
        "command": "cp",
        "description": "Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.Mandatory arguments to long options are mandatory for shortoptions too.-a, --archivesame as -dR --preserve=all--attributes-onlydon't copy the file data, just the attributes--backup[=CONTROL]make a backup of each existing destination file-blike --backup but does not accept an argument--copy-contentscopy contents of special files when recursive-dsame as --no-dereference --preserve=links--debugexplain how a file is copied.Implies -v-f, --forceif an existing destination file cannot be opened, removeit and try again (this option is ignored when the -noption is also used)-i, --interactiveprompt before overwrite (overrides a previous -n option)-Hfollow command-line symbolic links in SOURCE-l, --linkhard link files instead of copying-L, --dereferencealways follow symbolic links in SOURCE-n, --no-clobberdo not overwrite an existing file (overrides a -u orprevious -i option). See also --update-P, --no-dereferencenever follow symbolic links in SOURCE-psame as --preserve=mode,ownership,timestamps--preserve[=ATTR_LIST]preserve the specified attributes--no-preserve=ATTR_LISTdon't preserve the specified attributes--parentsuse full source file name under DIRECTORY-R, -r, --recursivecopy directories recursively--reflink[=WHEN]control clone/CoW copies. See below--remove-destinationremove each existing destination file before attempting toopen it (contrast with --force)--sparse=WHENcontrol creation of sparse files. See below--strip-trailing-slashesremove any trailing slashes from each SOURCE argument-s, --symbolic-linkmake symbolic links instead of copying-S, --suffix=SUFFIXoverride the usual backup suffix-t, --target-directory=DIRECTORYcopy all SOURCE arguments into DIRECTORY-T, --no-target-directorytreat DEST as a normal file--update[=UPDATE]control which existing files are updated;UPDATE={all,none,older(default)}.See below-uequivalent to --update[=older]-v, --verboseexplain what is being done-x, --one-file-systemstay on this file system-Zset SELinux security context of destination file todefault type--context[=CTX]like -Z, or if CTX is specified then set the SELinux orSMACK security context to CTX--help display this help and exit--versionoutput version information and exitATTR_LIST is a comma-separated list of attributes. Attributes are'mode' for permissions (including any ACL and xattr permissions),'ownership' for user and group, 'timestamps' for file timestamps,'links' for hard links, 'context' for security context, 'xattr'for extended attributes, and 'all' for all attributes.By default, sparse SOURCE files are detected by a crude heuristicand the corresponding DEST file is made sparse as well.That isthe behavior selected by --sparse=auto.Specify --sparse=alwaysto create a sparse DEST file whenever the SOURCE file contains along enough sequence of zero bytes.Use --sparse=never toinhibit creation of sparse files.UPDATE controls which existing files in the destination arereplaced.'all' is the default operation when an --update optionis not specified, and results in all existing files in thedestination being replaced.'none' is similar to the--no-clobber option, in that no files in the destination arereplaced, but also skipped files do not induce a failure.'older' is the default operation when --update is specified, andresults in files being replaced if they're older than thecorresponding source file.When --reflink[=always] is specified, perform a lightweight copy,where the data blocks are copied only when modified.If this isnot possible the copy fails, or if --reflink=auto is specified,fall back to a standard copy.Use --reflink=never to ensure astandard copy is performed.The backup suffix is '~', unless set with --suffix orSIMPLE_BACKUP_SUFFIX.The version control method may be selectedvia the --backup option or through the VERSION_CONTROLenvironment variable.Here are the values:none, offnever make backups (even if --backup is given)numbered, tmake numbered backupsexisting, nilnumbered if numbered backups exist, simple otherwisesimple, neveralways make simple backupsAs a special case, cp makes a backup of SOURCE when the force andbackup options are given and SOURCE and DEST are the same namefor an existing, regular file.",
        "name": "cp - copy files and directories",
        "section": 1
    },
    {
        "command": "cpp",
        "description": "The C preprocessor, often known as cpp, is a macro processor thatis used automatically by the C compiler to transform your programbefore compilation.It is called a macro processor because itallows you to define macros, which are brief abbreviations forlonger constructs.The C preprocessor is intended to be used only with C, C++, andObjective-C source code.In the past, it has been abused as ageneral text processor.It will choke on input which does notobey C's lexical rules.For example, apostrophes will beinterpreted as the beginning of character constants, and causeerrors.Also, you cannot rely on it preserving characteristicsof the input which are not significant to C-family languages.Ifa Makefile is preprocessed, all the hard tabs will be removed,and the Makefile will not work.Having said that, you can often get away with using cpp on thingswhich are not C.Other Algol-ish programming languages are oftensafe (Ada, etc.) So is assembly, with caution.-traditional-cppmode preserves more white space, and is otherwise morepermissive.Many of the problems can be avoided by writing C orC++ style comments instead of native language comments, andkeeping macros simple.Wherever possible, you should use a preprocessor geared to thelanguage you are writing in.Modern versions of the GNUassembler have macro facilities.Most high level programminglanguages have their own conditional compilation and inclusionmechanism.If all else fails, try a true general text processor,such as GNU M4.C preprocessors vary in some details.This manual discusses theGNU C preprocessor, which provides a small superset of thefeatures of ISO Standard C.In its default mode, the GNU Cpreprocessor does not do a few things required by the standard.These are features which are rarely, if ever, used, and may causesurprising changes to the meaning of a program which does notexpect them.To get strict ISO Standard C, you should use the-std=c90, -std=c99, -std=c11 or -std=c17 options, depending onwhich version of the standard you want.To get all the mandatorydiagnostics, you must also use -pedantic.This manual describes the behavior of the ISO preprocessor.Tominimize gratuitous differences, where the ISO preprocessor'sbehavior does not conflict with traditional semantics, thetraditional preprocessor should behave the same way.The variousdifferences that do exist are detailed in the section TraditionalMode.For clarity, unless noted otherwise, references to CPP in thismanual refer to GNU CPP.",
        "name": "cpp - The C Preprocessor",
        "section": 1
    },
    {
        "command": "cronnext",
        "description": "Determine the time cron will execute the next job.Withoutarguments, it prints that time considering all crontabs, innumber of seconds since the Epoch, rounded to the minute. Thisnumber can be converted into other formats using date(1), likedate --date @43243254The file arguments are optional. If provided, cronnext uses themas crontabs instead of the ones installed in the system.",
        "name": "cronnext - time of next job cron will execute",
        "section": 1
    },
    {
        "command": "crontab",
        "description": "Crontab is the program used to install a crontab table file,remove or list the existing tables used to serve the cron(8)daemon.Each user can have their own crontab, and though theseare files in /var/spool/, they are not intended to be editeddirectly.For SELinux in MLS mode, you can define more crontabsfor each range.For more information, see selinux(8).In this version of Cron it is possible to use a network-mountedshared /var/spool/cron across a cluster of hosts and specify thatonly one of the hosts should run the crontab jobs in theparticular directory at any one time.You may also use crontabfrom any of these hosts to edit the same shared set of crontabfiles, and to set and query which host should run the crontabjobs.Scheduling cron jobs with crontab can be allowed or disallowedfor different users.For this purpose, use the cron.allow andcron.deny files.If the cron.allow file exists, a user must belisted in it to be allowed to use crontab.If the cron.allowfile does not exist but the cron.deny file does exist, then auser must not be listed in the cron.deny file in order to usecrontab.If neither of these files exist, then only the superuser is allowed to use crontab.Another way to restrict the scheduling of cron jobs beyondcrontab is to use PAM authentication in /etc/security/access.confto set up users, which are allowed or disallowed to use crontabor modify system cron jobs in the /etc/cron.d/ directory.The temporary directory can be set in an environment variable.If it is not set by the user, the /tmp directory is used.When listing a crontab on a terminal the output will be colorizedunless an environment variable NO_COLOR is set.On edition or deletion of the crontab, a backup of the lastcrontab will be saved to $XDG_CACHE_HOME/crontab/crontab.bak or$XDG_CACHE_HOME/crontab/crontab.<user>.bak if -u is used.If theXDG_CACHE_HOME environment variable is not set, $HOME/.cache willbe used instead.",
        "name": "crontab - maintains crontab files for individual users",
        "section": 1
    },
    {
        "command": "csplit",
        "description": "Output pieces of FILE separated by PATTERN(s) to files 'xx00','xx01', ..., and output byte counts of each piece to standardoutput.Read standard input if FILE is -Mandatory arguments to long options are mandatory for shortoptions too.-b, --suffix-format=FORMATuse sprintf FORMAT instead of %02d-f, --prefix=PREFIXuse PREFIX instead of 'xx'-k, --keep-filesdo not remove output files on errors--suppress-matchedsuppress the lines matching PATTERN-n, --digits=DIGITSuse specified number of digits instead of 2-s, --quiet, --silentdo not print counts of output file sizes-z, --elide-empty-filessuppress empty output files--help display this help and exit--versionoutput version information and exitEach PATTERN may be:INTEGERcopy up to but not including specified line number/REGEXP/[OFFSET]copy up to but not including a matching line%REGEXP%[OFFSET]skip to, but not including a matching line{INTEGER}repeat the previous pattern specified number of times{*}repeat the previous pattern as many times as possibleA line OFFSET is an integer optionally preceded by '+' or '-'",
        "name": "csplit - split a file into sections determined by context lines",
        "section": 1
    },
    {
        "command": "cups",
        "description": "CUPS is the software you use to print from applications like wordprocessors, email readers, photo editors, and web browsers. Itconverts the page descriptions produced by your application (puta paragraph here, draw a line there, and so forth) into somethingyour printer can understand and then sends the information to theprinter for printing.Now, since every printer manufacturer does things differently,printing can be very complicated.CUPS does its best to hidethis from you and your application so that you can concentrate onprinting and less on how to print. Generally, the only time youneed to know anything about your printer is when you use it forthe first time, and even then CUPS can often figure things out onits own.HOW DOES IT WORK?The first time you print to a printer, CUPS creates a queue tokeep track of the current status of the printer (everything OK,out of paper, etc.) and any pages you have printed. Most of thetime the queue points to a printer connected directly to yourcomputer via a USB port, however it can also point to a printeron your network, a printer on the Internet, or multiple printersdepending on the configuration. Regardless of where the queuepoints, it will look like any other printer to you and yourapplications.Every time you print something, CUPS creates a job which containsthe queue you are sending the print to, the name of the documentyou are printing, and the page descriptions. Job are numbered(queue-1, queue-2, and so forth) so you can monitor the job as itis printed or cancel it if you see a mistake. When CUPS gets ajob for printing, it determines the best programs (filters,printer drivers, port monitors, and backends) to convert thepages into a printable format and then runs them to actuallyprint the job.When the print job is completely printed, CUPS removes the jobfrom the queue and moves on to any other jobs you have submitted.You can also be notified when the job is finished, or if thereare any errors during printing, in several different ways.WHERE DO I BEGIN?The easiest way to start is by using the web interface toconfigure your printer. Go to \"http://localhost:631\" and choosethe Administration tab at the top of the page. Click/press on theAdd Printer button and follow the prompts.When you are asked for a username and password, enter your loginusername and password or the \"root\" username and password.After the printer is added you will be asked to set the defaultprinter options (paper size, output mode, etc.) for the printer.Make any changes as needed and then click/press on the SetDefault Options button to save them. Some printers also supportauto-configuration - click/press on the Query Printer for DefaultOptions button to update the options automatically.Once you have added the printer, you can print to it from anyapplication. You can also choose Print Test Page from themaintenance menu to print a simple test page and verify thateverything is working properly.You can also use the lpadmin(8) and lpinfo(8) commands to addprinters to CUPS.Additionally, your operating system mayinclude graphical user interfaces or automatically create printerqueues when you connect a printer to your computer.HOW DO I GET HELP?The CUPS web site (http://www.CUPS.org) provides access to thecups and cups-devel mailing lists, additional documentation andresources, and a bug report database. Most vendors also provideonline discussion forums to ask printing questions for youroperating system of choice.",
        "name": "cups - a standards-based, open source printing system",
        "section": 1
    },
    {
        "command": "cups-config",
        "description": "The cups-config command allows application developers todetermine the necessary command-line options for the compiler andlinker, as well as the installation directories for filters,configuration files, and drivers.All values are reported to thestandard output.",
        "name": "cups-config - get cups api, compiler, directory, and linkinformation.",
        "section": 1
    },
    {
        "command": "cupstestppd",
        "description": "cupstestppd tests the conformance of PPD files to the AdobePostScript Printer Description file format specification version4.3.It can also be used to list the supported options andavailable fonts in a PPD file.The results of testing and anyother output are sent to the standard output.The first form of cupstestppd tests one or more PPD files on thecommand-line.The second form tests the PPD file provided on thestandard input.",
        "name": "cupstestppd - test conformance of ppd files",
        "section": 1
    },
    {
        "command": "curl",
        "description": "curl is a tool for transferring data from or to a server. Itsupports these protocols: DICT, FILE, FTP, FTPS, GOPHER, GOPHERS,HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS, MQTT, POP3, POP3S, RTMP,RTMPS, RTSP, SCP, SFTP, SMB, SMBS, SMTP, SMTPS, TELNET, TFTP, WSand WSS. The command is designed to work without userinteraction.curl offers a busload of useful tricks like proxy support, userauthentication, FTP upload, HTTP post, SSL connections, cookies,file transfer resume and more. As you will see below, the numberof features will make your head spin.curl is powered by libcurl for all transfer-related features. Seelibcurl(3) for details.",
        "name": "curl - transfer a URL",
        "section": 1
    },
    {
        "command": "curl-config",
        "description": "curl-config displays information about the curl and libcurlinstallation.",
        "name": "curl-config - Get information about a libcurl installation",
        "section": 1
    },
    {
        "command": "cut",
        "description": "Print selected parts of lines from each FILE to standard output.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-b, --bytes=LISTselect only these bytes-c, --characters=LISTselect only these characters-d, --delimiter=DELIMuse DELIM instead of TAB for field delimiter-f, --fields=LISTselect only these fields;also print any line thatcontains no delimiter character, unless the -s option isspecified-n(ignored)--complementcomplement the set of selected bytes, characters or fields-s, --only-delimiteddo not print lines not containing delimiters--output-delimiter=STRINGuse STRING as the output delimiter the default is to usethe input delimiter-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exitUse one, and only one of -b, -c or -f.Each LIST is made up ofone range, or many ranges separated by commas.Selected input iswritten in the same order that it is read, and is written exactlyonce.Each range is one of:NN'th byte, character or field, counted from 1N-from N'th byte, character or field, to end of lineN-Mfrom N'th to M'th (included) byte, character or field-Mfrom first to M'th (included) byte, character or field",
        "name": "cut - remove sections from each line of files",
        "section": 1
    },
    {
        "command": "cvtsudoers",
        "description": "The cvtsudoers utility accepts one or more security policies ineither sudoers or LDIF format as input, and generates a singlepolicy of the specified format as output.The default input formatis sudoers. The default output format is LDIF.It is only possibleto convert a policy file that is syntactically correct.If no input_file is specified, or if it is \u2018-\u2019, the policy is readfrom the standard input.Input files may be optionally prefixedwith a host name followed by a colon (\u2018:\u2019) to make the policy rulesspecific to a host when merging multiple files.By default, theresult is written to the standard output.The options are as follows:-b dn, --base=dnThe base DN (distinguished name) that will be used whenperforming LDAP queries.Typically this is of the form\u201cou=SUDOers,dc=my-domain,dc=com\u201d for the domain my-domain.com.If this option is not specified, the value ofthe SUDOERS_BASE environment variable will be used instead.Only necessary when converting to LDIF format.-c conf_file, --config=conf_fileSpecify the path to the configuration file.Defaults to/etc/cvtsudoers.conf.-d deftypes, --defaults=deftypesOnly convert Defaults entries of the specified types.Oneor more Defaults types may be specified, separated by acomma (\u2018,\u2019).The supported types are:allAll Defaults entries.globalGlobal Defaults entries that are appliedregardless of user, runas, host, or command.userPer-user Defaults entries.runasPer-runas user Defaults entries.hostPer-host Defaults entries.commandPer-command Defaults entries.See the Defaults section in sudoers(5) for moreinformation.If the -d option is not specified, all Defaults entrieswill be converted.-e, --expand-aliasesExpand aliases in input_file.Aliases are preserved bydefault when the output format is JSON or sudoers.-f output_format, --output-format=output_formatSpecify the output format (case-insensitive).Thefollowing formats are supported:CSVCSV (comma-separated value) files are often usedby spreadsheets and report generators.See CSVoutput format for more details.JSONJSON (JavaScript Object Notation) files areusually easier for third-party applications toconsume than the traditional sudoers format.Thevarious values have explicit types which removesmuch of the ambiguity of the sudoers format.SeeJSON output format for more details.LDIFLDIF (LDAP Data Interchange Format) files can beimported into an LDAP server for use withsudoers.ldap(5).Conversion to LDIF has the following limitations:\u2022Command, host, runas, and user-specificDefaults lines cannot be translated as theydon't have an equivalent in the sudoers LDAPschema.\u2022Command, host, runas, and user aliases are notsupported by the sudoers LDAP schema so theyare expanded during the conversion.sudoersTraditional sudoers format.A new sudoers filewill be reconstructed from the parsed input file.Comments are not preserved and data from anyinclude files will be output inline.--group-file=fileWhen the -M option is also specified, perform group queriesusing file instead of the system group database.-h, --helpDisplay a short help message to the standard output andexit.-i input_format, --input-format=input_formatSpecify the input format.The following formats aresupported:LDIFLDIF (LDAP Data Interchange Format) files can beexported from an LDAP server to convert securitypolicies used by sudoers.ldap(5).If a base DN(distinguished name) is specified, only sudoRoleobjects that match the base DN will be processed.Not all sudoOptions specified in a sudoRole can betranslated from LDIF to sudoers format.sudoersTraditional sudoers format.This is the defaultinput format.-I increment, --increment=incrementWhen generating LDIF output, increment each sudoOrderattribute by the specified number.Defaults to anincrement of 1.-l log_file, --logfile=log_fileLog conversion warnings to log_file instead of to thestandard error.This is particularly useful when mergingmultiple sudoers files, which can generate a large numberof warnings.-m filter, --match=filterOnly output rules that match the specified filter.Afilter expression is made up of one or more key = valuepairs, separated by a comma (\u2018,\u2019).The key may be \u201ccmnd\u201d(or \u201ccmd\u201d), \u201chost\u201d, \u201cgroup\u201d, or \u201cuser\u201d.For example, user= operator or host = www.An upper-case Cmnd_Alias,Host_alias, or User_Alias may be specified as the \u201ccmnd\u201d,\u201chost\u201d, or \u201cuser\u201d.A matching sudoers rule may also include users, groups, andhosts that are not part of the filter.This can happenwhen a rule includes multiple users, groups, or hosts.Toprune out any non-matching user, group, or host from therules, the -p option may be used.By default, the password and group databases are notconsulted when matching against the filter so the users andgroups do not need to be present on the local system (seethe -M option).Only aliases that are referenced by thefiltered policy rules will be displayed.-M, --match-localWhen the -m option is also specified, use password andgroup database information when matching users and groupsin the filter.Only users and groups in the filter thatexist on the local system will match, and a user's groupswill automatically be added to the filter.If the -M isnot specified, users and groups in the filter do not needto exist on the local system, but all groups used formatching must be explicitly listed in the filter.-o output_file, --output=output_fileWrite the converted output to output_file.If nooutput_file is specified, or if it is \u2018-\u2019, the convertedsudoers policy will be written to the standard output.-O start_point, --order-start=start_pointWhen generating LDIF output, use the number specified bystart_point in the sudoOrder attribute of the firstsudoRole object.Subsequent sudoRole object use asudoOrder value generated by adding an increment, see the-I option for details.Defaults to a starting point of 1.A starting point of 0 will disable the generation ofsudoOrder attributes in the resulting LDIF file.--passwd-file=fileWhen the -M option is also specified, perform passwdqueries using file instead of the system passwd database.-p, --prune-matchesWhen the -m option is also specified, cvtsudoers will pruneout non-matching users, groups, and hosts from matchingentries.-P padding, --padding=paddingWhen generating LDIF output, construct the initialsudoOrder value by concatenating order_start and increment,padding the increment with zeros until it consists ofpadding digits.For example, if order_start is 1027,padding is 3, and increment is 1, the value of sudoOrderfor the first entry will be 1027000, followed by 1027001,1027002, etc.If the number of sudoRole entries is largerthan the padding would allow, cvtsudoers will exit with anerror.By default, no padding is performed.-s sections, --suppress=sectionsSuppress the output of specific sections of the securitypolicy.One or more section names may be specified,separated by a comma (\u2018,\u2019).The supported section nameare: defaults, aliases and privileges (which may beshortened to privs).-V, --versionPrint the cvtsudoers and sudoers grammar versions and exit.Merging multiple filesWhen multiple input files are specified, cvtsudoers will attempt tomerge them into a single policy file.It is assumed that user andgroup names are consistent among the policy files to be merged.For example, user \u201cbob\u201d on one host is the same as user \u201cbob\u201d onanother host.When merging policy files, it is possible to prefix the input filename with a host name, separated by a colon (\u2018:\u2019).When the filesare merged, the host name will be used to restrict the policy rulesto that specific host where possible.The merging process is performed as follows:\u2022Each input file is parsed into internal sudoers data structures.\u2022Aliases are merged and renamed as necessary to avoid conflicts.In the event of a conflict, the first alias found is left as-isand subsequent aliases of the same name are renamed with anumeric suffix separated with a underscore (\u2018_\u2019).For example,if there are two different aliases named SERVERS, the first willbe left as-is and the second will be renamed SERVERS_1.References to the renamed alias are also updated in the policyfile.Duplicate aliases (those with identical contents) arepruned.\u2022Defaults settings are merged and duplicates are removed.Ifthere are conflicts in the Defaults settings, a warning isemitted for each conflict.If a host name is specified with theinput file, cvtsudoers will change the global Defaults settingsin that file to be host-specific.A warning is emitted forcommand, user, or runas-specific Defaults settings which cannotbe made host-specific.\u2022Per-user rules are merged and duplicates are removed.If a hostname is specified with the input file, cvtsudoers will changerules that specify a host name of ALL to the host nameassociated with the policy file being merged.The merging ofrules is currently fairly simplistic but will be improved in alater release.It is possible to merge policy files with differing formats.The cvtsudoers.conf fileOptions in the form \u201ckeyword = value\u201d may also be specified in aconfiguration file, /etc/cvtsudoers.conf by default.The followingkeywords are recognized:defaults = deftypesSee the description of the -d command line option.expand_aliases = yes | noSee the description of the -e command line option.group_file = fileSee the description of the --group-file command line option.input_format = ldif | sudoersSee the description of the -i command line option.match = filterSee the description of the -m command line option.match_local = yes | noSee the description of the -M command line option.order_increment = incrementSee the description of the -I command line option.order_start = start_pointSee the description of the -O command line option.output_format = csv | json | ldif | sudoersSee the description of the -f command line option.padding = paddingSee the description of the -P command line option.passwd_file = fileSee the description of the --passwd-file command line option.prune_matches = yes | noSee the description of the -p command line option.sudoers_base = dnSee the description of the -b command line option.suppress = sectionsSee the description of the -s command line option.Options on the command line will override values from theconfiguration file.JSON output formatThe sudoers JSON format may contain any of the following top-levelobjects:DefaultsAn array of objects, each containing an Options array and anoptional Binding array.The Options array consists of one or more objects, eachcontaining a \u201cname:value\u201d pair that corresponds to a sudoersDefaults setting.Options that operate on a list will alsoinclude an operation entry in the object, with a value of\u201clist_assign\u201d for \u2018=\u2019, \u201clist_add\u201d for \u2018+=\u2019, or \u201clist_remove\u201dfor \u2018-=\u2019.The optional Binding array consists of one or more objects,each containing a \u201cname:value\u201d pair and an optional negatedentry, which will negate any comparison performed with theobject.If a Binding is present, the setting will only takeeffect if one of the specified command, hostname, netgroup,networkaddr, nonunixgid, nonunixgroup, usergid, usergroup,userid, username, or alias entries match.For example, the following sudoers entry:Defaults@somehost set_home, env_keep += DISPLAYconverts to:\"Defaults\": [{\"Binding\": [{ \"hostname\": \"somehost\" }],\"Options\": [{ \"set_home\": true },{\"operation\": \"list_add\",\"env_keep\": [\"DISPLAY\"]}]}]User_AliasesA JSON object containing one or more sudoers User_Aliasentries where each named alias has as its value an arraycontaining one or more objects.Each object contains a\u201cname:value\u201d pair and an optional negated entry, which willnegate any comparison performed with the object.The namemay be one of netgroup, nonunixgid, nonunixgroup, useralias,usergid, usergroup, userid, or username.For example, the following sudoers entry:User_Alias SYSADMIN = will, %wheel, +adminconverts to:\"User_Aliases\": {\"SYSADMIN\": [{ \"username\": \"will\" },{ \"usergroup\": \"wheel\" },{ \"netgroup\": \"admin\" }]}Runas_AliasesA JSON object containing one or more sudoers Runas_Aliasentries, where each named alias has as its value an arraycontaining one or more objects.Each object contains a\u201cname:value\u201d pair and an optional negated entry, which willnegate any comparison performed with the object.The namemay be one of netgroup, nonunixgid, nonunixgroup, runasalias,usergid, usergroup, userid, or username.For example, the following sudoers entry:Runas_Alias DB = oracle, sybase : OP = root, operatorconverts to:\"Runas_Aliases\": {\"DB\": [{ \"username\": \"oracle\" },{ \"username\": \"sybase\" }],\"OP\": [{ \"username\": \"root\" },{ \"username\": \"operator\" }]}Host_AliasesA JSON object containing one or more sudoers Host_Aliasentries where each named alias has as its value an arraycontaining one or more objects.Each object contains a\u201cname:value\u201d pair and an optional negated entry, which willnegate any comparison performed with the object.The namemay be one of hostalias, hostname, netgroup, or networkaddr.For example, the following sudoers entries:Host_Alias DORMNET = 128.138.243.0, 128.138.204.0/24Host_Alias SERVERS = boulder, refugeconvert to:\"Host_Aliases\": {\"DORMNET\": [{ \"networkaddr\": \"128.138.243.0\" },{ \"networkaddr\": \"128.138.204.0/24\" }],\"SERVERS\": [{ \"hostname\": \"boulder\" },{ \"hostname\": \"refuge\" }]}Cmnd_AliasesA JSON object containing one or more sudoers Cmnd_Aliasentries where each named alias has as its value an arraycontaining one or more objects.Each object contains a\u201cname:value\u201d pair and an optional negated entry, which willnegate any comparison performed with the object.The namemay be either another cmndalias or a command.For example,the following sudoers entries:Cmnd_Alias SHELLS = /bin/bash, /bin/csh, /bin/sh, /bin/zshCmnd_Alias VIPW = /usr/bin/chpass, /usr/bin/chfn, /usr/bin/chsh, \\/usr/bin/passwd, /usr/sbin/vigr, /usr/sbin/vipwconvert to:\"Cmnd_Aliases\": {\"SHELLS\": [{ \"command\": \"/bin/bash\" },{ \"command\": \"/bin/csh\" },{ \"command\": \"/bin/sh\" },{ \"command\": \"/bin/zsh\" }],\"VIPW\": [{ \"command\": \"/usr/bin/chpass\" },{ \"command\": \"/usr/bin/chfn\" },{ \"command\": \"/usr/bin/chsh\" },{ \"command\": \"/usr/bin/passwd\" },{ \"command\": \"/usr/sbin/vigr\" },{ \"command\": \"/usr/sbin/vipw\" }]}User_SpecsA JSON array containing one or more objects, eachrepresenting a sudoers User_Spec.Each object in theUser_Specs array should contain a User_List array, aHost_List array and a Cmnd_Specs array.A User_List consists of one or more objects.Each objectcontains a \u201cname:value\u201d pair and an optional negated entry,which will negate any comparison performed with the object.The name may be one of netgroup, nonunixgid, nonunixgroup,useralias, usergid, usergroup, userid, or username.Ifusername is set to the special value ALL, it will match anyuser.A Host_List consists of one or more objects.Each objectcontains a \u201cname:value\u201d pair and an optional negated entry,which will negate any comparison performed with the object.The name may be one of hostalias, hostname, netgroup, ornetworkaddr.If hostname is set to the special value ALL, itwill match any host.The Cmnd_Specs array consists of one or more JSON objectsdescribing a command that may be run.Each Cmnd_Specs ismade up of a Commands array, an optional runasusers array, anoptional runasgroups array, and an optional Options array.The Commands array consists of one or more objects containing\u201cname:value\u201d pair elements.The following names and valuesare supported:commandA string containing the command to run.The specialvalue ALL it will match any command.negatedA boolean value that, if true, will negate anycomparison performed with the object.sha224A string containing the SHA224 digest of thecommand.sha256A string containing the SHA256 digest of thecommand.sha384A string containing the SHA384 digest of thecommand.sha512A string containing the SHA512 digest of thecommand.The runasusers array consists of objects describing users thecommand may be run as.Each object contains a \u201cname:value\u201dpair and an optional negated entry, which will negate anycomparison performed with the object.The name may be one ofnetgroup, nonunixgid, nonunixgroup, runasalias, usergid,usergroup, userid, or username.If username is set to thespecial value ALL, it will match any user.If username isset to the empty string \u201c\u201d, it will match the invoking user.The runasgroups array consists of objects describing groupsthe command may be run as.Each object contains a\u201cname:value\u201d pair and an optional negated entry, which willnegate any comparison performed with the object.The namemay be one of runasalias, usergid, or usergroup.Ifusergroup is set to the special value ALL, it will match anygroup.The Options array is of the same format as the one in theDefaults object.Any Tag_Spec entries in sudoers areconverted to Options.A user with \u201csudo ALL\u201d privileges willautomatically have the setenv option enabled to match theimplicit behavior provided by sudoers.For example, the following sudoers entry:millert ALL = (ALL : ALL) NOPASSWD: ALL, !/usr/bin/idconverts to:\"User_Specs\": [{\"User_List\": [{ \"username\": \"millert\" }],\"Host_List\": [{ \"hostname\": \"ALL\" }],\"Cmnd_Specs\": [{\"runasusers\": [{ \"username\": \"ALL\" }],\"runasgroups\": [{ \"usergroup\": \"ALL\" }],\"Options\": [{ \"authenticate\": false },{ \"setenv\": true }],\"Commands\": [{ \"command\": \"ALL\" },{\"command\": \"/usr/bin/id\",\"negated\": true}]}]}]CSV output formatCSV (comma-separated value) files are often used by spreadsheetsand report generators.For CSV output, cvtsudoers double quotesstrings that contain commas.For each literal double quotecharacter present inside the string, two double quotes are output.This method of quoting commas is compatible with most spreadsheetprograms.There are three possible sections in cvtsudoers's CSV output, eachseparated by a blank line:defaultsThis section includes any Defaults settings in sudoers.Thedefaults section begins with the following heading:defaults_type,binding,name,operator,valueThe fields are as follows:defaults_typeThe type of Defaults setting; one of defaults,defaults_command, defaults_host, defaults_runas, ordefaults_user.bindingFor defaults_command, defaults_host, defaults_runas,and defaults_user this is the value that must match forthe setting to be applied.nameThe name of the Defaults setting.operatorThe operator determines how the value is applied to thesetting.It may be either \u2018=\u2019 (assignment), \u2018+=\u2019(append), or \u2018-=\u2019 (remove).valueThe setting's value, usually a string or, for settingsused in a boolean context, true or false.aliasesThis section includes any Cmnd_Alias Host_Alias, Runas_Alias,or User_Alias, entries from sudoers.The aliases sectionbegins with the following heading:alias_type,alias_name,membersThe fields are as follows:alias_typeThe type of alias; one of Cmnd_Alias, Host_Alias,Runas_Alias, or User_Alias.alias_nameThe name of the alias; a string starting with an upper-case letter that consists of upper-case letters,digits, or underscores.membersA comma-separated list of members belonging to thealias.Due to the use of commas, members is surroundedby double quotes if it contains more than one member.rulesThis section includes the sudoers rules that grantprivileges.The rules section begins with the followingheading:rule,user,host,runusers,rungroups,options,commandThe fields are as follows:ruleThis field indicates a sudoers rule entry.userThe user the rule applies to.This may also be a Unixgroup (preceded by a \u2018%\u2019 character), a non-Unix group(preceded by \u2018%:\u2019) or a netgroup (preceded by a \u2018+\u2019character) or a User_Alias.If set to the specialvalue ALL, it will match any user.hostThe host the rule applies to.This may also be anetgroup (preceded by a \u2018+\u2019 character) or a Host_Alias.If set to the special value ALL, it will match anyhost.runusersAn optional comma-separated list of users (orRunas_Aliases) the command may be run as.If itcontains more than one member, the value is surroundedby double quotes.If set to the special value ALL, itwill match any user.If empty, the root user isassumed.rungroupsAn optional comma-separated list of groups (orRunas_Aliases) the command may be run as.If itcontains more than one member, the value is surroundedby double quotes.If set to the special value ALL, itwill match any group.If empty, the runuser's group isused.optionsAn optional list of Defaults settings to apply to thecommand.Any Tag_Spec entries in sudoers are convertedto options.commandsA list of commands, with optional arguments, that theuser is allowed to run.If set to the special valueALL, it will match any command.For example, the following sudoers entry:millert ALL = (ALL : ALL) NOPASSWD: ALL, !/usr/bin/idconverts to:rule,millert,ALL,ALL,ALL,\"!authenticate\",\"ALL,!/usr/bin/id\"",
        "name": "cvtsudoers \u2014 convert between sudoers file formats",
        "section": 1
    },
    {
        "command": "danetool",
        "description": "Tool to generate and check DNS resource records for the DANEprotocol.",
        "name": "danetool - GnuTLS DANE tool",
        "section": 1
    },
    {
        "command": "dash",
        "description": "dash is the standard command interpreter for the system.Thecurrent version of dash is in the process of being changed toconform with the POSIX 1003.2 and 1003.2a specifications for theshell.This version has many features which make it appear similarin some respects to the Korn shell, but it is not a Korn shellclone (see ksh(1)).Only features designated by POSIX, plus a fewBerkeley extensions, are being incorporated into this shell.Thisman page is not intended to be a tutorial or a completespecification of the shell.OverviewThe shell is a command that reads lines from either a file or theterminal, interprets them, and generally executes other commands.It is the program that is running when a user logs into the system(although a user can select a different shell with the chsh(1)command).The shell implements a language that has flow controlconstructs, a macro facility that provides a variety of features inaddition to data storage, along with built in history and lineediting capabilities.It incorporates many features to aidinteractive use and has the advantage that the interpretativelanguage is common to both interactive and non-interactive use(shell scripts).That is, commands can be typed directly to therunning shell or can be put into a file and the file can beexecuted directly by the shell.InvocationIf no args are present and if the standard input of the shell isconnected to a terminal (or if the -i flag is set), and the -coption is not present, the shell is considered an interactiveshell.An interactive shell generally prompts before each commandand handles programming and command errors differently (asdescribed below).When first starting, the shell inspects argument0, and if it begins with a dash \u2018-\u2019, the shell is also considered alogin shell.This is normally done automatically by the systemwhen the user first logs in.A login shell first reads commandsfrom the files /etc/profile and .profile if they exist.If theenvironment variable ENV is set on entry to an interactive shell,or is set in the .profile of a login shell, the shell next readscommands from the file named in ENV.Therefore, a user shouldplace commands that are to be executed only at login time in the.profile file, and commands that are executed for every interactiveshell inside the ENV file.To set the ENV variable to some file,place the following line in your .profile of your home directoryENV=$HOME/.shinit; export ENVsubstituting for \u201c.shinit\u201d any filename you wish.If command line arguments besides the options have been specified,then the shell treats the first argument as the name of a file fromwhich to read commands (a shell script), and the remainingarguments are set as the positional parameters of the shell ($1,$2, etc).Otherwise, the shell reads commands from its standardinput.Argument List ProcessingAll of the single letter options that have a corresponding name canbe used as an argument to the -o option.The set -o name isprovided next to the single letter option in the description below.Specifying a dash \u201c-\u201d turns the option on, while using a plus \u201c+\u201ddisables the option.The following options can be set from thecommand line or with the set builtin (described later).-a allexportExport all variables assigned to.-cRead commands from the command_stringoperand instead of from the standard input.Special parameter 0 will be set from thecommand_name operand and the positionalparameters ($1, $2, etc.)set from theremaining argument operands.-C noclobberDon't overwrite existing files with \u201c>\u201d.-e errexitIf not interactive, exit immediately if anyuntested command fails.The exit status ofa command is considered to be explicitlytested if the command is used to control anif, elif, while, or until; or if the commandis the left hand operand of an \u201c&&\u201d or \u201c||\u201doperator.-f noglobDisable pathname expansion.-n noexecIf not interactive, read commands but do notexecute them.This is useful for checkingthe syntax of shell scripts.-u nounsetWrite a message to standard error whenattempting to expand a variable that is notset, and if the shell is not interactive,exit immediately.-v verboseThe shell writes its input to standard erroras it is read.Useful for debugging.-x xtraceWrite each command to standard error(preceded by a \u2018+ \u2019) before it is executed.Useful for debugging.-I ignoreeofIgnore EOF's from input when interactive.-i interactiveForce the shell to behave interactively.-lMake dash act as if it had been invoked as alogin shell.-m monitorTurn on job control (set automatically wheninteractive).-s stdinRead commands from standard input (setautomatically if no file arguments arepresent).This option has no effect whenset after the shell has already startedrunning (i.e. with set).-V viEnable the built-in vi(1) command lineeditor (disables -E if it has been set).-E emacsEnable the built-in emacs(1) command lineeditor (disables -V if it has been set).-b notifyEnable asynchronous notification ofbackground job completion.(UNIMPLEMENTEDfor 4.4alpha)Lexical StructureThe shell reads input in terms of lines from a file and breaks itup into words at whitespace (blanks and tabs), and at certainsequences of characters that are special to the shell called\u201coperators\u201d.There are two types of operators: control operatorsand redirection operators (their meaning is discussed later).Following is a list of operators:Control operators:& && ( ) ; ;; | || <newline>Redirection operators:< > >| << >> <& >& <<- <>QuotingQuoting is used to remove the special meaning of certain charactersor words to the shell, such as operators, whitespace, or keywords.There are three types of quoting: matched single quotes, matcheddouble quotes, and backslash.BackslashA backslash preserves the literal meaning of the followingcharacter, with the exception of \u27e8newline\u27e9.A backslash precedinga \u27e8newline\u27e9 is treated as a line continuation.Single QuotesEnclosing characters in single quotes preserves the literal meaningof all the characters (except single quotes, making it impossibleto put single-quotes in a single-quoted string).Double QuotesEnclosing characters within double quotes preserves the literalmeaning of all characters except dollarsign ($), backquote (`), andbackslash (\\).The backslash inside double quotes is historicallyweird, and serves to quote only the following characters:$ ` \" \\ <newline>.Otherwise it remains literal.Reserved WordsReserved words are words that have special meaning to the shell andare recognized at the beginning of a line and after a controloperator.The following are reserved words:!eliffiwhilecaseelseforthen{}dodoneuntilifesacTheir meaning is discussed later.AliasesAn alias is a name and corresponding value set using the alias(1)builtin command.Whenever a reserved word may occur (see above),and after checking for reserved words, the shell checks the word tosee if it matches an alias.If it does, it replaces it in theinput stream with its value.For example, if there is an aliascalled \u201clf\u201d with the value \u201cls -F\u201d, then the input:lf foobar \u27e8return\u27e9would becomels -F foobar \u27e8return\u27e9Aliases provide a convenient way for naive users to createshorthands for commands without having to learn how to createfunctions with arguments.They can also be used to createlexically obscure code.This use is discouraged.CommandsThe shell interprets the words it reads according to a language,the specification of which is outside the scope of this man page(refer to the BNF in the POSIX 1003.2 document).Essentiallythough, a line is read and if the first word of the line (or aftera control operator) is not a reserved word, then the shell hasrecognized a simple command.Otherwise, a complex command or someother special construct may have been recognized.Simple CommandsIf a simple command has been recognized, the shell performs thefollowing actions:1.Leading words of the form \u201cname=value\u201d are stripped offand assigned to the environment of the simple command.Redirection operators and their arguments (as describedbelow) are stripped off and saved for processing.2.The remaining words are expanded as described in thesection called \u201cExpansions\u201d, and the first remainingword is considered the command name and the command islocated.The remaining words are considered thearguments of the command.If no command name resulted,then the \u201cname=value\u201d variable assignments recognized initem 1 affect the current shell.3.Redirections are performed as described in the nextsection.RedirectionsRedirections are used to change where a command reads its input orsends its output.In general, redirections open, close, orduplicate an existing reference to a file.The overall format usedfor redirection is:[n] redir-op filewhere redir-op is one of the redirection operators mentionedpreviously.Following is a list of the possible redirections.The[n] is an optional number between 0 and 9, as in \u20183\u2019 (not \u2018[3]\u2019),that refers to a file descriptor.[n]> fileRedirect standard output (or n) to file.[n]>| fileSame, but override the -C option.[n]>> fileAppend standard output (or n) to file.[n]< fileRedirect standard input (or n) from file.[n1]<&n2Copy file descriptor n2 as stdout (or fd n1).fdn2.[n]<&-Close standard input (or n).[n1]>&n2Copy file descriptor n2 as stdin (or fd n1).fdn2.[n]>&-Close standard output (or n).[n]<> fileOpen file for reading and writing on standardinput (or n).The following redirection is often called a \u201chere-document\u201d.[n]<< delimiterhere-doc-text ...delimiterAll the text on successive lines up to the delimiter is saved awayand made available to the command on standard input, or filedescriptor n if it is specified.If the delimiter as specified onthe initial line is quoted, then the here-doc-text is treatedliterally, otherwise the text is subjected to parameter expansion,command substitution, and arithmetic expansion (as described in thesection on \u201cExpansions\u201d).If the operator is \u201c<<-\u201d instead of\u201c<<\u201d, then leading tabs in the here-doc-text are stripped.Search and ExecutionThere are three types of commands: shell functions, builtincommands, and normal programs \u2013 and the command is searched for (byname) in that order.They each are executed in a different way.When a shell function is executed, all of the shell positionalparameters (except $0, which remains unchanged) are set to thearguments of the shell function.The variables which areexplicitly placed in the environment of the command (by placingassignments to them before the function name) are made local to thefunction and are set to the values given.Then the command givenin the function definition is executed.The positional parametersare restored to their original values when the command completes.This all occurs within the current shell.Shell builtins are executed internally to the shell, withoutspawning a new process.Otherwise, if the command name doesn't match a function or builtin,the command is searched for as a normal program in the file system(as described in the next section).When a normal program isexecuted, the shell runs the program, passing the arguments and theenvironment to the program.If the program is not a normalexecutable file (i.e., if it does not begin with the \"magic number\"whose ASCII representation is \"#!\", so execve(2) returns ENOEXECthen) the shell will interpret the program in a subshell.Thechild shell will reinitialize itself in this case, so that theeffect will be as if a new shell had been invoked to handle the ad-hoc shell script, except that the location of hashed commandslocated in the parent shell will be remembered by the child.Note that previous versions of this document and the source codeitself misleadingly and sporadically refer to a shell scriptwithout a magic number as a \"shell procedure\".Path SearchWhen locating a command, the shell first looks to see if it has ashell function by that name.Then it looks for a builtin commandby that name.If a builtin command is not found, one of two thingshappen:1.Command names containing a slash are simply executed withoutperforming any searches.2.The shell searches each entry in PATH in turn for the command.The value of the PATH variable should be a series of entriesseparated by colons.Each entry consists of a directory name.The current directory may be indicated implicitly by an emptydirectory name, or explicitly by a single period.Command Exit StatusEach command has an exit status that can influence the behaviour ofother shell commands.The paradigm is that a command exits withzero for normal or success, and non-zero for failure, error, or afalse indication.The man page for each command should indicatethe various exit codes and what they mean.Additionally, thebuiltin commands return exit codes, as does an executed shellfunction.If a command consists entirely of variable assignments then theexit status of the command is that of the last command substitutionif any, otherwise 0.Complex CommandsComplex commands are combinations of simple commands with controloperators or reserved words, together creating a larger complexcommand.More generally, a command is one of the following:\u2022simple command\u2022pipeline\u2022list or compound-list\u2022compound command\u2022function definitionUnless otherwise stated, the exit status of a command is that ofthe last simple command executed by the command.PipelinesA pipeline is a sequence of one or more commands separated by thecontrol operator |.The standard output of all but the lastcommand is connected to the standard input of the next command.The standard output of the last command is inherited from theshell, as usual.The format for a pipeline is:[!] command1 [| command2 ...]The standard output of command1 is connected to the standard inputof command2.The standard input, standard output, or both of acommand is considered to be assigned by the pipeline before anyredirection specified by redirection operators that are part of thecommand.If the pipeline is not in the background (discussed later), theshell waits for all commands to complete.If the reserved word ! does not precede the pipeline, the exitstatus is the exit status of the last command specified in thepipeline.Otherwise, the exit status is the logical NOT of theexit status of the last command.That is, if the last commandreturns zero, the exit status is 1; if the last command returnsgreater than zero, the exit status is zero.Because pipeline assignment of standard input or standard output orboth takes place before redirection, it can be modified byredirection.For example:$ command1 2>&1 | command2sends both the standard output and standard error of command1 tothe standard input of command2.A ; or \u27e8newline\u27e9 terminator causes the preceding AND-OR-list(described next) to be executed sequentially; a & causesasynchronous execution of the preceding AND-OR-list.Note that unlike some other shells, each process in the pipeline isa child of the invoking shell (unless it is a shell builtin, inwhich case it executes in the current shell \u2013 but any effect it hason the environment is wiped).Background Commands \u2013 &If a command is terminated by the control operator ampersand (&),the shell executes the command asynchronously \u2013 that is, the shelldoes not wait for the command to finish before executing the nextcommand.The format for running a command in background is:command1 & [command2 & ...]If the shell is not interactive, the standard input of anasynchronous command is set to /dev/null.Lists \u2013 Generally SpeakingA list is a sequence of zero or more commands separated bynewlines, semicolons, or ampersands, and optionally terminated byone of these three characters.The commands in a list are executedin the order they are written.If command is followed by anampersand, the shell starts the command and immediately proceedsonto the next command; otherwise it waits for the command toterminate before proceeding to the next one.Short-Circuit List Operators\u201c&&\u201d and \u201c||\u201d are AND-OR list operators.\u201c&&\u201d executes the firstcommand, and then executes the second command if and only if theexit status of the first command is zero.\u201c||\u201d is similar, butexecutes the second command if and only if the exit status of thefirst command is nonzero.\u201c&&\u201d and \u201c||\u201d both have the samepriority.Flow-Control Constructs \u2013 if, while, for, caseThe syntax of the if command isif listthen list[ elif listthenlist ] ...[ else list ]fiThe syntax of the while command iswhile listdolistdoneThe two lists are executed repeatedly while the exit status of thefirst list is zero.The until command is similar, but has the worduntil in place of while, which causes it to repeat until the exitstatus of the first list is zero.The syntax of the for command isfor variable [ in [ word ... ] ]dolistdoneThe words following in are expanded, and then the list is executedrepeatedly with the variable set to each word in turn.Omitting inword ... is equivalent to in \"$@\".The syntax of the break and continue command isbreak [ num ]continue [ num ]Break terminates the num innermost for or while loops.Continuecontinues with the next iteration of the innermost loop.These areimplemented as builtin commands.The syntax of the case command iscase word in[(]pattern) list ;;...esacThe pattern can actually be one or more patterns (see ShellPatterns described later), separated by \u201c|\u201d characters.The \u201c(\u201dcharacter before the pattern is optional.Grouping Commands TogetherCommands may be grouped by writing either(list)or{ list; }The first of these executes the commands in a subshell.Builtincommands grouped into a (list) will not affect the current shell.The second form does not fork another shell so is slightly moreefficient.Grouping commands together this way allows you toredirect their output as though they were one program:{ printf \" hello \" ; printf \" world\\n\" ; } > greetingNote that \u201c}\u201d must follow a control operator (here, \u201c;\u201d) so that itis recognized as a reserved word and not as another commandargument.FunctionsThe syntax of a function definition isname () commandA function definition is an executable statement; when executed itinstalls a function named name and returns an exit status of zero.The command is normally a list enclosed between \u201c{\u201d and \u201c}\u201d.Variables may be declared to be local to a function by using alocal command.This should appear as the first statement of afunction, and the syntax islocal [variable | -] ...Local is implemented as a builtin command.When a variable is made local, it inherits the initial value andexported and readonly flags from the variable with the same name inthe surrounding scope, if there is one.Otherwise, the variable isinitially unset.The shell uses dynamic scoping, so that if youmake the variable x local to function f, which then calls functiong, references to the variable x made inside g will refer to thevariable x declared inside f, not to the global variable named x.The only special parameter that can be made local is \u201c-\u201d.Making\u201c-\u201d local any shell options that are changed via the set commandinside the function to be restored to their original values whenthe function returns.The syntax of the return command isreturn [exitstatus]It terminates the currently executing function.Return isimplemented as a builtin command.Variables and ParametersThe shell maintains a set of parameters.A parameter denoted by aname is called a variable.When starting up, the shell turns allthe environment variables into shell variables.New variables canbe set using the formname=valueVariables set by the user must have a name consisting solely ofalphabetics, numerics, and underscores - the first of which mustnot be numeric.A parameter can also be denoted by a number or aspecial character as explained below.Positional ParametersA positional parameter is a parameter denoted by a number (n > 0).The shell sets these initially to the values of its command linearguments that follow the name of the shell script.The setbuiltin can also be used to set or reset them.Special ParametersA special parameter is a parameter denoted by one of the followingspecial characters.The value of the parameter is listed next toits character.*Expands to the positional parameters, starting fromone.When the expansion occurs within a double-quotedstring it expands to a single field with the value ofeach parameter separated by the first character of theIFS variable, or by a \u27e8space\u27e9 if IFS is unset.@Expands to the positional parameters, starting fromone.When the expansion occurs within double-quotes,each positional parameter expands as a separateargument.If there are no positional parameters, theexpansion of @ generates zero arguments, even when @is double-quoted.What this basically means, forexample, is if $1 is \u201cabc\u201d and $2 is \u201cdef ghi\u201d, then\"$@\" expands to the two arguments:\"abc\" \"def ghi\"#Expands to the number of positional parameters.?Expands to the exit status of the most recentpipeline.- (Hyphen.)Expands to the current option flags (the single-letteroption names concatenated into a string) as specifiedon invocation, by the set builtin command, orimplicitly by the shell.$Expands to the process ID of the invoked shell.Asubshell retains the same value of $ as its parent.!Expands to the process ID of the most recentbackground command executed from the current shell.For a pipeline, the process ID is that of the lastcommand in the pipeline.0 (Zero.)Expands to the name of the shell or shell script.Word ExpansionsThis clause describes the various expansions that are performed onwords.Not all expansions are performed on every word, asexplained later.Tilde expansions, parameter expansions, command substitutions,arithmetic expansions, and quote removals that occur within asingle word expand to a single field.It is only field splittingor pathname expansion that can create multiple fields from a singleword.The single exception to this rule is the expansion of thespecial parameter @ within double-quotes, as was described above.The order of word expansion is:1.Tilde Expansion, Parameter Expansion, Command Substitution,Arithmetic Expansion (these all occur at the same time).2.Field Splitting is performed on fields generated by step (1)unless the IFS variable is null.3.Pathname Expansion (unless set -f is in effect).4.Quote Removal.The $ character is used to introduce parameter expansion, commandsubstitution, or arithmetic evaluation.Tilde Expansion (substituting a user's home directory)A word beginning with an unquoted tilde character (~) is subjectedto tilde expansion.All the characters up to a slash (/) or theend of the word are treated as a username and are replaced with theuser's home directory.If the username is missing (as in~/foobar), the tilde is replaced with the value of the HOMEvariable (the current user's home directory).Parameter ExpansionThe format for parameter expansion is as follows:${expression}where expression consists of all characters until the matching \u201c}\u201d.Any \u201c}\u201d escaped by a backslash or within a quoted string, andcharacters in embedded arithmetic expansions, commandsubstitutions, and variable expansions, are not examined indetermining the matching \u201c}\u201d.The simplest form for parameter expansion is:${parameter}The value, if any, of parameter is substituted.The parameter name or symbol can be enclosed in braces, which areoptional except for positional parameters with more than one digitor when parameter is followed by a character that could beinterpreted as part of the name.If a parameter expansion occursinside double-quotes:1.Pathname expansion is not performed on the results of theexpansion.2.Field splitting is not performed on the results of theexpansion, with the exception of @.In addition, a parameter expansion can be modified by using one ofthe following formats.${parameter:-word}Use Default Values.If parameter is unset ornull, the expansion of word is substituted;otherwise, the value of parameter issubstituted.${parameter:=word}Assign Default Values.If parameter is unsetor null, the expansion of word is assigned toparameter.In all cases, the final value ofparameter is substituted.Only variables,not positional parameters or specialparameters, can be assigned in this way.${parameter:?[word]}Indicate Error if Null or Unset.Ifparameter is unset or null, the expansion ofword (or a message indicating it is unset ifword is omitted) is written to standard errorand the shell exits with a nonzero exitstatus.Otherwise, the value of parameter issubstituted.An interactive shell need notexit.${parameter:+word}Use Alternative Value.If parameter is unsetor null, null is substituted; otherwise, theexpansion of word is substituted.In the parameter expansions shown previously, use of the colon inthe format results in a test for a parameter that is unset or null;omission of the colon results in a test for a parameter that isonly unset.${#parameter}String Length.The length in characters ofthe value of parameter.The following four varieties of parameter expansion provide forsubstring processing.In each case, pattern matching notation (seeShell Patterns), rather than regular expression notation, is usedto evaluate the patterns.If parameter is * or @, the result ofthe expansion is unspecified.Enclosing the full parameterexpansion string in double-quotes does not cause the following fourvarieties of pattern characters to be quoted, whereas quotingcharacters within the braces has this effect.${parameter%word}Remove Smallest Suffix Pattern.The word isexpanded to produce a pattern.The parameterexpansion then results in parameter, with thesmallest portion of the suffix matched by thepattern deleted.${parameter%%word}Remove Largest Suffix Pattern.The word isexpanded to produce a pattern.The parameterexpansion then results in parameter, with thelargest portion of the suffix matched by thepattern deleted.${parameter#word}Remove Smallest Prefix Pattern.The word isexpanded to produce a pattern.The parameterexpansion then results in parameter, with thesmallest portion of the prefix matched by thepattern deleted.${parameter##word}Remove Largest Prefix Pattern.The word isexpanded to produce a pattern.The parameterexpansion then results in parameter, with thelargest portion of the prefix matched by thepattern deleted.Command SubstitutionCommand substitution allows the output of a command to besubstituted in place of the command name itself.Commandsubstitution occurs when the command is enclosed as follows:$(command)or (\u201cbackquoted\u201d version):`command`The shell expands the command substitution by executing command ina subshell environment and replacing the command substitution withthe standard output of the command, removing sequences of one ormore \u27e8newline\u27e9s at the end of the substitution.(Embedded\u27e8newline\u27e9s before the end of the output are not removed; however,during field splitting, they may be translated into \u27e8space\u27e9s,depending on the value of IFS and quoting that is in effect.)Arithmetic ExpansionArithmetic expansion provides a mechanism for evaluating anarithmetic expression and substituting its value.The format forarithmetic expansion is as follows:$((expression))The expression is treated as if it were in double-quotes, exceptthat a double-quote inside the expression is not treated specially.The shell expands all tokens in the expression for parameterexpansion, command substitution, and quote removal.Next, the shell treats this as an arithmetic expression andsubstitutes the value of the expression.White Space Splitting (Field Splitting)After parameter expansion, command substitution, and arithmeticexpansion the shell scans the results of expansions andsubstitutions that did not occur in double-quotes for fieldsplitting and multiple fields can result.The shell treats each character of the IFS as a delimiter and usesthe delimiters to split the results of parameter expansion andcommand substitution into fields.Pathname Expansion (File Name Generation)Unless the -f flag is set, file name generation is performed afterword splitting is complete.Each word is viewed as a series ofpatterns, separated by slashes.The process of expansion replacesthe word with the names of all existing files whose names can beformed by replacing each pattern with a string that matches thespecified pattern.There are two restrictions on this: first, apattern cannot match a string containing a slash, and second, apattern cannot match a string starting with a period unless thefirst character of the pattern is a period.The next sectiondescribes the patterns used for both Pathname Expansion and thecase command.Shell PatternsA pattern consists of normal characters, which match themselves,and meta-characters.The meta-characters are \u201c!\u201d, \u201c*\u201d, \u201c?\u201d, and\u201c[\u201d.These characters lose their special meanings if they arequoted.When command or variable substitution is performed and thedollar sign or back quotes are not double quoted, the value of thevariable or the output of the command is scanned for thesecharacters and they are turned into meta-characters.An asterisk (\u201c*\u201d) matches any string of characters.A questionmark matches any single character.A left bracket (\u201c[\u201d) introducesa character class.The end of the character class is indicated bya (\u201c]\u201d); if the \u201c]\u201d is missing then the \u201c[\u201d matches a \u201c[\u201d ratherthan introducing a character class.A character class matches anyof the characters between the square brackets.A range ofcharacters may be specified using a minus sign.The characterclass may be complemented by making an exclamation point the firstcharacter of the character class.To include a \u201c]\u201d in a character class, make it the first characterlisted (after the \u201c!\u201d, if any).To include a minus sign, make itthe first or last character listed.BuiltinsThis section lists the builtin commands which are builtin becausethey need to perform some operation that can't be performed by aseparate process.In addition to these, there are several othercommands that may be builtin for efficiency (e.g.printf(1),echo(1), test(1), etc).:trueA null command that returns a 0 (true) exit value.falseA null command that returns a 1 (false) exit value.. fileThe commands in the specified file are read and executed bythe shell.alias [name[=string ...]]If name=string is specified, the shell defines the aliasname with value string.If just name is specified, thevalue of the alias name is printed.With no arguments, thealias builtin prints the names and values of all definedaliases (see unalias).bg [job] ...Continue the specified jobs (or the current job if no jobsare given) in the background.command [-p] [-v] [-V] command [arg ...]Execute the specified command but ignore shell functionswhen searching for it.(This is useful when you have ashell function with the same name as a builtin command.)-psearch for command using a PATH that guarantees tofind all the standard utilities.-VDo not execute the command but search for the commandand print the resolution of the command search.Thisis the same as the type builtin.-vDo not execute the command but search for the commandand print the absolute pathname of utilities, thename for builtins or the expansion of aliases.cd|chdir -cd|chdir [-LP] [directory]Switch to the specified directory (default HOME).If anentry for CDPATH appears in the environment of the cdcommand or the shell variable CDPATH is set and thedirectory name does not begin with a slash, then thedirectories listed in CDPATH will be searched for thespecified directory.The format of CDPATH is the same asthat of PATH.If a single dash is specified as theargument, it will be replaced by the value of OLDPWD.Thecd command will print out the name of the directory that itactually switched to if this is different from the name thatthe user gave.These may be different either because theCDPATH mechanism was used or because the argument is asingle dash.The -P option causes the physical directorystructure to be used, that is, all symbolic links areresolved to their respective values.The -L option turnsoff the effect of any preceding -P options.echo [-n] args...Print the arguments on the standard output, separated byspaces.Unless the -n option is present, a newline isoutput following the arguments.If any of the following sequences of characters isencountered during output, the sequence is not output.Instead, the specified action is performed:\\bA backspace character is output.\\cSubsequent output is suppressed.This is normallyused at the end of the last argument to suppress thetrailing newline that echo would otherwise output.\\fOutput a form feed.\\nOutput a newline character.\\rOutput a carriage return.\\tOutput a (horizontal) tab character.\\vOutput a vertical tab.\\0digitsOutput the character whose value is given by zero tothree octal digits.If there are zero digits, a nulcharacter is output.\\\\Output a backslash.All other backslash sequences elicit undefined behaviour.eval string ...Concatenate all the arguments with spaces.Then re-parseand execute the command.exec [command arg ...]Unless command is omitted, the shell process is replacedwith the specified program (which must be a real program,not a shell builtin or function).Any redirections on theexec command are marked as permanent, so that they are notundone when the exec command finishes.exit [exitstatus]Terminate the shell process.If exitstatus is given it isused as the exit status of the shell; otherwise the exitstatus of the preceding command is used.export name ...export -pThe specified names are exported so that they will appear inthe environment of subsequent commands.The only way to un-export a variable is to unset it.The shell allows thevalue of a variable to be set at the same time it isexported by writingexport name=valueWith no arguments the export command lists the names of allexported variables.With the -p option specified the outputwill be formatted suitably for non-interactive use.fc [-e editor] [first [last]]fc -l [-nr] [first [last]]fc -s [old=new] [first]The fc builtin lists, or edits and re-executes, commandspreviously entered to an interactive shell.-e editorUse the editor named by editor to edit the commands.The editor string is a command name, subject tosearch via the PATH variable.The value in theFCEDIT variable is used as a default when -e is notspecified.If FCEDIT is null or unset, the value ofthe EDITOR variable is used.If EDITOR is null orunset, ed(1) is used as the editor.-l (ell)List the commands rather than invoking an editor onthem.The commands are written in the sequenceindicated by the first and last operands, as affectedby -r, with each command preceded by the commandnumber.-nSuppress command numbers when listing with -l.-rReverse the order of the commands listed (with -l) oredited (with neither -l nor -s).-sRe-execute the command without invoking an editor.firstlastSelect the commands to list or edit.The number ofprevious commands that can be accessed are determinedby the value of the HISTSIZE variable.The value offirst or last or both are one of the following:[+]numberA positive number representing a commandnumber; command numbers can be displayed withthe -l option.-numberA negative decimal number representing thecommand that was executed number of commandspreviously.For example, -1 is theimmediately previous command.stringA string indicating the most recently entered commandthat begins with that string.If the old=new operandis not also specified with -s, the string form of thefirst operand cannot contain an embedded equal sign.The following environment variables affect the execution offc:FCEDITName of the editor to use.HISTSIZEThe number of previous commands that areaccessible.fg [job]Move the specified job or the current job to the foreground.getopts optstring var [arg ...]The POSIX getopts command, not to be confused with the BellLabs-derived getopt(1).The first argument should be a series of letters, each ofwhich may be optionally followed by a colon to indicate thatthe option requires an argument.The variable specified isset to the parsed option.The getopts command deprecates the older getopt(1) utilitydue to its handling of arguments containing whitespace.The getopts builtin may be used to obtain options and theirarguments from a list of parameters.When invoked, getoptsplaces the value of the next option from the option stringin the list in the shell variable specified by var and itsindex in the shell variable OPTIND.When the shell isinvoked, OPTIND is initialized to 1.For each option thatrequires an argument, the getopts builtin will place it inthe shell variable OPTARG.If an option is not allowed forin the optstring, then OPTARG will be unset.By default, the variables $1, ..., $n are inspected; if argsare specified, they'll be parsed instead.optstring is a string of recognized option letters (seegetopt(3)).If a letter is followed by a colon, the optionis expected to have an argument which may or may not beseparated from it by white space.If an option character isnot found where expected, getopts will set the variable varto a \u201c?\u201d; getopts will then unset OPTARG and write output tostandard error.By specifying a colon as the firstcharacter of optstring all errors will be ignored.After the last option getopts will return a non-zero valueand set var to \u201c?\u201d.The following code fragment shows how one might process thearguments for a command that can take the options [a] and[b], and the option [c], which requires an argument.while getopts abc: fdocase $f ina | b)flag=$f;;c)carg=$OPTARG;;\\?)echo $USAGE; exit 1;;esacdoneshift $((OPTIND - 1))This code will accept any of the following as equivalent:cmd -acarg file filecmd -a -c arg file filecmd -carg -a file filecmd -a -carg -- file filehash [command ...]hash -rThe shell maintains a hash table which remembers thelocations of commands.With no arguments whatsoever, thehash command prints out the contents of this table.Entrieswhich have not been looked at since the last cd command aremarked with an asterisk; it is possible for these entries tobe invalid.With arguments, the hash command removes the specifiedcommands from the hash table (unless they are functions) andthen locates them.The -r option causes the hash command todelete all the entries in the hash table except forfunctions.jobs [-lp] [job ...]Display the status of all, or just the specified, jobs:By defaultdisplay the job number, currency (+-)status, if any, the job state, and itsshell command.-lalso output the PID of the group leader,and just the PID and shell commands ofother members of the job.-pDisplay only leader PIDs, one per line.kill [-s sigspec | -signum | -sigspec] [pid | job ...]Equivalent to kill(1), but a job spec may also be specified.Signals can be either case-insensitive names without SIGprefixes or decimal numbers; the default is TERM.kill -l [signum | exitstatus]List available signal names without the SIG prefix(sigspecs).If signum specified, display just the sigspecfor that signal.If exitstatus specified (> 128), displayjust the sigspec that caused it.pwd [-LP]builtin command remembers what the current directory israther than recomputing it each time.This makes it faster.However, if the current directory is renamed, the builtinversion of pwd will continue to print the old name for thedirectory.The -P option causes the physical value of thecurrent working directory to be shown, that is, all symboliclinks are resolved to their respective values.The -Loption turns off the effect of any preceding -P options.read [-p prompt] [-r] variable [...]The prompt is printed if the -p option is specified and thestandard input is a terminal.Then a line is read from thestandard input.The trailing newline is deleted from theline and the line is split as described in the section onword splitting above, and the pieces are assigned to thevariables in order.At least one variable must bespecified.If there are more pieces than variables, theremaining pieces (along with the characters in IFS thatseparated them) are assigned to the last variable.If thereare more variables than pieces, the remaining variables areassigned the null string.The read builtin will indicatesuccess unless EOF is encountered on input, in which casefailure is returned.By default, unless the -r option is specified, the backslash\u201c\\\u201d acts as an escape character, causing the followingcharacter to be treated literally.If a backslash isfollowed by a newline, the backslash and the newline will bedeleted.readonly name ...readonly -pThe specified names are marked as read only, so that theycannot be subsequently modified or unset.The shell allowsthe value of a variable to be set at the same time it ismarked read only by writingreadonly name=valueWith no arguments the readonly command lists the names ofall read only variables.With the -p option specified theoutput will be formatted suitably for non-interactive use.printf format [value]...printf formats and prints its arguments according to format,a character string which contains three types of objects:plain characters, which are simply copied to standardoutput, character escape sequences which are converted andcopied to the standard output, and format specifications,each of which causes printing of the next successive value.Each value is treated as a string if the correspondingformat specification is either b, c, or s; otherwise it isevaluated as a C constant, with the following additions:\u2022A leading plus or minus sign is allowed.\u2022If the leading character is a single or doublequote, the value of the next byte.The format string is reused as often as necessary until allvalues are consumed.Any extra format specifications areevaluated with zero or the null string.Character escape sequences are in backslash notation asdefined in ANSI X3.159-1989 (\u201cANSI C89\u201d).The charactersand their meanings are as follows:\\aWrite a <bell> character.\\bWrite a <backspace> character.\\fWrite a <form-feed> character.\\nWrite a <new-line> character.\\rWrite a <carriage return> character.\\tWrite a <tab> character.\\vWrite a <vertical tab> character.\\\\Write a backslash character.\\numWrite an 8-bit character whose ASCII value isthe 1-, 2-, or 3-digit octal number num.Each format specification is introduced by the percentcharacter (``%'').The remainder of the formatspecification includes, in the following order:Zero or more of the following flags:#A `#' character specifying that the valueshould be printed in an ``alternativeform''.For b, c, d, and s formats, thisoption has no effect.For the o format theprecision of the number is increased toforce the first character of the outputstring to a zero.For the x (X) format, anon-zero result has the string 0x (0X)prepended to it.For e, E, f, g, and Gformats, the result will always contain adecimal point, even if no digits follow thepoint (normally, a decimal point onlyappears in the results of those formats if adigit follows the decimal point).For g andG formats, trailing zeros are not removedfrom the result as they would otherwise be.-A minus sign `-' which specifies leftadjustment of the output in the indicatedfield;+A `+' character specifying that there shouldalways be a sign placed before the numberwhen using signed formats.\u2018 \u2019A space specifying that a blank should beleft before a positive number for a signedformat.A `+' overrides a space if both areused;0A zero `0' character indicating that zero-padding should be used rather than blank-padding.A `-' overrides a `0' if both areused;Field Width:An optional digit string specifying a field width;if the output string has fewer characters than thefield width it will be blank-padded on the left (orright, if the left-adjustment indicator has beengiven) to make up the field width (note that aleading zero is a flag, but an embedded zero is partof a field width);Precision:An optional period, \u2018.\u2019, followed by an optionaldigit string giving a precision which specifies thenumber of digits to appear after the decimal point,for e and f formats, or the maximum number of bytesto be printed from a string (b and s formats); ifthe digit string is missing, the precision istreated as zero;Format:A character which indicates the type of format touse (one of diouxXfwEgGbcs).A field width or precision may be \u2018*\u2019 instead of a digitstring.In this case an argument supplies the field widthor precision.The format characters and their meanings are:diouXxThe argument is printed as a signed decimal (dor i), unsigned octal, unsigned decimal, orunsigned hexadecimal (X or x), respectively.fThe argument is printed in the style [-]ddd.dddwhere the number of d's after the decimal pointis equal to the precision specification for theargument.If the precision is missing, 6 digitsare given; if the precision is explicitly 0, nodigits and no decimal point are printed.eEThe argument is printed in the style[-]d.ddde\u00b1dd where there is one digit before thedecimal point and the number after is equal tothe precision specification for the argument;when the precision is missing, 6 digits areproduced.An upper-case E is used for an `E'format.gGThe argument is printed in style f or in style e(E) whichever gives full precision in minimumspace.bCharacters from the string argument are printedwith backslash-escape sequences expanded.The following additional backslash-escapesequences are supported:\\cCauses dash to ignore any remainingcharacters in the string operandcontaining it, any remaining stringoperands, and any additional charactersin the format operand.\\0numWrite an 8-bit character whose ASCIIvalue is the 1-, 2-, or 3-digit octalnumber num.cThe first character of argument is printed.sCharacters from the string argument are printeduntil the end is reached or until the number ofbytes indicated by the precision specificationis reached; if the precision is omitted, allcharacters in the string are printed.%Print a `%'; no argument is used.In no case does a non-existent or small field width causetruncation of a field; padding takes place only if thespecified field width exceeds the actual width.set [{ -options | +options | -- }] arg ...The set command performs three different functions.With no arguments, it lists the values of all shellvariables.If options are given, it sets the specified option flags, orclears them as described in the section called Argument ListProcessing.As a special case, if the option is -o or +oand no argument is supplied, the shell prints the settingsof all its options.If the option is -o, the settings areprinted in a human-readable format; if the option is +o, thesettings are printed in a format suitable for reinput to theshell to affect the same option settings.The third use of the set command is to set the values of theshell's positional parameters to the specified args.Tochange the positional parameters without changing anyoptions, use \u201c--\u201d as the first argument to set.If no argsare present, the set command will clear all the positionalparameters (equivalent to executing \u201cshift $#\u201d.)shift [n]Shift the positional parameters n times.A shift sets thevalue of $1 to the value of $2, the value of $2 to the valueof $3, and so on, decreasing the value of $# by one.If nis greater than the number of positional parameters, shiftwill issue an error message, and exit with return status 2.test expression[ expression ]The test utility evaluates the expression and, if itevaluates to true, returns a zero (true) exit status;otherwise it returns 1 (false).If there is no expression,test also returns 1 (false).All operators and flags are separate arguments to the testutility.The following primaries are used to construct expression:-b fileTrue if file exists and is a block specialfile.-c fileTrue if file exists and is a character specialfile.-d fileTrue if file exists and is a directory.-e fileTrue if file exists (regardless of type).-f fileTrue if file exists and is a regular file.-g fileTrue if file exists and its set group ID flagis set.-h fileTrue if file exists and is a symbolic link.-k fileTrue if file exists and its sticky bit is set.-n stringTrue if the length of string is nonzero.-p fileTrue if file is a named pipe (FIFO).-r fileTrue if file exists and is readable.-s fileTrue if file exists and has a size greaterthan zero.-t file_descriptorTrue if the file whose file descriptor numberis file_descriptor is open and is associatedwith a terminal.-u fileTrue if file exists and its set user ID flagis set.-w fileTrue if file exists and is writable.Trueindicates only that the write flag is on.Thefile is not writable on a read-only filesystem even if this test indicates true.-x fileTrue if file exists and is executable.Trueindicates only that the execute flag is on.If file is a directory, true indicates thatfile can be searched.-z stringTrue if the length of string is zero.-L fileTrue if file exists and is a symbolic link.This operator is retained for compatibilitywith previous versions of this program.Donot rely on its existence; use -h instead.-O fileTrue if file exists and its owner matches theeffective user id of this process.-G fileTrue if file exists and its group matches theeffective group id of this process.-S fileTrue if file exists and is a socket.file1 -nt file2True if file1 and file2 exist and file1 isnewer than file2.file1 -ot file2True if file1 and file2 exist and file1 isolder than file2.file1 -ef file2True if file1 and file2 exist and refer to thesame file.stringTrue if string is not the null string.s1 = s2True if the strings s1 and s2 are identical.s1 != s2True if the strings s1 and s2 are notidentical.s1 < s2True if string s1 comes before s2 based on theASCII value of their characters.s1 > s2True if string s1 comes after s2 based on theASCII value of their characters.n1 -eq n2True if the integers n1 and n2 arealgebraically equal.n1 -ne n2True if the integers n1 and n2 are notalgebraically equal.n1 -gt n2True if the integer n1 is algebraicallygreater than the integer n2.n1 -ge n2True if the integer n1 is algebraicallygreater than or equal to the integer n2.n1 -lt n2True if the integer n1 is algebraically lessthan the integer n2.n1 -le n2True if the integer n1 is algebraically lessthan or equal to the integer n2.These primaries can be combined with the followingoperators:! expressionTrue if expression is false.expression1 -a expression2True if both expression1 and expression2 aretrue.expression1 -o expression2True if either expression1 or expression2 aretrue.(expression)True if expression is true.The -a operator has higher precedence than the -o operator.timesPrint the accumulated user and system times for the shelland for processes run from the shell.The return status is0.trap [action signal ...]Cause the shell to parse and execute action when any of thespecified signals are received.The signals are specifiedby signal number or as the name of the signal.If signal is0 or EXIT, the action is executed when the shell exits.action may be empty (''), which causes the specified signalsto be ignored.With action omitted or set to `-' thespecified signals are set to their default action.When theshell forks off a subshell, it resets trapped (but notignored) signals to the default action.The trap commandhas no effect on signals that were ignored on entry to theshell.trap without any arguments cause it to write a listof signals and their associated action to the standardoutput in a format that is suitable as an input to the shellthat achieves the same trapping results.Examples:trapList trapped signals and their corresponding actiontrap '' INT QUIT tstp 30Ignore signals INT QUIT TSTP USR1trap date INTPrint date upon receiving signal INTtype [name ...]Interpret each name as a command and print the resolution ofthe command search.Possible resolutions are: shellkeyword, alias, shell builtin, command, tracked alias andnot found.For aliases the alias expansion is printed; forcommands and tracked aliases the complete pathname of thecommand is printed.ulimit [-H | -S] [-a | -tfdscmlpnvwr [value]]Inquire about or set the hard or soft limits on processes orset new limits.The choice between hard limit (which noprocess is allowed to violate, and which may not be raisedonce it has been lowered) and soft limit (which causesprocesses to be signaled but not necessarily killed, andwhich may be raised) is made with these flags:-Hset or inquire about hard limits-Sset or inquire about soft limits.If neither -Hnor -S is specified, the soft limit is displayedor both limits are set.If both are specified,the last one wins.The limit to be interrogated or set, then, is chosen byspecifying any one of these flags:-ashow all the current limits-tshow or set the limit on CPU time (in seconds)-fshow or set the limit on the largest file thatcan be created (in 512-byte blocks)-dshow or set the limit on the data segment sizeof a process (in kilobytes)-sshow or set the limit on the stack size of aprocess (in kilobytes)-cshow or set the limit on the largest core dumpsize that can be produced (in 512-byte blocks)-mshow or set the limit on the total physicalmemory that can be in use by a process (inkilobytes)-lshow or set the limit on how much memory aprocess can lock with mlock(2) (in kilobytes)-pshow or set the limit on the number of processesthis user can have at one time-nshow or set the limit on the number files aprocess can have open at once-vshow or set the limit on the total virtualmemory that can be in use by a process (inkilobytes)-wshow or set the limit on the total number oflocks held by a process-rshow or set the limit on the real-timescheduling priority of a processIf none of these is specified, it is the limit on file sizethat is shown or set.If value is specified, the limit isset to that number; otherwise the current limit isdisplayed.Limits of an arbitrary process can be displayed or set usingthe sysctl(8) utility.umask [mask]Set the value of umask (see umask(2)) to the specified octalvalue.If the argument is omitted, the umask value isprinted.unalias [-a] [name]If name is specified, the shell removes that alias.If -ais specified, all aliases are removed.unset [-fv] name ...The specified variables and functions are unset andunexported.If -f or -v is specified, the correspondingfunction or variable is unset, respectively.If a givenname corresponds to both a variable and a function, and nooptions are given, only the variable is unset.wait [job]Wait for the specified job to complete and return the exitstatus of the last process in the job.If the argument isomitted, wait for all jobs to complete and return an exitstatus of zero.Command Line EditingWhen dash is being used interactively from a terminal, the currentcommand and the command history (see fc in Builtins) can be editedusing vi-mode command-line editing.This mode uses commands,described below, similar to a subset of those described in the viman page.The command \u2018set -o vi\u2019 enables vi-mode editing andplaces sh into vi insert mode.With vi-mode enabled, sh can beswitched between insert mode and command mode.It is similar tovi: typing \u27e8ESC\u27e9 enters vi command mode.Hitting \u27e8return\u27e9 while incommand mode will pass the line to the shell.",
        "name": "dash \u2014 command interpreter (shell)",
        "section": 1
    },
    {
        "command": "date",
        "description": "Display date and time in the given FORMAT.With -s, or with[MMDDhhmm[[CC]YY][.ss]], set the date and time.Mandatory arguments to long options are mandatory for shortoptions too.-d, --date=STRINGdisplay time described by STRING, not 'now'--debugannotate the parsed date, and warn about questionableusage to stderr-f, --file=DATEFILElike --date; once for each line of DATEFILE-I[FMT], --iso-8601[=FMT]output date/time in ISO 8601 format.FMT='date' for dateonly (the default), 'hours', 'minutes', 'seconds', or 'ns'for date and time to the indicated precision.Example:2006-08-14T02:34:56-06:00--resolutionoutput the available resolution of timestamps Example:0.000000001-R, --rfc-emailoutput date and time in RFC 5322 format.Example: Mon, 14Aug 2006 02:34:56 -0600--rfc-3339=FMToutput date/time in RFC 3339 format.FMT='date','seconds', or 'ns' for date and time to the indicatedprecision.Example: 2006-08-14 02:34:56-06:00-r, --reference=FILEdisplay the last modification time of FILE-s, --set=STRINGset time described by STRING-u, --utc, --universalprint or set Coordinated Universal Time (UTC)--help display this help and exit--versionoutput version information and exitAll options that specify the date to display are mutuallyexclusive.I.e.: --date, --file, --reference, --resolution.FORMAT controls the output.Interpreted sequences are:%%a literal %%alocale's abbreviated weekday name (e.g., Sun)%Alocale's full weekday name (e.g., Sunday)%blocale's abbreviated month name (e.g., Jan)%Blocale's full month name (e.g., January)%clocale's date and time (e.g., Thu Mar3 23:05:25 2005)%Ccentury; like %Y, except omit last two digits (e.g., 20)%dday of month (e.g., 01)%Ddate; same as %m/%d/%y%eday of month, space padded; same as %_d%Ffull date; like %+4Y-%m-%d%glast two digits of year of ISO week number (see %G)%Gyear of ISO week number (see %V); normally useful onlywith %V%hsame as %b%Hhour (00..23)%Ihour (01..12)%jday of year (001..366)%khour, space padded ( 0..23); same as %_H%lhour, space padded ( 1..12); same as %_I%mmonth (01..12)%Mminute (00..59)%na newline%Nnanoseconds (000000000..999999999)%plocale's equivalent of either AM or PM; blank if not known%Plike %p, but lower case%qquarter of year (1..4)%rlocale's 12-hour clock time (e.g., 11:11:04 PM)%R24-hour hour and minute; same as %H:%M%sseconds since the Epoch (1970-01-01 00:00 UTC)%Ssecond (00..60)%ta tab%Ttime; same as %H:%M:%S%uday of week (1..7); 1 is Monday%Uweek number of year, with Sunday as first day of week(00..53)%VISO week number, with Monday as first day of week (01..53)%wday of week (0..6); 0 is Sunday%Wweek number of year, with Monday as first day of week(00..53)%xlocale's date representation (e.g., 12/31/99)%Xlocale's time representation (e.g., 23:13:48)%ylast two digits of year (00..99)%Yyear%z+hhmm numeric time zone (e.g., -0400)%:z+hh:mm numeric time zone (e.g., -04:00)%::z+hh:mm:ss numeric time zone (e.g., -04:00:00)%:::znumeric time zone with : to necessary precision (e.g.,-04, +05:30)%Zalphabetic time zone abbreviation (e.g., EDT)By default, date pads numeric fields with zeroes.The followingoptional flags may follow '%':-(hyphen) do not pad the field_(underscore) pad with spaces0(zero) pad with zeros+pad with zeros, and put '+' before future years with >4digits^use upper case if possible#use opposite case if possibleAfter any flags comes an optional field width, as a decimalnumber; then an optional modifier, which is either E to use thelocale's alternate representations if available, or O to use thelocale's alternate numeric symbols if available.",
        "name": "date - print or set the system date and time",
        "section": 1
    },
    {
        "command": "dbpmda",
        "description": "dbpmda is an interactive interface to the interactions between aPerformance Metric Domain Agent (PMDA(3)) and the PerformanceMetric Collector Daemon (pmcd(1)).This allows PMDAs to beattached, initialized and exercised to test for correctness.dbpmda interactively prompts the user for commands, many of whichemulate the Protocol Data Units (PDUs) that may be sent by apmcd(1) process.After running dbpmda, enter the command help toget a list of the available commands.The example section belowillustrates a session using dbpmda to test a PMDA.To simplify repetitive testing of a PMDA, the file .dbpmdarc inthe current working directory can contain a list of commands thatwill be executed by dbpmda on startup, before the user isprompted to enter further commands interactively.Whileprocessing the .dbpmdarc file, interactive mode and commandechoing are enabled and then reset at the end of the .dbpmdarcfile (see the -i and -e command line options below).The -f command line option prevents startup processing of a.dbpmdarc file (if it exists).If the system supports readline(3) then this will be used to readcommands when input is from a tty device, so history and commandline editing are available.As there are no timeout constraints on a PMDA while using dbpmda(as compared to pmcd(1)), another debugger like gdb(1) can beused on the PMDA process once it has been attached to dbpmda.",
        "name": "dbpmda - debugger for Performance Co-Pilot PMDAs",
        "section": 1
    },
    {
        "command": "dbprobe",
        "description": "The dbprobe utility is used by pmdadbping(1) to measure responsetime from a database.A given query is executed on the databaseat the requested interval (delay, which defaults to 60 seconds).This response time measure can be exported via the PerformanceCo-Pilot framework for live and historical monitoring usingpmdadbping(1).",
        "name": "dbprobe - database response time and availability information",
        "section": 1
    },
    {
        "command": "dd",
        "description": "Copy a file, converting and formatting according to the operands.bs=BYTESread and write up to BYTES bytes at a time (default: 512);overrides ibs and obscbs=BYTESconvert BYTES bytes at a timeconv=CONVSconvert the file as per the comma separated symbol listcount=Ncopy only N input blocksibs=BYTESread up to BYTES bytes at a time (default: 512)if=FILEread from FILE instead of stdiniflag=FLAGSread as per the comma separated symbol listobs=BYTESwrite BYTES bytes at a time (default: 512)of=FILEwrite to FILE instead of stdoutoflag=FLAGSwrite as per the comma separated symbol listseek=N (or oseek=N) skip N obs-sized output blocksskip=N (or iseek=N) skip N ibs-sized input blocksstatus=LEVELThe LEVEL of information to print to stderr; 'none'suppresses everything but error messages, 'noxfer'suppresses the final transfer statistics, 'progress' showsperiodic transfer statisticsN and BYTES may be followed by the following multiplicativesuffixes: c=1, w=2, b=512, kB=1000, K=1024, MB=1000*1000,M=1024*1024, xM=M, GB=1000*1000*1000, G=1024*1024*1024, and so onfor T, P, E, Z, Y, R, Q.Binary prefixes can be used, too:KiB=K, MiB=M, and so on.If N ends in 'B', it counts bytes notblocks.Each CONV symbol may be:asciifrom EBCDIC to ASCIIebcdic from ASCII to EBCDICibmfrom ASCII to alternate EBCDICblockpad newline-terminated records with spaces to cbs-sizeunblockreplace trailing spaces in cbs-size records with newlinelcasechange upper case to lower caseucasechange lower case to upper casesparse try to seek rather than write all-NUL output blocksswabswap every pair of input bytessyncpad every input block with NULs to ibs-size; when usedwith block or unblock, pad with spaces rather than NULsexclfail if the output file already existsnocreatdo not create the output filenotruncdo not truncate the output filenoerrorcontinue after read errorsfdatasyncphysically write output file data before finishingfsynclikewise, but also write metadataEach FLAG symbol may be:append append mode (makes sense only for output; conv=notruncsuggested)direct use direct I/O for datadirectoryfail unless a directorydsyncuse synchronized I/O for datasynclikewise, but also for metadatafullblockaccumulate full blocks of input (iflag only)nonblockuse non-blocking I/Onoatimedo not update access timenocacheRequest to drop cache.See also oflag=syncnoctty do not assign controlling terminal from filenofollowdo not follow symlinksSending a USR1 signal to a running 'dd' process makes it printI/O statistics to standard error and then resume copying.Options are:--help display this help and exit--versionoutput version information and exit",
        "name": "dd - convert and copy a file",
        "section": 1
    },
    {
        "command": "deallocvt",
        "description": "The command deallocvt deallocates kernel memory and datastructures for all unused virtual consoles.If one or morearguments N ...are given, only the corresponding consoles/dev/ttyN are deallocated.A virtual console is unused if it is not the foreground console,and no process has it open for reading or writing, and no texthas been selected on its screen.",
        "name": "deallocvt - deallocate unused virtual consoles",
        "section": 1
    },
    {
        "command": "debuginfo-install",
        "description": "debuginfo-install is a program which installs the RPMs needed todebug the specified package.The package argument can be awildcard, but will only match installed packages.debuginfo-install will then enable any debuginfo repositories, and installthe relevant debuginfo rpm.",
        "name": "debuginfo-install - install debuginfo packages and theirdependencies",
        "section": 1
    },
    {
        "command": "derb",
        "description": "derb reads the compiled resource bundle files passed on thecommand line and write them back in text form.The resultingtext files have a .txt extension while compiled resource bundlesource files typically have a .res extension.It is customary to name the resource bundles by their localename, i.e. to use a local identifier for the bundle filename,e.g.ja_JP.res for Japanese (Japan) data, or root.res for theroot bundle.This is especially important for derb since thelocale name is not accessible directly from the compiled resourcebundle, and to know which locale to ask for when opening thebundle.derb will produce a file whose base name is the basename of the compiled resource file itself.If the --to-stdout,-c option is used, however, the text will be written on thestandard output.",
        "name": "derb - disassemble a resource bundle",
        "section": 1
    },
    {
        "command": "df",
        "description": "This manual page documents the GNU version of df.df displaysthe amount of space available on the file system containing eachfile name argument.If no file name is given, the spaceavailable on all currently mounted file systems is shown.Spaceis shown in 1K blocks by default, unless the environment variablePOSIXLY_CORRECT is set, in which case 512-byte blocks are used.If an argument is the absolute file name of a device nodecontaining a mounted file system, df shows the space available onthat file system rather than on the file system containing thedevice node.This version of df cannot show the space availableon unmounted file systems, because on most kinds of systems doingso requires very nonportable intimate knowledge of file systemstructures.",
        "name": "df - report file system space usage",
        "section": 1
    },
    {
        "command": "dh",
        "description": "dh runs a sequence of debhelper commands. The supported sequencescorrespond to the targets of a debian/rules file: build-arch,build-indep, build, clean, install-indep, install-arch, install,binary-arch, binary-indep, and binary.",
        "name": "dh - debhelper command sequencer",
        "section": 1
    },
    {
        "command": "dh_assistant",
        "description": "dh_assistant is a debhelper program that provides introspectioninto the debhelper stack to assist third-party tools (e.g.linters) or third-party debhelper implementations not using thedebhelper script API (e.g., because they are not written inPerl).",
        "name": "dh_assistant - tool for supporting debhelper tools and provideintrospection",
        "section": 1
    },
    {
        "command": "dh_auto_build",
        "description": "dh_auto_build is a debhelper program that tries to automaticallybuild a package. It does so by running the appropriate commandfor the build system it detects the package uses. For example, ifa Makefile is found, this is done by running make (or MAKE, ifthe environment variable is set). If there's a setup.py, orBuild.PL, it is run to build the package.This is intended to work for about 90% of packages. If it doesn'twork, you're encouraged to skip using dh_auto_build at all, andjust run the build process manually.",
        "name": "dh_auto_build - automatically builds a package",
        "section": 1
    },
    {
        "command": "dh_auto_clean",
        "description": "dh_auto_clean is a debhelper program that tries to automaticallyclean up after a package build. It does so by running theappropriate command for the build system it detects the packageuses. For example, if there's a Makefile and it contains adistclean, realclean, or clean target, then this is done byrunning make (or MAKE, if the environment variable is set). Ifthere is a setup.py or Build.PL, it is run to clean the package.This is intended to work for about 90% of packages. If it doesn'twork, or tries to use the wrong clean target, you're encouragedto skip using dh_auto_clean at all, and just run make cleanmanually.",
        "name": "dh_auto_clean - automatically cleans up after a build",
        "section": 1
    },
    {
        "command": "dh_auto_configure",
        "description": "dh_auto_configure is a debhelper program that tries toautomatically configure a package prior to building. It does soby running the appropriate command for the build system itdetects the package uses.For example, it looks for and runs a./configure script, Makefile.PL, Build.PL, or cmake. A standardset of parameters is determined and passed to the program that isrun. Some build systems, such as make, do not need a configurestep; for these dh_auto_configure will exit without doinganything.This is intended to work for about 90% of packages. If it doesn'twork, you're encouraged to skip using dh_auto_configure at all,and just run ./configure or its equivalent manually.",
        "name": "dh_auto_configure - automatically configure a package prior tobuilding",
        "section": 1
    },
    {
        "command": "dh_auto_install",
        "description": "dh_auto_install is a debhelper program that tries toautomatically install built files. It does so by running theappropriate command for the build system it detects the packageuses. For example, if there's a Makefile and it contains ainstall target, then this is done by running make (or MAKE, ifthe environment variable is set). If there is a setup.py orBuild.PL, it is used. Note that the Ant build system does notsupport installation, so dh_auto_install will not install filesbuilt using Ant.In compat 15 or later, dh_auto_install will use debian/tmp as thedefault --destdir and should be moved from there to theappropriate package build directory using dh_install(1) orsimilar tools. Though if the single-binary addon for dh(1) isactivated, then it will pass an explicit--destdir=debian/package/ to dh_auto_install.For earlier compat levels then unless --destdir option isspecified, the files are installed into debian/package/ if thereis only one binary package. In the multiple binary package case,the files are instead installed into debian/tmp/, and should bemoved from there to the appropriate package build directory usingdh_install(1) or similar tools.DESTDIR is used to tell make where to install the files.If theMakefile was generated by MakeMaker from a Makefile.PL, it willautomatically set PREFIX=/usr too, since such Makefiles needthat.This is intended to work for about 90% of packages. If it doesn'twork, or tries to use the wrong install target, you're encouragedto skip using dh_auto_install at all, and just run make installmanually.",
        "name": "dh_auto_install - automatically runs make install or similar",
        "section": 1
    },
    {
        "command": "dh_auto_test",
        "description": "dh_auto_test is a debhelper program that tries to automaticallyrun a package's test suite. It does so by running the appropriatecommand for the build system it detects the package uses. Forexample, if there's a Makefile and it contains a test or checktarget, then this is done by running make (or MAKE, if theenvironment variable is set). If the test suite fails, thecommand will exit nonzero. If there's no test suite, it will exitzero without doing anything.This is intended to work for about 90% of packages with a testsuite. If it doesn't work, you're encouraged to skip usingdh_auto_test at all, and just run the test suite manually.",
        "name": "dh_auto_test - automatically runs a package's test suites",
        "section": 1
    },
    {
        "command": "dh_bugfiles",
        "description": "dh_bugfiles is a debhelper program that is responsible forinstalling bug reporting customization files (bug scripts and/orbug control files and/or presubj files) into package builddirectories.",
        "name": "dh_bugfiles - install bug reporting customization files intopackage build directories",
        "section": 1
    },
    {
        "command": "dh_builddeb",
        "description": "dh_builddeb simply calls dpkg-deb(1) to build a Debian package orpackages.It will also build dbgsym packages when dh_strip(1)and dh_gencontrol(1) have prepared them.It supports building multiple binary packages in parallel, whenenabled by DEB_BUILD_OPTIONS.When the Rules-Requires-Root field is not (effectively) binary-targets, dh_builddeb will pass --root-owner-group to dpkg-deb(1).",
        "name": "dh_builddeb - build Debian binary packages",
        "section": 1
    },
    {
        "command": "dh_clean",
        "description": "dh_clean is a debhelper program that is responsible for cleaningup.It should be the last step of the clean target and otherdebhelper commands generally assume that dh_clean will clean upafter them.It removes the package build directories, and removes some otherfiles including debian/files, and any detritus left behind byother debhelper commands. It also removes common files thatshould not appear in a Debian diff:#*# *~ DEADJOE *.orig *.rej *.SUMS TAGS .deps/* *.P *-stampIt does not run \"make clean\" to clean up after the build process.Use dh_auto_clean(1) to do things like that.",
        "name": "dh_clean - clean up package build directories",
        "section": 1
    },
    {
        "command": "dh_compress",
        "description": "dh_compress is a debhelper program that is responsible forcompressing the files in package build directories, and makessure that any symlinks that pointed to the files before they werecompressed are updated to point to the new files.By default, dh_compress compresses files that Debian policymandates should be compressed, namely all files inusr/share/info, usr/share/man, files in usr/share/doc that arelarger than 4k in size, (except the copyright file, .html andother web files, image files, and files that appear to be alreadycompressed based on their extensions), and all changelog files.Plus PCF fonts underneath usr/share/fonts/X11/",
        "name": "dh_compress - compress files and fix symlinks in package builddirectories",
        "section": 1
    },
    {
        "command": "dh_dwz",
        "description": "dh_dwz is a debhelper program that will optimize the(uncompressed) size of the DWARF debug information in ELFbinaries.It does so by running dwz(1) on all the ELF binariesin the package.",
        "name": "dh_dwz - optimize DWARF debug information in ELF binaries via dwz",
        "section": 1
    },
    {
        "command": "dh_fixperms",
        "description": "dh_fixperms is a debhelper program that is responsible forsetting the permissions of files and directories in package builddirectories to a sane state -- a state that complies with Debianpolicy.dh_fixperms makes all files in usr/share/doc in the package builddirectory (excluding files in the examples/ directory) be mode644. It also changes the permissions of all man pages to mode644. It removes group and other write permission from all files.It removes execute permissions from any libraries, headers, Perlmodules, or desktop files that have it set. It makes all files inthe standard bin and sbin directories, usr/games/ and etc/init.dexecutable (since v4). Finally, it removes the setuid and setgidbits from all files in the package.When the Rules-Requires-Root field has the (effective) value ofbinary-targets, dh_fixperms will also reset the ownership of allpaths to \"root:root\".",
        "name": "dh_fixperms - fix permissions of files in package builddirectories",
        "section": 1
    },
    {
        "command": "dh_gencontrol",
        "description": "dh_gencontrol is a debhelper program that is responsible forgenerating control files, and installing them into the DEBIANdirectory with the proper permissions.This program is merely a wrapper around dpkg-gencontrol(1), whichcalls it once for each package being acted on (plus relateddbgsym packages), and passes in some additional useful flags.Note that if you use dh_gencontrol, you must also usedh_builddeb(1) to build the packages.Otherwise, your build mayfail to build as dh_gencontrol (via dpkg-gencontrol(1)) declareswhich packages are built.As debhelper automatically generatesdbgsym packages, it some times adds additional packages, whichwill be built by dh_builddeb(1).",
        "name": "dh_gencontrol - generate and install control file",
        "section": 1
    },
    {
        "command": "dh_icons",
        "description": "dh_icons is a debhelper program that updates caches ofFreedesktop icons when needed, using the update-icon-cachesprogram provided by GTK+2.12.Currently this program does nothandle installation of the files, though it may do so at a laterdate, so should be run after icons are installed in the packagebuild directories.It takes care of adding maintainer script fragments to callupdate-icon-caches for icon directories. (This is not done forgnome and hicolor icons, as those are handled by triggers.)These commands are inserted into the maintainer scripts bydh_installdeb(1).",
        "name": "dh_icons - Update caches of Freedesktop icons",
        "section": 1
    },
    {
        "command": "dh_install",
        "description": "dh_install is a debhelper program that handles installing filesinto package build directories. There are many dh_install*commands that handle installing specific types of files such asdocumentation, examples, man pages, and so on, and they should beused when possible as they often have extra intelligence forthose particular tasks. dh_install, then, is useful forinstalling everything else, for which no particular intelligenceis needed. It is a replacement for the old dh_movefiles command.This program may be used in one of two ways. If you just have afile or two that the upstream Makefile does not install for you,you can run dh_install on them to move them into place. On theother hand, maybe you have a large package that builds multiplebinary packages. You can use the upstream Makefile to install itall into debian/tmp, and then use dh_install to copy directoriesand files from there into the proper package build directories.From debhelper compatibility level 7 on, dh_install will fallback to looking in debian/tmp for files, if it does not find themin the current directory (or wherever you've told it to lookusing --sourcedir).",
        "name": "dh_install - install files into package build directories",
        "section": 1
    },
    {
        "command": "dh_installalternatives",
        "description": "dh_installalternatives is a debhelper program that is responsiblefor parsing the declarative alternatives format and insert therelevant maintscripts snippets to interface withupdate-alternatives(1)",
        "name": "dh_installalternatives - install declarative alternative rules",
        "section": 1
    },
    {
        "command": "dh_installcatalogs",
        "description": "dh_installcatalogs is a debhelper program that installs andregisters SGML catalogs. It complies with the Debian XML/SGMLpolicy.Catalogs will be registered in a supercatalog, in/etc/sgml/package.cat.This command automatically adds maintainer script snippets forregistering and unregistering the catalogs and supercatalogs(unless -n is used). These snippets are inserted into themaintainer scripts and the triggers file by dh_installdeb; seedh_installdeb(1) for an explanation of Debhelper maintainerscript snippets.A dependency on sgml-base will be added to ${misc:Depends}, so besure your package uses that variable in debian/control.",
        "name": "dh_installcatalogs - install and register SGML Catalogs",
        "section": 1
    },
    {
        "command": "dh_installchangelogs",
        "description": "dh_installchangelogs is a debhelper program that is responsiblefor installing changelogs into package build directories.An upstream changelog file may be specified as an option. If noneis specified, dh_installchangelogs may look for files with namesthat seem likely to be changelogs as described in the nextparagraphs.In non-native packages, dh_installchangelogs will first look forchangelog files installed by the upstream build system intousr/share/doc/package (of the package build directory) and renamethe most likely candidate (if any) tousr/share/doc/package/changelog.Note that dh_installchangelogsdoes not look into any source directory (such as debian/tmp).Otherwise, dh_installchangelogs (at compatibility level 7 or anylater) will look for changelog files in the source directory(e.g. the root or the docs subdirectory).It will look forchangelog, changes and history optionally with common extensions(such as .txt, .md and .rst).If a changelog file is specified and is an html file (determinedby file extension), it will be installed asusr/share/doc/package/changelog.html instead. If the htmlchangelog is converted to plain text, that variant can bespecified as a second parameter. When no plain text variant isspecified, a short usr/share/doc/package/changelog is generated,pointing readers at the html changelog file.The debchange-style Debian changelogs are trimmed to include onlyentries more recent than the release date of oldstable.Notrimming will be performed if the --no-trim option is passed orif the DEB_BUILD_OPTIONS environment variable contains notrimdch.",
        "name": "dh_installchangelogs - install changelogs into package builddirectories",
        "section": 1
    },
    {
        "command": "dh_installcron",
        "description": "dh_installcron is a debhelper program that is responsible forinstalling cron scripts.",
        "name": "dh_installcron - install cron scripts into etc/cron.*",
        "section": 1
    },
    {
        "command": "dh_installdeb",
        "description": "dh_installdeb is a debhelper program that is responsible forinstalling files into the DEBIAN directories in package builddirectories with the correct permissions.",
        "name": "dh_installdeb - install files into the DEBIAN directory",
        "section": 1
    },
    {
        "command": "dh_installdebconf",
        "description": "dh_installdebconf is a debhelper program that is responsible forinstalling files used by debconf into package build directories.It also automatically generates the postrm commands needed tointerface with debconf. The commands are added to the maintainerscripts by dh_installdeb. See dh_installdeb(1) for an explanationof how that works.Note that if you use debconf, your package probably needs todepend on it (it will be added to ${misc:Depends} by thisprogram).Note that for your config script to be called by dpkg, yourpostinst needs to source debconf's confmodule. dh_installdebconfdoes not install this statement into the postinst automaticallyas it is too hard to do it right.",
        "name": "dh_installdebconf - install files used by debconf in packagebuild directories",
        "section": 1
    },
    {
        "command": "dh_installdirs",
        "description": "dh_installdirs is a debhelper program that is responsible forcreating subdirectories in package build directories.Many packages can get away with omitting the call todh_installdirs completely.Notably, other dh_* commands areexpected to create directories as needed.",
        "name": "dh_installdirs - create subdirectories in package builddirectories",
        "section": 1
    },
    {
        "command": "dh_installdocs",
        "description": "dh_installdocs is a debhelper program that is responsible forinstalling documentation into usr/share/doc/package in packagebuild directories.In compat 10 and earlier, dh_install(1) may be a better tool forhandling the upstream documentation, when upstream's own buildsystem installs all the desired documentation correctly.In thiscase, dh_installdocs is still useful for installing packagingrelated documentation (e.g. the debian/copyright file).From debhelper compatibility level 11 on, dh_install will fallback to looking in debian/tmp for files, if it does not find themin the current directory (or wherever you've told it to lookusing --sourcedir).In compat 11 and later, dh_installdocs offers many of thefeatures that dh_install(1) also has.Furthermore,dh_installdocs also supports the nodoc build profile to excludedocumentation (regardless of compat level).",
        "name": "dh_installdocs - install documentation into package builddirectories",
        "section": 1
    },
    {
        "command": "dh_installemacsen",
        "description": "dh_installemacsen is a debhelper program that is responsible forinstalling files used by the Debian emacsen-common package intopackage build directories.It also automatically generates the preinst postinst and prermcommands needed to register a package as an Emacs add on package.The commands are added to the maintainer scripts bydh_installdeb. See dh_installdeb(1) for an explanation of howthis works.",
        "name": "dh_installemacsen - register an Emacs add on package",
        "section": 1
    },
    {
        "command": "dh_installexamples",
        "description": "dh_installexamples is a debhelper program that is responsible forinstalling examples into usr/share/doc/package/examples inpackage build directories.From debhelper compatibility level 11 on, dh_install will fallback to looking in debian/tmp for files, if it does not find themin the current directory (or wherever you've told it to lookusing --sourcedir).",
        "name": "dh_installexamples - install example files into package builddirectories",
        "section": 1
    },
    {
        "command": "dh_installgsettings",
        "description": "dh_installgsettings is a debhelper program that is responsiblefor installing GSettings override files and generatingappropriate dependencies on the GSettings backend.The dependency on the backend will be generated in${misc:Depends}.",
        "name": "dh_installgsettings - install GSettings overrides and setdependencies",
        "section": 1
    },
    {
        "command": "dh_installifupdown",
        "description": "dh_installifupdown is a debhelper program that is responsible forinstalling if-up, if-down, if-pre-up, and if-post-down hookscripts into package build directories.",
        "name": "dh_installifupdown - install if-up and if-down hooks",
        "section": 1
    },
    {
        "command": "dh_installinfo",
        "description": "dh_installinfo is a debhelper program that is responsible forinstalling info files into usr/share/info in the package builddirectory.From debhelper compatibility level 11 on, dh_install will fallback to looking in debian/tmp for files, if it does not find themin the current directory (or wherever you've told it to lookusing --sourcedir).",
        "name": "dh_installinfo - install info files",
        "section": 1
    },
    {
        "command": "dh_installinit",
        "description": "dh_installinit is a debhelper program that is responsible forinstalling init scripts with associated defaults files.Incompatibility levels up to and including 10, dh_installinit willalso install some systemd related files provided by the debianpackaging (see the \"FILES\" section below).In compatibilitylevels up to and including 11, dh_installinit will also handleupstart jobs provided in the debian packaging (see the \"FILES\"for more information on this as well).It also automatically generates the postinst and postrm and prermcommands needed to set up the symlinks in /etc/rc*.d/ to startand stop the init scripts.In compat 10 or earlier: If a package only ships a systemdservice file and no sysvinit script is provided, you may want toexclude the call to dh_installinit for that package (e.g. via-N).Otherwise, you may get warnings from lintian about init.dscripts not being included in the package.",
        "name": "dh_installinit - install service init files into package builddirectories",
        "section": 1
    },
    {
        "command": "dh_installinitramfs",
        "description": "dh_installinitramfs is a debhelper program that is responsiblefor installing Debian package provided initramfs hooks.If dh_installinitramfs installs or detects one or more initramfshooks in the package, then it also automatically generates thenoawait trigger update-initframfs command needed to interfacewith the Debian initramfs system.This trigger is inserted intothe packaging by dh_installdeb(1).",
        "name": "dh_installinitramfs - install initramfs hooks and setupmaintscripts",
        "section": 1
    },
    {
        "command": "dh_installlogcheck",
        "description": "dh_installlogcheck is a debhelper program that is responsible forinstalling logcheck rule files.",
        "name": "dh_installlogcheck - install logcheck rulefiles intoetc/logcheck/",
        "section": 1
    },
    {
        "command": "dh_installlogrotate",
        "description": "dh_installlogrotate is a debhelper program that is responsiblefor installing logrotate config files into etc/logrotate.d inpackage build directories.Files named debian/package.logrotateare installed.",
        "name": "dh_installlogrotate - install logrotate config files",
        "section": 1
    },
    {
        "command": "dh_installman",
        "description": "dh_installman is a debhelper program that handles installing manpages into the correct locations in package build directories.In compat 10 and earlier, this program was primarily for whenupstream's build system does not properly install them as a partof its install step (or it does not have an install step).Incompat 11 and later, it also supports the default searchdir plus--sourcedir like dh_install(1) and has the advantage that itrespects the nodoc build profile (unlike dh_install(1)).Even if you prefer to use dh_install(1) for installing themanpages, dh_installman can still be useful for converting themanpage encoding to UTF-8 and for converting .so links (asdescribed below).However, that part happens automaticallywithout any explicit configuration.You tell dh_installman what man pages go in your packages, and itfigures out where to install them based on the section field intheir .TH or .Dt line. If you have a properly formatted .TH or.Dt line, your man page will be installed into the rightdirectory, with the right name (this includes proper handling ofpages with a subsection, like 3perl, which are placed in man3,and given an extension of .3perl). If your .TH or .Dt line isincorrect or missing, the program may guess wrong based on thefile extension.It also supports translated man pages, by looking for extensionslike .ll.8 and .ll_LL.8, or by use of the --language switch.If dh_installman seems to install a man page into the wrongsection or with the wrong extension, this is because the man pagehas the wrong section listed in its .TH or .Dt line. Edit the manpage and correct the section, and dh_installman will follow suit.See man(7) for details about the .TH section, and mdoc(7) for the.Dt section. If dh_installman seems to install a man page into adirectory like /usr/share/man/pl/man1/, that is because yourprogram has a name like foo.pl, and dh_installman assumes thatmeans it is translated into Polish. Use --language=C to avoidthis.After the man page installation step, dh_installman will check tosee if any of the man pages in the temporary directories of anyof the packages it is acting on contain .so links. If so, itchanges them to symlinks.Also, dh_installman will use man to guess the character encodingof each manual page and convert it to UTF-8. If the guessworkfails for some reason, you can override it using an encodingdeclaration. See manconv(1) for details.From debhelper compatibility level 11 on, dh_install will fallback to looking in debian/tmp for files, if it does not find themin the current directory (or wherever you've told it to lookusing --sourcedir).",
        "name": "dh_installman - install man pages into package build directories",
        "section": 1
    },
    {
        "command": "dh_installmanpages",
        "description": "dh_installmanpages is a debhelper program that is responsible forautomatically installing man pages into usr/share/man/ in packagebuild directories.This is a DWIM-style program, with an interface unlike the restof debhelper. It is deprecated, and you are encouraged to usedh_installman(1) instead.dh_installmanpages scans the current directory and allsubdirectories for filenames that look like man pages. (Note thatonly real files are looked at; symlinks are ignored.) It usesfile(1) to verify that the files are in the correct format. Then,based on the files' extensions, it installs them into the correctman directory.All filenames specified as parameters will be skipped bydh_installmanpages.This is useful if by default it installssome man pages that you do not want to be installed.After the man page installation step, dh_installmanpages willcheck to see if any of the man pages are .so links. If so, itchanges them to symlinks.",
        "name": "dh_installmanpages - old-style man page installer (deprecated)",
        "section": 1
    },
    {
        "command": "dh_installmenu",
        "description": "dh_installmenu is a debhelper program that is responsible forinstalling files used by the Debian menu package into packagebuild directories.It also automatically generates the postinst and postrm commandsneeded to interface with the Debian menu package. These commandsare inserted into the maintainer scripts by dh_installdeb(1).",
        "name": "dh_installmenu - install Debian menu files into package builddirectories",
        "section": 1
    },
    {
        "command": "dh_installmime",
        "description": "dh_installmime is a debhelper program that is responsible forinstalling mime files into package build directories.",
        "name": "dh_installmime - install mime files into package builddirectories",
        "section": 1
    },
    {
        "command": "dh_installmodules",
        "description": "dh_installmodules is a debhelper program that is responsible forregistering kernel modules.Kernel modules are searched for in the package build directoryand if found, preinst, postinst and postrm commands areautomatically generated to run depmod and register the moduleswhen the package is installed.These commands are inserted intothe maintainer scripts by dh_installdeb(1).",
        "name": "dh_installmodules - register kernel modules",
        "section": 1
    },
    {
        "command": "dh_installpam",
        "description": "dh_installpam is a debhelper program that is responsible forinstalling files used by PAM into package build directories.",
        "name": "dh_installpam - install pam support files",
        "section": 1
    },
    {
        "command": "dh_installppp",
        "description": "dh_installppp is a debhelper program that is responsible forinstalling ppp ip-up and ip-down scripts into package builddirectories.",
        "name": "dh_installppp - install ppp ip-up and ip-down files",
        "section": 1
    },
    {
        "command": "dh_installsystemd",
        "description": "dh_installsystemd is a debhelper program that is responsible forinstalling package maintainer supplied systemd unit files.It also finds the service files installed by a package andgenerates preinst, postinst, and prerm code blocks for enabling,disabling, starting, stopping, and restarting the correspondingsystemd services, when the package is installed, updated, orremoved. These snippets are added to the maintainer scripts bydh_installdeb(1).deb-systemd-helper(1) is used to enable and disable systemdunits, thus it is not necessary that the machine actually runssystemd during package installation time, enabling happens on allmachines in order to be able to switch from sysvinit to systemdand back.dh_installsystemd operates on all unit files installed by apackage. For only generating blocks for specific unit files, passthem as arguments, \"dh_installsystemd quota.service\". Specificunit files can be excluded from processing using the -X commondebhelper(1) option.",
        "name": "dh_installsystemd - install systemd unit files",
        "section": 1
    },
    {
        "command": "dh_installsystemduser",
        "description": "dh_installsystemduser finds the systemd user instance servicefiles installed by a package and generates preinst, postinst, andprerm code blocks for enabling, disabling, starting, stopping,and restarting the corresponding systemd user instance services,when the package is installed, updated, or removed. Thesesnippets are added to the maintainer scripts by dh_installdeb(1).deb-systemd-helper(1) is used to enable and disable the systemdunits, thus it is not necessary that the machine actually runssystemd during package installation time, enabling happens on allmachines.dh_installsystemduser operates on all user instance unit filesinstalled by a package. For only generating blocks for specificunit files, pass them as arguments. Specific unit files can beexcluded from processing using the -X common debhelper(1) option.",
        "name": "dh_installsystemduser - install systemd unit files",
        "section": 1
    },
    {
        "command": "dh_installsysusers",
        "description": "dh_installsysusers is a debhelper program that is responsible forinstalling package maintainer supplied systemd sysusers files.It also finds the systemd sysusers files installed in a packageand generates relevant integration snippets for enabling theusers on installation.These snippets are added to the packageby dh_installdeb(1).",
        "name": "dh_installsysusers - install and integrates systemd sysusersfiles",
        "section": 1
    },
    {
        "command": "dh_installtmpfiles",
        "description": "dh_installtmpfiles is a debhelper program that is responsible forinstalling package maintainer supplied tmpfiles.d configurationfiles (e.g. for systemd-tmpfiles).It also finds the tmpfiles.d configuration files installed by apackage and generates postinst code blocks for activating thetmpfiles.d configuration when the package is installed. Thesesnippets are added to the maintainer scripts by dh_installdeb(1).",
        "name": "dh_installtmpfiles - install tmpfiles.d configuration files",
        "section": 1
    },
    {
        "command": "dh_installudev",
        "description": "dh_installudev is a debhelper program that is responsible forinstalling udev rules files.",
        "name": "dh_installudev - install udev rules files",
        "section": 1
    },
    {
        "command": "dh_installwm",
        "description": "dh_installwm is a debhelper program that is responsible forgenerating the postinst and prerm commands that register a windowmanager with update-alternatives(8). The window manager's manpage is also registered as a slave symlink (in v6 mode and up).It must be installed in usr/share/man/man1/ in the package builddirectory prior to calling dh_installwm.In compat 9 andearlier, the manpage was optional.",
        "name": "dh_installwm - register a window manager",
        "section": 1
    },
    {
        "command": "dh_installxfonts",
        "description": "dh_installxfonts is a debhelper program that is responsible forregistering X fonts, so their corresponding fonts.dir,fonts.alias, and fonts.scale be rebuilt properly at install time.Before calling this program, you should have installed any Xfonts provided by your package into the appropriate location inthe package build directory, and if you have fonts.alias orfonts.scale files, you should install them into the correctlocation under etc/X11/fonts in your package build directory.Your package should depend on xfonts-utils so that theupdate-fonts-* commands are available. (This program adds thatdependency to ${misc:Depends}.)This program automatically generates the postinst and postrmcommands needed to register X fonts. These commands are insertedinto the maintainer scripts by dh_installdeb. Seedh_installdeb(1) for an explanation of how this works.",
        "name": "dh_installxfonts - register X fonts",
        "section": 1
    },
    {
        "command": "dh_link",
        "description": "dh_link is a debhelper program that creates symlinks in packagebuild directories.dh_link accepts a list of pairs of source and destination files.The source files are the already existing files that will besymlinked from (called target by ln(1)). The destination filesare the symlinks that will be created (called link name byln(1)). There must be an equal number of source and destinationfiles specified.Be sure you do specify the absolute path to both the source anddestination files (unlike you would do if you were usingsomething like ln(1)).Please note that the leading slash isoptional.dh_link will generate symlinks that comply with Debian policy -absolute when policy says they should be absolute, and relativelinks with as short a path as possible. It will also create anysubdirectories it needs to put the symlinks in.Any pre-existing destination files will be replaced withsymlinks.dh_link also scans the package build tree for existing symlinkswhich do not conform to Debian policy, and corrects them (v4 orlater).",
        "name": "dh_link - create symlinks in package build directories",
        "section": 1
    },
    {
        "command": "dh_lintian",
        "description": "dh_lintian is a debhelper program that is responsible forinstalling override files used by lintian into package builddirectories.",
        "name": "dh_lintian - install lintian override files into package builddirectories",
        "section": 1
    },
    {
        "command": "dh_listpackages",
        "description": "dh_listpackages is a debhelper program that outputs a list of allbinary packages debhelper commands will act on. If you pass itsome options, it will change the list to match the packages otherdebhelper commands would act on if passed the same options.Packages are listed in the order they appear in debian/control.",
        "name": "dh_listpackages - list binary packages debhelper will act on",
        "section": 1
    },
    {
        "command": "dh_makeshlibs",
        "description": "dh_makeshlibs is a debhelper program that automatically scans forshared libraries, and generates a shlibs file for the librariesit finds.It will also ensure that ldconfig is invoked during install andremoval when it finds shared libraries.Since debhelper9.20151004, this is done via a dpkg trigger.In older versionsof debhelper, dh_makeshlibs would generate a maintainer scriptfor this purpose.Since debhelper 12.3, dh_makeshlibs will by default add anadditional udeb line for udebs in the shlibs file, when the udebhas the same name as the deb followed by a \"-udeb\" suffix (e.g.if the deb is called \"libfoo1\", then debhelper will auto-detectthe udeb if it is named \"libfoo1-udeb\"). Please use the--add-udeb and --no-add-udeb options below when this auto-detection is insufficient.If you previously used --add-udeb and are considering to migrateto using the new auto-detection feature in 12.3, then pleaseremember to test that the resulting DEBIAN/shlibs files are asexpected.There are some known corner cases, where the auto-detection is insufficient.These include when the udeb containslibrary files from multiple regular deb packages or when thepackages do not follow the expected naming convention.",
        "name": "dh_makeshlibs - automatically create shlibs file and calldpkg-gensymbols",
        "section": 1
    },
    {
        "command": "dh_md5sums",
        "description": "dh_md5sums is a debhelper program that is responsible forgenerating a DEBIAN/md5sums file, which lists the md5sums of eachfile in the package.These files are used by dpkg --verify orthe debsums(1) program.All files in DEBIAN/ are omitted from the md5sums file, as areall conffiles (unless you use the --include-conffiles switch).The md5sums file is installed with proper permissions andownerships.",
        "name": "dh_md5sums - generate DEBIAN/md5sums file",
        "section": 1
    },
    {
        "command": "dh_missing",
        "description": "dh_missing compares the list of installed files with the files inthe source directory. If any of the files (and symlinks) in thesource directory were not installed to somewhere, it will warn onstderr about that (--list-missing) or fail (--fail-missing).Please note that in compat 11 and earlier without either of theseoptions, dh_missing will silently do nothing.In compat 12,--list-missing is the defaultIn compat 13 and later,--fail-missing is the default.This may be useful if you have a large package and want to makesure that you don't miss installing newly added files in newupstream releases.Remember to test different kinds of builds (dpkg-buildpackage-A/-B/...) as you may experience varying results when only asubset of the packages are built.",
        "name": "dh_missing - check for missing files",
        "section": 1
    },
    {
        "command": "dh_movefiles",
        "description": "dh_movefiles is a debhelper program that is responsible formoving files out of debian/tmp or some other directory and intoother package build directories. This may be useful if yourpackage has a Makefile that installs everything into debian/tmp,and you need to break that up into subpackages.Note: dh_install is a much better program, and you arerecommended to use it instead of dh_movefiles.",
        "name": "dh_movefiles - move files out of debian/tmp into subpackages",
        "section": 1
    },
    {
        "command": "dh_perl",
        "description": "dh_perl is a debhelper program that is responsible for generatingthe ${perl:Depends} substitutions and adding them to substvarsfiles.The program will look at Perl scripts and modules in yourpackage, and will use this information to generate a dependencyon perl or perlapi. The dependency will be substituted into yourpackage's control file wherever you place the token${perl:Depends}.dh_perl also cleans up empty directories that MakeMaker cangenerate when installing Perl modules.",
        "name": "dh_perl - calculates Perl dependencies and cleans up afterMakeMaker",
        "section": 1
    },
    {
        "command": "dh_prep",
        "description": "dh_prep is a debhelper program that performs some file cleanupsin preparation for building a binary package. (This is whatdh_clean -k used to do.) It removes the package builddirectories, debian/tmp, and some temp files that are generatedwhen building a binary package.It is typically run at the top of the binary-arch and binary-indep targets, or at the top of a target such as install thatthey depend on.",
        "name": "dh_prep - perform cleanups in preparation for building a binarypackage",
        "section": 1
    },
    {
        "command": "dh_shlibdeps",
        "description": "dh_shlibdeps is a debhelper program that is responsible forcalculating shared library dependencies for packages.This program is merely a wrapper around dpkg-shlibdeps(1) thatcalls it once for each package listed in the control file,passing it a list of ELF executables and shared libraries it hasfound.",
        "name": "dh_shlibdeps - calculate shared library dependencies",
        "section": 1
    },
    {
        "command": "dh_strip",
        "description": "dh_strip is a debhelper program that is responsible for strippingout debug symbols in executables, shared libraries, and staticlibraries that are not needed during execution.This program examines your package build directories and worksout what to strip on its own. It uses file(1) and filepermissions and filenames to figure out what files are sharedlibraries (*.so), executable binaries, and static (lib*.a) anddebugging libraries (lib*_g.a, debug/*.so), and strips each asmuch as is possible. (Which is not at all for debugginglibraries.) In general it seems to make very good guesses, andwill do the right thing in almost all cases.Since it is very hard to automatically guess if a file is amodule, and hard to determine how to strip a module, dh_stripdoes not currently deal with stripping binary modules such as .ofiles.",
        "name": "dh_strip - strip executables, shared libraries, and some staticlibraries",
        "section": 1
    },
    {
        "command": "dh_systemd_enable",
        "description": "dh_systemd_enable is a debhelper program that is responsible forenabling and disabling systemd unit files.In the simple case, it finds all unit files installed by apackage (e.g.bacula-fd.service) and enables them. It is notnecessary that the machine actually runs systemd during packageinstallation time, enabling happens on all machines in order tobe able to switch from sysvinit to systemd and back.In the complex case, you can call dh_systemd_enable anddh_systemd_start manually (by overwriting the debian/rulestargets) and specify flags per unit file. An example is colord,which ships colord.service, a dbus-activated service without an[Install] section. This service file cannot be enabled ordisabled (a state called \"static\" by systemd) because it has no[Install] section. Therefore, running dh_systemd_enable does notmake sense.For only generating blocks for specific service files, you needto pass them as arguments, e.g. dh_systemd_enable quota.serviceand dh_systemd_enable --name=quotarpc quotarpc.service.",
        "name": "dh_systemd_enable - enable/disable systemd unit files",
        "section": 1
    },
    {
        "command": "dh_systemd_start",
        "description": "dh_systemd_start is a debhelper program that is responsible forstarting/stopping or restarting systemd unit files in case nocorresponding sysv init script is available.As with dh_installinit, the unit file is stopped before upgradesand started afterwards (unless --restart-after-upgrade isspecified, in which case it will only be restarted after theupgrade).This logic is not used when there is a correspondingSysV init script because invoke-rc.d performs thestop/start/restart in that case.",
        "name": "dh_systemd_start - start/stop/restart systemd unit files",
        "section": 1
    },
    {
        "command": "dh_testdir",
        "description": "dh_testdir tries to make sure that you are in the correctdirectory when building a Debian package. It makes sure that thefile debian/control exists, as well as any other files youspecify. If not, it exits with an error.",
        "name": "dh_testdir - test directory before building Debian package",
        "section": 1
    },
    {
        "command": "dh_testroot",
        "description": "dh_testroot is used to determine if the target is being run withsuffient access to root(-like) features.The definition of sufficient access depends on whether thebuilder (the tool invoking the debian/rules target) supports theRules-Requires-Root (R\u00b3) field.If the builder supports R\u00b3, thenit will set the environment variable DEB_RULES_REQUIRES_ROOT anddh_testroot will validate that the builder followed the minimumrequirements for the given value of DEB_RULES_REQUIRES_ROOT.If the builder does not support Rules-Requires-Root, then it willnot set the DEB_RULES_REQUIRES_ROOT environment variable.Thiswill in turn make dh_testroot (and the rest of debhelper) fallback to assuming that (fake)root is implied.The following is a summary of how dh_testroot behaves based onthe DEB_RULES_REQUIRES_ROOT environment variable (leading andtrailing whitespace in the variable is ignored).-If unset, or set to \"binary-targets\", then dh_testrootasserts that it is run as root or under fakeroot(1).-If set to \"no\", then dh_testroot returns successfully(without performing any additional checks).-If set to any other value than the above, then dh_testrootasserts that it is either run as root (or under fakeroot(1))or the builder has provided the DEB_GAIN_ROOT_CMD environmentvariable (e.g. via dpkg-buildpackage -r).Please note that dh_testroot does not read the Rules-Requires-Root field.Which implies that dh_testroot may produce incorrectresult if the builder lies in DEB_RULES_REQUIRES_ROOT.On theflip side, it also enables things like testing for what willhappen when DEB_RULES_REQUIRES_ROOT is set to a given value.",
        "name": "dh_testroot - ensure that a package is built with necessary levelof root permissions",
        "section": 1
    },
    {
        "command": "dh_ucf",
        "description": "dh_ucf is a debhelper program that is responsible for generatingthe postinst and postrm commands that register files with ucf(1)and ucfr(1).",
        "name": "dh_ucf - register configuration files with ucf",
        "section": 1
    },
    {
        "command": "dh_update_autotools_config",
        "description": "dh_update_autotools_config replaces all occurrences of config.suband config.guess in the source tree by the up-to-date versionsfound in the autotools-dev package.The original files arebacked up and restored by dh_clean.",
        "name": "dh_update_autotools_config - Update autotools config files",
        "section": 1
    },
    {
        "command": "dh_usrlocal",
        "description": "dh_usrlocal is a debhelper program that can be used for buildingpackages that will provide a subdirectory in /usr/local wheninstalled.It finds subdirectories of usr/local in the package builddirectory, and removes them, replacing them with maintainerscript snippets (unless -n is used) to create the directories atinstall time, and remove them when the package is removed, in amanner compliant with Debian policy. These snippets are insertedinto the maintainer scripts by dh_installdeb. Seedh_installdeb(1) for an explanation of debhelper maintainerscript snippets.When the DEB_RULES_REQUIRES_ROOT environment variable is not(effectively) binary-targets, the directories in /usr/local willbe handled as if they were owned by root:root (see below).When the DEB_RULES_REQUIRES_ROOT environment variable has aneffective value of binary-targets, the owners, groups andpermissions will be preserved with the sole exception where thedirectory is owned by root:root.If a directory is owned by root:root, then ownership will bedetermined at install time.The ownership and permission bitswill either be root:root mode 0755 or root:staff mode 02775.Theactual choice depends on whether the system has/etc/staff-group-for-usr-local (as documented in the DebianPolicy Manual \u00a79.1.2 since version 4.1.4)",
        "name": "dh_usrlocal - migrate usr/local directories to maintainer scripts",
        "section": 1
    },
    {
        "command": "diff",
        "description": "Compare FILES line by line.Mandatory arguments to long options are mandatory for shortoptions too.--normaloutput a normal diff (the default)-q, --briefreport only when files differ-s, --report-identical-filesreport when two files are the same-c, -C NUM, --context[=NUM]output NUM (default 3) lines of copied context-u, -U NUM, --unified[=NUM]output NUM (default 3) lines of unified context-e, --edoutput an ed script-n, --rcsoutput an RCS format diff-y, --side-by-sideoutput in two columns-W, --width=NUMoutput at most NUM (default 130) print columns--left-columnoutput only the left column of common lines--suppress-common-linesdo not output common lines-p, --show-c-functionshow which C function each change is in-F, --show-function-line=REshow the most recent line matching RE--label LABELuse LABEL instead of file name and timestamp (can berepeated)-t, --expand-tabsexpand tabs to spaces in output-T, --initial-tabmake tabs line up by prepending a tab--tabsize=NUMtab stops every NUM (default 8) print columns--suppress-blank-emptysuppress space or tab before empty output lines-l, --paginatepass output through 'pr' to paginate it-r, --recursiverecursively compare any subdirectories found--no-dereferencedon't follow symbolic links-N, --new-filetreat absent files as empty--unidirectional-new-filetreat absent first files as empty--ignore-file-name-caseignore case when comparing file names--no-ignore-file-name-caseconsider case when comparing file names-x, --exclude=PATexclude files that match PAT-X, --exclude-from=FILEexclude files that match any pattern in FILE-S, --starting-file=FILEstart with FILE when comparing directories--from-file=FILE1compare FILE1 to all operands; FILE1 can be a directory--to-file=FILE2compare all operands to FILE2; FILE2 can be a directory-i, --ignore-caseignore case differences in file contents-E, --ignore-tab-expansionignore changes due to tab expansion-Z, --ignore-trailing-spaceignore white space at line end-b, --ignore-space-changeignore changes in the amount of white space-w, --ignore-all-spaceignore all white space-B, --ignore-blank-linesignore changes where lines are all blank-I, --ignore-matching-lines=REignore changes where all lines match RE-a, --texttreat all files as text--strip-trailing-crstrip trailing carriage return on input-D, --ifdef=NAMEoutput merged file with '#ifdef NAME' diffs--GTYPE-group-format=GFMTformat GTYPE input groups with GFMT--line-format=LFMTformat all input lines with LFMT--LTYPE-line-format=LFMTformat LTYPE input lines with LFMTThese format options provide fine-grained control over theoutputof diff, generalizing -D/--ifdef.LTYPE is 'old', 'new', or 'unchanged'.GTYPE is LTYPE or 'changed'.GFMT (only) may contain:%<lines from FILE1%>lines from FILE2%=lines common to FILE1 and FILE2%[-][WIDTH][.[PREC]]{doxX}LETTERprintf-style spec for LETTERLETTERs are as follows for new group, lower case for oldgroup:Ffirst line numberLlast line numberNnumber of lines = L-F+1EF-1ML+1%(A=B?T:E)if A equals B then T else ELFMT (only) may contain:%Lcontents of line%lcontents of line, excluding any trailing newline%[-][WIDTH][.[PREC]]{doxX}nprintf-style spec for input line numberBoth GFMT and LFMT may contain:%%%%c'C'the single character C%c'\\OOO'the character with octal code OOOCthe character C (other characters represent themselves)-d, --minimaltry hard to find a smaller set of changes--horizon-lines=NUMkeep NUM lines of the common prefix and suffix--speed-large-filesassume large files and many scattered small changes--color[=WHEN]color output; WHEN is 'never', 'always', or 'auto'; plain--color means --color='auto'--palette=PALETTEthe colors to use when --color is active; PALETTE is acolon-separated list of terminfo capabilities--help display this help and exit-v, --versionoutput version information and exitFILES are 'FILE1 FILE2' or 'DIR1 DIR2' or 'DIR FILE' or 'FILEDIR'.If --from-file or --to-file is given, there are norestrictions on FILE(s).If a FILE is '-', read standard input.Exit status is 0 if inputs are the same, 1 if different, 2 iftrouble.",
        "name": "diff - compare files line by line",
        "section": 1
    },
    {
        "command": "diff3",
        "description": "Compare three files line by line.Mandatory arguments to long options are mandatory for shortoptions too.-A, --show-alloutput all changes, bracketing conflicts-e, --edoutput ed script incorporating changes from OLDFILE toYOURFILE into MYFILE-E, --show-overlaplike -e, but bracket conflicts-3, --easy-onlylike -e, but incorporate only nonoverlapping changes-x, --overlap-onlylike -e, but incorporate only overlapping changes-Xlike -x, but bracket conflicts-iappend 'w' and 'q' commands to ed scripts-m, --mergeoutput actual merged file, according to -A if no otheroptions are given-a, --texttreat all files as text--strip-trailing-crstrip trailing carriage return on input-T, --initial-tabmake tabs line up by prepending a tab--diff-program=PROGRAMuse PROGRAM to compare files-L, --label=LABELuse LABEL instead of file name (can be repeated up tothree times)--help display this help and exit-v, --versionoutput version information and exitThe default output format is a somewhat human-readablerepresentation of the changes.The -e, -E, -x, -X (and corresponding long) options cause an edscript to be output instead of the default.Finally, the -m (--merge) option causes diff3 to do the mergeinternally and output the actual merged file.For unusual input,this is more robust than using ed.If a FILE is '-', read standard input.Exit status is 0 ifsuccessful, 1 if conflicts, 2 if trouble.",
        "name": "diff3 - compare three files line by line",
        "section": 1
    },
    {
        "command": "dir",
        "description": "List information about the FILEs (the current directory bydefault).Sort entries alphabetically if none of -cftuvSUX nor--sort is specified.Mandatory arguments to long options are mandatory for shortoptions too.-a, --alldo not ignore entries starting with .-A, --almost-alldo not list implied . and ..--authorwith -l, print the author of each file-b, --escapeprint C-style escapes for nongraphic characters--block-size=SIZEwith -l, scale sizes by SIZE when printing them; e.g.,'--block-size=M'; see SIZE format below-B, --ignore-backupsdo not list implied entries ending with ~-cwith -lt: sort by, and show, ctime (time of last change offile status information); with -l: show ctime and sort byname; otherwise: sort by ctime, newest first-Clist entries by columns--color[=WHEN]color the output WHEN; more info below-d, --directorylist directories themselves, not their contents-D, --diredgenerate output designed for Emacs' dired mode-flist all entries in directory order-F, --classify[=WHEN]append indicator (one of */=>@|) to entries WHEN--file-typelikewise, except do not append '*'--format=WORDacross -x, commas -m, horizontal -x, long -l,single-column -1, verbose -l, vertical -C--full-timelike -l --time-style=full-iso-glike -l, but do not list owner--group-directories-firstgroup directories before files; can be augmented with a--sort option, but any use of --sort=none (-U) disablesgrouping-G, --no-groupin a long listing, don't print group names-h, --human-readablewith -l and -s, print sizes like 1K 234M 2G etc.--silikewise, but use powers of 1000 not 1024-H, --dereference-command-linefollow symbolic links listed on the command line--dereference-command-line-symlink-to-dirfollow each command line symbolic link that points to adirectory--hide=PATTERNdo not list implied entries matching shell PATTERN(overridden by -a or -A)--hyperlink[=WHEN]hyperlink file names WHEN--indicator-style=WORDappend indicator with style WORD to entry names: none(default), slash (-p), file-type (--file-type), classify(-F)-i, --inodeprint the index number of each file-I, --ignore=PATTERNdo not list implied entries matching shell PATTERN-k, --kibibytesdefault to 1024-byte blocks for file system usage; usedonly with -s and per directory totals-luse a long listing format-L, --dereferencewhen showing file information for a symbolic link, showinformation for the file the link references rather thanfor the link itself-mfill width with a comma separated list of entries-n, --numeric-uid-gidlike -l, but list numeric user and group IDs-N, --literalprint entry names without quoting-olike -l, but do not list group information-p, --indicator-style=slashappend / indicator to directories-q, --hide-control-charsprint ? instead of nongraphic characters--show-control-charsshow nongraphic characters as-is (the default, unlessprogram is 'ls' and output is a terminal)-Q, --quote-nameenclose entry names in double quotes--quoting-style=WORDuse quoting style WORD for entry names: literal, locale,shell, shell-always, shell-escape, shell-escape-always, c,escape (overrides QUOTING_STYLE environment variable)-r, --reversereverse order while sorting-R, --recursivelist subdirectories recursively-s, --sizeprint the allocated size of each file, in blocks-Ssort by file size, largest first--sort=WORDsort by WORD instead of name: none (-U), size (-S), time(-t), version (-v), extension (-X), width--time=WORDselect which timestamp used to display or sort; accesstime (-u): atime, access, use; metadata change time (-c):ctime, status; modified time (default): mtime,modification; birth time: birth, creation;with -l, WORD determines which time to show; with--sort=time, sort by WORD (newest first)--time-style=TIME_STYLEtime/date format with -l; see TIME_STYLE below-tsort by time, newest first; see --time-T, --tabsize=COLSassume tab stops at each COLS instead of 8-uwith -lt: sort by, and show, access time; with -l: showaccess time and sort by name; otherwise: sort by accesstime, newest first-Udo not sort; list entries in directory order-vnatural sort of (version) numbers within text-w, --width=COLSset output width to COLS.0 means no limit-xlist entries by lines instead of by columns-Xsort alphabetically by entry extension-Z, --contextprint any security context of each file--zero end each output line with NUL, not newline-1list one file per line--help display this help and exit--versionoutput version information and exitThe SIZE argument is an integer and optional unit (example: 10Kis 10*1024).Units are K,M,G,T,P,E,Z,Y,R,Q (powers of 1024) orKB,MB,... (powers of 1000).Binary prefixes can be used, too:KiB=K, MiB=M, and so on.The TIME_STYLE argument can be full-iso, long-iso, iso, locale,or +FORMAT.FORMAT is interpreted like in date(1).If FORMAT isFORMAT1<newline>FORMAT2, then FORMAT1 applies to non-recent filesand FORMAT2 to recent files.TIME_STYLE prefixed with 'posix-'takes effect only outside the POSIX locale.Also the TIME_STYLEenvironment variable sets the default style to use.The WHEN argument defaults to 'always' and can also be 'auto' or'never'.Using color to distinguish file types is disabled both by defaultand with --color=never.With --color=auto, ls emits color codesonly when standard output is connected to a terminal.TheLS_COLORS environment variable can change the settings.Use thedircolors(1) command to set it.Exit status:0if OK,1if minor problems (e.g., cannot access subdirectory),2if serious trouble (e.g., cannot access command-lineargument).",
        "name": "dir - list directory contents",
        "section": 1
    },
    {
        "command": "dircolors",
        "description": "Output commands to set the LS_COLORS environment variable.Determine format of output:-b, --sh, --bourne-shelloutput Bourne shell code to set LS_COLORS-c, --csh, --c-shelloutput C shell code to set LS_COLORS-p, --print-databaseoutput defaults--print-ls-colorsoutput fully escaped colors for display--help display this help and exit--versionoutput version information and exitIf FILE is specified, read it to determine which colors to usefor which file types and extensions.Otherwise, a precompileddatabase is used.For details on the format of these files, run'dircolors --print-database'.",
        "name": "dircolors - color setup for ls",
        "section": 1
    },
    {
        "command": "dirname",
        "description": "Output each NAME with its last non-slash component and trailingslashes removed; if NAME contains no /'s, output '.' (meaning thecurrent directory).-z, --zeroend each output line with NUL, not newline--help display this help and exit--versionoutput version information and exit",
        "name": "dirname - strip last component from file name",
        "section": 1
    },
    {
        "command": "dlltool",
        "description": "dlltool reads its inputs, which can come from the -d and -boptions as well as object files specified on the command line.It then processes these inputs and if the -e option has beenspecified it creates a exports file.If the -l option has beenspecified it creates a library file and if the -z option has beenspecified it creates a def file.Any or all of the -e, -l and -zoptions can be present in one invocation of dlltool.When creating a DLL, along with the source for the DLL, it isnecessary to have three other files.dlltool can help with thecreation of these files.The first file is a .def file which specifies which functions areexported from the DLL, which functions the DLL imports, and soon.This is a text file and can be created by hand, or dlltoolcan be used to create it using the -z option.In this casedlltool will scan the object files specified on its command linelooking for those functions which have been specially marked asbeing exported and put entries for them in the .def file itcreates.In order to mark a function as being exported from a DLL, itneeds to have an -export:<name_of_function> entry in the .drectvesection of the object file.This can be done in C by using theasm() operator:asm (\".section .drectve\");asm (\".ascii \\\"-export:my_func\\\"\");int my_func (void) { ... }The second file needed for DLL creation is an exports file.Thisfile is linked with the object files that make up the body of theDLL and it handles the interface between the DLL and the outsideworld.This is a binary file and it can be created by giving the-e option to dlltool when it is creating or reading in a .deffile.The third file needed for DLL creation is the library file thatprograms will link with in order to access the functions in theDLL (an `import library').This file can be created by givingthe -l option to dlltool when it is creating or reading in a .deffile.If the -y option is specified, dlltool generates a delay-importlibrary that can be used instead of the normal import library toallow a program to link to the dll only as soon as an importedfunction is called for the first time. The resulting executablewill need to be linked to the static delayimp library containing__delayLoadHelper2(), which in turn will import LoadLibraryA andGetProcAddress from kernel32.dlltool builds the library file by hand, but it builds theexports file by creating temporary files containing assemblerstatements and then assembling these.The -S command-line optioncan be used to specify the path to the assembler that dlltoolwill use, and the -f option can be used to pass specific flags tothat assembler.The -n can be used to prevent dlltool fromdeleting these temporary assembler files when it is done, and if-n is specified twice then this will prevent dlltool fromdeleting the temporary object files it used to build the library.Here is an example of creating a DLL from a source file dll.c andalso creating a program (from an object file called program.o)that uses that DLL:gcc -c dll.cdlltool -e exports.o -l dll.lib dll.ogcc dll.o exports.o -o dll.dllgcc program.o dll.lib -o programdlltool may also be used to query an existing import library todetermine the name of the DLL to which it is associated.See thedescription of the -I or --identify option.",
        "name": "dlltool - create files needed to build and use DLLs",
        "section": 1
    },
    {
        "command": "dmesg",
        "description": "dmesg is used to examine or control the kernel ring buffer.The default action is to display all messages from the kernelring buffer.",
        "name": "dmesg - print or control the kernel ring buffer",
        "section": 1
    },
    {
        "command": "dnsdomainname",
        "description": "Hostname is the program that is used to either set or display thecurrent host, domain or node name of the system.These names areused by many of the networking programs to identify the machine.The domain name is also used by NIS/YP.GET NAMEWhen called without any arguments, the program displays thecurrent names:hostname will print the name of the system as returned by thegethostname(2) function.domainname, nisdomainname, ypdomainname will print the name ofthe system as returned by the getdomainname(2) function. This isalso known as the YP/NIS domain name of the system.nodename will print the DECnet node name of the system asreturned by the getnodename(2) function.dnsdomainname will print the domain part of the FQDN (FullyQualified Domain Name). The complete FQDN of the system isreturned with hostname --fqdn.SET NAMEWhen called with one argument or with the --file option, thecommands set the host name, the NIS/YP domain name or the nodename.Note, that only the super-user can change the names.It is not possible to set the FQDN or the DNS domain name withthe dnsdomainname command (see THE FQDN below).The host name is usually set once at system startup by readingthe contents of a file which contains the host name, e.g./etc/hostname).THE FQDNYou can't change the FQDN (as returned by hostname --fqdn) or theDNS domain name (as returned by dnsdomainname) with this command.The FQDN of the system is the name that the resolver(3) returnsfor the host name.Technically: The FQDN is the canonical name returned bygethostbyname2(2) when resolving the result of the gethostname(2)name. The DNS domain name is the part after the first dot.Therefore it depends on the configuration (usually in/etc/host.conf) how you can change it. If hosts is the firstlookup method, you can change the FQDN in /etc/hosts.",
        "name": "hostname - show or set the system's host namednsdomainname - show the system's DNS domain namedomainname - show or set the system's NIS/YP domain namenisdomainname - show or set system's NIS/YP domain namenodename - show or set the system's DECnet node nameypdomainname - show or set the system's NIS/YP domain name",
        "section": 1
    },
    {
        "command": "domainname",
        "description": "Hostname is the program that is used to either set or display thecurrent host, domain or node name of the system.These names areused by many of the networking programs to identify the machine.The domain name is also used by NIS/YP.GET NAMEWhen called without any arguments, the program displays thecurrent names:hostname will print the name of the system as returned by thegethostname(2) function.domainname, nisdomainname, ypdomainname will print the name ofthe system as returned by the getdomainname(2) function. This isalso known as the YP/NIS domain name of the system.nodename will print the DECnet node name of the system asreturned by the getnodename(2) function.dnsdomainname will print the domain part of the FQDN (FullyQualified Domain Name). The complete FQDN of the system isreturned with hostname --fqdn.SET NAMEWhen called with one argument or with the --file option, thecommands set the host name, the NIS/YP domain name or the nodename.Note, that only the super-user can change the names.It is not possible to set the FQDN or the DNS domain name withthe dnsdomainname command (see THE FQDN below).The host name is usually set once at system startup by readingthe contents of a file which contains the host name, e.g./etc/hostname).THE FQDNYou can't change the FQDN (as returned by hostname --fqdn) or theDNS domain name (as returned by dnsdomainname) with this command.The FQDN of the system is the name that the resolver(3) returnsfor the host name.Technically: The FQDN is the canonical name returned bygethostbyname2(2) when resolving the result of the gethostname(2)name. The DNS domain name is the part after the first dot.Therefore it depends on the configuration (usually in/etc/host.conf) how you can change it. If hosts is the firstlookup method, you can change the FQDN in /etc/hosts.",
        "name": "hostname - show or set the system's host namednsdomainname - show the system's DNS domain namedomainname - show or set the system's NIS/YP domain namenisdomainname - show or set system's NIS/YP domain namenodename - show or set the system's DECnet node nameypdomainname - show or set the system's NIS/YP domain name",
        "section": 1
    },
    {
        "command": "dpkg",
        "description": "dpkg is a medium-level tool to install, build, remove and manageDebian packages.The primary and more user-friendly front-endfor dpkg as a CLI (command-line interface) is apt(8) and as a TUI(terminal user interface) is aptitude(8).dpkg itself iscontrolled entirely via command line parameters, which consist ofexactly one action and zero or more options. The action-parametertells dpkg what to do and options control the behavior of theaction in some way.dpkg can also be used as a front-end to dpkg-deb(1) anddpkg-query(1). The list of supported actions can be found lateron in the ACTIONS section. If any such action is encountered dpkgjust runs dpkg-deb or dpkg-query with the parameters given to it,but no specific options are currently passed to them, to use anysuch option the back-ends need to be called directly.",
        "name": "dpkg - package manager for Debian",
        "section": 1
    },
    {
        "command": "dpkg-architecture",
        "description": "dpkg-architecture provides a facility to determine and set thebuild and host architecture for package building.The build architecture is always determined by either theDEB_BUILD_ARCH variable if set (and --force not being specified)or by an external call to dpkg(1), and cannot be set at thecommand line.You can specify the host architecture by providing one or both ofthe options --host-arch and --host-type, otherwise theDEB_HOST_ARCH variable is used if set (and --force not beingspecified). The default is determined by an external call togcc(1), or the same as the build architecture if CC or gcc areboth not available. One out of --host-arch and --host-type issufficient, the value of the other will be set to a usabledefault. Indeed, it is often better to only specify one, becausedpkg-architecture will warn you if your choice does not match thedefault.",
        "name": "dpkg-architecture - set and determine the architecture forpackage building",
        "section": 1
    },
    {
        "command": "dpkg-buildflags",
        "description": "dpkg-buildflags is a tool to retrieve compilation flags to useduring build of Debian packages.The default flags are defined by the vendor but they can beextended/overridden in several ways:1.system-wide with /usr/local/etc/dpkg/buildflags.conf;2.for the current user with$XDG_CONFIG_HOME/dpkg/buildflags.conf where $XDG_CONFIG_HOMEdefaults to $HOME/.config;3.temporarily by the user with environment variables (seesection ENVIRONMENT);4.dynamically by the package maintainer with environmentvariables set via debian/rules (see section ENVIRONMENT).The configuration files can contain four types of directives:SET flag valueOverride the flag named flag to have the value value.STRIP flag valueStrip from the flag named flag all the build flags listed invalue.Since dpkg 1.16.1.APPEND flag valueExtend the flag named flag by appending the options given invalue.A space is prepended to the appended value if theflag's current value is non-empty.PREPEND flag valueExtend the flag named flag by prepending the options given invalue.A space is appended to the prepended value if theflag's current value is non-empty.Since dpkg 1.16.1.The configuration files can contain comments on lines startingwith a hash (#). Empty lines are also ignored.",
        "name": "dpkg-buildflags - returns build flags to use during package build",
        "section": 1
    },
    {
        "command": "dpkg-buildpackage",
        "description": "dpkg-buildpackage is a program that automates the process ofbuilding a Debian package. It consists of the following steps:1.It runs the preinit hook before reading any source file.Itprepares the build environment by setting various environmentvariables (see ENVIRONMENT), runs the init hook, and callsdpkg-source --before-build (unless -T or --target has beenused).2.It checks that the build-dependencies and build-conflicts aresatisfied (unless -d or --no-check-builddeps is specified).3.If one or more specific targets have been selected with the-T or --target option, it calls those targets and stops here.Otherwise it runs the preclean hook and calls fakerootdebian/rules clean to clean the build-tree (unless -nc or--no-pre-clean is specified).4.It runs the source hook and calls dpkg-source -b to generatethe source package (if a source build has been requested with--build or equivalent options).5.It runs the build hook and calls debian/rules build-target,then runs the binary hook followed by fakeroot debian/rulesbinary-target (unless a source-only build has been requestedwith --build=source or equivalent options).Note that build-target and binary-target are either build and binary (defaultcase, or if an any and all build has been requested with--build or equivalent options), or build-arch and binary-arch(if an any and not all build has been requested with --buildor equivalent options), or build-indep and binary-indep (ifan all and not any build has been requested with --build orequivalent options).6.It runs the buildinfo hook and calls dpkg-genbuildinfo togenerate a .buildinfo file.Several dpkg-buildpackageoptions are forwarded to dpkg-genbuildinfo.7.It runs the changes hook and calls dpkg-genchanges togenerate a .changes file.The name of the .changes file willdepend on the type of build and will be as specific asnecessary but not more; the name will be:source-name_binary-version_arch.changesfor a build that includes anysource-name_binary-version_all.changesotherwise for a build that includes allsource-name_source-version_source.changes.otherwise for a build that includes sourceMany dpkg-buildpackage options are forwarded to dpkg-genchanges.8.It runs the postclean hook and if -tc or --post-clean isspecified, it will call fakeroot debian/rules clean again.9.It calls dpkg-source --after-build.10. It runs the check hook and calls a package checker for the.changes file (if a command is specified in DEB_CHECK_COMMANDor with --check-command).11. It runs the sign hook and signs using the OpenPGP backend (aslong as it is not an UNRELEASED build, or --no-sign isspecified) to sign the .dsc file (if any, unless -us or--unsigned-source is specified), the .buildinfo file (unless-ui, --unsigned-buildinfo, -uc or --unsigned-changes isspecified) and the .changes file (unless -uc or--unsigned-changes is specified).12. It runs the done hook.",
        "name": "dpkg-buildpackage - build binary or source packages from sources",
        "section": 1
    },
    {
        "command": "dpkg-checkbuilddeps",
        "description": "This program checks the installed packages in the system againstthe build dependencies and build conflicts listed in the controlfile. If any are not met, it displays them and exits with anonzero return code.By default, debian/control is read, but an alternate controlfilename may be specified on the command line.",
        "name": "dpkg-checkbuilddeps - check build dependencies and conflicts",
        "section": 1
    },
    {
        "command": "dpkg-deb",
        "description": "dpkg-deb packs, unpacks and provides information about Debianarchives.Use dpkg to install and remove packages from your system.You can also invoke dpkg-deb by calling dpkg with whateveroptions you want to pass to dpkg-deb. dpkg will spot that youwanted dpkg-deb and run it for you.For most commands taking an input archive argument, the archivecan be read from standard input if the archive name is given as asingle minus character (\u00ab-\u00bb); otherwise lack of support will bedocumented in their respective command description.",
        "name": "dpkg-deb - Debian package archive (.deb) manipulation tool",
        "section": 1
    },
    {
        "command": "dpkg-distaddfile",
        "description": "dpkg-distaddfile adds an entry for a named file to debian/files.It takes three non-option arguments, the filename and the sectionand priority for the .changes file.The filename should be specified relative to the directory wheredpkg-genchanges will expect to find the files, usually .., ratherthan being a pathname relative to the current directory whendpkg-distaddfile is run.",
        "name": "dpkg-distaddfile - add entries to debian/files",
        "section": 1
    },
    {
        "command": "dpkg-divert",
        "description": "dpkg-divert is the utility used to set up and update the list ofdiversions.File diversions are a way of forcing dpkg(1) not to install afile into its location, but to a diverted location. Diversionscan be used through the Debian package scripts to move a fileaway when it causes a conflict. System administrators can alsouse it to override some package's configuration file, or wheneversome files (which aren't marked as \u201cconffiles\u201d) need to bepreserved by dpkg, when installing a newer version of a packagewhich contains those files.",
        "name": "dpkg-divert - override a package's version of a file",
        "section": 1
    },
    {
        "command": "dpkg-genbuildinfo",
        "description": "dpkg-genbuildinfo reads information from an unpacked and builtDebian source tree and from the files it has generated andgenerates a Debian control file describing the build environmentand the build artifacts (.buildinfo file).This command was introduced in dpkg 1.18.11.",
        "name": "dpkg-genbuildinfo - generate Debian .buildinfo files",
        "section": 1
    },
    {
        "command": "dpkg-genchanges",
        "description": "dpkg-genchanges reads information from an unpacked and builtDebian source tree and from the files it has generated andgenerates a Debian upload control file (.changes file).",
        "name": "dpkg-genchanges - generate Debian .changes files",
        "section": 1
    },
    {
        "command": "dpkg-gencontrol",
        "description": "dpkg-gencontrol reads information from an unpacked Debian sourcetree and generates a binary package control file (which defaultsto debian/tmp/DEBIAN/control); during this process it willsimplify the relation fields.Thus Pre-Depends, Depends, Recommends and Suggests are simplifiedin this order by removing dependencies which are known to be trueaccording to the stronger dependencies already parsed. It willalso remove any self-dependency (in fact it will remove anydependency which evaluates to true given the current version ofthe package as installed). Logically it keeps the intersection ofmultiple dependencies on the same package. The order ofdependencies is preserved as best as possible: if any dependencymust be discarded due to another dependency appearing further inthe field, the superseding dependency will take the place of thediscarded one.The other relation fields (Enhances, Conflicts, Breaks, Replacesand Provides) are also simplified individually by computing theunion of the various dependencies when a package is listedmultiple times in the field.dpkg-gencontrol also adds an entry for the binary package todebian/files.",
        "name": "dpkg-gencontrol - generate Debian control files",
        "section": 1
    },
    {
        "command": "dpkg-gensymbols",
        "description": "dpkg-gensymbols scans a temporary build tree (debian/tmp bydefault) looking for libraries and generates a symbols filedescribing them. This file, if non-empty, is then installed inthe DEBIAN subdirectory of the build tree so that it ends upincluded in the control information of the package.When generating those files, it uses as input some symbols filesprovided by the maintainer. It looks for the following files (anduses the first that is found):\u2022debian/package.symbols.arch\u2022debian/symbols.arch\u2022debian/package.symbols\u2022debian/symbolsThe main interest of those files is to provide the minimalversion associated to each symbol provided by the libraries.Usually it corresponds to the first version of that package thatprovided the symbol, but it can be manually incremented by themaintainer if the ABI of the symbol is extended without breakingbackwards compatibility. It's the responsibility of themaintainer to keep those files up-to-date and accurate, but dpkg-gensymbols helps with that.When the generated symbols files differ from the maintainersupplied one, dpkg-gensymbols will print a diff between the twoversions.Furthermore if the difference is too significant, itwill even fail (you can customize how much difference you cantolerate, see the -c option).",
        "name": "dpkg-gensymbols - generate symbols files (shared librarydependency information)",
        "section": 1
    },
    {
        "command": "dpkg-maintscript-helper",
        "description": "This program is designed to be run within maintainer scripts toachieve some tasks that dpkg can't (yet) handle natively eitherbecause of design decisions or due to current limitations.Many of those tasks require coordinated actions from severalmaintainer scripts (preinst, postinst, prerm, postrm). To avoidmistakes the same call simply needs to be put in all scripts andthe program will automatically adapt its behavior based on theenvironment variable DPKG_MAINTSCRIPT_NAME and on the maintainerscripts arguments that you have to forward after a double hyphen.",
        "name": "dpkg-maintscript-helper - works around known dpkg limitations inmaintainer scripts",
        "section": 1
    },
    {
        "command": "dpkg-mergechangelogs",
        "description": "This program will use the 3 provided versions of the Debianchangelog to generate a merged changelog file. The resultingchangelog is stored in the file out or output to the standardoutput if that parameter is not given.Each entry is identified by its version number and they areassumed to be not conflicting, they are simply merged in theright order (by decreasing version number). When--merge-prereleases is used, the part of the version number afterthe last tilde is dropped so that 1.0-1~exp1 and 1.0-1~exp5 areconsidered to be the same entry. When the same version isavailable in both new-a and new-b, a standard line-based 3-waymerge is attempted (provided that the module Algorithm::Merge isavailable \u2014 it's part of the package libalgorithm-merge-perl \u2014otherwise you get a global conflict on the content of the entry).",
        "name": "dpkg-mergechangelogs - 3-way merge of debian/changelog files",
        "section": 1
    },
    {
        "command": "dpkg-name",
        "description": "This manual page documents the dpkg-name program which providesan easy way to rename Debian packages into their full packagenames. A full package name consists ofpackage_version_architecture.package-type as specified in thecontrol file of the package. The version part of the filenameconsists of the upstream version information optionally followedby a hyphen and the revision information. The package-type partcomes from that field if present or fallbacks to deb.",
        "name": "dpkg-name - rename Debian packages to full package names",
        "section": 1
    },
    {
        "command": "dpkg-parsechangelog",
        "description": "dpkg-parsechangelog reads and parses the changelog of an unpackedDebian source tree and outputs the information in it to standardoutput in a machine-readable form.",
        "name": "dpkg-parsechangelog - parse Debian changelog files",
        "section": 1
    },
    {
        "command": "dpkg-query",
        "description": "dpkg-query is a tool to show information about packages listed inthe dpkg database.",
        "name": "dpkg-query - a tool to query the dpkg database",
        "section": 1
    },
    {
        "command": "dpkg-realpath",
        "description": "dpkg-realpath is a tool (since dpkg 1.20.1) to resolve apathname, that takes the dpkg(1) root directory into account,either implicitly from the DPKG_ROOT environment variable or fromthe command-line --root or --instdir options, and returns anabsolute pathname relative to the root directory.The rootdirectory must not be prefixed to the pathname to be resolved.This is intended to be used by other dpkg helpers, or bymaintainer scripts instead of using realpath(1) or readlink(1) tocanonicalize pathnames, as these latter commands do not supportcanonicalization relative to a different root than /.",
        "name": "dpkg-realpath - print the resolved pathname with DPKG_ROOTsupport",
        "section": 1
    },
    {
        "command": "dpkg-scanpackages",
        "description": "dpkg-scanpackages sorts through a tree of Debian binary packagesand creates a Packages file, used by apt(8), dselect(1), etc, totell the user what packages are available for installation. ThesePackages files are the same as those found on Debian archivesites and CD-ROMs. You might use dpkg-scanpackages yourself ifmaking a directory of local packages to install on a cluster ofmachines.Note: If you want to access the generated Packages file withapt(8) you will probably need to compress the file with xz(1)(generating a Packages.xz file), bzip2(1) (generating aPackages.bz2 file) or gzip(1) (generating a Packages.gz file).apt(8) ignores uncompressed Packages files except on local access(i.e.file:// sources).binary-path is the name of the tree of the binary packages toprocess (for example, contrib/binary-i386).It is best to makethis relative to the root of the Debian archive, because everyFilename field in the new Packages file will start with thisstring.override-file is the name of a file to read which containsinformation about how the package fits into the distribution (thefile can be compressed since dpkg 1.15.5); see deb-override(5).path-prefix is an optional string to be prepended to the Filenamefields.If more than one version of a package is found only the newestone is included in the output. If they have the same version andonly differ in architecture only the first one found is used.",
        "name": "dpkg-scanpackages - create Packages index files",
        "section": 1
    },
    {
        "command": "dpkg-scansources",
        "description": "dpkg-scansources scans the given binary-dir for .dsc files.These are used to create a Debian source index, which is outputto stdout.The override-file, if given, is used to set priorities in theresulting index stanzas and to override the maintainer fieldgiven in the .dsc files.The file can be compressed (since dpkg1.15.5).See deb-override(5) for the format of this file.Note: Since the override file is indexed by binary, not sourcepackages, there's a bit of a problem here. The currentimplementation uses the highest priority of all the binarypackages produced by a .dsc file for the priority of the sourcepackage, and the override entry for the first binary packagelisted in the .dsc file to modify maintainer information. Thismight change.The path-prefix, if given, is prepended to the directory field inthe generated source index. You generally use this to make thedirectory fields contain the path from the top of the Debianarchive hierarchy.Note: If you want to access the generated Sources file withapt(8) you will probably need to compress the file with gzip(1)(generating a Sources.gz file).apt(8) ignores uncompressedSources files except on local access (i.e.file:// sources).",
        "name": "dpkg-scansources - create Sources index files",
        "section": 1
    },
    {
        "command": "dpkg-shlibdeps",
        "description": "dpkg-shlibdeps calculates shared library dependencies forexecutables named in its arguments. The dependencies are added tothe substitution variables file debian/substvars as variablenames shlibs:dependency-field where dependency-field is adependency field name. Any other variables starting with shlibs:are removed from the file.dpkg-shlibdeps has two possible sources of information togenerate dependency information. Either symbols files or shlibsfiles. For each binary that dpkg-shlibdeps analyzes, it finds outthe list of libraries that it's linked with.Then, for eachlibrary, it looks up either the symbols file, or the shlibs file(if the former doesn't exist or if debian/shlibs.local containsthe relevant dependency). Both files are supposed to be providedby the library package and should thus be available as/usr/local/var/lib/dpkg/info/package.symbols or/usr/local/var/lib/dpkg/info/package.shlibs. The package name isidentified in two steps: find the library file on the system(looking in the same directories that ld.so would use), then usedpkg -S library-file to lookup the package providing the library.Symbols filesSymbols files contain finer-grained dependency information byproviding the minimum dependency for each symbol that the libraryexports. The script tries to find a symbols file associated to alibrary package in the following places (first match is used):debian/*/DEBIAN/symbolsShared library information generated by the current buildprocess that also invoked dpkg-shlibdeps.They are generatedby dpkg-gensymbols(1).They are only used if the library isfound in a package's build tree. The symbols file in thatbuild tree takes precedence over symbols files from otherbinary packages./usr/local/etc/dpkg/symbols/package.symbols.arch/usr/local/etc/dpkg/symbols/package.symbolsPer-system overriding shared library dependency information.arch is the architecture of the current system (obtained bydpkg-architecture -qDEB_HOST_ARCH).Output from \u201cdpkg-query --control-path package symbols\u201dPackage-provided shared library dependency information.Unless overridden by --admindir, those files are located in/usr/local/var/lib/dpkg.While scanning the symbols used by all binaries, dpkg-shlibdepsremembers the (biggest) minimal version needed for each library.At the end of the process, it is able to write out the minimaldependency for every library used (provided that the informationof the symbols files are accurate).As a safe-guard measure, a symbols file can provide a Build-Depends-Package meta-information field and dpkg-shlibdeps willextract the minimal version required by the corresponding packagein the Build-Depends field and use this version if it's higherthan the minimal version computed by scanning symbols.Shlibs filesShlibs files associate directly a library to a dependency(without looking at the symbols). It's thus often stronger thanreally needed but very safe and easy to handle.The dependencies for a library are looked up in several places.The first file providing information for the library of interestis used:debian/shlibs.localPackage-local overriding shared library dependencyinformation./usr/local/etc/dpkg/shlibs.overridePer-system overriding shared library dependency information.debian/*/DEBIAN/shlibsShared library information generated by the current buildprocess that also invoked dpkg-shlibdeps.They are only usedif the library is found in a package's build tree. The shlibsfile in that build tree takes precedence over shlibs filesfrom other binary packages.Output from \u201cdpkg-query --control-path package shlibs\u201dPackage-provided shared library dependency information.Unless overridden by --admindir, those files are located in/usr/local/var/lib/dpkg./usr/local/etc/dpkg/shlibs.defaultPer-system default shared library dependency information.The extracted dependencies are then directly used (except if theyare filtered out because they have been identified as duplicate,or as weaker than another dependency).",
        "name": "dpkg-shlibdeps - generate shared library substvar dependencies",
        "section": 1
    },
    {
        "command": "dpkg-source",
        "description": "dpkg-source packs and unpacks Debian source archives.None of these commands allow multiple options to be combined intoone, and they do not allow the value for an option to bespecified in a separate argument.",
        "name": "dpkg-source - Debian source package (.dsc) manipulation tool",
        "section": 1
    },
    {
        "command": "dpkg-split",
        "description": "dpkg-split splits Debian binary package files into smaller partsand reassembles them again, to support the storage of largepackage files on small media such as floppy disks.It can be operated manually using the --split, --join and --infooptions.It also has an automatic mode, invoked using the --auto option,where it maintains a queue of parts seen but not yet reassembledand reassembles a package file when it has seen all of its parts.The --listq and --discard options allow the management of thequeue.All splitting, joining and queueing operations produceinformative messages on standard output; these may safely beignored.",
        "name": "dpkg-split - Debian package archive split/join tool",
        "section": 1
    },
    {
        "command": "dpkg-statoverride",
        "description": "\u201cstat overrides\u201d are a way to tell dpkg(1) to use a differentowner or mode for a path when a package is installed (thisapplies to any filesystem object that dpkg handles, includingdirectories, devices, etc.). This can be used to force programsthat are normally setuid to be install without a setuid flag, oronly executable by a certain group.dpkg-statoverride is a utility to manage the list of statoverrides. It has three basic functions: adding, removing andlisting overrides.",
        "name": "dpkg-statoverride - override ownership and mode of files",
        "section": 1
    },
    {
        "command": "dpkg-trigger",
        "description": "dpkg-trigger is a tool to explicitly activate triggers and checkfor its support on the running dpkg.This can be used by maintainer scripts in complex and conditionalsituations where the file triggers, or the declarative activatetriggers control file directive, are insufficiently rich. It canalso be used for testing and by system administrators (but notethat the triggers won't actually be run by dpkg-trigger).Unrecognized trigger name syntaxes are an error for dpkg-trigger.",
        "name": "dpkg-trigger - a package trigger utility",
        "section": 1
    },
    {
        "command": "dpkg-vendor",
        "description": "dpkg-vendor is a tool to query information about vendors listedin /usr/local/etc/dpkg/origins./usr/local/etc/dpkg/origins/default contains information aboutthe current vendor.",
        "name": "dpkg-vendor - queries information about distribution vendors",
        "section": 1
    },
    {
        "command": "dselect",
        "description": "dselect is one of the primary user interfaces for managingpackages on a Debian system. At the dselect main menu, the systemadministrator can:\u2022Update the list of available package versions,\u2022View the status of installed and available packages,\u2022Alter package selections and manage dependencies,\u2022Install new packages or upgrade to newer versions.dselect operates as a front-end to dpkg(1), the low-level Debianpackage handling tool. It features a full-screen packageselections manager with package depends and conflicts resolver.When run with administrator privileges, packages can beinstalled, upgraded and removed. Various access methods can beconfigured to retrieve available package version information andinstallable packages from package repositories.Depending on theused access method, these repositories can be public archiveservers on the internet, local archive servers or CD-ROMs.Therecommended access method is apt, which is provided by thepackage apt(8).Normally dselect is invoked without parameters. An interactivemenu is presented, offering the user a list of commands. If acommand is given as argument, then that command is startedimmediately. Several command line parameters are still availableto modify the running behaviour of dselect or show additionalinformation about the program.",
        "name": "dselect - Debian package management frontend",
        "section": 1
    },
    {
        "command": "dtrace",
        "description": "The dtrace command converts probe descriptions defined in file.dinto a probe header file via the -h option or a probe descriptionfile via the -G option.",
        "name": "dtrace - Dtrace compatible user application static probegeneration tool.",
        "section": 1
    },
    {
        "command": "du",
        "description": "Summarize device usage of the set of FILEs, recursively fordirectories.Mandatory arguments to long options are mandatory for shortoptions too.-0, --nullend each output line with NUL, not newline-a, --allwrite counts for all files, not just directories--apparent-sizeprint apparent sizes rather than device usage; althoughthe apparent size is usually smaller, it may be larger dueto holes in ('sparse') files, internal fragmentation,indirect blocks, and the like-B, --block-size=SIZEscale sizes by SIZE before printing them; e.g., '-BM'prints sizes in units of 1,048,576 bytes; see SIZE formatbelow-b, --bytesequivalent to '--apparent-size --block-size=1'-c, --totalproduce a grand total-D, --dereference-argsdereference only symlinks that are listed on the commandline-d, --max-depth=Nprint the total for a directory (or file, with --all) onlyif it is N or fewer levels below the command lineargument;--max-depth=0 is the same as --summarize--files0-from=Fsummarize device usage of the NUL-terminated file namesspecified in file F; if F is -, then read names fromstandard input-Hequivalent to --dereference-args (-D)-h, --human-readableprint sizes in human readable format (e.g., 1K 234M 2G)--inodeslist inode usage information instead of block usage-klike --block-size=1K-L, --dereferencedereference all symbolic links-l, --count-linkscount sizes many times if hard linked-mlike --block-size=1M-P, --no-dereferencedon't follow any symbolic links (this is the default)-S, --separate-dirsfor directories do not include size of subdirectories--silike -h, but use powers of 1000 not 1024-s, --summarizedisplay only a total for each argument-t, --threshold=SIZEexclude entries smaller than SIZE if positive, or entriesgreater than SIZE if negative--time show time of the last modification of any file in thedirectory, or any of its subdirectories--time=WORDshow time as WORD instead of modification time: atime,access, use, ctime or status--time-style=STYLEshow times using STYLE, which can be: full-iso, long-iso,iso, or +FORMAT; FORMAT is interpreted like in 'date'-X, --exclude-from=FILEexclude files that match any pattern in FILE--exclude=PATTERNexclude files that match PATTERN-x, --one-file-systemskip directories on different file systems--help display this help and exit--versionoutput version information and exitDisplay values are in units of the first available SIZE from--block-size, and the DU_BLOCK_SIZE, BLOCK_SIZE and BLOCKSIZEenvironment variables.Otherwise, units default to 1024 bytes(or 512 if POSIXLY_CORRECT is set).The SIZE argument is an integer and optional unit (example: 10Kis 10*1024).Units are K,M,G,T,P,E,Z,Y,R,Q (powers of 1024) orKB,MB,... (powers of 1000).Binary prefixes can be used, too:KiB=K, MiB=M, and so on.",
        "name": "du - estimate file space usage",
        "section": 1
    },
    {
        "command": "dumpkeys",
        "description": "dumpkeys writes, to the standard output, the current contents ofthe keyboard driver's translation tables, in the format specifiedby keymaps(5).Using the various options, the format of the output can becontrolled and also other information from the kernel and theprograms dumpkeys(1) and loadkeys(1) can be obtained.",
        "name": "dumpkeys - dump keyboard translation tables",
        "section": 1
    },
    {
        "command": "echo",
        "description": "Echo the STRING(s) to standard output.-ndo not output the trailing newline-eenable interpretation of backslash escapes-Edisable interpretation of backslash escapes (default)--help display this help and exit--versionoutput version information and exitIf -e is in effect, the following sequences are recognized:\\\\backslash\\aalert (BEL)\\bbackspace\\cproduce no further output\\eescape\\fform feed\\nnew line\\rcarriage return\\thorizontal tab\\vvertical tab\\0NNNbyte with octal value NNN (1 to 3 digits)\\xHHbyte with hexadecimal value HH (1 to 2 digits)NOTE: your shell may have its own version of echo, which usuallysupersedes the version described here.Please refer to yourshell's documentation for details about the options it supports.NOTE: printf(1) is a preferred alternative, which does not haveissues outputting option-like strings.",
        "name": "echo - display a line of text",
        "section": 1
    },
    {
        "command": "eject",
        "description": "eject allows removable media (typically a CD-ROM, floppy disk,tape, JAZ, ZIP or USB disk) to be ejected under software control.The command can also control some multi-disc CD-ROM changers, theauto-eject feature supported by some devices, and close the disctray of some CD-ROM drives.The device corresponding to device or mountpoint is ejected. Ifno name is specified, the default name /dev/cdrom is used. Thedevice may be addressed by device name (e.g., 'sda'), device path(e.g., '/dev/sda'), UUID=uuid or LABEL=label tags.There are four different methods of ejecting, depending onwhether the device is a CD-ROM, SCSI device, removable floppy, ortape. By default eject tries all four methods in order until itsucceeds.If a device partition is specified, the whole-disk device isused.If the device or a device partition is currently mounted, it isunmounted before ejecting. The eject is processed on exclusiveopen block device file descriptor if --no-unmount or --force arenot specified.",
        "name": "eject - eject removable mediaeject [options] device|mountpoint",
        "section": 1
    },
    {
        "command": "elfedit",
        "description": "elfedit updates the ELF header and program property of ELF fileswhich have the matching ELF machine and file types.The optionscontrol how and which fields in the ELF header and programproperty should be updated.elffile... are the ELF files to be updated.32-bit and 64-bitELF files are supported, as are archives containing ELF files.",
        "name": "elfedit - update ELF header and program property of ELF files",
        "section": 1
    },
    {
        "command": "enosys",
        "description": "enosys is a simple command to execute a child process for whichcertain syscalls fail with errno ENOSYS.It can be used to test the behavior of applications in the faceof missing syscalls as would happen when running on old kernels.",
        "name": "enosys - utility make syscalls fail with ENOSYS",
        "section": 1
    },
    {
        "command": "env",
        "description": "Set each NAME to VALUE in the environment and run COMMAND.Mandatory arguments to long options are mandatory for shortoptions too.-i, --ignore-environmentstart with an empty environment-0, --nullend each output line with NUL, not newline-u, --unset=NAMEremove variable from the environment-C, --chdir=DIRchange working directory to DIR-S, --split-string=Sprocess and split S into separate arguments; used to passmultiple arguments on shebang lines--block-signal[=SIG]block delivery of SIG signal(s) to COMMAND--default-signal[=SIG]reset handling of SIG signal(s) to the default--ignore-signal[=SIG]set handling of SIG signal(s) to do nothing--list-signal-handlinglist non default signal handling to stderr-v, --debugprint verbose information for each processing step--help display this help and exit--versionoutput version information and exitA mere - implies -i.If no COMMAND, print the resultingenvironment.SIG may be a signal name like 'PIPE', or a signal number like'13'.Without SIG, all known signals are included.Multiplesignals can be comma-separated.An empty SIG argument is ano-op.Exit status:125if the env command itself fails126if COMMAND is found but cannot be invoked127if COMMAND cannot be found-the exit status of COMMAND otherwise",
        "name": "env - run a program in a modified environment",
        "section": 1
    },
    {
        "command": "envsubst",
        "description": "Substitutes the values of environment variables.Operation mode:-v, --variablesoutput the variables occurring in SHELL-FORMATInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exitIn normal operation mode, standard input is copied to standardoutput, with references to environment variables of the form$VARIABLE or ${VARIABLE} being replaced with the correspondingvalues.If a SHELL-FORMAT is given, only those environmentvariables that are referenced in SHELL-FORMAT are substituted;otherwise all environment variables references occurring instandard input are substituted.When --variables is used, standard input is ignored, and theoutput consists of the environment variables that are referencedin SHELL-FORMAT, one per line.",
        "name": "envsubst - substitutes environment variables in shell formatstrings",
        "section": 1
    },
    {
        "command": "eqn",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "eqn2graph",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "expand",
        "description": "Convert tabs in each FILE to spaces, writing to standard output.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-i, --initialdo not convert tabs after non blanks-t, --tabs=Nhave tabs N characters apart, not 8-t, --tabs=LISTuse comma separated list of tab positions.The lastspecified position can be prefixed with '/' to specify atab size to use after the last explicitly specified tabstop.Also a prefix of '+' can be used to align remainingtab stops relative to the last specified tab stop insteadof the first column--help display this help and exit--versionoutput version information and exit",
        "name": "expand - convert tabs to spaces",
        "section": 1
    },
    {
        "command": "expect",
        "description": null,
        "name": "expect - programmed dialogue with interactive programs, Version 5",
        "section": 1
    },
    {
        "command": "expiry",
        "description": "The expiry command checks (-c) the current password expirationand forces (-f) changes when required. It is callable as a normaluser command.",
        "name": "expiry - check and enforce password expiration policy",
        "section": 1
    },
    {
        "command": "expr",
        "description": "--help display this help and exit--versionoutput version information and exitPrint the value of EXPRESSION to standard output.A blank linebelow separates increasing precedence groups.EXPRESSION may be:ARG1 | ARG2ARG1 if it is neither null nor 0, otherwise ARG2ARG1 & ARG2ARG1 if neither argument is null or 0, otherwise 0ARG1 < ARG2ARG1 is less than ARG2ARG1 <= ARG2ARG1 is less than or equal to ARG2ARG1 = ARG2ARG1 is equal to ARG2ARG1 != ARG2ARG1 is unequal to ARG2ARG1 >= ARG2ARG1 is greater than or equal to ARG2ARG1 > ARG2ARG1 is greater than ARG2ARG1 + ARG2arithmetic sum of ARG1 and ARG2ARG1 - ARG2arithmetic difference of ARG1 and ARG2ARG1 * ARG2arithmetic product of ARG1 and ARG2ARG1 / ARG2arithmetic quotient of ARG1 divided by ARG2ARG1 % ARG2arithmetic remainder of ARG1 divided by ARG2STRING : REGEXPanchored pattern match of REGEXP in STRINGmatch STRING REGEXPsame as STRING : REGEXPsubstr STRING POS LENGTHsubstring of STRING, POS counted from 1index STRING CHARSindex in STRING where any CHARS is found, or 0length STRINGlength of STRING+ TOKENinterpret TOKEN as a string, even if it is akeyword like 'match' or an operator like '/'( EXPRESSION )value of EXPRESSIONBeware that many operators need to be escaped or quoted forshells.Comparisons are arithmetic if both ARGs are numbers,else lexicographical.Pattern matches return the string matchedbetween \\( and \\) or null; if \\( and \\) are not used, they returnthe number of characters matched or 0.Exit status is 0 if EXPRESSION is neither null nor 0, 1 ifEXPRESSION is null or 0, 2 if EXPRESSION is syntacticallyinvalid, and 3 if an error occurred.",
        "name": "expr - evaluate expressions",
        "section": 1
    },
    {
        "command": "factor",
        "description": "Print the prime factors of each specified integer NUMBER.Ifnone are specified on the command line, read them from standardinput.-h, --exponentsprint repeated factors in form p^e unless e is 1--help display this help and exit--versionoutput version information and exit",
        "name": "factor - factor numbers",
        "section": 1
    },
    {
        "command": "fadvise",
        "description": "fadvise is a simple command wrapping posix_fadvise system callthat is for predeclaring an access pattern for file data.",
        "name": "fadvise - utility to use the posix_fadvise system call",
        "section": 1
    },
    {
        "command": "fallocate",
        "description": "fallocate is used to manipulate the allocated disk space for afile, either to deallocate or preallocate it. For filesystemswhich support the fallocate(2) system call, preallocation is donequickly by allocating blocks and marking them as uninitialized,requiring no IO to the data blocks. This is much faster thancreating a file by filling it with zeroes.The exit status returned by fallocate is 0 on success and 1 onfailure.",
        "name": "fallocate - preallocate or deallocate space to a file",
        "section": 1
    },
    {
        "command": "false",
        "description": "Exit with a status code indicating failure.--help display this help and exit--versionoutput version information and exitNOTE: your shell may have its own version of false, which usuallysupersedes the version described here.Please refer to yourshell's documentation for details about the options it supports.",
        "name": "false - do nothing, unsuccessfully",
        "section": 1
    },
    {
        "command": "fedabipkgdiff",
        "description": null,
        "name": "fedabipkgdiff - compare ABIs of Fedora packagesfedabipkgdiff compares the ABI of shared libraries in Fedorapackages.It's a convenient way to do so without having tomanually download packages from the Fedora Build System.fedabipkgdiff knows how to talk with the Fedora Build System tofind the right packages versions, their associated debuginformation and development packages, download them, comparetheir ABI locally, and report about the possible ABI changes.Note that by default, this tool reports ABI changes about typesthat are defined in public header files found in the developmentpackages associated with the packages being compared.It alsoreports ABI changes about functions and global variables whosesymbols are defined and exported in the ELF binaries found in thepackages being compared.",
        "section": 1
    },
    {
        "command": "fgconsole",
        "description": "If the active Virtual Terminal is /dev/ttyN, then prints N onstandard output.If the console is a serial console, then \"serial\" is printedinstead.-h --helpPrints short usage message and exits.-V --versionPrints version number and exits.--next-availableWill show the next unallocated virtual terminal. Normally6 virtual terminals are allocated, with number 7 used forX; this will return \"8\" in this case.",
        "name": "fgconsole - print the number of the active VT.",
        "section": 1
    },
    {
        "command": "file",
        "description": "This manual page documents version 5.44 of the file command.file tests each argument in an attempt to classify it.There arethree sets of tests, performed in this order: filesystem tests,magic tests, and language tests.The first test that succeedscauses the file type to be printed.The type printed will usually contain one of the words text (thefile contains only printing characters and a few common controlcharacters and is probably safe to read on an ASCII terminal),executable (the file contains the result of compiling a program ina form understandable to some UNIX kernel or another), or datameaning anything else (data is usually \u201cbinary\u201d or non-printable).Exceptions are well-known file formats (core files, tar archives)that are known to contain binary data.When modifying magic filesor the program itself, make sure to preserve these keywords.Usersdepend on knowing that all the readable files in a directory havethe word \u201ctext\u201d printed.Don't do as Berkeley did and change\u201cshell commands text\u201d to \u201cshell script\u201d.The filesystem tests are based on examining the return from astat(2) system call.The program checks to see if the file isempty, or if it's some sort of special file.Any known file typesappropriate to the system you are running on (sockets, symboliclinks, or named pipes (FIFOs) on those systems that implement them)are intuited if they are defined in the system header file<sys/stat.h>.The magic tests are used to check for files with data in particularfixed formats.The canonical example of this is a binaryexecutable (compiled program) a.out file, whose format is definedin <elf.h>, <a.out.h> and possibly <exec.h> in the standard includedirectory.These files have a \u201cmagic number\u201d stored in aparticular place near the beginning of the file that tells the UNIXoperating system that the file is a binary executable, and which ofseveral types thereof.The concept of a \u201cmagic number\u201d has beenapplied by extension to data files.Any file with some invariantidentifier at a small fixed offset into the file can usually bedescribed in this way.The information identifying these files isread from the compiled magic file /usr/local/share/misc/magic.mgc,or the files in the directory /usr/local/share/misc/magic if thecompiled file does not exist.In addition, if $HOME/.magic.mgc or$HOME/.magic exists, it will be used in preference to the systemmagic files.If a file does not match any of the entries in the magic file, itis examined to see if it seems to be a text file.ASCII,ISO-8859-x, non-ISO 8-bit extended-ASCII character sets (such asthose used on Macintosh and IBM PC systems), UTF-8-encoded Unicode,UTF-16-encoded Unicode, and EBCDIC character sets can bedistinguished by the different ranges and sequences of bytes thatconstitute printable text in each set.If a file passes any ofthese tests, its character set is reported.ASCII, ISO-8859-x,UTF-8, and extended-ASCII files are identified as \u201ctext\u201d becausethey will be mostly readable on nearly any terminal; UTF-16 andEBCDIC are only \u201ccharacter data\u201d because, while they contain text,it is text that will require translation before it can be read.Inaddition, file will attempt to determine other characteristics oftext-type files.If the lines of a file are terminated by CR,CRLF, or NEL, instead of the Unix-standard LF, this will bereported.Files that contain embedded escape sequences oroverstriking will also be identified.Once file has determined the character set used in a text-typefile, it will attempt to determine in what language the file iswritten.The language tests look for particular strings (cf.<names.h>) that can appear anywhere in the first few blocks of afile.For example, the keyword .br indicates that the file is mostlikely a troff(1) input file, just as the keyword struct indicatesa C program.These tests are less reliable than the previous twogroups, so they are performed last.The language test routinesalso test for some miscellany (such as tar(1) archives, JSONfiles).Any file that cannot be identified as having been written in any ofthe character sets listed above is simply said to be \u201cdata\u201d.",
        "name": "file \u2014 determine file type",
        "section": 1
    },
    {
        "command": "fincore",
        "description": "fincore counts pages of file contents being resident in memory(in core), and reports the numbers. If an error occurs duringcounting, then an error message is printed to the stderr andfincore continues processing the rest of files listed in acommand line.The default output is subject to change. So whenever possible,you should avoid using default outputs in your scripts. Alwaysexplicitly define expected columns by using --output columns-listin environments where a stable output is required.",
        "name": "fincore - count pages of file contents in core",
        "section": 1
    },
    {
        "command": "find",
        "description": "This manual page documents the GNU version of find.GNU findsearches the directory tree rooted at each given starting-pointby evaluating the given expression from left to right, accordingto the rules of precedence (see section OPERATORS), until theoutcome is known (the left hand side is false for and operations,true for or), at which point find moves on to the next file name.If no starting-point is specified, `.' is assumed.If you are using find in an environment where security isimportant (for example if you are using it to search directoriesthat are writable by other users), you should read the `SecurityConsiderations' chapter of the findutils documentation, which iscalled Finding Files and comes with findutils.That documentalso includes a lot more detail and discussion than this manualpage, so you may find it a more useful source of information.",
        "name": "find - search for files in a directory hierarchy",
        "section": 1
    },
    {
        "command": "find-filter",
        "description": "find-filter is a filter for a list of file names read on standardinput, and if the files match the predicate their names arewritten on standard output.The supported predicate forms are based on the file's creationtime or modification time, and take the form ctime or mtimefollowed by a time specification.A time specification takes theform of a + or - followed by days (a non-negative integer),optionally followed by a colon (:) and hours (an integer in therange 0 to 23), optionally followed by another colon (:) andminutes (an integer in the range 0 to 59).The semantics of the time specification are that a file matchesthe predicate if the chosen time attribute for the file is lessthan (+) the current time minus the time specification, else morethan or equal to (-) the current time minus the timespecification.Alternatively, + means before the current timeminus the time specification, and - means at or after the currenttime minus the time specification.find-filter is intended to be used to provide finer-grain andplatform independent selection compared to the -mtime or -ctimeoptions of find(1).find-filter is expected to be used as apost-filter for the output from find(1), and this is how it isused in the pmlogger_daily(1) scripts to select files to becompressed or culled.",
        "name": "find-filter - fine-grained file selection based on timeattributes",
        "section": 1
    },
    {
        "command": "find-repos-of-install",
        "description": "find-repos-of-install is a program which reports the Yumrepository that a specified package was installed from.",
        "name": "find-repos-of-install - report which Yum repository a package wasinstalled from",
        "section": 1
    },
    {
        "command": "firecfg",
        "description": "Firecfg is the desktop integration utility for Firejail sandbox.It allows the user to sandbox applications automatically byclicking on desktop manager icons and menus.The integration covers:- programs started in a terminal - typing \"firefox\" wouldbe enough to start a sandboxed Firefox browser- programs started by clicking on desktop manager menus -all major desktop managers are supported- programs started by clicking on file icons in filemanager - only Cinnamon, KDE, LXDE/LXQT, MATE and XFCEdesktop managers are supported in this momentTo set it up, run \"sudo firecfg\" after installing Firejailsoftware.The same command should also be run after installingnew programs. If the program is supported by Firejail, thesymbolic link in /usr/local/bin will be created. For a full listof programs supported by default run \"cat/etc/firejail/firecfg.config\".For user-driven manual integration, see DESKTOP INTEGRATIONsection in man 1 firejail.",
        "name": "Firecfg - Desktop integration utility for Firejail software.",
        "section": 1
    },
    {
        "command": "firejail",
        "description": "Firejail is a SUID sandbox program that reduces the risk ofsecurity breaches by restricting the running environment ofuntrusted applications using Linux namespaces, seccomp-bpf andLinux capabilities.It allows a process and all its descendantsto have their own private view of the globally shared kernelresources, such as the network stack, process table, mount table.Firejail can work in a SELinux or AppArmor environment, and it isintegrated with Linux Control Groups.Written in C with virtually no dependencies, the software runs onany Linux computer with a 3.x kernel version or newer.It cansandbox any type of processes: servers, graphical applications,and even user login sessions.Firejail allows the user to manage application security usingsecurity profiles.Each profile defines a set of permissions fora specific application or group of applications. The softwareincludes security profiles for a number of more common Linuxprograms, such as Mozilla Firefox, Chromium, VLC, Transmissionetc.Firejail is currently implemented as an SUID binary, which meansthat if a malicious or compromised user account manages toexploit a bug in Firejail, that could ultimately lead to aprivilege escalation to root.To mitigate this, it isrecommended to only allow trusted users to run firejail (seefirejail-users(5) for details on how to achieve that).For moredetails on the security/usability tradeoffs of Firejail, see:#4601 \u27e8https://github.com/netblue30/firejail/discussions/4601\u27e9Alternative sandbox technologies like snap(https://snapcraft.io/) and flatpak (https://flatpak.org/) arenot supported. Snap and flatpak packages have their own nativemanagement tools and will not work when sandboxed with Firejail.",
        "name": "Firejail - Linux namespaces sandbox program",
        "section": 1
    },
    {
        "command": "firemon",
        "description": "Firemon monitors programs started in a Firejail sandbox.Withouta PID specified, all processes started by Firejail are monitored.Descendants of these processes are also being monitored. OnGrsecurity systems only root user can run this program.",
        "name": "Firemon - Monitoring program for processes started in a Firejailsandbox.",
        "section": 1
    },
    {
        "command": "flatpak",
        "description": "Flatpak is a tool for managing applications and the runtimes theyuse. In the Flatpak model, applications can be built anddistributed independently from the host system they are used on,and they are isolated from the host system ('sandboxed') to somedegree, at runtime.Flatpak can operate in system-wide or per-user mode. Thesystem-wide data (runtimes, applications and configuration) islocated in $prefix/var/lib/flatpak/, and the per-user data is in$HOME/.local/share/flatpak/. Below these locations, there is alocal repository in the repo/ subdirectory and installed runtimesand applications are in the corresponding runtime/ and app/subdirectories.System-wide remotes can be statically preconfigured by droppingflatpakrepo files into /etc/flatpak/remotes.d/.In addition to the system-wide installation in$prefix/var/lib/flatpak/, which is always considered the defaultone unless overridden, more system-wide installations can bedefined via configuration files in /etc/flatpak/installations.d/,which must define at least the id of the installation and theabsolute path to it. Other optional parameters like DisplayName,Priority or StorageType are also supported.Flatpak uses OSTree to distribute and deploy data. Therepositories it uses are OSTree repositories and can bemanipulated with the ostree utility. Installed runtimes andapplications are OSTree checkouts.Basic commands for building flatpaks such as build-init, buildand build-finish are included in the flatpak utility. Forhigher-level build support, see the separate flatpak-builder(1)tool.Flatpak supports installing from sideload repos. These arepartial copies of a repository (generated by flatpak create-usb)that are used as an installation source when offline (and onlineas a performance improvement). Such repositories are configuredby creating symlinks to the sideload sources in thesideload-repos subdirectory of the installation directory (i.e.typically /var/lib/flatpak/sideload-repos or~/.local/share/flatpak/sideload-repos). Additionally symlinks canbe created in /run/flatpak/sideload-repos which is a betterlocation for non-persistent sources (as it is cleared on reboot).These symlinks can point to either the directory given to flatpakcreate-usb which by default writes to the subpath .ostree/repo,or directly to an ostree repo.",
        "name": "flatpak - Build, install and run applications and runtimes",
        "section": 1
    },
    {
        "command": "flatpak-build",
        "description": "Runs a build command in a directory.DIRECTORY must have beeninitialized with flatpak build-init.The sdk that is specified in the metadata file in the directoryis mounted at /usr and the files and var subdirectories aremounted at /app and /var, respectively. They are writable, andtheir contents are preserved between build commands, to allowaccumulating build artifacts there.",
        "name": "flatpak-build - Build in a directory",
        "section": 1
    },
    {
        "command": "flatpak-build-bundle",
        "description": "Creates a single-file named FILENAME for the application (orruntime) named NAME in the repository at LOCATION. If a BRANCH isspecified, this branch of the application is used.The collection ID set on the repository at LOCATION (if set) willbe used for the bundle.Unless --oci is used, the format of the bundle file is that of anostree static delta (against an empty base) with some flatpakspecific metadata for the application icons and appdata.",
        "name": "flatpak-build-bundle - Create a single-file bundle from a localrepository",
        "section": 1
    },
    {
        "command": "flatpak-build-commit-from",
        "description": "Creates new commits on the DST-REF branch in the DST-REPO, withthe contents (and most of the metadata) taken from anotherbranch, either from another repo, or from another branch in thesame repository.The collection ID set on DST-REPO (if set) will be used for thenewly created commits.This command is very useful when you want to maintain a branchwith a clean history that has no unsigned or broken commits. Forinstance, you can import the head from a different repositoryfrom an automatic builder when you've verified that it worked.The new commit will have no parents or signatures from theautobuilder, and can be properly signed with the official key.Any deltas that affect the original commit and that match parentcommits in the destination repository are copied and rewrittenfor the new commit id.",
        "name": "flatpak-build-commit-from - Create new commits based on existingone (possibly from another repository)",
        "section": 1
    },
    {
        "command": "flatpak-build-export",
        "description": "Creates or updates a repository with an application build.LOCATION is the location of the repository.DIRECTORY must be afinalized build directory. If BRANCH is not specified, it isassumed to be \"master\".If LOCATION exists, it is assumed to be an OSTree repository,otherwise a new OSTree repository is created at this location.The repository can be inspected with the ostree tool.The contents of DIRECTORY are committed on the branch with nameapp/APPNAME/ARCH/BRANCH, where ARCH is the architecture of theruntime that the application is using. A commit filter is used toenforce that only the contents of the files/ and export/subdirectories and the metadata file are included in the commit,anything else is ignored.When exporting a flatpak to be published to the internet,--collection-id=COLLECTION-ID should be specified as a globallyunique reverse DNS value to identify the collection of flatpaksthis will be added to. Setting a globally unique collection IDallows the apps in the repository to be shared over peer to peersystems without needing further configuration.The build-update-repo command should be used to update repositorymetadata whenever application builds are added to a repository.",
        "name": "flatpak-build-export - Create a repository from a build directory",
        "section": 1
    },
    {
        "command": "flatpak-build-finish",
        "description": "Finalizes a build directory, to prepare it for exporting.DIRECTORY is the name of the directory.The result of this command is that desktop files, icons, D-Busservice files, and AppStream metainfo files from the filessubdirectory are copied to a new export subdirectory. In themetadata file, the command key is set in the [Application] group,and the supported keys in the [Environment] group are setaccording to the options.As part of finalization you can also specify permissions that theapp needs, using the various options specified below.Additionally during finalization the permissions from the runtimeare inherited into the app unless you specify--no-inherit-permissionsYou should review the exported files and the application metadatabefore creating and distributing an application bundle.It is an error to run build-finish on a directory that has notbeen initialized as a build directory, or has already beenfinalized.",
        "name": "flatpak-build-finish - Finalize a build directory",
        "section": 1
    },
    {
        "command": "flatpak-build-import-bundle",
        "description": "Imports a bundle from a file named FILENAME into the repositoryat LOCATION.The format of the bundle file is that generated by build-bundle.",
        "name": "flatpak-build-import-bundle - Import a file bundle into a localrepository",
        "section": 1
    },
    {
        "command": "flatpak-build-init",
        "description": "Initializes a separate build directory.DIRECTORY is the name ofthe directory.APPNAME is the application id of the app thatwill be built.SDK and RUNTIME specify the sdk and runtime thatthe application should be built against and run in.BRANCHspecify the version of sdk and runtimeInitializes a directory as build directory which can be used astarget directory of flatpak build. It creates a metadata insidethe given directory. Additionally, empty files and varsubdirectories are created.It is an error to run build-init on a directory that has alreadybeen initialized as a build directory.",
        "name": "flatpak-build-init - Initialize a build directory",
        "section": 1
    },
    {
        "command": "flatpak-build-sign",
        "description": "Signs the commit for a specified application or runtime in alocal repository.LOCATION is the location of the repository.ID is the name of the application, or runtime if --runtime isspecified. If BRANCH is not specified, it is assumed to be\"master\".Applications can also be signed during build-export, but it issometimes useful to add additional signatures later.",
        "name": "flatpak-build-sign - Sign an application or runtime",
        "section": 1
    },
    {
        "command": "flatpak-build-update-repo",
        "description": "Updates repository metadata for the repository at LOCATION. Thiscommand generates an OSTree summary file that lists the contentsof the repository. The summary is used by flatpak remote-ls andother commands to display the contents of remote repositories.After this command, LOCATION can be used as the repositorylocation for flatpak remote-add, either by exporting it overhttp, or directly with a file: url.",
        "name": "flatpak-build-update-repo - Create a repository from a builddirectory",
        "section": 1
    },
    {
        "command": "flatpak-config",
        "description": "The flatpak config command shows or modifies the configuration ofa flatpak installation. The following keys are supported:languagesThe languages that are included when installing Localeextensions. The value is a semicolon-separated list oftwo-letter language codes, or one of the special values * orall. If this key is unset, flatpak defaults to including theextra-languages key and the current locale.extra-languagesThis key is used when languages is not set, and it definesextra locale extensions on top of the system configuredlanguages. The value is a semicolon-separated list of localeidentifiers (language, optional locale, optional codeset,optional modifier) as documented by setlocale(3) (forexample, en;en_DK;zh_HK.big5hkscs;uz_UZ.utf8@cyrillic).For configuration of individual remotes, seeflatpak-remote-modify(1). For configuration of individualapplications, see flatpak-override(1).",
        "name": "flatpak-config - Manage configuration",
        "section": 1
    },
    {
        "command": "flatpak-create-usb",
        "description": "Copies the specified apps and/or runtimes REFs onto the removablemedia mounted at MOUNT-PATH, along with all the dependencies andmetadata needed for installing them. This is one way oftransferring flatpaks between computers that doesn't require anInternet connection. After using this command, the USB drive canbe connected to another computer which already has the relevantremote(s) configured, and Flatpak will install or update from thedrive offline (see below). If online, the drive will be used as acache, meaning some objects will be pulled from it and othersfrom the Internet. For this process to work a collection ID mustbe configured on the relevant remotes on both the source anddestination computers, and on the remote server.On the destination computer one can install from the USB (or anymounted filesystem) using the --sideload-repo option with flatpakinstall. It's also possible to configure sideload paths usingsymlinks; see flatpak(1). Flatpak also includes systemd units toautomatically sideload from hot-plugged USB drives, but these mayor may not be enabled depending on your Linux distribution.Each REF argument is a full or partial identifier in the flatpakref format, which looks like \"(app|runtime)/ID/ARCH/BRANCH\". Allelements except ID are optional and can be left out, includingthe slashes, so most of the time you need only specify ID. Anypart left out will be matched against what is installed, and ifthere are multiple matches an error message will list thealternatives.By default this looks for both installed apps and runtimes withthe given REF, but you can limit this by using the --app or--runtime option.All REFs must be in the same installation (user, system, orother). Otherwise it's ambiguous which repository metadata refsto put on the USB drive.By default flatpak create-usb uses .ostree/repo as thedestination directory under MOUNT-PATH but if you specify anotherlocation using --destination-repo a symbolic link will be createdfor you in .ostree/repos.d. This ensures that either way therepository will be found by flatpak (and other consumers oflibostree) for install/update operations.Unless overridden with the --system, --user, or --installationoptions, this command searches both the system-wide installationand the per-user one for REF and errors out if it exists in morethan one.",
        "name": "flatpak-create-usb - Copy apps and/or runtimes onto removablemedia.",
        "section": 1
    },
    {
        "command": "flatpak-document-export",
        "description": "Creates a document id for a local file that can be exposed tosandboxed applications, allowing them access to files that theywould not otherwise see. The exported files are exposed in a fusefilesystem at /run/user/$UID/doc/.This command also lets you modify the per-application permissionsof the documents, granting or revoking access to the file on aper-application basis.",
        "name": "flatpak-document-export - Export a file to a sandboxedapplication",
        "section": 1
    },
    {
        "command": "flatpak-document-info",
        "description": "Shows information about an exported file, such as the documentid, the fuse path, the original location in the filesystem, andthe per-application permissions.FILE can either be a file in the fuse filesystem at/run/user/$UID/doc/, or a file anywhere else.",
        "name": "flatpak-document-info - Show information about exported files",
        "section": 1
    },
    {
        "command": "flatpak-document-unexport",
        "description": "Removes the document id for the file from the document portal.This will make the document unavailable to all sandboxedapplications.",
        "name": "flatpak-document-unexport - Stop exporting a file",
        "section": 1
    },
    {
        "command": "flatpak-documents",
        "description": "Lists exported files, with their document id and the full path totheir origin. If an APPID is specified, only the files exportedto this app are listed.",
        "name": "flatpak-documents - List exported files",
        "section": 1
    },
    {
        "command": "flatpak-enter",
        "description": "Enter a running sandbox.INSTANCE must be either the pid of a process running in a flatpaksandbox, or the ID of a running application, or the instance IDof a running sandbox. You can use flatpak ps to find the instanceIDs of running flatpaks.COMMAND is the command to run in the sandbox. Extra arguments arepassed on to the command.This creates a new process within the running sandbox, with thesame environment. This is useful when you want to debug a problemwith a running application.This command works as a regular user if the system supportunprivileged user namespace. If that is not available you need torun run it like: sudo -E flatpak enter.",
        "name": "flatpak-enter - Enter an application or runtime's sandbox",
        "section": 1
    },
    {
        "command": "flatpak-history",
        "description": "Shows changes to the flatpak installations on the system. Thisincludes installs, updates and removals of applications andruntimes.By default, both per-user and system-wide installations areshown. Use the --user, --installation or --system options tochange this.The information for the history command is taken from the systemdjournal, and can also be accessed using e.g.journalctlMESSAGE_ID=c7b39b1e006b464599465e105b361485",
        "name": "flatpak-history - Show history",
        "section": 1
    },
    {
        "command": "flatpak-info",
        "description": "Show info about an installed application or runtime.By default, the output is formatted in a friendly format. If youspecify any of the --show-...or --file-access options, theoutput is instead formatted in a machine-readable format.By default, both per-user and system-wide installations arequeried. Use the --user, --system or --installation options tochange this.",
        "name": "flatpak-info - Show information about an installed application orruntime",
        "section": 1
    },
    {
        "command": "flatpak-install",
        "description": "Installs an application or runtime. The primary way to install isto specify a REMOTE name as the source and one ore more REFs tospecify the application or runtime to install. If REMOTE isomitted, the configured remotes are searched for the first REFand the user is asked to confirm the resulting choice.Each REF argument is a full or partial identifier in the flatpakref format, which looks like \"(app|runtime)/ID/ARCH/BRANCH\". Allelements except ID are optional and can be left out, includingthe slashes, so most of the time you need only specify ID. Anypart left out will be matched against what is in the remote, andif there are multiple matches you will be prompted to choose oneof them. You will also be prompted with choices if REF doesn'tmatch anything in the remote exactly but is similar to one ormore refs in the remote (e.g. \"devhelp\" is similar to\"org.gnome.Devhelp\"), but this fuzzy matching behavior isdisabled if REF contains any slashes or periods.By default this looks for both apps and runtimes with the givenREF in the specified REMOTE, but you can limit this by using the--app or --runtime option, or by supplying the initial element inthe REF.If REMOTE is a uri or a path (absolute or relative starting with./) to a local repository, then that repository will be used asthe source, and a temporary remote will be created for thelifetime of the REF.If the specified REMOTE has a collection ID configured on it,Flatpak will search the sideload-repos directories configuredeither with the --sideload-repo option, or on a per-installationor system-wide basis (see flatpak(1)).The alternative form of the command (with --from or --bundle)allows to install directly from a source such as a .flatpaksingle-file bundle or a .flatpakref application description. Theoptions are optional if the first argument has the expectedfilename extension.Note that flatpak allows to have multiple branches of anapplication and runtimes installed and used at the same time.However, only one version of an application can be current,meaning its exported files (for instance desktop files and icons)are visible to the host. The last installed version is madecurrent by default, but this can manually changed with flatpakmake-current.Unless overridden with the --user or the --installation option,this command installs the application or runtime in the defaultsystem-wide installation.",
        "name": "flatpak-install - Install an application or runtime",
        "section": 1
    },
    {
        "command": "flatpak-kill",
        "description": "Stop a running Flatpak instance.INSTANCE can be either the numeric instance ID or the applicationID of a running Flatpak. You can use flatpak ps to find theinstance IDs of running flatpaks.",
        "name": "flatpak-kill - Stop a running application",
        "section": 1
    },
    {
        "command": "flatpak-list",
        "description": "Lists the names of the installed applications and runtimes.By default, both apps and runtimes are shown, but you can changethis by using the --app or --runtime options.By default, both per-user and system-wide installations areshown. Use the --user, --installation or --system options tochange this.The list command can also be used to find installed apps that usea certain runtime, with the --app-runtime option.",
        "name": "flatpak-list - List installed applications and/or runtimes",
        "section": 1
    },
    {
        "command": "flatpak-make-current",
        "description": "Makes a particular branch of an application current. Only thecurrent branch of an app has its exported files (such as desktopfiles and icons) made visible to the host.When a new branch is installed it will automatically be madecurrent, so this command is often not needed.Unless overridden with the --user or --installation options, thiscommand changes the default system-wide installation.",
        "name": "flatpak-make-current - Make a specific version of an app current",
        "section": 1
    },
    {
        "command": "flatpak-mask",
        "description": "Flatpak maintains a list of patterns that define which refs aremasked. A masked ref will never be updated or automaticallyinstalled (for example a masked extension marked auto-downloadwill not be downloaded). You can still manually install suchrefs, but once they are installed the version will be pinned.The patterns are just a partial ref, with the * charactermatching anything within that part of the ref. Here are someexample patterns:org.some.Apporg.some.App//unstableapp/org.domain.*org.some.App/armTo list the current set of masks, run this command without anypatterns.",
        "name": "flatpak-mask - Mask out updates and automatic installation",
        "section": 1
    },
    {
        "command": "flatpak-override",
        "description": "Overrides the application specified runtime requirements. Thiscan be used to grant a sandboxed application more or lessresources than it requested.By default the application gets access to the resources itrequested when it is started. But the user can override it on aparticular instance by specifying extra arguments to flatpak run,or every time by using flatpak override.The application overrides are saved in text files residing in$XDG_DATA_HOME/flatpak/overrides in user mode.If the application ID APP is not specified then the overridesaffect all applications, but the per-application overrides canoverride the global overrides.Unless overridden with the --user or --installation options, thiscommand changes the default system-wide installation.",
        "name": "flatpak-override - Override application requirements",
        "section": 1
    },
    {
        "command": "flatpak-permission-remove",
        "description": "Removes an entry for the object with id ID to the permissionstore table TABLE. The ID must be in a suitable format for thetable. If APP_ID is specified, only the entry for thatapplication is removed.The permission store is used by portals. Each portal generallyhas its own table in the permission store, and the format of thetable entries is specific to each portal.",
        "name": "flatpak-permission-remove - Remove permissions",
        "section": 1
    },
    {
        "command": "flatpak-permission-reset",
        "description": "Removes all permissions for the given app from the Flatpakpermission store.The permission store is used by portals. Each portal generallyhas its own table in the permission store, and the format of thetable entries is specific to each portal.",
        "name": "flatpak-permission-reset - Reset permissions",
        "section": 1
    },
    {
        "command": "flatpak-permission-set",
        "description": "Set the permissions for an application in an entry in thepermission store. The entry is identified by TABLE and ID, theapplication is identified by APP_ID. The PERMISSION strings mustbe in a format suitable for the table.The permission store is used by portals. Each portal generallyhas its own table in the permission store, and the format of thetable entries is specific to each portal.",
        "name": "flatpak-permission-set - Set permissions",
        "section": 1
    },
    {
        "command": "flatpak-permission-show",
        "description": "Lists dynamic permissions for the given app which are stored inthe Flatpak permission store.When called without arguments, lists all the entries in allpermission store tables. When called with one argument, lists allthe entries in the named table. When called with two arguments,lists the entry in the named table for the given object ID.The permission store is used by portals. Each portal generallyhas its own table in the permission store, and the format of thetable entries is specific to each portal.",
        "name": "flatpak-permission-show - Show permissions",
        "section": 1
    },
    {
        "command": "flatpak-permissions",
        "description": "Lists dynamic permissions which are stored in the Flatpakpermission store.When called without arguments, lists all the entries in allpermission store tables. When called with one argument, lists allthe entries in the named table. When called with two arguments,lists the entry in the named table for the given object ID.The permission store is used by portals. Each portal generallyhas its own table in the permission store, and the format of thetable entries is specific to each portal.",
        "name": "flatpak-permissions - List permissions",
        "section": 1
    },
    {
        "command": "flatpak-pin",
        "description": "Flatpak maintains a list of patterns that define which refs arepinned. A pinned ref will never be automatically uninstalled (asare unused runtimes periodically). This can be useful if forexample you are using a runtime for development purposes.Runtimes that are explicitly installed, rather than installed asa dependency of something else, are automatically pinned.The patterns are just a partial ref, with the * charactermatching anything within that part of the ref. Only runtimes canbe pinned, not apps. Here are some example patterns:org.some.Runtimeorg.some.Runtime//unstableruntime/org.domain.*org.some.Runtime/armTo list the current set of pins, run this command without anypatterns.",
        "name": "flatpak-pin - Pin runtimes to prevent automatic removal",
        "section": 1
    },
    {
        "command": "flatpak-ps",
        "description": "Lists useful information about running Flatpak instances.To see full details of a running instance, you can open the file/run/user/$UID/.flatpak/$INSTANCE/info, where $INSTANCE is theinstance ID reported by flatpak ps.",
        "name": "flatpak-ps - Enumerate running instances",
        "section": 1
    },
    {
        "command": "flatpak-remote-add",
        "description": "Adds a remote repository to the flatpak repository configuration.NAME is the name for the new remote, and LOCATION is a url orpathname. The LOCATION is either a flatpak repository, or a.flatpakrepo file which describes a repository. In the formercase you may also have to specify extra options, such as the gpgkey for the repo.Unless overridden with the --user or --installation options, thiscommand changes the default system-wide installation.",
        "name": "flatpak-remote-add - Add a remote repository",
        "section": 1
    },
    {
        "command": "flatpak-remote-delete",
        "description": "Removes a remote repository from the flatpak repositoryconfiguration.NAME is the name of an existing remote.Unless overridden with the --system, --user, or --installationoptions, this command uses either the default system-wideinstallation or the per-user one, depending on which has thespecified REMOTE.",
        "name": "flatpak-remote-delete - Delete a remote repository",
        "section": 1
    },
    {
        "command": "flatpak-remote-info",
        "description": "Shows information about the runtime or application REF from theremote repository with the name REMOTE. You can find allconfigured remote repositories with flatpak remotes.By default, the output is formatted in a friendly format. If youspecify one of the --show-...options, the output is insteadformatted in a machine-readable format.Unless overridden with the --system, --user, or --installationoptions, this command uses either the default system-wideinstallation or the per-user one, depending on which has thespecified REMOTE.",
        "name": "flatpak-remote-info - Show information about an application orruntime in a remote",
        "section": 1
    },
    {
        "command": "flatpak-remote-ls",
        "description": "Shows runtimes and applications that are available in the remoterepository with the name REMOTE, or all remotes if one isn'tspecified. You can find all configured remote repositories withflatpak remotes.REMOTE can be a file:// URI pointing to a local repositoryinstead of a remote name.Unless overridden with the --system, --user, or --installationoptions, this command uses either the default system-wideinstallation or the per-user one, depending on which has thespecified REMOTE.",
        "name": "flatpak-remote-ls - Show available runtimes and applications",
        "section": 1
    },
    {
        "command": "flatpak-remote-modify",
        "description": "Modifies options for an existing remote repository in the flatpakrepository configuration.NAME is the name for the remote.Unless overridden with the --system, --user, or --installationoptions, this command uses either the default system-wideinstallation or the per-user one, depending on which has thespecified REMOTE.",
        "name": "flatpak-remote-modify - Modify a remote repository",
        "section": 1
    },
    {
        "command": "flatpak-remotes",
        "description": "Lists the known remote repositories, in priority order.By default, both per-user and system-wide installations areshown. Use the --user, --system or --installation options tochange this.",
        "name": "flatpak-remotes - List remote repositories",
        "section": 1
    },
    {
        "command": "flatpak-repair",
        "description": "Repair a flatpak installation by pruning and reinstalling invalidobjects. The repair command does all of the following:\u2022Scan all locally available refs, removing any that don'tcorrespond to a deployed ref.\u2022Verify each commit they point to, removing any invalidobjects and noting any missing objects.\u2022Remove any refs that had an invalid object, and anynon-partial refs that had missing objects.\u2022Prune all objects not referenced by a ref, which gets rid ofany possibly invalid non-scanned objects.\u2022Enumerate all deployed refs and re-install any that are notin the repo (or are partial for a non-subdir deploy).Note that flatpak repair has to be run with root privileges tooperate on the system installation.An alternative command for repairing OSTree repositories isostree fsck.",
        "name": "flatpak-repair - Repair a flatpak installation",
        "section": 1
    },
    {
        "command": "flatpak-repo",
        "description": "Show information about a local repository.If you need to modify a local repository, see the flatpakbuild-update-repo command, or use the ostree tool.",
        "name": "flatpak-repo - Show information about a local repository",
        "section": 1
    },
    {
        "command": "flatpak-run",
        "description": "If REF names an installed application, Flatpak runs theapplication in a sandboxed environment. Extra arguments arepassed on to the application. The current branch and arch of theapplication is used unless otherwise specified with --branch or--arch. See flatpak-make-current(1).If REF names a runtime, a shell is opened in the runtime. This isuseful for development and testing. If there is ambiguity aboutwhich branch to use, you will be prompted to choose. Use --branchto avoid this. The primary arch is used unless otherwisespecified with --arch.By default, Flatpak will look for the application or runtime inthe per-user installation first, then in all systeminstallations. This can be overridden with the --user, --systemand --installation options.Flatpak creates a sandboxed environment for the application torun in by mounting the right runtime at /usr and a writabledirectory at /var, whose content is preserved between applicationruns. The application itself is mounted at /app.The details of the sandboxed environment are controlled by theapplication metadata and various options like --share and--socket that are passed to the run command: Access is allowed ifit was requested either in the application metadata file or withan option and the user hasn't overridden it.The remaining arguments are passed to the command that gets runin the sandboxed environment. See the --file-forwarding optionfor handling of file arguments.Environment variables are generally passed on to the sandboxedapplication, with certain exceptions. The application metadatacan override environment variables, as well as the --env option.Apart from that, Flatpak always unsets or overrides the followingvariables, since their session values are likely to interferewith the functioning of the sandbox:PATHLD_LIBRARY_PATHXDG_CONFIG_DIRSXDG_DATA_DIRSXDG_RUNTIME_DIRSHELLTEMPTEMPDIRTMPTMPDIRPYTHONPATHPERLLIBPERL5LIBXCURSOR_PATHKRB5CCNAMEXKB_CONFIG_ROOTGIO_EXTRA_MODULESGDK_BACKENDAlso several environment variables with the prefix \"GST_\" thatare used by gstreamer are unset (since Flatpak 1.12.5).Flatpak also overrides the XDG environment variables to pointsandboxed applications at their writable filesystem locationsbelow ~/.var/app/$APPID/:XDG_DATA_HOMEXDG_CONFIG_HOMEXDG_CACHE_HOMEXDG_STATE_HOME (since Flatpak 1.13)Apps can use the --persist=.local/state and--unset-env=XDG_STATE_HOME options to get a Flatpak1.13-compatible ~/.local/state on older versions of Flatpak.The host values of these variables are made available inside thesandbox via these HOST_-prefixed variables:HOST_XDG_DATA_HOMEHOST_XDG_CONFIG_HOMEHOST_XDG_CACHE_HOMEHOST_XDG_STATE_HOME (since Flatpak 1.13)Flatpak sets the environment variable FLATPAK_ID to theapplication ID of the running app.Flatpak also bind-mounts as read-only the host's /etc/os-release(if available, or /usr/lib/os-release as a fallback) to/run/host/os-release in accordance with the os-releasespecification[1].If parental controls support is enabled, flatpak will check thecurrent user\u2019s parental controls settings, and will refuse to runan app if it is blocklisted for the current user.",
        "name": "flatpak-run - Run an application or open a shell in a runtime",
        "section": 1
    },
    {
        "command": "flatpak-search",
        "description": "Searches for applications and runtimes matching TEXT. Note thatthis uses appstream data that can be updated with the flatpakupdate command. The appstream data is updated automatically onlyif it's at least a day old.",
        "name": "flatpak-search - Search for applications and runtimes",
        "section": 1
    },
    {
        "command": "flatpak-spawn",
        "description": "Unlike other flatpak commands, flatpak-spawn is available toapplications inside the sandbox. It runs COMMAND outside thesandbox: either in another sandbox, or on the host.When called without --host, flatpak-spawn uses the Flatpak portalto create a copy of the sandbox it was called from, optionallyusing tighter permissions and optionally the latest version ofthe app and runtime (see --latest-version).",
        "name": "flatpak-spawn - Run commands in a sandbox",
        "section": 1
    },
    {
        "command": "flatpak-uninstall",
        "description": "Uninstalls an application or runtime.REF is a reference to theapplication or runtime to uninstall.Each REF argument is a full or partial identifier in the flatpakref format, which looks like \"(app|runtime)/ID/ARCH/BRANCH\". Allelements except ID are optional and can be left out, includingthe slashes, so most of the time you need only specify ID. Anypart left out will be matched against what is installed, and ifthere are multiple matches you will be prompted to choose betweenthem. You will also be prompted if REF doesn't match anyinstalled ref exactly but is similar (e.g. \"gedit\" is similar to\"org.gnome.gedit\"), but this fuzzy matching behavior is disabledif REF contains any slashes or periods.By default this looks for both installed apps and runtimes withthe given REF, but you can limit this by using the --app or--runtime option, or by supplying the initial element in the REF.Normally, this command removes the ref for thisapplication/runtime from the local OSTree repository and purgesany objects that are no longer needed to free up disk space. Ifthe same application is later reinstalled, the objects will bepulled from the remote repository again. The --keep-ref optioncan be used to prevent this.When --delete-data is specified while removing an app, its datadirectory in ~/.var/app and any permissions it might have areremoved. When --delete-data is used without a REF, all 'unowned'app data is removed.Unless overridden with the --system, --user, or --installationoptions, this command searches both the system-wide installationand the per-user one for REF and errors out if it exists in morethan one.",
        "name": "flatpak-uninstall - Uninstall an application or runtime",
        "section": 1
    },
    {
        "command": "flatpak-update",
        "description": "Updates applications and runtimes.REF is a reference to theapplication or runtime to update. If no REF is given, everythingis updated, as well as appstream info for all remotes.Each REF argument is a full or partial identifier in the flatpakref format, which looks like \"(app|runtime)/ID/ARCH/BRANCH\". Allelements except ID are optional and can be left out, includingthe slashes, so most of the time you need only specify ID. Anypart left out will be matched against what is installed, and ifthere are multiple matches an error message will list thealternatives.By default this looks for both apps and runtimes with the givenREF, but you can limit this by using the --app or --runtimeoption, or by supplying the initial element in the REF.Normally, this command updates the application to the tip of itsbranch. But it is possible to check out another commit, with the--commit option.If the configured remote for a ref being updated has a collectionID configured on it, Flatpak will search the sideload-reposdirectories configured either with the --sideload-repo option, oron a per-installation or system-wide basis (see flatpak(1)).Note that updating a runtime is different from installing adifferent branch, and runtime updates are expected to keep strictcompatibility. If an application update does cause a problem, itis possible to go back to the previous version, with the --commitoption.In addition to updates, this command will offer to uninstall anyunused end-of-life runtimes. Runtimes that were explicitlyinstalled (not as a dependency) or explicitly pinned (seeflatpak-pin(1)) are left installed even if unused andend-of-life.Unless overridden with the --user, --system or --installationoption, this command updates any matching refs in the standardsystem-wide installation and the per-user one.",
        "name": "flatpak-update - Update an application or runtime",
        "section": 1
    },
    {
        "command": "flock",
        "description": "This utility manages flock(2) locks from within shell scripts orfrom the command line.The first and second of the above forms wrap the lock around theexecution of a command, in a manner similar to su(1) ornewgrp(1). They lock a specified file or directory, which iscreated (assuming appropriate permissions) if it does not alreadyexist. By default, if the lock cannot be immediately acquired,flock waits until the lock is available.The third form uses an open file by its file descriptor number.See the examples below for how that can be used.",
        "name": "flock - manage locks from shell scripts",
        "section": 1
    },
    {
        "command": "fmt",
        "description": "Reformat each paragraph in the FILE(s), writing to standardoutput.The option -WIDTH is an abbreviated form of--width=DIGITS.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-c, --crown-marginpreserve indentation of first two lines-p, --prefix=STRINGreformat only lines beginning with STRING, reattaching theprefix to reformatted lines-s, --split-onlysplit long lines, but do not refill-t, --tagged-paragraphindentation of first line different from second-u, --uniform-spacingone space between words, two after sentences-w, --width=WIDTHmaximum line width (default of 75 columns)-g, --goal=WIDTHgoal width (default of 93% of width)--help display this help and exit--versionoutput version information and exit",
        "name": "fmt - simple optimal text formatter",
        "section": 1
    },
    {
        "command": "fold",
        "description": "Wrap input lines in each FILE, writing to standard output.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-b, --bytescount bytes rather than columns-s, --spacesbreak at spaces-w, --width=WIDTHuse WIDTH columns instead of 80--help display this help and exit--versionoutput version information and exit",
        "name": "fold - wrap each input line to fit in specified width",
        "section": 1
    },
    {
        "command": "free",
        "description": "free displays the total amount of free and used physical and swapmemory in the system, as well as the buffers and caches used bythe kernel. The information is gathered by parsing /proc/meminfo.The displayed columns are:totalTotal usable memory (MemTotal and SwapTotal in/proc/meminfo). This includes the physical and swap memoryminus a few reserved bits and kernel binary code.usedUsed or unavailable memory (calculated as total -available)freeUnused memory (MemFree and SwapFree in /proc/meminfo)shared Memory used (mostly) by tmpfs (Shmem in /proc/meminfo)buffersMemory used by kernel buffers (Buffers in /proc/meminfo)cacheMemory used by the page cache and slabs (Cached andSReclaimable in /proc/meminfo)buff/cacheSum of buffers and cacheavailableEstimation of how much memory is available for startingnew applications, without swapping. Unlike the dataprovided by the cache or free fields, this field takesinto account page cache and also that not all reclaimablememory slabs will be reclaimed due to items being in use(MemAvailable in /proc/meminfo, available on kernels 3.14,emulated on kernels 2.6.27+, otherwise the same as free)",
        "name": "free - Display amount of free and used memory in the system",
        "section": 1
    },
    {
        "command": "fsnotifywait",
        "description": "inotifywait efficiently waits for changes to files using Linux'sinotify(7) interface.It is suitable for waiting for changes tofiles from shell scripts.It can either exit once an eventoccurs, or continually execute and output events as they occur.fsnotifywait is similar to inotifywait but it is using Linux'sfanotify(7) interface by default. If explicitly specified, ituses the inotify(7) interface.",
        "name": "inotifywait, fsnotifywait - wait for changes to files usinginotify or fanotify",
        "section": 1
    },
    {
        "command": "fsnotifywatch",
        "description": "inotifywatch listens for filesystem events using Linux'sinotify(7) interface, then outputs a summary count of the eventsreceived on each file or directory.fsnotifywatch is similar to inotifywatch but it is using Linux'sfanotify(7) interface by default. If explicitly specified, ituses the inotify(7) interface.",
        "name": "inotifywatch, fsnotifywatch - gather filesystem access statisticsusing inotify or fanotify",
        "section": 1
    },
    {
        "command": "fuse2fs",
        "description": "fuse2fs is a FUSE file system client that supports reading andwriting from devices or image files containing ext2, ext3, andext4 file systems.",
        "name": "fuse2fs - FUSE file system client for ext2/ext3/ext4 file systems",
        "section": 1
    },
    {
        "command": "fuser",
        "description": "fuser displays the PIDs of processes using the specified files orfile systems.In the default display mode, each file name isfollowed by a letter denoting the type of access:ccurrent directory.eexecutable being run.fopen file.f is omitted in default display mode.Fopen file for writing.F is omitted in defaultdisplay mode.rroot directory.mmmap'ed file or shared library..Placeholder, omitted in default display mode.fuser returns a non-zero return code if none of the specifiedfiles is accessed or in case of a fatal error.If at least oneaccess has been found, fuser returns zero.In order to look up processes using TCP and UDP sockets, thecorresponding name space has to be selected with the -n option.By default fuser will look in both IPv6 and IPv4 sockets.Tochange the default behavior, use the -4 and -6 options.Thesocket(s) can be specified by the local and remote port, and theremote address.All fields are optional, but commas in front ofmissing fields must be present:[lcl_port][,[rmt_host][,[rmt_port]]]Either symbolic or numeric values can be used for IP addressesand port numbers.fuser outputs only the PIDs to stdout, everything else is sent tostderr.",
        "name": "fuser - identify processes using files or sockets",
        "section": 1
    },
    {
        "command": "fusermount3",
        "description": "Filesystem in Userspace (FUSE) is a simple interface foruserspace programs to export a virtual filesystem to the Linuxkernel. It also aims to provide a secure method for nonprivileged users to create and mount their own filesystemimplementations.fusermount3 is a program to mount and unmount FUSE filesystems.It should be called directly only for unmounting FUSE filesystems. To allow mounting and unmounting by unprivileged users,fusermount3 needs to be installed set-uid root.",
        "name": "fusermount3 - mount and unmount FUSE filesystems",
        "section": 1
    },
    {
        "command": "g++",
        "description": "When you invoke GCC, it normally does preprocessing, compilation,assembly and linking.The \"overall options\" allow you to stopthis process at an intermediate stage.For example, the -coption says not to run the linker.Then the output consists ofobject files output by the assembler.Other options are passed on to one or more stages of processing.Some options control the preprocessor and others the compileritself.Yet other options control the assembler and linker; mostof these are not documented here, since you rarely need to useany of them.Most of the command-line options that you can use with GCC areuseful for C programs; when an option is only useful with anotherlanguage (usually C++), the explanation says so explicitly.Ifthe description for a particular option does not mention a sourcelanguage, you can use that option with all supported languages.The usual way to run GCC is to run the executable called gcc, ormachine-gcc when cross-compiling, or machine-gcc-version to run aspecific version of GCC.When you compile C++ programs, youshould invoke GCC as g++ instead.The gcc program accepts options and file names as operands.Manyoptions have multi-letter names; therefore multiple single-letteroptions may not be grouped: -dv is very different from -d -v.You can mix options and other arguments.For the most part, theorder you use doesn't matter.Order does matter when you useseveral options of the same kind; for example, if you specify -Lmore than once, the directories are searched in the orderspecified.Also, the placement of the -l option is significant.Many options have long names starting with -f or with -W---forexample, -fmove-loop-invariants, -Wformat and so on.Most ofthese have both positive and negative forms; the negative form of-ffoo is -fno-foo.This manual documents only one of these twoforms, whichever one is not the default.Some options take one or more arguments typically separatedeither by a space or by the equals sign (=) from the option name.Unless documented otherwise, an argument can be either numeric ora string.Numeric arguments must typically be small unsigneddecimal or hexadecimal integers.Hexadecimal arguments mustbegin with the 0x prefix.Arguments to options that specify asize threshold of some sort may be arbitrarily large decimal orhexadecimal integers followed by a byte size suffix designating amultiple of bytes such as \"kB\" and \"KiB\" for kilobyte andkibibyte, respectively, \"MB\" and \"MiB\" for megabyte and mebibyte,\"GB\" and \"GiB\" for gigabyte and gigibyte, and so on.Sucharguments are designated by byte-size in the following text.Refer to the NIST, IEC, and other relevant national andinternational standards for the full listing and explanation ofthe binary and decimal byte size prefixes.",
        "name": "gcc - GNU project C and C++ compiler",
        "section": 1
    },
    {
        "command": "galera_new_cluster",
        "description": "Used to bootstrap a new Galera Cluster when all nodes are down.Run galera_new_cluster on the first node only.On the remainingnodes simply run 'service @DAEMON_NAME@ start'.\u2022--help, -hDisplay a help message and exit.",
        "name": "galera_new_cluster - starting a new Galera cluster",
        "section": 1
    },
    {
        "command": "galera_recovery",
        "description": "Use: Recover from non-graceful shutdown.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "galera_recovery - recover from non-graceful shutdown",
        "section": 1
    },
    {
        "command": "ganglia2pcp",
        "description": "ganglia2pcp is intended to read a set of ganglia files stored inthe rrd format and translate them into a Performance Co-Pilot(PCP) archive with the basename outfile.The intent of this converter is to take all of the rrd filesfound in the input_dir and generate a single pcp archive file.The mapping of ganglia metrics to pcp metrics is definedinternally.The resultant PCP achive may be used with all the PCP clienttools to graph subsets of the data using pmchart(1), perform datareduction and reporting, filter with the PCP inference enginepmie(1), etc.A series of physical files will be created with the prefixoutfile.These are outfile.0 (the performance data),outfile.meta (the metadata that describes the performance data)and outfile.index (a temporal index to improve efficiency ofreplay operations for the archive).If any of these files existsalready, then sar2pcp will not overwrite them and will exit withan error message of the form__pmLogNewFile: ``blah.0'' already exists, not over-writtenganglia2pcp is a Perl script that uses the PCP::LogImport Perlwrapper around the PCP libpcp_import library, and as such couldbe used as an example to develop new tools to import other typesof performance data and create PCP archives.The RRDs Perlwrapper is used to parse the raw rrd format files.",
        "name": "ganglia2pcp - import ganglia data and create a PCP archive",
        "section": 1
    },
    {
        "command": "gawk",
        "description": "Gawk is the GNU Project's implementation of the AWK programminglanguage.It conforms to the definition of the language in thePOSIX 1003.1 standard.This version in turn is based on thedescription in The AWK Programming Language, by Aho, Kernighan,and Weinberger.Gawk provides the additional features found inthe current version of Brian Kernighan's awk and numerous GNU-specific extensions.The command line consists of options to gawk itself, the AWKprogram text (if not supplied via the -f or --include options),and values to be made available in the ARGC and ARGV pre-definedAWK variables.",
        "name": "gawk - pattern scanning and processing language",
        "section": 1
    },
    {
        "command": "gcc",
        "description": "When you invoke GCC, it normally does preprocessing, compilation,assembly and linking.The \"overall options\" allow you to stopthis process at an intermediate stage.For example, the -coption says not to run the linker.Then the output consists ofobject files output by the assembler.Other options are passed on to one or more stages of processing.Some options control the preprocessor and others the compileritself.Yet other options control the assembler and linker; mostof these are not documented here, since you rarely need to useany of them.Most of the command-line options that you can use with GCC areuseful for C programs; when an option is only useful with anotherlanguage (usually C++), the explanation says so explicitly.Ifthe description for a particular option does not mention a sourcelanguage, you can use that option with all supported languages.The usual way to run GCC is to run the executable called gcc, ormachine-gcc when cross-compiling, or machine-gcc-version to run aspecific version of GCC.When you compile C++ programs, youshould invoke GCC as g++ instead.The gcc program accepts options and file names as operands.Manyoptions have multi-letter names; therefore multiple single-letteroptions may not be grouped: -dv is very different from -d -v.You can mix options and other arguments.For the most part, theorder you use doesn't matter.Order does matter when you useseveral options of the same kind; for example, if you specify -Lmore than once, the directories are searched in the orderspecified.Also, the placement of the -l option is significant.Many options have long names starting with -f or with -W---forexample, -fmove-loop-invariants, -Wformat and so on.Most ofthese have both positive and negative forms; the negative form of-ffoo is -fno-foo.This manual documents only one of these twoforms, whichever one is not the default.Some options take one or more arguments typically separatedeither by a space or by the equals sign (=) from the option name.Unless documented otherwise, an argument can be either numeric ora string.Numeric arguments must typically be small unsigneddecimal or hexadecimal integers.Hexadecimal arguments mustbegin with the 0x prefix.Arguments to options that specify asize threshold of some sort may be arbitrarily large decimal orhexadecimal integers followed by a byte size suffix designating amultiple of bytes such as \"kB\" and \"KiB\" for kilobyte andkibibyte, respectively, \"MB\" and \"MiB\" for megabyte and mebibyte,\"GB\" and \"GiB\" for gigabyte and gigibyte, and so on.Sucharguments are designated by byte-size in the following text.Refer to the NIST, IEC, and other relevant national andinternational standards for the full listing and explanation ofthe binary and decimal byte size prefixes.",
        "name": "gcc - GNU project C and C++ compiler",
        "section": 1
    },
    {
        "command": "gcore",
        "description": "Generate core dumps of one or more running programs with processIDs pid1, pid2, etc.A core file produced by gcore is equivalentto one produced by the kernel when the process crashes (and when\"ulimit -c\" was used to set up an appropriate core dump limit).However, unlike after a crash, after gcore finishes its job theprogram remains running without any change.",
        "name": "gcore - Generate a core file of a running program",
        "section": 1
    },
    {
        "command": "gcov",
        "description": "gcov is a test coverage program.Use it in concert with GCC toanalyze your programs to help create more efficient, fasterrunning code and to discover untested parts of your program.Youcan use gcov as a profiling tool to help discover where youroptimization efforts will best affect your code.You can alsouse gcov along with the other profiling tool, gprof, to assesswhich parts of your code use the greatest amount of computingtime.Profiling tools help you analyze your code's performance.Usinga profiler such as gcov or gprof, you can find out some basicperformance statistics, such as:*how often each line of code executes*what lines of code are actually executed*how much computing time each section of code usesOnce you know these things about how your code works whencompiled, you can look at each module to see which modules shouldbe optimized.gcov helps you determine where to work onoptimization.Software developers also use coverage testing in concert withtestsuites, to make sure software is actually good enough for arelease.Testsuites can verify that a program works as expected;a coverage program tests to see how much of the program isexercised by the testsuite.Developers can then determine whatkinds of test cases need to be added to the testsuites to createboth better testing and a better final product.You should compile your code without optimization if you plan touse gcov because the optimization, by combining some lines ofcode into one function, may not give you as much information asyou need to look for `hot spots' where the code is using a greatdeal of computer time.Likewise, because gcov accumulatesstatistics by line (at the lowest resolution), it works best witha programming style that places only one statement on each line.If you use complicated macros that expand to loops or to othercontrol structures, the statistics are less helpful---they onlyreport on the line where the macro call appears.If your complexmacros behave like functions, you can replace them with inlinefunctions to solve this problem.gcov creates a logfile called sourcefile.gcov which indicates howmany times each line of a source file sourcefile.c has executed.You can use these logfiles along with gprof to aid in fine-tuningthe performance of your programs.gprof gives timing informationyou can use along with the information you get from gcov.gcov works only on code compiled with GCC.It is not compatiblewith any other profiling or test coverage mechanism.",
        "name": "gcov - coverage testing tool",
        "section": 1
    },
    {
        "command": "gcov-dump",
        "description": "gcov-dump is a tool you can use in conjunction with GCC to dumpcontent of gcda and gcno profile files offline.",
        "name": "gcov-dump - offline gcda and gcno profile dump tool",
        "section": 1
    },
    {
        "command": "gcov-tool",
        "description": "gcov-tool is an offline tool to process gcc's gcda profile files.Current gcov-tool supports the following functionalities:*merge two sets of profiles with weights.*read one set of profile and rewrite profile contents. One canscale or normalize the count values.Examples of the use cases for this tool are:*Collect the profiles for different set of inputs, and usethis tool to merge them. One can specify the weight to factorin the relative importance of each input.*Rewrite the profile after removing a subset of the gcdafiles, while maintaining the consistency of the summary andthe histogram.*It can also be used to debug or libgcov code as the toolsshares the majority code as the runtime library.Note that for the merging operation, this profile generatedoffline may contain slight different values from the onlinemerged profile. Here are a list of typical differences:*histogram difference: This offline tool recomputes thehistogram after merging the counters. The resultinghistogram, therefore, is precise. The online merging does nothave this capability -- the histogram is merged from twohistograms and the result is an approximation.*summary checksum difference: Summary checksum uses a CRC32operation. The value depends on the link list order of gcov-info objects. This order is different in gcov-tool from thatin the online merge. It's expected to have different summarychecksums. It does not really matter as the compiler does notuse this checksum anywhere.*value profile counter values difference: Some counter valuesfor value profile are runtime dependent, like heap addresses.It's normal to see some difference in these kind of counters.",
        "name": "gcov-tool - offline gcda profile processing tool",
        "section": 1
    },
    {
        "command": "gdb",
        "description": "The purpose of a debugger such as GDB is to allow you to see whatis going on \"inside\" another program while it executes -- or whatanother program was doing at the moment it crashed.GDB can do four main kinds of things (plus other things insupport of these) to help you catch bugs in the act:\u2022Start your program, specifying anything that might affect itsbehavior.\u2022Make your program stop on specified conditions.\u2022Examine what has happened, when your program has stopped.\u2022Change things in your program, so you can experiment withcorrecting the effects of one bug and go on to learn aboutanother.You can use GDB to debug programs written in C, C++, Fortran andModula-2.GDB is invoked with the shell command \"gdb\".Once started, itreads commands from the terminal until you tell it to exit withthe GDB command \"quit\" or \"exit\".You can get online help fromGDB itself by using the command \"help\".You can run \"gdb\" with no arguments or options; but the mostusual way to start GDB is with one argument or two, specifying anexecutable program as the argument:gdb programYou can also start with both an executable program and a corefile specified:gdb program coreYou can, instead, specify a process ID as a second argument oruse option \"-p\", if you want to debug a running process:gdb program 1234gdb -p 1234would attach GDB to process 1234.With option -p you can omitthe program filename.Here are some of the most frequently needed GDB commands:break [file:][function|line]Set a breakpoint at function or line (in file).run [arglist]Start your program (with arglist, if specified).btBacktrace: display the program stack.print exprDisplay the value of an expression.cContinue running your program (after stopping, e.g. at abreakpoint).nextExecute next program line (after stopping); step over anyfunction calls in the line.edit [file:]functionlook at the program line where it is presently stopped.list [file:]functiontype the text of the program in the vicinity of where it ispresently stopped.stepExecute next program line (after stopping); step into anyfunction calls in the line.help [name]Show information about GDB command name, or generalinformation about using GDB.quitexitExit from GDB.For full details on GDB, see Using GDB: A Guide to the GNUSource-Level Debugger, by Richard M. Stallman and Roland H.Pesch.The same text is available online as the \"gdb\" entry inthe \"info\" program.",
        "name": "gdb - The GNU Debugger",
        "section": 1
    },
    {
        "command": "gdb-add-index",
        "description": "When GDB finds a symbol file, it scans the symbols in the file inorder to construct an internal symbol table.This lets most GDBoperations work quickly--at the cost of a delay early on.Forlarge programs, this delay can be quite lengthy, so GDB providesa way to build an index, which speeds up startup.To determine whether a file contains such an index, use thecommand \"readelf -S filename\": the index is stored in a sectionnamed \".gdb_index\".The index file can only be produced onsystems which use ELF binaries and DWARF debug information (i.e.,sections named \".debug_*\").gdb-add-index uses GDB and objdump found in the PATH environmentvariable.If you want to use different versions of theseprograms, you can specify them through the GDB and OBJDUMPenvironment variables.See more in the GDB manual in node \"Index Files\" -- shell command\"info -f gdb -n \"Index Files\"\".",
        "name": "gdb-add-index - Add index files to speed up GDB",
        "section": 1
    },
    {
        "command": "gdbserver",
        "description": "gdbserver is a program that allows you to run GDB on a differentmachine than the one which is running the program being debugged.Usage (server (target) side):First, you need to have a copy of the program you want to debugput onto the target system.The program can be stripped to savespace if needed, as gdbserver doesn't care about symbols.Allsymbol handling is taken care of by the GDB running on the hostsystem.To use the server, you log on to the target system, and run thegdbserver program.You must tell it (a) how to communicate withGDB, (b) the name of your program, and (c) its arguments.Thegeneral syntax is:target> gdbserver <comm> <program> [<args> ...]For example, using a serial port, you might say:target> gdbserver /dev/com1 emacs foo.txtThis tells gdbserver to debug emacs with an argument of foo.txt,and to communicate with GDB via /dev/com1.gdbserver now waitspatiently for the host GDB to communicate with it.To use a TCP connection, you could say:target> gdbserver host:2345 emacs foo.txtThis says pretty much the same thing as the last example, exceptthat we are going to communicate with the \"host\" GDB via TCP.The \"host:2345\" argument means that we are expecting to see a TCPconnection from \"host\" to local TCP port 2345.(Currently, the\"host\" part is ignored.)You can choose any number you want forthe port number as long as it does not conflict with any existingTCP ports on the target system.This same port number must beused in the host GDBs \"target remote\" command, which will bedescribed shortly.Note that if you chose a port number thatconflicts with another service, gdbserver will print an errormessage and exit.gdbserver can also attach to running programs.This isaccomplished via the --attach argument.The syntax is:target> gdbserver --attach <comm> <pid>pid is the process ID of a currently running process.It isn'tnecessary to point gdbserver at a binary for the running process.To start \"gdbserver\" without supplying an initial command to runor process ID to attach, use the --multi command line option.Insuch case you should connect using \"target extended-remote\" tostart the program you want to debug.target> gdbserver --multi <comm>Usage (host side):You need an unstripped copy of the target program on your hostsystem, since GDB needs to examine its symbol tables and such.Start up GDB as you normally would, with the target program asthe first argument.(You may need to use the --baud option ifthe serial line is running at anything except 9600 baud.)Thatis \"gdb TARGET-PROG\", or \"gdb --baud BAUD TARGET-PROG\".Afterthat, the only new command you need to know about is \"targetremote\" (or \"target extended-remote\").Its argument is either adevice name (usually a serial device, like /dev/ttyb), or a\"HOST:PORT\" descriptor.For example:(gdb) target remote /dev/ttybcommunicates with the server via serial line /dev/ttyb, and:(gdb) target remote the-target:2345communicates via a TCP connection to port 2345 on host`the-target', where you previously started up gdbserver with thesame port number.Note that for TCP connections, you must startup gdbserver prior to using the `target remote' command,otherwise you may get an error that looks something like`Connection refused'.gdbserver can also debug multiple inferiors at once, described inthe GDB manual in node \"Inferiors Connections and Programs\" --shell command \"info -f gdb -n 'Inferiors Connections andPrograms'\".In such case use the \"extended-remote\" GDB commandvariant:(gdb) target extended-remote the-target:2345The gdbserver option --multi may or may not be used in such case.",
        "name": "gdbserver - Remote Server for the GNU Debugger",
        "section": 1
    },
    {
        "command": "gdiffmk",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "genbrk",
        "description": "genbrk reads the break (boundary) rule source code from rule-fileand creates a break iteration data file. Normally this data filehas the .brk extension.The details of the rule syntax can be found in ICU's User Guide.",
        "name": "genbrk - Compiles ICU break iteration rules source files intobinary data files",
        "section": 1
    },
    {
        "command": "gencfu",
        "description": "gencfu reads confusable character definitions in the input file,which are plain text files containing confusable characterdefinitions in the input format defined by Unicode UAX39 for thefiles confusables.txt and confusablesWholeScript.txt.Thissource (.txt) format is also accepted by ICU spoof detectors.The files must be encoded in utf-8 format, with or without a BOM.Normally the output data file has the .cfu extension.",
        "name": "gencfu - Generates Unicode Confusable data files",
        "section": 1
    },
    {
        "command": "gencnval",
        "description": "gencnval converts the ICU aliases file converterfile into thebinary file cnvalias.icu.This binary file can then be readdirectly by ICU, or used by pkgdata(1) for incorporation into alarger archive or library.If converterfile is not provided, the default ICU convrtrs.txtfile is used.",
        "name": "gencnval - compile the converters aliases file",
        "section": 1
    },
    {
        "command": "gendict",
        "description": "gendict reads the word list from dictionary-file and creates astring trie dictionary file. Normally this data file has the.dict extension.Words begin at the beginning of a line and are terminated by thefirst whitespace.Lines that begin with whitespace are ignored.",
        "name": "gendict - Compiles word list into ICU string trie dictionary",
        "section": 1
    },
    {
        "command": "gendiff",
        "description": "gendiff is a rather simple script which aids in generating a difffile from a single directory.It takes a directory name and a\"diff-extension\" as its only arguments.The diff extensionshould be a unique sequence of characters added to the end of alloriginal, unmodified files.The output of the program is a difffile which may be applied with the patch program to recreate thechanges.The usual sequence of events for creating a diff is to create twoidentical directories, make changes in one directory, and thenuse the diff utility to create a list of differences between thetwo.Using gendiff eliminates the need for the extra, originaland unmodified directory copy.Instead, only the individualfiles that are modified need to be saved.Before editing a file, copy the file, appending the extension youhave chosen to the filename.I.e.if you were going to editsomefile.cpp and have chosen the extension \"fix\", copy it tosomefile.cpp.fix before editing it.Then edit the first copy(somefile.cpp).After editing all the files you need to edit in this fashion,enter the directory one level above where your source coderesides, and then type$ gendiff somedirectory .fix > mydiff-fix.patchYou should redirect the output to a file (as illustrated) unlessyou want to see the results on stdout.",
        "name": "gendiff - utility to aid in error-free diff file generation",
        "section": 1
    },
    {
        "command": "genload",
        "description": "pmdatxmon is an example Performance Metrics Domain Agent (PMDA)which exports a small number of performance metrics from asimulated transaction monitor.The txmon PMDA is shipped as both binary and source code and isdesigned to be an aid for PMDA developers; the txmon PMDAdemonstrates how performance data can be exported from anapplication (in this case txrecord) to the PCP infrastructure viaa shared memory segment.As a matter of convenience, pmdatxmoncreates (and destroys on exit) the shared memory segment.The tx_type arguments are arbitrary unique tags used to identifydifferent transaction types.The txrecord application simulates the processing of one or moretransactions identified by tx_type and with an observed servicetime of servtime .With the -l option, txrecord displays the current summary of thetransaction activity from the shared memory segment.genload is a shell and awk(1) script that acts as a front-end totxrecord to generate a constant load of simulated transactionactivity.A brief description of the pmdatxmon command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedtxmon.log is written in the current directory of pmcd(1)when pmdatxmon is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdatxmon, txrecord, genload - txmon performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "genpmda",
        "description": "genpmda is a rapid application development tool for creating newPerformance Metrics Domain Agents, see PMDA(3).It provides avery easy and efficient way to extend the Performance Co-pilot(PCP) with new performance metrics without needing to understandthe low level details of how PMDAs are constructed.genpmda reads a config file containing an augmented PerformanceMetrics Name Space, see PMNS(5), and automatically generatesvirtually all of the source code to implement a fully functionalPMDA, including the Makefile, name space, support scripts forconfiguring the new PMDA, and the metrics help text.Fairlysimple PMDAs can be automatically generated from the config filewithout writing any additional code.More complicated PMDAs,e.g. containing multiple instance domains, require only therefresh methods for the instance domains to be written manually.An example of the config file format accepted by genpmda is givenbelow.",
        "name": "genpmda - Performance Co-Pilot PMDA Generator",
        "section": 1
    },
    {
        "command": "genrb",
        "description": "genrb converts the resource bundle source files passed on thecommand line to their binary form or to a Java source file foruse with ICU4J.The resulting binary files have a .res extensionwhile resource bundle source files typically have a .txtextension. Java source files have a java extension and follow theICU4J naming conventions.It is customary to name the resource bundles by their localename, i.e. to use a local identifier for the bundle filename,e.g.ja_JP.txt for Japanese (Japan) data, or root.txt for theroot bundle.In any case, genrb will produce a file whose basename is the name of the locale found in the resource file, notthe base name of the resource file itself.The binary files can be read directly by ICU, or used bypkgdata(1) for incorporation into a larger archive or library.",
        "name": "genrb - compile a resource bundle",
        "section": 1
    },
    {
        "command": "getent",
        "description": "The getent command displays entries from databases supported bythe Name Service Switch libraries, which are configured in/etc/nsswitch.conf.If one or more key arguments are provided,then only the entries that match the supplied keys will bedisplayed.Otherwise, if no key is provided, all entries will bedisplayed (unless the database does not support enumeration).The database may be any of those supported by the GNU C Library,listed below:ahosts When no key is provided, use sethostent(3),gethostent(3), and endhostent(3) to enumerate the hostsdatabase.This is identical to using hosts.When oneor more key arguments are provided, pass each key insuccession to getaddrinfo(3) with the address familyAF_UNSPEC, enumerating each socket address structurereturned.ahostsv4Same as ahosts, but use the address family AF_INET.ahostsv6Same as ahosts, but use the address family AF_INET6.The call to getaddrinfo(3) in this case includes theAI_V4MAPPED flag.aliasesWhen no key is provided, use setaliasent(3),getaliasent(3), and endaliasent(3) to enumerate thealiases database.When one or more key arguments areprovided, pass each key in succession togetaliasbyname(3) and display the result.ethers When one or more key arguments are provided, pass eachkey in succession to ether_aton(3) and ether_hostton(3)until a result is obtained, and display the result.Enumeration is not supported on ethers, so a key mustbe provided.groupWhen no key is provided, use setgrent(3), getgrent(3),and endgrent(3) to enumerate the group database.Whenone or more key arguments are provided, pass eachnumeric key to getgrgid(3) and each nonnumeric key togetgrnam(3) and display the result.gshadowWhen no key is provided, use setsgent(3), getsgent(3),and endsgent(3) to enumerate the gshadow database.When one or more key arguments are provided, pass eachkey in succession to getsgnam(3) and display theresult.hostsWhen no key is provided, use sethostent(3),gethostent(3), and endhostent(3) to enumerate the hostsdatabase.When one or more key arguments are provided,pass each key to gethostbyaddr(3) or gethostbyname2(3),depending on whether a call to inet_pton(3) indicatesthat the key is an IPv6 or IPv4 address or not, anddisplay the result.initgroupsWhen one or more key arguments are provided, pass eachkey in succession to getgrouplist(3) and display theresult.Enumeration is not supported on initgroups, soa key must be provided.netgroupWhen one key is provided, pass the key tosetnetgrent(3) and, using getnetgrent(3) display theresulting string triple (hostname, username,domainname).Alternatively, three keys may beprovided, which are interpreted as the hostname,username, and domainname to match to a netgroup namevia innetgr(3).Enumeration is not supported onnetgroup, so either one or three keys must be provided.networksWhen no key is provided, use setnetent(3),getnetent(3), and endnetent(3) to enumerate thenetworks database.When one or more key arguments areprovided, pass each numeric key to getnetbyaddr(3) andeach nonnumeric key to getnetbyname(3) and display theresult.passwd When no key is provided, use setpwent(3), getpwent(3),and endpwent(3) to enumerate the passwd database.Whenone or more key arguments are provided, pass eachnumeric key to getpwuid(3) and each nonnumeric key togetpwnam(3) and display the result.protocolsWhen no key is provided, use setprotoent(3),getprotoent(3), and endprotoent(3) to enumerate theprotocols database.When one or more key arguments areprovided, pass each numeric key to getprotobynumber(3)and each nonnumeric key to getprotobyname(3) anddisplay the result.rpcWhen no key is provided, use setrpcent(3),getrpcent(3), and endrpcent(3) to enumerate the rpcdatabase.When one or more key arguments are provided,pass each numeric key to getrpcbynumber(3) and eachnonnumeric key to getrpcbyname(3) and display theresult.servicesWhen no key is provided, use setservent(3),getservent(3), and endservent(3) to enumerate theservices database.When one or more key arguments areprovided, pass each numeric key to getservbynumber(3)and each nonnumeric key to getservbyname(3) and displaythe result.shadow When no key is provided, use setspent(3), getspent(3),and endspent(3) to enumerate the shadow database.Whenone or more key arguments are provided, pass each keyin succession to getspnam(3) and display the result.",
        "name": "getent - get entries from Name Service Switch libraries",
        "section": 1
    },
    {
        "command": "getfacl",
        "description": "For each file, getfacl displays the file name, owner, the group,and the Access Control List (ACL). If a directory has a defaultACL, getfacl also displays the default ACL. Non-directoriescannot have default ACLs.If getfacl is used on a file system that does not support ACLs,getfacl displays the access permissions defined by thetraditional file mode permission bits.The output format of getfacl is as follows:1:# file: somedir/2:# owner: lisa3:# group: staff4:# flags: -s-5:user::rwx6:user:joe:rwx#effective:r-x7:group::rwx#effective:r-x8:group:cool:r-x9:mask::r-x10:other::r-x11:default:user::rwx12:default:user:joe:rwx#effective:r-x13:default:group::r-x14:default:mask::r-x15:default:other::---Lines 1--3 indicate the file name, owner, and owning group.Line 4 indicates the setuid (s), setgid (s), and sticky (t) bits:either the letter representing the bit, or else a dash (-). Thisline is included if any of those bits is set and left outotherwise, so it will not be shown for most files. (SeeCONFORMANCE TO POSIX 1003.1e DRAFT STANDARD 17 below.)Lines 5, 7 and 10 correspond to the user, group and other fieldsof the file mode permission bits. These three are called the baseACL entries. Lines 6 and 8 are named user and named groupentries. Line 9 is the effective rights mask. This entry limitsthe effective rights granted to all groups and to named users.(The file owner and others permissions are not affected by theeffective rights mask; all other entries are.)Lines 11--15display the default ACL associated with this directory.Directories may have a default ACL. Regular files never have adefault ACL.The default behavior for getfacl is to display both the ACL andthe default ACL, and to include an effective rights comment forlines where the rights of the entry differ from the effectiverights.If output is to a terminal, the effective rights comment isaligned to column 40. Otherwise, a single tab character separatesthe ACL entry and the effective rights comment.The ACL listings of multiple files are separated by blank lines.The output of getfacl can also be used as input to setfacl.PERMISSIONSProcess with search access to a file (i.e., processes with readaccess to the containing directory of a file) are also grantedread access to the file's ACLs.This is analogous to thepermissions required for accessing the file mode.",
        "name": "getfacl - get file access control lists",
        "section": 1
    },
    {
        "command": "getfattr",
        "description": "For each file, getfattr displays the file name, and the set ofextended attribute names (and optionally values) which areassociated with that file. Per default only attributes in theuser namespace are displayed, see -m.The output format of getfattr -d is as follows:1:# file: somedir/2:user.name0=\"value0\"3:user.name1=\"value1\"4:user.name2=\"value2\"5:...Line 1 identifies the file name for which the following lines arebeing reported.The remaining lines (lines 2 to 4 above) showthe name and value pairs associated with the specified file.",
        "name": "getfattr - get extended attributes of filesystem objects",
        "section": 1
    },
    {
        "command": "getopt",
        "description": "getopt is used to break up (parse) options in command lines foreasy parsing by shell procedures, and to check for valid options.It uses the GNU getopt(3) routines to do this.The parameters getopt is called with can be divided into twoparts: options which modify the way getopt will do the parsing(the options and the optstring in the SYNOPSIS), and theparameters which are to be parsed (parameters in the SYNOPSIS).The second part will start at the first non-option parameter thatis not an option argument, or after the first occurrence of '--'.If no '-o' or '--options' option is found in the first part, thefirst parameter of the second part is used as the short optionsstring.If the environment variable GETOPT_COMPATIBLE is set, or if thefirst parameter is not an option (does not start with a '-', thefirst format in the SYNOPSIS), getopt will generate output thatis compatible with that of other versions of getopt(1). It willstill do parameter shuffling and recognize optional arguments(see the COMPATIBILITY section for more information).Traditional implementations of getopt(1) are unable to cope withwhitespace and other (shell-specific) special characters inarguments and non-option parameters. To solve this problem, thisimplementation can generate quoted output which must once againbe interpreted by the shell (usually by using the eval command).This has the effect of preserving those characters, but you mustcall getopt in a way that is no longer compatible with otherversions (the second or third format in the SYNOPSIS). Todetermine whether this enhanced version of getopt(1) isinstalled, a special test option (-T) can be used.",
        "name": "getopt - parse command options (enhanced)",
        "section": 1
    },
    {
        "command": "getsubids",
        "description": "The getsubids command lists the subordinate user ID ranges for agiven user. The subordinate group IDs can be listed using the -goption.",
        "name": "getsubids - get the subordinate id ranges for a user",
        "section": 1
    },
    {
        "command": "gettext",
        "description": "The gettext program translates a natural language message intothe user's language, by looking up the translation in a messagecatalog.Display native language translation of a textual message.-d, --domain=TEXTDOMAINretrieve translated messages from TEXTDOMAIN-c, --context=CONTEXTspecify context for MSGID-eenable expansion of some escape sequences-nsuppress trailing newline-E(ignored for compatibility)[TEXTDOMAIN] MSGIDretrieve translated message corresponding to MSGID fromTEXTDOMAINInformative output:-h, --helpdisplay this help and exit-V, --versiondisplay version information and exitIf the TEXTDOMAIN parameter is not given, the domain isdetermined from the environment variable TEXTDOMAIN.If themessage catalog is not found in the regular directory, anotherlocation can be specified with the environment variableTEXTDOMAINDIR.When used with the -s option the program behaveslike the 'echo' command.But it does not simply copy itsarguments to stdout.Instead those messages found in theselected catalog are translated.Standard search directory:/usr/local/share/locale",
        "name": "gettext - translate message",
        "section": 1
    },
    {
        "command": "gettextize",
        "description": "Prepares a source package to use gettext.",
        "name": "gettextize - install or upgrade gettext infrastructure",
        "section": 1
    },
    {
        "command": "gfortran",
        "description": "The gfortran command supports all the options supported by thegcc command.Only options specific to GNU Fortran are documentedhere.All GCC and GNU Fortran options are accepted both by gfortran andby gcc (as well as any other drivers built at the same time, suchas g++), since adding GNU Fortran to the GCC distribution enablesacceptance of GNU Fortran options by all of the relevant drivers.In some cases, options have positive and negative forms; thenegative form of -ffoo would be -fno-foo.This manual documentsonly one of these two forms, whichever one is not the default.",
        "name": "gfortran - GNU Fortran compiler",
        "section": 1
    },
    {
        "command": "git",
        "description": "Git is a fast, scalable, distributed revision control system withan unusually rich command set that provides both high-leveloperations and full access to internals.See gittutorial(7) to get started, then see giteveryday(7) for auseful minimum set of commands. The Git User\u2019s Manual[1] has amore in-depth introduction.After you mastered the basic concepts, you can come back to thispage to learn what commands Git offers. You can learn more aboutindividual Git commands with \"git help command\". gitcli(7) manualpage gives you an overview of the command-line command syntax.A formatted and hyperlinked copy of the latest Git documentationcan be viewed at https://git.github.io/htmldocs/git.html orhttps://git-scm.com/docs .",
        "name": "git - the stupid content tracker",
        "section": 1
    },
    {
        "command": "git-add",
        "description": "This command updates the index using the current content found inthe working tree, to prepare the content staged for the nextcommit. It typically adds the current content of existing pathsas a whole, but with some options it can also be used to addcontent with only part of the changes made to the working treefiles applied, or remove paths that do not exist in the workingtree anymore.The \"index\" holds a snapshot of the content of the working tree,and it is this snapshot that is taken as the contents of the nextcommit. Thus after making any changes to the working tree, andbefore running the commit command, you must use the add commandto add any new or modified files to the index.This command can be performed multiple times before a commit. Itonly adds the content of the specified file(s) at the time theadd command is run; if you want subsequent changes included inthe next commit, then you must run git add again to add the newcontent to the index.The git status command can be used to obtain a summary of whichfiles have changes that are staged for the next commit.The git add command will not add ignored files by default. If anyignored files were explicitly specified on the command line, gitadd will fail with a list of ignored files. Ignored files reachedby directory recursion or filename globbing performed by Git(quote your globs before the shell) will be silently ignored. Thegit add command can be used to add ignored files with the -f(force) option.Please see git-commit(1) for alternative ways to add content to acommit.",
        "name": "git-add - Add file contents to the index",
        "section": 1
    },
    {
        "command": "git-am",
        "description": "Splits mail messages in a mailbox into commit log message,authorship information and patches, and applies them to thecurrent branch. You could think of it as a reverse operation ofgit-format-patch(1) run on a branch with a straight historywithout merges.",
        "name": "git-am - Apply a series of patches from a mailbox",
        "section": 1
    },
    {
        "command": "git-annotate",
        "description": "Annotates each line in the given file with information from thecommit which introduced the line. Optionally annotates from agiven revision.The only difference between this command and git-blame(1) is thatthey use slightly different output formats, and this commandexists only for backward compatibility to support existingscripts, and provide a more familiar command name for peoplecoming from other SCM systems.",
        "name": "git-annotate - Annotate file lines with commit information",
        "section": 1
    },
    {
        "command": "git-apply",
        "description": "Reads the supplied diff output (i.e. \"a patch\") and applies it tofiles. When running from a subdirectory in a repository, patchedpaths outside the directory are ignored. With the --index optionthe patch is also applied to the index, and with the --cachedoption the patch is only applied to the index. Without theseoptions, the command applies the patch only to files, and doesnot require them to be in a Git repository.This command applies the patch but does not create a commit. Usegit-am(1) to create commits from patches generated bygit-format-patch(1) and/or received by email.",
        "name": "git-apply - Apply a patch to files and/or to the index",
        "section": 1
    },
    {
        "command": "git-archimport",
        "description": "Imports a project from one or more GNU Arch repositories. It willfollow branches and repositories within the namespaces defined bythe <archive>/<branch> parameters supplied. If it cannot find theremote branch a merge comes from it will just import it as aregular commit. If it can find it, it will mark it as a mergewhenever possible (see discussion below).The script expects you to provide the key roots where it canstart the import from an initial import or tag type of Archcommit. It will follow and import new branches within theprovided roots.It expects to be dealing with one project only. If it seesbranches that have different roots, it will refuse to run. Inthat case, edit your <archive>/<branch> parameters to defineclearly the scope of the import.git archimport uses tla extensively in the background to accessthe Arch repository. Make sure you have a recent version of tlaavailable in the path. tla must know about the repositories youpass to git archimport.For the initial import, git archimport expects to find itself inan empty directory. To follow the development of a project thatuses Arch, rerun git archimport with the same parameters as theinitial import to perform incremental imports.While git archimport will try to create sensible branch names forthe archives that it imports, it is also possible to specify Gitbranch names manually. To do so, write a Git branch name aftereach <archive>/<branch> parameter, separated by a colon. Thisway, you can shorten the Arch branch names and convert Archjargon to Git jargon, for example mapping a\"PROJECT--devo--VERSION\" branch to \"master\".Associating multiple Arch branches to one Git branch is possible;the result will make the most sense only if no commits are madeto the first branch, after the second branch is created. Still,this is useful to convert Arch repositories that had been rotatedperiodically.",
        "name": "git-archimport - Import a GNU Arch repository into Git",
        "section": 1
    },
    {
        "command": "git-archive",
        "description": "Creates an archive of the specified format containing the treestructure for the named tree, and writes it out to the standardoutput. If <prefix> is specified it is prepended to the filenamesin the archive.git archive behaves differently when given a tree ID versus whengiven a commit ID or tag ID. In the first case the current timeis used as the modification time of each file in the archive. Inthe latter case the commit time as recorded in the referencedcommit object is used instead. Additionally the commit ID isstored in a global extended pax header if the tar format is used;it can be extracted using git get-tar-commit-id. In ZIP files itis stored as a file comment.",
        "name": "git-archive - Create an archive of files from a named tree",
        "section": 1
    },
    {
        "command": "git-bisect",
        "description": "The command takes various subcommands, and different optionsdepending on the subcommand:git bisect start [--term-{new,bad}=<term> --term-{old,good}=<term>][--no-checkout] [--first-parent] [<bad> [<good>...]] [--] [<paths>...]git bisect (bad|new|<term-new>) [<rev>]git bisect (good|old|<term-old>) [<rev>...]git bisect terms [--term-good | --term-bad]git bisect skip [(<rev>|<range>)...]git bisect reset [<commit>]git bisect (visualize|view)git bisect replay <logfile>git bisect loggit bisect run <cmd>...git bisect helpThis command uses a binary search algorithm to find which commitin your project\u2019s history introduced a bug. You use it by firsttelling it a \"bad\" commit that is known to contain the bug, and a\"good\" commit that is known to be before the bug was introduced.Then git bisect picks a commit between those two endpoints andasks you whether the selected commit is \"good\" or \"bad\". Itcontinues narrowing down the range until it finds the exactcommit that introduced the change.In fact, git bisect can be used to find the commit that changedany property of your project; e.g., the commit that fixed a bug,or the commit that caused a benchmark\u2019s performance to improve.To support this more general usage, the terms \"old\" and \"new\" canbe used in place of \"good\" and \"bad\", or you can choose your ownterms. See section \"Alternate terms\" below for more information.Basic bisect commands: start, bad, goodAs an example, suppose you are trying to find the commit thatbroke a feature that was known to work in version v2.6.13-rc2 ofyour project. You start a bisect session as follows:$ git bisect start$ git bisect bad# Current version is bad$ git bisect good v2.6.13-rc2# v2.6.13-rc2 is known to be goodOnce you have specified at least one bad and one good commit, gitbisect selects a commit in the middle of that range of history,checks it out, and outputs something similar to the following:Bisecting: 675 revisions left to test after this (roughly 10 steps)You should now compile the checked-out version and test it. Ifthat version works correctly, type$ git bisect goodIf that version is broken, type$ git bisect badThen git bisect will respond with something likeBisecting: 337 revisions left to test after this (roughly 9 steps)Keep repeating the process: compile the tree, test it, anddepending on whether it is good or bad run git bisect good or gitbisect bad to ask for the next commit that needs testing.Eventually there will be no more revisions left to inspect, andthe command will print out a description of the first bad commit.The reference refs/bisect/bad will be left pointing at thatcommit.Bisect resetAfter a bisect session, to clean up the bisection state andreturn to the original HEAD, issue the following command:$ git bisect resetBy default, this will return your tree to the commit that waschecked out before git bisect start. (A new git bisect start willalso do that, as it cleans up the old bisection state.)With an optional argument, you can return to a different commitinstead:$ git bisect reset <commit>For example, git bisect reset bisect/bad will check out the firstbad revision, while git bisect reset HEAD will leave you on thecurrent bisection commit and avoid switching commits at all.Alternate termsSometimes you are not looking for the commit that introduced abreakage, but rather for a commit that caused a change betweensome other \"old\" state and \"new\" state. For example, you might belooking for the commit that introduced a particular fix. Or youmight be looking for the first commit in which the source-codefilenames were finally all converted to your company\u2019s namingstandard. Or whatever.In such cases it can be very confusing to use the terms \"good\"and \"bad\" to refer to \"the state before the change\" and \"thestate after the change\". So instead, you can use the terms \"old\"and \"new\", respectively, in place of \"good\" and \"bad\". (But notethat you cannot mix \"good\" and \"bad\" with \"old\" and \"new\" in asingle session.)In this more general usage, you provide git bisect with a \"new\"commit that has some property and an \"old\" commit that doesn\u2019thave that property. Each time git bisect checks out a commit, youtest if that commit has the property. If it does, mark the commitas \"new\"; otherwise, mark it as \"old\". When the bisection isdone, git bisect will report which commit introduced theproperty.To use \"old\" and \"new\" instead of \"good\" and bad, you must rungit bisect start without commits as argument and then run thefollowing commands to add the commits:git bisect old [<rev>]to indicate that a commit was before the sought change, orgit bisect new [<rev>...]to indicate that it was after.To get a reminder of the currently used terms, usegit bisect termsYou can get just the old (respectively new) term with git bisectterms --term-old or git bisect terms --term-good.If you would like to use your own terms instead of \"bad\"/\"good\"or \"new\"/\"old\", you can choose any names you like (exceptexisting bisect subcommands like reset, start, ...) by startingthe bisection usinggit bisect start --term-old <term-old> --term-new <term-new>For example, if you are looking for a commit that introduced aperformance regression, you might usegit bisect start --term-old fast --term-new slowOr if you are looking for the commit that fixed a bug, you mightusegit bisect start --term-new fixed --term-old brokenThen, use git bisect <term-old> and git bisect <term-new> insteadof git bisect good and git bisect bad to mark commits.Bisect visualize/viewTo see the currently remaining suspects in gitk, issue thefollowing command during the bisection process (the subcommandview can be used as an alternative to visualize):$ git bisect visualizeIf the DISPLAY environment variable is not set, git log is usedinstead. You can also give command-line options such as -p and--stat.$ git bisect visualize --statBisect log and bisect replayAfter having marked revisions as good or bad, issue the followingcommand to show what has been done so far:$ git bisect logIf you discover that you made a mistake in specifying the statusof a revision, you can save the output of this command to a file,edit it to remove the incorrect entries, and then issue thefollowing commands to return to a corrected state:$ git bisect reset$ git bisect replay that-fileAvoiding testing a commitIf, in the middle of a bisect session, you know that thesuggested revision is not a good one to test (e.g. it fails tobuild and you know that the failure does not have anything to dowith the bug you are chasing), you can manually select a nearbycommit and test that one instead.For example:$ git bisect good/bad# previous round was good or bad.Bisecting: 337 revisions left to test after this (roughly 9 steps)$ git bisect visualize# oops, that is uninteresting.$ git reset --hard HEAD~3# try 3 revisions before what# was suggestedThen compile and test the chosen revision, and afterwards markthe revision as good or bad in the usual manner.Bisect skipInstead of choosing a nearby commit by yourself, you can ask Gitto do it for you by issuing the command:$ git bisect skip# Current version cannot be testedHowever, if you skip a commit adjacent to the one you are lookingfor, Git will be unable to tell exactly which of those commitswas the first bad one.You can also skip a range of commits, instead of just one commit,using range notation. For example:$ git bisect skip v2.5..v2.6This tells the bisect process that no commit after v2.5, up toand including v2.6, should be tested.Note that if you also want to skip the first commit of the rangeyou would issue the command:$ git bisect skip v2.5 v2.5..v2.6This tells the bisect process that the commits between v2.5 andv2.6 (inclusive) should be skipped.Cutting down bisection by giving more parameters to bisect startYou can further cut down the number of trials, if you know whatpart of the tree is involved in the problem you are trackingdown, by specifying path parameters when issuing the bisect startcommand:$ git bisect start -- arch/i386 include/asm-i386If you know beforehand more than one good commit, you can narrowthe bisect space down by specifying all of the good commitsimmediately after the bad commit when issuing the bisect startcommand:$ git bisect start v2.6.20-rc6 v2.6.20-rc4 v2.6.20-rc1 --# v2.6.20-rc6 is bad# v2.6.20-rc4 and v2.6.20-rc1 are goodBisect runIf you have a script that can tell if the current source code isgood or bad, you can bisect by issuing the command:$ git bisect run my_script argumentsNote that the script (my_script in the above example) should exitwith code 0 if the current source code is good/old, and exit witha code between 1 and 127 (inclusive), except 125, if the currentsource code is bad/new.Any other exit code will abort the bisect process. It should benoted that a program that terminates via exit(-1) leaves $? =255, (see the exit(3) manual page), as the value is chopped with& 0377.The special exit code 125 should be used when the current sourcecode cannot be tested. If the script exits with this code, thecurrent revision will be skipped (see git bisect skip above). 125was chosen as the highest sensible value to use for this purpose,because 126 and 127 are used by POSIX shells to signal specificerror status (127 is for command not found, 126 is for commandfound but not executable\u2014these details do not matter, as they arenormal errors in the script, as far as bisect run is concerned).You may often find that during a bisect session you want to havetemporary modifications (e.g. s/#define DEBUG 0/#define DEBUG 1/in a header file, or \"revision that does not have this commitneeds this patch applied to work around another problem thisbisection is not interested in\") applied to the revision beingtested.To cope with such a situation, after the inner git bisect findsthe next revision to test, the script can apply the patch beforecompiling, run the real test, and afterwards decide if therevision (possibly with the needed patch) passed the test andthen rewind the tree to the pristine state. Finally the scriptshould exit with the status of the real test to let the gitbisect run command loop determine the eventual outcome of thebisect session.",
        "name": "git-bisect - Use binary search to find the commit that introduceda bug",
        "section": 1
    },
    {
        "command": "git-blame",
        "description": "Annotates each line in the given file with information from therevision which last modified the line. Optionally, startannotating from the given revision.When specified one or more times, -L restricts annotation to therequested lines.The origin of lines is automatically followed across whole-filerenames (currently there is no option to turn therename-following off). To follow lines moved from one file toanother, or to follow lines that were copied and pasted fromanother file, etc., see the -C and -M options.The report does not tell you anything about lines which have beendeleted or replaced; you need to use a tool such as git diff orthe \"pickaxe\" interface briefly mentioned in the followingparagraph.Apart from supporting file annotation, Git also supportssearching the development history for when a code snippetoccurred in a change. This makes it possible to track when a codesnippet was added to a file, moved or copied between files, andeventually deleted or replaced. It works by searching for a textstring in the diff. A small example of the pickaxe interface thatsearches for blame_usage:$ git log --pretty=oneline -S'blame_usage'5040f17eba15504bad66b14a645bddd9b015ebb7 blame -S <ancestry-file>ea4c7f9bf69e781dd0cd88d2bccb2bf5cc15c9a7 git-blame: Make the output",
        "name": "git-blame - Show what revision and author last modified each lineof a file",
        "section": 1
    },
    {
        "command": "git-branch",
        "description": "If --list is given, or if there are no non-option arguments,existing branches are listed; the current branch will behighlighted in green and marked with an asterisk. Any brancheschecked out in linked worktrees will be highlighted in cyan andmarked with a plus sign. Option -r causes the remote-trackingbranches to be listed, and option -a shows both local and remotebranches.If a <pattern> is given, it is used as a shell wildcard torestrict the output to matching branches. If multiple patternsare given, a branch is shown if it matches any of the patterns.Note that when providing a <pattern>, you must use --list;otherwise the command may be interpreted as branch creation.With --contains, shows only the branches that contain the namedcommit (in other words, the branches whose tip commits aredescendants of the named commit), --no-contains inverts it. With--merged, only branches merged into the named commit (i.e. thebranches whose tip commits are reachable from the named commit)will be listed. With --no-merged only branches not merged intothe named commit will be listed. If the <commit> argument ismissing it defaults to HEAD (i.e. the tip of the current branch).The command\u2019s second form creates a new branch head named<branchname> which points to the current HEAD, or <start-point>if given. As a special case, for <start-point>, you may use\"A...B\" as a shortcut for the merge base of A and B if there isexactly one merge base. You can leave out at most one of A and B,in which case it defaults to HEAD.Note that this will create the new branch, but it will not switchthe working tree to it; use \"git switch <newbranch>\" to switch tothe new branch.When a local branch is started off a remote-tracking branch, Gitsets up the branch (specifically the branch.<name>.remote andbranch.<name>.merge configuration entries) so that git pull willappropriately merge from the remote-tracking branch. Thisbehavior may be changed via the global branch.autoSetupMergeconfiguration flag. That setting can be overridden by using the--track and --no-track options, and changed later using gitbranch --set-upstream-to.With a -m or -M option, <oldbranch> will be renamed to<newbranch>. If <oldbranch> had a corresponding reflog, it isrenamed to match <newbranch>, and a reflog entry is created toremember the branch renaming. If <newbranch> exists, -M must beused to force the rename to happen.The -c and -C options have the exact same semantics as -m and -M,except instead of the branch being renamed, it will be copied toa new name, along with its config and reflog.With a -d or -D option, <branchname> will be deleted. You mayspecify more than one branch for deletion. If the branchcurrently has a reflog then the reflog will also be deleted.Use -r together with -d to delete remote-tracking branches. Note,that it only makes sense to delete remote-tracking branches ifthey no longer exist in the remote repository or if git fetch wasconfigured not to fetch them again. See also the prune subcommandof git-remote(1) for a way to clean up all obsoleteremote-tracking branches.",
        "name": "git-branch - List, create, or delete branches",
        "section": 1
    },
    {
        "command": "git-bugreport",
        "description": "Captures information about the user\u2019s machine, Git client, andrepository state, as well as a form requesting information aboutthe behavior the user observed, into a single text file which theuser can then share, for example to the Git mailing list, inorder to report an observed bug.The following information is requested from the user:\u2022Reproduction steps\u2022Expected behavior\u2022Actual behaviorThe following information is captured automatically:\u2022git version --build-options\u2022uname sysname, release, version, and machine strings\u2022Compiler-specific info string\u2022A list of enabled hooks\u2022$SHELLAdditional information may be gathered into a separate ziparchive using the --diagnose option, and can be attachedalongside the bugreport document to provide additional context toreaders.This tool is invoked via the typical Git setup process, whichmeans that in some cases, it might not be able to launch - forexample, if a relevant config file is unreadable. In this kind ofscenario, it may be helpful to manually gather the kind ofinformation listed above when manually asking for help.",
        "name": "git-bugreport - Collect information for user to file a bug report",
        "section": 1
    },
    {
        "command": "git-bundle",
        "description": "Create, unpack, and manipulate \"bundle\" files. Bundles are usedfor the \"offline\" transfer of Git objects without an active\"server\" sitting on the other side of the network connection.They can be used to create both incremental and full backups of arepository, and to relay the state of the references in onerepository to another.Git commands that fetch or otherwise \"read\" via protocols such asssh:// and https:// can also operate on bundle files. It ispossible git-clone(1) a new repository from a bundle, to usegit-fetch(1) to fetch from one, and to list the referencescontained within it with git-ls-remote(1). There\u2019s nocorresponding \"write\" support, i.e.a git push into a bundle isnot supported.See the \"EXAMPLES\" section below for examples of how to usebundles.",
        "name": "git-bundle - Move objects and refs by archive",
        "section": 1
    },
    {
        "command": "git-cat-file",
        "description": "In its first form, the command provides the content or the typeof an object in the repository. The type is required unless -t or-p is used to find the object type, or -s is used to find theobject size, or --textconv or --filters is used (which imply type\"blob\").In the second form, a list of objects (separated by linefeeds) isprovided on stdin, and the SHA-1, type, and size of each objectis printed on stdout. The output format can be overridden usingthe optional <format> argument. If either --textconv or --filterswas specified, the input is expected to list the object namesfollowed by the path name, separated by a single whitespace, sothat the appropriate drivers can be determined.",
        "name": "git-cat-file - Provide content or type and size information forrepository objects",
        "section": 1
    },
    {
        "command": "git-check-attr",
        "description": "For every pathname, this command will list if each attribute isunspecified, set, or unset as a gitattribute on that pathname.",
        "name": "git-check-attr - Display gitattributes information",
        "section": 1
    },
    {
        "command": "git-check-ignore",
        "description": "For each pathname given via the command-line or from a file via--stdin, check whether the file is excluded by .gitignore (orother input files to the exclude mechanism) and output the pathif it is excluded.By default, tracked files are not shown at all since they are notsubject to exclude rules; but see \u2018--no-index\u2019.",
        "name": "git-check-ignore - Debug gitignore / exclude files",
        "section": 1
    },
    {
        "command": "git-check-mailmap",
        "description": "For each \u201cName <user@host>\u201d or \u201c<user@host>\u201d from thecommand-line or standard input (when using --stdin), look up theperson\u2019s canonical name and email address (see \"Mapping Authors\"below). If found, print them; otherwise print the input as-is.",
        "name": "git-check-mailmap - Show canonical names and email addresses ofcontacts",
        "section": 1
    },
    {
        "command": "git-check-ref-format",
        "description": "Checks if a given refname is acceptable, and exits with anon-zero status if it is not.A reference is used in Git to specify branches and tags. A branchhead is stored in the refs/heads hierarchy, while a tag is storedin the refs/tags hierarchy of the ref namespace (typically in$GIT_DIR/refs/heads and $GIT_DIR/refs/tags directories or, asentries in file $GIT_DIR/packed-refs if refs are packed by gitgc).Git imposes the following rules on how references are named:1. They can include slash / for hierarchical (directory)grouping, but no slash-separated component can begin with adot .or end with the sequence .lock.2. They must contain at least one /. This enforces the presenceof a category like heads/, tags/ etc. but the actual namesare not restricted. If the --allow-onelevel option is used,this rule is waived.3. They cannot have two consecutive dots ..anywhere.4. They cannot have ASCII control characters (i.e. bytes whosevalues are lower than \\040, or \\177 DEL), space, tilde ~,caret ^, or colon : anywhere.5. They cannot have question-mark ?, asterisk *, or open bracket[ anywhere. See the --refspec-pattern option below for anexception to this rule.6. They cannot begin or end with a slash / or contain multipleconsecutive slashes (see the --normalize option below for anexception to this rule)7. They cannot end with a dot ..8. They cannot contain a sequence @{.9. They cannot be the single character @.10. They cannot contain a \\.These rules make it easy for shell script based tools to parsereference names, pathname expansion by the shell when a referencename is used unquoted (by mistake), and also avoid ambiguities incertain reference name expressions (see gitrevisions(7)):1. A double-dot ..is often used as in ref1..ref2, and in somecontexts this notation means ^ref1 ref2 (i.e. not in ref1 andin ref2).2. A tilde ~ and caret ^ are used to introduce the postfix nthparent and peel onion operation.3. A colon : is used as in srcref:dstref to mean \"use srcref\u2019svalue and store it in dstref\" in fetch and push operations.It may also be used to select a specific object such as withgit cat-file: \"git cat-file blob v1.3.3:refs.c\".4. at-open-brace @{ is used as a notation to access a reflogentry.With the --branch option, the command takes a name and checks ifit can be used as a valid branch name (e.g. when creating a newbranch). But be cautious when using the previous checkout syntaxthat may refer to a detached HEAD state. The rule gitcheck-ref-format --branch $name implements may be stricter thanwhat git check-ref-format refs/heads/$name says (e.g. a dash mayappear at the beginning of a ref component, but it is explicitlyforbidden at the beginning of a branch name). When run with--branch option in a repository, the input is first expanded forthe \u201cprevious checkout syntax\u201d @{-n}. For example, @{-1} is a wayto refer the last thing that was checked out using \"git switch\"or \"git checkout\" operation. This option should be used byporcelains to accept this syntax anywhere a branch name isexpected, so they can act as if you typed the branch name. As anexception note that, the \u201cprevious checkout operation\u201d mightresult in a commit object name when the N-th last thing checkedout was not a branch.",
        "name": "git-check-ref-format - Ensures that a reference name is wellformed",
        "section": 1
    },
    {
        "command": "git-checkout",
        "description": "Updates files in the working tree to match the version in theindex or the specified tree. If no pathspec was given, gitcheckout will also update HEAD to set the specified branch as thecurrent branch.git checkout [<branch>]To prepare for working on <branch>, switch to it by updatingthe index and the files in the working tree, and by pointingHEAD at the branch. Local modifications to the files in theworking tree are kept, so that they can be committed to the<branch>.If <branch> is not found but there does exist a trackingbranch in exactly one remote (call it <remote>) with amatching name and --no-guess is not specified, treat asequivalent to$ git checkout -b <branch> --track <remote>/<branch>You could omit <branch>, in which case the commanddegenerates to \"check out the current branch\", which is aglorified no-op with rather expensive side-effects to showonly the tracking information, if exists, for the currentbranch.git checkout -b|-B <new-branch> [<start-point>]Specifying -b causes a new branch to be created as ifgit-branch(1) were called and then checked out. In this caseyou can use the --track or --no-track options, which will bepassed to git branch. As a convenience, --track without -bimplies branch creation; see the description of --trackbelow.If -B is given, <new-branch> is created if it doesn\u2019t exist;otherwise, it is reset. This is the transactional equivalentof$ git branch -f <branch> [<start-point>]$ git checkout <branch>that is to say, the branch is not reset/created unless \"gitcheckout\" is successful.git checkout --detach [<branch>], git checkout [--detach]<commit>Prepare to work on top of <commit>, by detaching HEAD at it(see \"DETACHED HEAD\" section), and updating the index and thefiles in the working tree. Local modifications to the filesin the working tree are kept, so that the resulting workingtree will be the state recorded in the commit plus the localmodifications.When the <commit> argument is a branch name, the --detachoption can be used to detach HEAD at the tip of the branch(git checkout <branch> would check out that branch withoutdetaching HEAD).Omitting <branch> detaches HEAD at the tip of the currentbranch.git checkout [-f|--ours|--theirs|-m|--conflict=<style>][<tree-ish>] [--] <pathspec>..., git checkout[-f|--ours|--theirs|-m|--conflict=<style>] [<tree-ish>]--pathspec-from-file=<file> [--pathspec-file-nul]Overwrite the contents of the files that match the pathspec.When the <tree-ish> (most often a commit) is not given,overwrite working tree with the contents in the index. Whenthe <tree-ish> is given, overwrite both the index and theworking tree with the contents at the <tree-ish>.The index may contain unmerged entries because of a previousfailed merge. By default, if you try to check out such anentry from the index, the checkout operation will fail andnothing will be checked out. Using -f will ignore theseunmerged entries. The contents from a specific side of themerge can be checked out of the index by using --ours or--theirs. With -m, changes made to the working tree file canbe discarded to re-create the original conflicted mergeresult.git checkout (-p|--patch) [<tree-ish>] [--] [<pathspec>...]This is similar to the previous mode, but lets you use theinteractive interface to show the \"diff\" output and choosewhich hunks to use in the result. See below for thedescription of --patch option.",
        "name": "git-checkout - Switch branches or restore working tree files",
        "section": 1
    },
    {
        "command": "git-checkout-index",
        "description": "Will copy all files listed from the index to the workingdirectory (not overwriting existing files).",
        "name": "git-checkout-index - Copy files from the index to the workingtree",
        "section": 1
    },
    {
        "command": "git-cherry",
        "description": "Determine whether there are commits in <head>..<upstream> thatare equivalent to those in the range <limit>..<head>.The equivalence test is based on the diff, after removingwhitespace and line numbers. git-cherry therefore detects whencommits have been \"copied\" by means of git-cherry-pick(1),git-am(1) or git-rebase(1).Outputs the SHA1 of every commit in <limit>..<head>, prefixedwith - for commits that have an equivalent in <upstream>, and +for commits that do not.",
        "name": "git-cherry - Find commits yet to be applied to upstream",
        "section": 1
    },
    {
        "command": "git-cherry-pick",
        "description": "Given one or more existing commits, apply the change each oneintroduces, recording a new commit for each. This requires yourworking tree to be clean (no modifications from the HEAD commit).When it is not obvious how to apply a change, the followinghappens:1. The current branch and HEAD pointer stay at the last commitsuccessfully made.2. The CHERRY_PICK_HEAD ref is set to point at the commit thatintroduced the change that is difficult to apply.3. Paths in which the change applied cleanly are updated both inthe index file and in your working tree.4. For conflicting paths, the index file records up to threeversions, as described in the \"TRUE MERGE\" section ofgit-merge(1). The working tree files will include adescription of the conflict bracketed by the usual conflictmarkers <<<<<<< and >>>>>>>.5. No other modifications are made.See git-merge(1) for some hints on resolving such conflicts.",
        "name": "git-cherry-pick - Apply the changes introduced by some existingcommits",
        "section": 1
    },
    {
        "command": "git-citool",
        "description": "A Tcl/Tk based graphical interface to review modified files,stage them into the index, enter a commit message and record thenew commit onto the current branch. This interface is analternative to the less interactive git commit program.git citool is actually a standard alias for git gui citool. Seegit-gui(1) for more details.",
        "name": "git-citool - Graphical alternative to git-commit",
        "section": 1
    },
    {
        "command": "git-clean",
        "description": "Cleans the working tree by recursively removing files that arenot under version control, starting from the current directory.Normally, only files unknown to Git are removed, but if the -xoption is specified, ignored files are also removed. This can,for example, be useful to remove all build products.If any optional <pathspec>... arguments are given, only thosepaths that match the pathspec are affected.",
        "name": "git-clean - Remove untracked files from the working tree",
        "section": 1
    },
    {
        "command": "git-clone",
        "description": "Clones a repository into a newly created directory, createsremote-tracking branches for each branch in the cloned repository(visible using git branch --remotes), and creates and checks outan initial branch that is forked from the cloned repository\u2019scurrently active branch.After the clone, a plain git fetch without arguments will updateall the remote-tracking branches, and a git pull withoutarguments will in addition merge the remote master branch intothe current master branch, if any (this is untrue when\"--single-branch\" is given; see below).This default configuration is achieved by creating references tothe remote branch heads under refs/remotes/origin and byinitializing remote.origin.url and remote.origin.fetchconfiguration variables.",
        "name": "git-clone - Clone a repository into a new directory",
        "section": 1
    },
    {
        "command": "git-column",
        "description": "This command formats the lines of its standard input into a tablewith multiple columns. Each input line occupies one cell of thetable. It is used internally by other git commands to formatoutput into columns.",
        "name": "git-column - Display data in columns",
        "section": 1
    },
    {
        "command": "git-commit",
        "description": "Create a new commit containing the current contents of the indexand the given log message describing the changes. The new commitis a direct child of HEAD, usually the tip of the current branch,and the branch is updated to point to it (unless no branch isassociated with the working tree, in which case HEAD is\"detached\" as described in git-checkout(1)).The content to be committed can be specified in several ways:1. by using git-add(1) to incrementally \"add\" changes to theindex before using the commit command (Note: even modifiedfiles must be \"added\");2. by using git-rm(1) to remove files from the working tree andthe index, again before using the commit command;3. by listing files as arguments to the commit command (without--interactive or --patch switch), in which case the commitwill ignore changes staged in the index, and instead recordthe current content of the listed files (which must alreadybe known to Git);4. by using the -a switch with the commit command toautomatically \"add\" changes from all known files (i.e. allfiles that are already listed in the index) and toautomatically \"rm\" files in the index that have been removedfrom the working tree, and then perform the actual commit;5. by using the --interactive or --patch switches with thecommit command to decide one by one which files or hunksshould be part of the commit in addition to contents in theindex, before finalizing the operation. See the \u201cInteractiveMode\u201d section of git-add(1) to learn how to operate thesemodes.The --dry-run option can be used to obtain a summary of what isincluded by any of the above for the next commit by giving thesame set of parameters (options and paths).If you make a commit and then find a mistake immediately afterthat, you can recover from it with git reset.",
        "name": "git-commit - Record changes to the repository",
        "section": 1
    },
    {
        "command": "git-commit-graph",
        "description": "Manage the serialized commit-graph file.",
        "name": "git-commit-graph - Write and verify Git commit-graph files",
        "section": 1
    },
    {
        "command": "git-commit-tree",
        "description": "This is usually not what an end user wants to run directly. Seegit-commit(1) instead.Creates a new commit object based on the provided tree object andemits the new commit object id on stdout. The log message is readfrom the standard input, unless -m or -F options are given.The -m and -F options can be given any number of times, in anyorder. The commit log message will be composed in the order inwhich the options are given.A commit object may have any number of parents. With exactly oneparent, it is an ordinary commit. Having more than one parentmakes the commit a merge between several lines of history.Initial (root) commits have no parents.While a tree represents a particular directory state of a workingdirectory, a commit represents that state in \"time\", and explainshow to get there.Normally a commit would identify a new \"HEAD\" state, and whileGit doesn\u2019t care where you save the note about that state, inpractice we tend to just write the result to the file that ispointed at by .git/HEAD, so that we can always see what the lastcommitted state was.",
        "name": "git-commit-tree - Create a new commit object",
        "section": 1
    },
    {
        "command": "git-config",
        "description": "You can query/set/replace/unset options with this command. Thename is actually the section and the key separated by a dot, andthe value will be escaped.Multiple lines can be added to an option by using the --addoption. If you want to update or unset an option which can occuron multiple lines, a value-pattern (which is an extended regularexpression, unless the --fixed-value option is given) needs to begiven. Only the existing values that match the pattern areupdated or unset. If you want to handle the lines that do notmatch the pattern, just prepend a single exclamation mark infront (see also the section called \u201cEXAMPLES\u201d), but note thatthis only works when the --fixed-value option is not in use.The --type=<type> option instructs git config to ensure thatincoming and outgoing values are canonicalize-able under thegiven <type>. If no --type=<type> is given, no canonicalizationwill be performed. Callers may unset an existing --type specifierwith --no-type.When reading, the values are read from the system, global andrepository local configuration files by default, and options--system, --global, --local, --worktree and --file <filename> canbe used to tell the command to read from only that location (seethe section called \u201cFILES\u201d).When writing, the new value is written to the repository localconfiguration file by default, and options --system, --global,--worktree, --file <filename> can be used to tell the command towrite to that location (you can say --local but that is thedefault).This command will fail with non-zero status upon error. Some exitcodes are:\u2022The section or key is invalid (ret=1),\u2022no section or name was provided (ret=2),\u2022the config file is invalid (ret=3),\u2022the config file cannot be written (ret=4),\u2022you try to unset an option which does not exist (ret=5),\u2022you try to unset/set an option for which multiple lines match(ret=5), or\u2022you try to use an invalid regexp (ret=6).On success, the command returns the exit code 0.A list of all available configuration variables can be obtainedusing the git help --config command.",
        "name": "git-config - Get and set repository or global options",
        "section": 1
    },
    {
        "command": "git-count-objects",
        "description": "This counts the number of unpacked object files and disk spaceconsumed by them, to help you decide when it is a good time torepack.",
        "name": "git-count-objects - Count unpacked number of objects and theirdisk consumption",
        "section": 1
    },
    {
        "command": "git-credential",
        "description": "Git has an internal interface for storing and retrievingcredentials from system-specific helpers, as well as promptingthe user for usernames and passwords. The git-credential commandexposes this interface to scripts which may want to retrieve,store, or prompt for credentials in the same manner as Git. Thedesign of this scriptable interface models the internal C API;see credential.h for more background on the concepts.git-credential takes an \"action\" option on the command-line (oneof fill, approve, or reject) and reads a credential descriptionon stdin (see INPUT/OUTPUT FORMAT).If the action is fill, git-credential will attempt to add\"username\" and \"password\" attributes to the description byreading config files, by contacting any configured credentialhelpers, or by prompting the user. The username and passwordattributes of the credential description are then printed tostdout together with the attributes already provided.If the action is approve, git-credential will send thedescription to any configured credential helpers, which may storethe credential for later use.If the action is reject, git-credential will send the descriptionto any configured credential helpers, which may erase any storedcredential matching the description.If the action is approve or reject, no output should be emitted.",
        "name": "git-credential - Retrieve and store user credentials",
        "section": 1
    },
    {
        "command": "git-credential-cache",
        "description": "This command caches credentials for use by future Git programs.The stored credentials are kept in memory of the cache-daemonprocess (instead of written to a file) and are forgotten after aconfigurable timeout. Credentials are forgotten sooner if thecache-daemon dies, for example if the system restarts. The cacheis accessible over a Unix domain socket, restricted to thecurrent user by filesystem permissions.You probably don\u2019t want to invoke this command directly; it ismeant to be used as a credential helper by other parts of Git.See gitcredentials(7) or EXAMPLES below.",
        "name": "git-credential-cache - Helper to temporarily store passwords inmemory",
        "section": 1
    },
    {
        "command": "git-credential-cache--daemon",
        "description": "NoteYou probably don\u2019t want to invoke this command yourself; itis started automatically when you usegit-credential-cache(1).This command listens on the Unix domain socket specified by<socket-path> for git-credential-cache clients. Clients may storeand retrieve credentials. Each credential is held for a timeoutspecified by the client; once no credentials are held, the daemonexits.If the --debug option is specified, the daemon does not close itsstderr stream, and may output extra diagnostics to it even afterit has begun listening for clients.",
        "name": "git-credential-cache--daemon - Temporarily store user credentialsin memory",
        "section": 1
    },
    {
        "command": "git-credential-store",
        "description": "NoteUsing this helper will store your passwords unencrypted ondisk, protected only by filesystem permissions. If this isnot an acceptable security tradeoff, trygit-credential-cache(1), or find a helper that integrateswith secure storage provided by your operating system.This command stores credentials indefinitely on disk for use byfuture Git programs.You probably don\u2019t want to invoke this command directly; it ismeant to be used as a credential helper by other parts of git.See gitcredentials(7) or EXAMPLES below.",
        "name": "git-credential-store - Helper to store credentials on disk",
        "section": 1
    },
    {
        "command": "git-cvsexportcommit",
        "description": "Exports a commit from Git to a CVS checkout, making it easier tomerge patches from a Git repository into a CVS repository.Specify the name of a CVS checkout using the -w switch or executeit from the root of the CVS working copy. In the latter caseGIT_DIR must be defined. See examples below.It does its best to do the safe thing, it will check that thefiles are unchanged and up to date in the CVS checkout, and itwill not autocommit by default.Supports file additions, removals, and commits that affect binaryfiles.If the commit is a merge commit, you must tell gitcvsexportcommit what parent the changeset should be done against.",
        "name": "git-cvsexportcommit - Export a single commit to a CVS checkout",
        "section": 1
    },
    {
        "command": "git-cvsimport",
        "description": "WARNING: git cvsimport uses cvsps version 2, which is considereddeprecated; it does not work with cvsps version 3 and later. Ifyou are performing a one-shot import of a CVS repository considerusing cvs2git[1] or cvs-fast-export[2].Imports a CVS repository into Git. It will either create a newrepository, or incrementally import into an existing one.Splitting the CVS log into patch sets is done by cvsps. At leastversion 2.1 is required.WARNING: for certain situations the import leads to incorrectresults. Please see the section ISSUES for further reference.You should never do any work of your own on the branches that arecreated by git cvsimport. By default initial import will createand populate a \"master\" branch from the CVS repository\u2019s mainbranch which you\u2019re free to work with; after that, you need togit merge incremental imports, or any CVS branches, yourself. Itis advisable to specify a named remote via -r to separate andprotect the incoming branches.If you intend to set up a shared public repository that alldevelopers can read/write, or if you want to usegit-cvsserver(1), then you probably want to make a bare clone ofthe imported repository, and use the clone as the sharedrepository. See gitcvs-migration(7).",
        "name": "git-cvsimport - Salvage your data out of another SCM people loveto hate",
        "section": 1
    },
    {
        "command": "git-cvsserver",
        "description": "This application is a CVS emulation layer for Git.It is highly functional. However, not all methods areimplemented, and for those methods that are implemented, not allswitches are implemented.Testing has been done using both the CLI CVS client, and theEclipse CVS plugin. Most functionality works fine with both ofthese clients.",
        "name": "git-cvsserver - A CVS server emulator for Git",
        "section": 1
    },
    {
        "command": "git-daemon",
        "description": "A really simple TCP Git daemon that normally listens on port\"DEFAULT_GIT_PORT\" aka 9418. It waits for a connection asking fora service, and will serve that service if it is enabled.It verifies that the directory has the magic file\"git-daemon-export-ok\", and it will refuse to export any Gitdirectory that hasn\u2019t explicitly been marked for export this way(unless the --export-all parameter is specified). If you passsome directory paths as git daemon arguments, the offers arelimited to repositories within those directories.By default, only upload-pack service is enabled, which serves gitfetch-pack and git ls-remote clients, which are invoked from gitfetch, git pull, and git clone.This is ideally suited for read-only updates, i.e., pulling fromGit repositories.An upload-archive also exists to serve git archive.",
        "name": "git-daemon - A really simple server for Git repositories",
        "section": 1
    },
    {
        "command": "git-describe",
        "description": "The command finds the most recent tag that is reachable from acommit. If the tag points to the commit, then only the tag isshown. Otherwise, it suffixes the tag name with the number ofadditional commits on top of the tagged object and theabbreviated object name of the most recent commit. The result isa \"human-readable\" object name which can also be used to identifythe commit to other git commands.By default (without --all or --tags) git describe only showsannotated tags. For more information about creating annotatedtags see the -a and -s options to git-tag(1).If the given object refers to a blob, it will be described as<commit-ish>:<path>, such that the blob can be found at <path> inthe <commit-ish>, which itself describes the first commit inwhich this blob occurs in a reverse revision walk from HEAD.",
        "name": "git-describe - Give an object a human readable name based on anavailable ref",
        "section": 1
    },
    {
        "command": "git-diagnose",
        "description": "Collects detailed information about the user\u2019s machine, Gitclient, and repository state and packages that information into azip archive. The generated archive can then, for example, beshared with the Git mailing list to help debug an issue or serveas a reference for independent debugging.By default, the following information is captured in the archive:\u2022git version --build-options\u2022The path to the repository root\u2022The available disk space on the filesystem\u2022The name and size of each packfile, including those inalternate object stores\u2022The total count of loose objects, as well as counts brokendown by .git/objects subdirectoryAdditional information can be collected by selecting a differentdiagnostic mode using the --mode option.This tool differs from git-bugreport(1) in that it collects muchmore detailed information with a greater focus on reporting thesize and data shape of repository contents.",
        "name": "git-diagnose - Generate a zip archive of diagnostic information",
        "section": 1
    },
    {
        "command": "git-diff",
        "description": "Show changes between the working tree and the index or a tree,changes between the index and a tree, changes between two trees,changes resulting from a merge, changes between two blob objects,or changes between two files on disk.git diff [<options>] [--] [<path>...]This form is to view the changes you made relative to theindex (staging area for the next commit). In other words, thedifferences are what you could tell Git to further add to theindex but you still haven\u2019t. You can stage these changes byusing git-add(1).git diff [<options>] --no-index [--] <path> <path>This form is to compare the given two paths on thefilesystem. You can omit the --no-index option when runningthe command in a working tree controlled by Git and at leastone of the paths points outside the working tree, or whenrunning the command outside a working tree controlled by Git.This form implies --exit-code.git diff [<options>] --cached [--merge-base] [<commit>] [--][<path>...]This form is to view the changes you staged for the nextcommit relative to the named <commit>. Typically you wouldwant comparison with the latest commit, so if you do not give<commit>, it defaults to HEAD. If HEAD does not exist (e.g.unborn branches) and <commit> is not given, it shows allstaged changes. --staged is a synonym of --cached.If --merge-base is given, instead of using <commit>, use themerge base of <commit> and HEAD.git diff --cached--merge-base A is equivalent to git diff --cached $(gitmerge-base A HEAD).git diff [<options>] [--merge-base] <commit> [--] [<path>...]This form is to view the changes you have in your workingtree relative to the named <commit>. You can use HEAD tocompare it with the latest commit, or a branch name tocompare with the tip of a different branch.If --merge-base is given, instead of using <commit>, use themerge base of <commit> and HEAD.git diff --merge-base A isequivalent to git diff $(git merge-base A HEAD).git diff [<options>] [--merge-base] <commit> <commit> [--][<path>...]This is to view the changes between two arbitrary <commit>.If --merge-base is given, use the merge base of the twocommits for the \"before\" side.git diff --merge-base A B isequivalent to git diff $(git merge-base A B) B.git diff [<options>] <commit> <commit>... <commit> [--][<path>...]This form is to view the results of a merge commit. The firstlisted <commit> must be the merge itself; the remaining twoor more commits should be its parents. Convenient ways toproduce the desired set of revisions are to use the suffixes^@ and ^!. If A is a merge commit, then git diff A A^@, gitdiff A^!and git show A all give the same combined diff.git diff [<options>] <commit>..<commit> [--] [<path>...]This is synonymous to the earlier form (without the ..) forviewing the changes between two arbitrary <commit>. If<commit> on one side is omitted, it will have the same effectas using HEAD instead.git diff [<options>] <commit>...<commit> [--] [<path>...]This form is to view the changes on the branch containing andup to the second <commit>, starting at a common ancestor ofboth <commit>.git diff A...B is equivalent to git diff$(git merge-base A B) B. You can omit any one of <commit>,which has the same effect as using HEAD instead.Just in case you are doing something exotic, it should be notedthat all of the <commit> in the above description, except in the--merge-base case and in the last two forms that use ..notations, can be any <tree>. A tree of interest is the onepointed to by the special ref AUTO_MERGE, which is written by theort merge strategy upon hitting merge conflicts (seegit-merge(1)). Comparing the working tree with AUTO_MERGE showschanges you\u2019ve made so far to resolve textual conflicts (see theexamples below).For a more complete list of ways to spell <commit>, see\"SPECIFYING REVISIONS\" section in gitrevisions(7). However,\"diff\" is about comparing two endpoints, not ranges, and therange notations (<commit>..<commit> and <commit>...<commit>) donot mean a range as defined in the \"SPECIFYING RANGES\" section ingitrevisions(7).git diff [<options>] <blob> <blob>This form is to view the differences between the raw contentsof two blob objects.",
        "name": "git-diff - Show changes between commits, commit and working tree,etc",
        "section": 1
    },
    {
        "command": "git-diff-files",
        "description": "Compares the files in the working tree and the index. When pathsare specified, compares only those named paths. Otherwise allentries in the index are compared. The output format is the sameas for git diff-index and git diff-tree.",
        "name": "git-diff-files - Compares files in the working tree and the index",
        "section": 1
    },
    {
        "command": "git-diff-index",
        "description": "Compares the content and mode of the blobs found in a tree objectwith the corresponding tracked files in the working tree, or withthe corresponding paths in the index. When <path> arguments arepresent, compares only paths matching those patterns. Otherwiseall tracked files are compared.",
        "name": "git-diff-index - Compare a tree to the working tree or index",
        "section": 1
    },
    {
        "command": "git-diff-tree",
        "description": "Compares the content and mode of the blobs found via two treeobjects.If there is only one <tree-ish> given, the commit is comparedwith its parents (see --stdin below).Note that git diff-tree can use the tree encapsulated in a commitobject.",
        "name": "git-diff-tree - Compares the content and mode of blobs found viatwo tree objects",
        "section": 1
    },
    {
        "command": "git-difftool",
        "description": "git difftool is a Git command that allows you to compare and editfiles between revisions using common diff tools. git difftool isa frontend to git diff and accepts the same options andarguments. See git-diff(1).",
        "name": "git-difftool - Show changes using common diff tools",
        "section": 1
    },
    {
        "command": "git-fast-export",
        "description": "This program dumps the given revisions in a form suitable to bepiped into git fast-import.You can use it as a human-readable bundle replacement (seegit-bundle(1)), or as a format that can be edited before beingfed to git fast-import in order to do history rewrites (anability relied on by tools like git filter-repo).",
        "name": "git-fast-export - Git data exporter",
        "section": 1
    },
    {
        "command": "git-fast-import",
        "description": "This program is usually not what the end user wants to rundirectly. Most end users want to use one of the existing frontendprograms, which parses a specific type of foreign source andfeeds the contents stored there to git fast-import.fast-import reads a mixed command/data stream from standard inputand writes one or more packfiles directly into the currentrepository. When EOF is received on standard input, fast importwrites out updated branch and tag refs, fully updating thecurrent repository with the newly imported data.The fast-import backend itself can import into an emptyrepository (one that has already been initialized by git init) orincrementally update an existing populated repository. Whether ornot incremental imports are supported from a particular foreignsource depends on the frontend program in use.",
        "name": "git-fast-import - Backend for fast Git data importers",
        "section": 1
    },
    {
        "command": "git-fetch",
        "description": "Fetch branches and/or tags (collectively, \"refs\") from one ormore other repositories, along with the objects necessary tocomplete their histories. Remote-tracking branches are updated(see the description of <refspec> below for ways to control thisbehavior).By default, any tag that points into the histories being fetchedis also fetched; the effect is to fetch tags that point atbranches that you are interested in. This default behavior can bechanged by using the --tags or --no-tags options or byconfiguring remote.<name>.tagOpt. By using a refspec that fetchestags explicitly, you can fetch tags that do not point intobranches you are interested in as well.git fetch can fetch from either a single named repository or URL,or from several repositories at once if <group> is given andthere is a remotes.<group> entry in the configuration file. (Seegit-config(1)).When no remote is specified, by default the origin remote will beused, unless there\u2019s an upstream branch configured for thecurrent branch.The names of refs that are fetched, together with the objectnames they point at, are written to .git/FETCH_HEAD. Thisinformation may be used by scripts or other git commands, such asgit-pull(1).",
        "name": "git-fetch - Download objects and refs from another repository",
        "section": 1
    },
    {
        "command": "git-fetch-pack",
        "description": "Usually you would want to use git fetch, which is a higher levelwrapper of this command, instead.Invokes git-upload-pack on a possibly remote repository and asksit to send objects missing from this repository, to update thenamed heads. The list of commits available locally is found outby scanning the local refs/ hierarchy and sent to git-upload-packrunning on the other end.This command degenerates to download everything to complete theasked refs from the remote side when the local side does not havea common ancestor commit.",
        "name": "git-fetch-pack - Receive missing objects from another repository",
        "section": 1
    },
    {
        "command": "git-filter-branch",
        "description": "Lets you rewrite Git revision history by rewriting the branchesmentioned in the <rev-list options>, applying custom filters oneach revision. Those filters can modify each tree (e.g. removinga file or running a perl rewrite on all files) or informationabout each commit. Otherwise, all information (including originalcommit times or merge information) will be preserved.The command will only rewrite the positive refs mentioned in thecommand line (e.g. if you pass a..b, only b will be rewritten).If you specify no filters, the commits will be recommittedwithout any changes, which would normally have no effect.Nevertheless, this may be useful in the future for compensatingfor some Git bugs or such, therefore such a usage is permitted.NOTE: This command honors .git/info/grafts file and refs in therefs/replace/ namespace. If you have any grafts or replacementrefs defined, running this command will make them permanent.WARNING! The rewritten history will have different object namesfor all the objects and will not converge with the originalbranch. You will not be able to easily push and distribute therewritten branch on top of the original branch. Please do not usethis command if you do not know the full implications, and avoidusing it anyway, if a simple single commit would suffice to fixyour problem. (See the \"RECOVERING FROM UPSTREAM REBASE\" sectionin git-rebase(1) for further information about rewritingpublished history.)Always verify that the rewritten version is correct: The originalrefs, if different from the rewritten ones, will be stored in thenamespace refs/original/.Note that since this operation is very I/O expensive, it might bea good idea to redirect the temporary directory off-disk with the-d option, e.g. on tmpfs. Reportedly the speedup is verynoticeable.FiltersThe filters are applied in the order as listed below. The<command> argument is always evaluated in the shell context usingthe eval command (with the notable exception of the commitfilter, for technical reasons). Prior to that, the $GIT_COMMITenvironment variable will be set to contain the id of the commitbeing rewritten. Also, GIT_AUTHOR_NAME, GIT_AUTHOR_EMAIL,GIT_AUTHOR_DATE, GIT_COMMITTER_NAME, GIT_COMMITTER_EMAIL, andGIT_COMMITTER_DATE are taken from the current commit and exportedto the environment, in order to affect the author and committeridentities of the replacement commit created bygit-commit-tree(1) after the filters have run.If any evaluation of <command> returns a non-zero exit status,the whole operation will be aborted.A map function is available that takes an \"original sha1 id\"argument and outputs a \"rewritten sha1 id\" if the commit has beenalready rewritten, and \"original sha1 id\" otherwise; the mapfunction can return several ids on separate lines if your commitfilter emitted multiple commits.",
        "name": "git-filter-branch - Rewrite branches",
        "section": 1
    },
    {
        "command": "git-fmt-merge-msg",
        "description": "Takes the list of merged objects on stdin and produces a suitablecommit message to be used for the merge commit, usually to bepassed as the <merge-message> argument of git merge.This command is intended mostly for internal use by scriptsautomatically invoking git merge.",
        "name": "git-fmt-merge-msg - Produce a merge commit message",
        "section": 1
    },
    {
        "command": "git-for-each-ref",
        "description": "Iterate over all refs that match <pattern> and show themaccording to the given <format>, after sorting them according tothe given set of <key>. If <count> is given, stop after showingthat many refs. The interpolated values in <format> canoptionally be quoted as string literals in the specified hostlanguage allowing their direct evaluation in that language.",
        "name": "git-for-each-ref - Output information on each ref",
        "section": 1
    },
    {
        "command": "git-for-each-repo",
        "description": "Run a Git command on a list of repositories. The arguments afterthe known options or -- indicator are used as the arguments forthe Git subprocess.THIS COMMAND IS EXPERIMENTAL. THE BEHAVIOR MAY CHANGE.For example, we could run maintenance on each of a list ofrepositories stored in a maintenance.repo config variable usinggit for-each-repo --config=maintenance.repo maintenance runThis will run git -C <repo> maintenance run for each value <repo>in the multi-valued config variable maintenance.repo.",
        "name": "git-for-each-repo - Run a Git command on a list of repositories",
        "section": 1
    },
    {
        "command": "git-format-patch",
        "description": "Prepare each non-merge commit with its \"patch\" in one \"message\"per commit, formatted to resemble a UNIX mailbox. The output ofthis command is convenient for e-mail submission or for use withgit am.A \"message\" generated by the command consists of three parts:\u2022A brief metadata header that begins with From <commit> with afixed Mon Sep 17 00:00:00 2001 datestamp to help programslike \"file(1)\" to recognize that the file is an output fromthis command, fields that record the author identity, theauthor date, and the title of the change (taken from thefirst paragraph of the commit log message).\u2022The second and subsequent paragraphs of the commit logmessage.\u2022The \"patch\", which is the \"diff -p --stat\" output (seegit-diff(1)) between the commit and its parent.The log message and the patch is separated by a line with athree-dash line.There are two ways to specify which commits to operate on.1. A single commit, <since>, specifies that the commits leadingto the tip of the current branch that are not in the historythat leads to the <since> to be output.2. Generic <revision range> expression (see \"SPECIFYINGREVISIONS\" section in gitrevisions(7)) means the commits inthe specified range.The first rule takes precedence in the case of a single <commit>.To apply the second rule, i.e., format everything since thebeginning of history up until <commit>, use the --root option:git format-patch --root <commit>. If you want to format only<commit> itself, you can do this with git format-patch -1<commit>.By default, each output file is numbered sequentially from 1, anduses the first line of the commit message (massaged for pathnamesafety) as the filename. With the --numbered-files option, theoutput file names will only be numbers, without the first line ofthe commit appended. The names of the output files are printed tostandard output, unless the --stdout option is specified.If -o is specified, output files are created in <dir>. Otherwisethey are created in the current working directory. The defaultpath can be set with the format.outputDirectory configurationoption. The -o option takes precedence overformat.outputDirectory. To store patches in the current workingdirectory even when format.outputDirectory points elsewhere, use-o .. All directory components will be created.By default, the subject of a single patch is \"[PATCH] \" followedby the concatenation of lines from the commit message up to thefirst blank line (see the DISCUSSION section of git-commit(1)).When multiple patches are output, the subject prefix will insteadbe \"[PATCH n/m] \". To force 1/1 to be added for a single patch,use -n. To omit patch numbers from the subject, use -N.If given --thread, git-format-patch will generate In-Reply-To andReferences headers to make the second and subsequent patch mailsappear as replies to the first mail; this also generates aMessage-ID header to reference.",
        "name": "git-format-patch - Prepare patches for e-mail submission",
        "section": 1
    },
    {
        "command": "git-fsck",
        "description": "Verifies the connectivity and validity of the objects in thedatabase.",
        "name": "git-fsck - Verifies the connectivity and validity of the objectsin the database",
        "section": 1
    },
    {
        "command": "git-fsck-objects",
        "description": "This is a synonym for git-fsck(1). Please refer to thedocumentation of that command.",
        "name": "git-fsck-objects - Verifies the connectivity and validity of theobjects in the database",
        "section": 1
    },
    {
        "command": "git-fsmonitor--daemon",
        "description": "A daemon to watch the working directory for file and directorychanges using platform-specific filesystem notificationfacilities.This daemon communicates directly with commands like git statususing the simple IPC[1] interface instead of the slowergithooks(5) interface.This daemon is built into Git so that no third-party tools arerequired.",
        "name": "git-fsmonitor--daemon - A Built-in Filesystem Monitor",
        "section": 1
    },
    {
        "command": "git-gc",
        "description": "Runs a number of housekeeping tasks within the currentrepository, such as compressing file revisions (to reduce diskspace and increase performance), removing unreachable objectswhich may have been created from prior invocations of git add,packing refs, pruning reflog, rerere metadata or stale workingtrees. May also update ancillary indexes such as thecommit-graph.When common porcelain operations that create objects are run,they will check whether the repository has grown substantiallysince the last maintenance, and if so run git gc automatically.See gc.auto below for how to disable this behavior.Running git gc manually should only be needed when adding objectsto a repository without regularly running such porcelaincommands, to do a one-off repository optimization, or e.g. toclean up a suboptimal mass-import. See the \"PACKFILEOPTIMIZATION\" section in git-fast-import(1) for more details onthe import case.",
        "name": "git-gc - Cleanup unnecessary files and optimize the localrepository",
        "section": 1
    },
    {
        "command": "git-get-tar-commit-id",
        "description": "Read a tar archive created by git archive from the standard inputand extract the commit ID stored in it. It reads only the first1024 bytes of input, thus its runtime is not influenced by thesize of the tar archive very much.If no commit ID is found, git get-tar-commit-id quietly existswith a return code of 1. This can happen if the archive had notbeen created using git archive or if the first parameter of gitarchive had been a tree ID instead of a commit ID or tag.",
        "name": "git-get-tar-commit-id - Extract commit ID from an archive createdusing git-archive",
        "section": 1
    },
    {
        "command": "git-grep",
        "description": "Look for specified patterns in the tracked files in the worktree, blobs registered in the index file, or blobs in given treeobjects. Patterns are lists of one or more search expressionsseparated by newline characters. An empty string as searchexpression matches all lines.",
        "name": "git-grep - Print lines matching a pattern",
        "section": 1
    },
    {
        "command": "git-gui",
        "description": "A Tcl/Tk based graphical user interface to Git. git gui focuseson allowing users to make changes to their repository by makingnew commits, amending existing ones, creating branches,performing local merges, and fetching/pushing to remoterepositories.Unlike gitk, git gui focuses on commit generation and single fileannotation and does not show project history. It does howeversupply menu actions to start a gitk session from within git gui.git gui is known to work on all popular UNIX systems, Mac OS X,and Windows (under both Cygwin and MSYS). To the extent possibleOS specific user interface guidelines are followed, making gitgui a fairly native interface for users.",
        "name": "git-gui - A portable graphical interface to Git",
        "section": 1
    },
    {
        "command": "git-hash-object",
        "description": "Computes the object ID value for an object with specified typewith the contents of the named file (which can be outside of thework tree), and optionally writes the resulting object into theobject database. Reports its object ID to its standard output.When <type> is not specified, it defaults to \"blob\".",
        "name": "git-hash-object - Compute object ID and optionally creates a blobfrom a file",
        "section": 1
    },
    {
        "command": "git-help",
        "description": "With no options and no <command> or <doc> given, the synopsis ofthe git command and a list of the most commonly used Git commandsare printed on the standard output.If the option --all or -a is given, all available commands areprinted on the standard output.If the option --guides or -g is given, a list of the Git conceptguides is also printed on the standard output.If a command or other documentation is given, the relevant manualpage will be brought up. The man program is used by default forthis purpose, but this can be overridden by other options orconfiguration variables.If an alias is given, git shows the definition of the alias onstandard output. To get the manual page for the aliased command,use git <command> --help.Note that git --help ... is identical to git help ... because theformer is internally converted into the latter.To display the git(1) man page, use git help git.This page can be displayed with git help help or git help --help",
        "name": "git-help - Display help information about Git",
        "section": 1
    },
    {
        "command": "git-hook",
        "description": "A command interface to running git hooks (see githooks(5)), foruse by other scripted git commands.",
        "name": "git-hook - Run git hooks",
        "section": 1
    },
    {
        "command": "git-http-backend",
        "description": "A simple CGI program to serve the contents of a Git repository toGit clients accessing the repository over http:// and https://protocols. The program supports clients fetching using both thesmart HTTP protocol and the backwards-compatible dumb HTTPprotocol, as well as clients pushing using the smart HTTPprotocol. It also supports Git\u2019s more-efficient \"v2\" protocol ifproperly configured; see the discussion of GIT_PROTOCOL in theENVIRONMENT section below.It verifies that the directory has the magic file\"git-daemon-export-ok\", and it will refuse to export any Gitdirectory that hasn\u2019t explicitly been marked for export this way(unless the GIT_HTTP_EXPORT_ALL environmental variable is set).By default, only the upload-pack service is enabled, which servesgit fetch-pack and git ls-remote clients, which are invoked fromgit fetch, git pull, and git clone. If the client isauthenticated, the receive-pack service is enabled, which servesgit send-pack clients, which is invoked from git push.",
        "name": "git-http-backend - Server side implementation of Git over HTTP",
        "section": 1
    },
    {
        "command": "git-http-fetch",
        "description": "Downloads a remote Git repository via HTTP.This command always gets all objects. Historically, there werethree options -a, -c and -t for choosing which objects todownload. They are now silently ignored.",
        "name": "git-http-fetch - Download from a remote Git repository via HTTP",
        "section": 1
    },
    {
        "command": "git-http-push",
        "description": "Sends missing objects to remote repository, and updates theremote branch.NOTE: This command is temporarily disabled if your libcurl isolder than 7.16, as the combination has been reported not to workand sometimes corrupts repository.",
        "name": "git-http-push - Push objects over HTTP/DAV to another repository",
        "section": 1
    },
    {
        "command": "git-imap-send",
        "description": "This command uploads a mailbox generated with git format-patchinto an IMAP drafts folder. This allows patches to be sent asother email is when using mail clients that cannot read mailboxfiles directly. The command also works with any general mailboxin which emails have the fields \"From\", \"Date\", and \"Subject\" inthat order.Typical usage is something like:git format-patch --signoff --stdout --attach origin | gitimap-send",
        "name": "git-imap-send - Send a collection of patches from stdin to anIMAP folder",
        "section": 1
    },
    {
        "command": "git-index-pack",
        "description": "Reads a packed archive (.pack) from the specified file, andbuilds a pack index file (.idx) for it. Optionally writes areverse-index (.rev) for the specified pack. The packed archivetogether with the pack index can then be placed in theobjects/pack/ directory of a Git repository.",
        "name": "git-index-pack - Build pack index file for an existing packedarchive",
        "section": 1
    },
    {
        "command": "git-init",
        "description": "This command creates an empty Git repository - basically a .gitdirectory with subdirectories for objects, refs/heads, refs/tags,and template files. An initial branch without any commits will becreated (see the --initial-branch option below for its name).If the $GIT_DIR environment variable is set then it specifies apath to use instead of ./.git for the base of the repository.If the object storage directory is specified via the$GIT_OBJECT_DIRECTORY environment variable then the sha1directories are created underneath - otherwise the default$GIT_DIR/objects directory is used.Running git init in an existing repository is safe. It will notoverwrite things that are already there. The primary reason forrerunning git init is to pick up newly added templates (or tomove the repository to another place if --separate-git-dir isgiven).",
        "name": "git-init - Create an empty Git repository or reinitialize anexisting one",
        "section": 1
    },
    {
        "command": "git-init-db",
        "description": "This is a synonym for git-init(1). Please refer to thedocumentation of that command.",
        "name": "git-init-db - Creates an empty Git repository",
        "section": 1
    },
    {
        "command": "git-instaweb",
        "description": "A simple script to set up gitweb and a web server for browsingthe local repository.",
        "name": "git-instaweb - Instantly browse your working repository in gitweb",
        "section": 1
    },
    {
        "command": "git-interpret-trailers",
        "description": "Add or parse trailer lines that look similar to RFC 822 e-mailheaders, at the end of the otherwise free-form part of a commitmessage. For example, in the following commit messagesubjectLorem ipsum dolor sit amet, consectetur adipiscing elit.Signed-off-by: Alice <alice@example.com>Signed-off-by: Bob <bob@example.com>the last two lines starting with \"Signed-off-by\" are trailers.This command reads commit messages from either the <file>arguments or the standard input if no <file> is specified. If--parse is specified, the output consists of the parsed trailers.Otherwise, this command applies the arguments passed using the--trailer option, if any, to each input file. The result isemitted on the standard output.This command can also operate on the output ofgit-format-patch(1), which is more elaborate than a plain commitmessage. Namely, such output includes a commit message (asabove), a \"---\" divider line, and a patch part. For these inputs,the divider and patch parts are not modified by this command andare emitted as is on the output, unless --no-divider isspecified.Some configuration variables control the way the --trailerarguments are applied to each input and the way any existingtrailer in the input is changed. They also make it possible toautomatically add some trailers.By default, a <token>=<value> or <token>:<value> argument givenusing --trailer will be appended after the existing trailers onlyif the last trailer has a different (<token>, <value>) pair (orif there is no existing trailer). The <token> and <value> partswill be trimmed to remove starting and trailing whitespace, andthe resulting trimmed <token> and <value> will appear in theoutput like this:token: valueThis means that the trimmed <token> and <value> will be separatedby ': ' (one colon followed by one space). For convenience, the<token> can be a shortened string key (e.g., \"sign\") instead ofthe full string which should appear before the separator on theoutput (e.g., \"Signed-off-by\"). This can be configured using thetrailer.<token>.key configuration variable.By default the new trailer will appear at the end of all theexisting trailers. If there is no existing trailer, the newtrailer will appear at the end of the input. A blank line will beadded before the new trailer if there isn\u2019t one already.Existing trailers are extracted from the input by looking for agroup of one or more lines that (i) is all trailers, or (ii)contains at least one Git-generated or user-configured trailerand consists of at least 25% trailers. The group must be precededby one or more empty (or whitespace-only) lines. The group musteither be at the end of the input or be the last non-whitespacelines before a line that starts with --- (followed by a space orthe end of the line).When reading trailers, there can be no whitespace before orinside the <token>, but any number of regular space and tabcharacters are allowed between the <token> and the separator.There can be whitespaces before, inside or after the <value>. The<value> may be split over multiple lines with each subsequentline starting with at least one whitespace, like the \"folding\" inRFC 822. Example:token: This is a very long value, with spaces andnewlines in it.Note that trailers do not follow (nor are they intended tofollow) many of the rules for RFC 822 headers. For example theydo not follow the encoding rule.",
        "name": "git-interpret-trailers - Add or parse structured information incommit messages",
        "section": 1
    },
    {
        "command": "git-log",
        "description": "Shows the commit logs.List commits that are reachable by following the parent linksfrom the given commit(s), but exclude commits that are reachablefrom the one(s) given with a ^ in front of them. The output isgiven in reverse chronological order by default.You can think of this as a set operation. Commits reachable fromany of the commits given on the command line form a set, and thencommits reachable from any of the ones given with ^ in front aresubtracted from that set. The remaining commits are what comesout in the command\u2019s output. Various other options and pathsparameters can be used to further limit the result.Thus, the following command:$ git log foo bar ^bazmeans \"list all the commits which are reachable from foo or bar,but not from baz\".A special notation \"<commit1>..<commit2>\" can be used as ashort-hand for \"^<commit1> <commit2>\". For example, either of thefollowing may be used interchangeably:$ git log origin..HEAD$ git log HEAD ^originAnother special notation is \"<commit1>...<commit2>\" which isuseful for merges. The resulting set of commits is the symmetricdifference between the two operands. The following two commandsare equivalent:$ git log A B --not $(git merge-base --all A B)$ git log A...BThe command takes options applicable to the git-rev-list(1)command to control what is shown and how, and options applicableto the git-diff(1) command to control how the changes each commitintroduces are shown.",
        "name": "git-log - Show commit logs",
        "section": 1
    },
    {
        "command": "git-ls-files",
        "description": "This merges the file listing in the index with the actual workingdirectory list, and shows different combinations of the two.One or more of the options below may be used to determine thefiles shown, and each file may be printed multiple times if thereare multiple entries in the index or multiple statuses areapplicable for the relevant file selection options.",
        "name": "git-ls-files - Show information about files in the index and theworking tree",
        "section": 1
    },
    {
        "command": "git-ls-remote",
        "description": "Displays references available in a remote repository along withthe associated commit IDs.",
        "name": "git-ls-remote - List references in a remote repository",
        "section": 1
    },
    {
        "command": "git-ls-tree",
        "description": "Lists the contents of a given tree object, like what \"/bin/ls -a\"does in the current working directory. Note that:\u2022the behaviour is slightly different from that of \"/bin/ls\" inthat the <path> denotes just a list of patterns to match,e.g. so specifying directory name (without -r) will behavedifferently, and order of the arguments does not matter.\u2022the behaviour is similar to that of \"/bin/ls\" in that the<path> is taken as relative to the current working directory.E.g. when you are in a directory sub that has a directorydir, you can run git ls-tree -r HEAD dir to list the contentsof the tree (that is sub/dir in HEAD). You don\u2019t want to givea tree that is not at the root level (e.g.git ls-tree -rHEAD:sub dir) in this case, as that would result in askingfor sub/sub/dir in the HEAD commit. However, the currentworking directory can be ignored by passing --full-treeoption.",
        "name": "git-ls-tree - List the contents of a tree object",
        "section": 1
    },
    {
        "command": "git-mailinfo",
        "description": "Reads a single e-mail message from the standard input, and writesthe commit log message in <msg> file, and the patches in <patch>file. The author name, e-mail and e-mail subject are written outto the standard output to be used by git am to create a commit.It is usually not necessary to use this command directly. Seegit-am(1) instead.",
        "name": "git-mailinfo - Extracts patch and authorship from a single e-mailmessage",
        "section": 1
    },
    {
        "command": "git-mailsplit",
        "description": "Splits a mbox file or a Maildir into a list of files: \"0001\"\"0002\" .. in the specified directory so you can process themfurther from there.ImportantMaildir splitting relies upon filenames being sorted tooutput patches in the correct order.",
        "name": "git-mailsplit - Simple UNIX mbox splitter program",
        "section": 1
    },
    {
        "command": "git-maintenance",
        "description": "Run tasks to optimize Git repository data, speeding up other Gitcommands and reducing storage requirements for the repository.Git commands that add repository data, such as git add or gitfetch, are optimized for a responsive user experience. Thesecommands do not take time to optimize the Git data, since suchoptimizations scale with the full size of the repository whilethese user commands each perform a relatively small action.The git maintenance command provides flexibility for how tooptimize the Git repository.",
        "name": "git-maintenance - Run tasks to optimize Git repository data",
        "section": 1
    },
    {
        "command": "git-merge",
        "description": "Incorporates changes from the named commits (since the time theirhistories diverged from the current branch) into the currentbranch. This command is used by git pull to incorporate changesfrom another repository and can be used by hand to merge changesfrom one branch into another.Assume the following history exists and the current branch is\"master\":A---B---C topic/D---E---F---G masterThen \"git merge topic\" will replay the changes made on the topicbranch since it diverged from master (i.e., E) until its currentcommit (C) on top of master, and record the result in a newcommit along with the names of the two parent commits and a logmessage from the user describing the changes. Before theoperation, ORIG_HEAD is set to the tip of the current branch (C).A---B---C topic/\\D---E---F---G---H masterThe second syntax (\"git merge --abort\") can only be run after themerge has resulted in conflicts. git merge --abort will abort themerge process and try to reconstruct the pre-merge state.However, if there were uncommitted changes when the merge started(and especially if those changes were further modified after themerge was started), git merge --abort will in some cases beunable to reconstruct the original (pre-merge) changes.Therefore:Warning: Running git merge with non-trivial uncommitted changesis discouraged: while possible, it may leave you in a state thatis hard to back out of in the case of a conflict.The third syntax (\"git merge --continue\") can only be run afterthe merge has resulted in conflicts.",
        "name": "git-merge - Join two or more development histories together",
        "section": 1
    },
    {
        "command": "git-merge-base",
        "description": "git merge-base finds best common ancestor(s) between two commitsto use in a three-way merge. One common ancestor is better thananother common ancestor if the latter is an ancestor of theformer. A common ancestor that does not have any better commonancestor is a best common ancestor, i.e. a merge base. Note thatthere can be more than one merge base for a pair of commits.",
        "name": "git-merge-base - Find as good common ancestors as possible for amerge",
        "section": 1
    },
    {
        "command": "git-merge-file",
        "description": "git merge-file incorporates all changes that lead from the<base-file> to <other-file> into <current-file>. The resultordinarily goes into <current-file>. git merge-file is useful forcombining separate changes to an original. Suppose <base-file> isthe original, and both <current-file> and <other-file> aremodifications of <base-file>, then git merge-file combines bothchanges.A conflict occurs if both <current-file> and <other-file> havechanges in a common segment of lines. If a conflict is found, gitmerge-file normally outputs a warning and brackets the conflictwith lines containing <<<<<<< and >>>>>>> markers. A typicalconflict will look like this:<<<<<<< Alines in file A=======lines in file B>>>>>>> BIf there are conflicts, the user should edit the result anddelete one of the alternatives. When --ours, --theirs, or --unionoption is in effect, however, these conflicts are resolvedfavouring lines from <current-file>, lines from <other-file>, orlines from both respectively. The length of the conflict markerscan be given with the --marker-size option.The exit value of this program is negative on error, and thenumber of conflicts otherwise (truncated to 127 if there are morethan that many conflicts). If the merge was clean, the exit valueis 0.git merge-file is designed to be a minimal clone of RCS merge;that is, it implements all of RCS merge's functionality which isneeded by git(1).",
        "name": "git-merge-file - Run a three-way file merge",
        "section": 1
    },
    {
        "command": "git-merge-index",
        "description": "This looks up the <file>(s) in the index and, if there are anymerge entries, passes the SHA-1 hash for those files as arguments1, 2, 3 (empty argument if no file), and <file> as argument 4.File modes for the three files are passed as arguments 5, 6 and7.",
        "name": "git-merge-index - Run a merge for files needing merging",
        "section": 1
    },
    {
        "command": "git-merge-one-file",
        "description": "This is the standard helper program to use with git merge-indexto resolve a merge after the trivial merge done with gitread-tree -m.",
        "name": "git-merge-one-file - The standard helper program to use withgit-merge-index",
        "section": 1
    },
    {
        "command": "git-merge-tree",
        "description": "This command has a modern --write-tree mode and a deprecated--trivial-merge mode. With the exception of the DEPRECATEDDESCRIPTION section at the end, the rest of this documentationdescribes modern --write-tree mode.Performs a merge, but does not make any new commits and does notread from or write to either the working tree or index.The performed merge will use the same feature as the \"real\"git-merge(1), including:\u2022three way content merges of individual files\u2022rename detection\u2022proper directory/file conflict handling\u2022recursive ancestor consolidation (i.e. when there is morethan one merge base, creating a virtual merge base by mergingthe merge bases)\u2022etc.After the merge completes, a new toplevel tree object is created.See OUTPUT below for details.",
        "name": "git-merge-tree - Perform merge without touching index or workingtree",
        "section": 1
    },
    {
        "command": "git-mergetool",
        "description": "Use git mergetool to run one of several merge utilities toresolve merge conflicts. It is typically run after git merge.If one or more <file> parameters are given, the merge toolprogram will be run to resolve differences on each file (skippingthose without conflicts). Specifying a directory will include allunresolved files in that path. If no <file> names are specified,git mergetool will run the merge tool program on every file withmerge conflicts.",
        "name": "git-mergetool - Run merge conflict resolution tools to resolvemerge conflicts",
        "section": 1
    },
    {
        "command": "git-mergetool--lib",
        "description": "This is not a command the end user would want to run. Ever. Thisdocumentation is meant for people who are studying thePorcelain-ish scripts and/or are writing new ones.The git-mergetool--lib scriptlet is designed to be sourced (using.) by other shell scripts to set up functions for working withGit merge tools.Before sourcing git-mergetool--lib, your script must setTOOL_MODE to define the operation mode for the functions listedbelow. diff and merge are valid values.",
        "name": "git-mergetool--lib - Common Git merge tool shell scriptlets",
        "section": 1
    },
    {
        "command": "git-mktag",
        "description": "Reads a tag contents on standard input and creates a tag object.The output is the new tag\u2019s <object> identifier.This command is mostly equivalent to git-hash-object(1) invokedwith -t tag -w --stdin. I.e. both of these will create and writea tag found in my-tag:git mktag <my-taggit hash-object -t tag -w --stdin <my-tagThe difference is that mktag will die before writing the tag ifthe tag doesn\u2019t pass a git-fsck(1) check.The \"fsck\" check done mktag is stricter than what git-fsck(1)would run by default in that all fsck.<msg-id> messages arepromoted from warnings to errors (so e.g. a missing \"tagger\" lineis an error).Extra headers in the object are also an error under mktag, butignored by git-fsck(1). This extra check can be turned off bysetting the appropriate fsck.<msg-id> variable:git -c fsck.extraHeaderEntry=ignore mktag <my-tag-with-headers",
        "name": "git-mktag - Creates a tag object with extra validation",
        "section": 1
    },
    {
        "command": "git-mktree",
        "description": "Reads standard input in non-recursive ls-tree output format, andcreates a tree object. The order of the tree entries isnormalized by mktree so pre-sorting the input is not required.The object name of the tree object built is written to thestandard output.",
        "name": "git-mktree - Build a tree-object from ls-tree formatted text",
        "section": 1
    },
    {
        "command": "git-multi-pack-index",
        "description": "Write or verify a multi-pack-index (MIDX) file.",
        "name": "git-multi-pack-index - Write and verify multi-pack-indexes",
        "section": 1
    },
    {
        "command": "git-mv",
        "description": "Move or rename a file, directory or symlink.git mv [-v] [-f] [-n] [-k] <source> <destination>git mv [-v] [-f] [-n] [-k] <source> ... <destination directory>In the first form, it renames <source>, which must exist and beeither a file, symlink or directory, to <destination>. In thesecond form, the last argument has to be an existing directory;the given sources will be moved into this directory.The index is updated after successful completion, but the changemust still be committed.",
        "name": "git-mv - Move or rename a file, a directory, or a symlink",
        "section": 1
    },
    {
        "command": "git-name-rev",
        "description": "Finds symbolic names suitable for human digestion for revisionsgiven in any format parsable by git rev-parse.",
        "name": "git-name-rev - Find symbolic names for given revs",
        "section": 1
    },
    {
        "command": "git-notes",
        "description": "Adds, removes, or reads notes attached to objects, withouttouching the objects themselves.By default, notes are saved to and read from refs/notes/commits,but this default can be overridden. See the OPTIONS,CONFIGURATION, and ENVIRONMENT sections below. If this ref doesnot exist, it will be quietly created when it is first needed tostore a note.A typical use of notes is to supplement a commit message withoutchanging the commit itself. Notes can be shown by git log alongwith the original commit message. To distinguish these notes fromthe message stored in the commit object, the notes are indentedlike the message, after an unindented line saying \"Notes(<refname>):\" (or \"Notes:\" for refs/notes/commits).Notes can also be added to patches prepared with git format-patchby using the --notes option. Such notes are added as a patchcommentary after a three dash separator line.To change which notes are shown by git log, see the\"notes.displayRef\" discussion in the section called\u201cCONFIGURATION\u201d.See the \"notes.rewrite.<command>\" configuration for a way tocarry notes across commands that rewrite commits.",
        "name": "git-notes - Add or inspect object notes",
        "section": 1
    },
    {
        "command": "git-p4",
        "description": "This command provides a way to interact with p4 repositoriesusing Git.Create a new Git repository from an existing p4 repository usinggit p4 clone, giving it one or more p4 depot paths. Incorporatenew commits from p4 changes with git p4 sync. The sync command isalso used to include new branches from other p4 depot paths.Submit Git changes back to p4 using git p4 submit. The commandgit p4 rebase does a sync plus rebases the current branch ontothe updated p4 remote branch.",
        "name": "git-p4 - Import from and submit to Perforce repositories",
        "section": 1
    },
    {
        "command": "git-pack-objects",
        "description": "Reads list of objects from the standard input, and writes eitherone or more packed archives with the specified base-name to disk,or a packed archive to the standard output.A packed archive is an efficient way to transfer a set of objectsbetween two repositories as well as an access efficient archivalformat. In a packed archive, an object is either stored as acompressed whole or as a difference from some other object. Thelatter is often called a delta.The packed archive format (.pack) is designed to beself-contained so that it can be unpacked without any furtherinformation. Therefore, each object that a delta depends uponmust be present within the pack.A pack index file (.idx) is generated for fast, random access tothe objects in the pack. Placing both the index file (.idx) andthe packed archive (.pack) in the pack/ subdirectory of$GIT_OBJECT_DIRECTORY (or any of the directories on$GIT_ALTERNATE_OBJECT_DIRECTORIES) enables Git to read from thepack archive.The git unpack-objects command can read the packed archive andexpand the objects contained in the pack into \"one-fileone-object\" format; this is typically done by the smart-pullcommands when a pack is created on-the-fly for efficient networktransport by their peers.",
        "name": "git-pack-objects - Create a packed archive of objects",
        "section": 1
    },
    {
        "command": "git-pack-redundant",
        "description": "This program computes which packs in your repository areredundant. The output is suitable for piping to xargs rm if youare in the root of the repository.git pack-redundant accepts a list of objects on standard input.Any objects given will be ignored when checking which packs arerequired. This makes the following command useful when wanting toremove packs which contain unreachable objects.git fsck --full --unreachable | cut -d ' ' -f3 | \\ gitpack-redundant --all | xargs rm",
        "name": "git-pack-redundant - Find redundant pack files",
        "section": 1
    },
    {
        "command": "git-pack-refs",
        "description": "Traditionally, tips of branches and tags (collectively known asrefs) were stored one file per ref in a (sub)directory under$GIT_DIR/refs directory. While many branch tips tend to beupdated often, most tags and some branch tips are never updated.When a repository has hundreds or thousands of tags, thisone-file-per-ref format both wastes storage and hurtsperformance.This command is used to solve the storage and performance problemby storing the refs in a single file, $GIT_DIR/packed-refs. Whena ref is missing from the traditional $GIT_DIR/refs directoryhierarchy, it is looked up in this file and used if found.Subsequent updates to branches always create new files under$GIT_DIR/refs directory hierarchy.A recommended practice to deal with a repository with too manyrefs is to pack its refs with --all once, and occasionally rungit pack-refs. Tags are by definition stationary and are notexpected to change. Branch heads will be packed with the initialpack-refs --all, but only the currently active branch heads willbecome unpacked, and the next pack-refs (without --all) willleave them unpacked.",
        "name": "git-pack-refs - Pack heads and tags for efficient repositoryaccess",
        "section": 1
    },
    {
        "command": "git-patch-id",
        "description": "Read a patch from the standard input and compute the patch ID forit.A \"patch ID\" is nothing but a sum of SHA-1 of the file diffsassociated with a patch, with line numbers ignored. As such, it\u2019s\"reasonably stable\", but at the same time also reasonably unique,i.e., two patches that have the same \"patch ID\" are almostguaranteed to be the same thing.The main usecase for this command is to look for likely duplicatecommits.When dealing with git diff-tree output, it takes advantage of thefact that the patch is prefixed with the object name of thecommit, and outputs two 40-byte hexadecimal strings. The firststring is the patch ID, and the second string is the commit ID.This can be used to make a mapping from patch ID to commit ID.",
        "name": "git-patch-id - Compute unique ID for a patch",
        "section": 1
    },
    {
        "command": "git-prune",
        "description": "NoteIn most cases, users should run git gc, which calls gitprune. See the section \"NOTES\", below.This runs git fsck --unreachable using all the refs available inrefs/, optionally with additional set of objects specified on thecommand line, and prunes all unpacked objects unreachable fromany of these head objects from the object database. In addition,it prunes the unpacked objects that are also found in packs byrunning git prune-packed. It also removes entries from.git/shallow that are not reachable by any ref.Note that unreachable, packed objects will remain. If this is notdesired, see git-repack(1).",
        "name": "git-prune - Prune all unreachable objects from the objectdatabase",
        "section": 1
    },
    {
        "command": "git-prune-packed",
        "description": "This program searches the $GIT_OBJECT_DIRECTORY for all objectsthat currently exist in a pack file as well as the independentobject directories.All such extra objects are removed.A pack is a collection of objects, individually compressed, withdelta compression applied, stored in a single file, with anassociated index file.Packs are used to reduce the load on mirror systems, backupengines, disk storage, etc.",
        "name": "git-prune-packed - Remove extra objects that are already in packfiles",
        "section": 1
    },
    {
        "command": "git-pull",
        "description": "Incorporates changes from a remote repository into the currentbranch. If the current branch is behind the remote, then bydefault it will fast-forward the current branch to match theremote. If the current branch and the remote have diverged, theuser needs to specify how to reconcile the divergent brancheswith --rebase or --no-rebase (or the corresponding configurationoption in pull.rebase).More precisely, git pull runs git fetch with the given parametersand then depending on configuration options or command lineflags, will call either git rebase or git merge to reconcilediverging branches.<repository> should be the name of a remote repository as passedto git-fetch(1). <refspec> can name an arbitrary remote ref (forexample, the name of a tag) or even a collection of refs withcorresponding remote-tracking branches (e.g.,refs/heads/*:refs/remotes/origin/*), but usually it is the nameof a branch in the remote repository.Default values for <repository> and <branch> are read from the\"remote\" and \"merge\" configuration for the current branch as setby git-branch(1) --track.Assume the following history exists and the current branch is\"master\":A---B---C master on origin/D---E---F---G master^origin/master in your repositoryThen \"git pull\" will fetch and replay the changes from the remotemaster branch since it diverged from the local master (i.e., E)until its current commit (C) on top of master and record theresult in a new commit along with the names of the two parentcommits and a log message from the user describing the changes.A---B---C origin/master/\\D---E---F---G---H masterSee git-merge(1) for details, including how conflicts arepresented and handled.In Git 1.7.0 or later, to cancel a conflicting merge, use gitreset --merge. Warning: In older versions of Git, running gitpull with uncommitted changes is discouraged: while possible, itleaves you in a state that may be hard to back out of in the caseof a conflict.If any of the remote changes overlap with local uncommittedchanges, the merge will be automatically canceled and the worktree untouched. It is generally best to get any local changes inworking order before pulling or stash them away withgit-stash(1).",
        "name": "git-pull - Fetch from and integrate with another repository or alocal branch",
        "section": 1
    },
    {
        "command": "git-push",
        "description": "Updates remote refs using local refs, while sending objectsnecessary to complete the given refs.You can make interesting things happen to a repository every timeyou push into it, by setting up hooks there. See documentationfor git-receive-pack(1).When the command line does not specify where to push with the<repository> argument, branch.*.remote configuration for thecurrent branch is consulted to determine where to push. If theconfiguration is missing, it defaults to origin.When the command line does not specify what to push with<refspec>... arguments or --all, --mirror, --tags options, thecommand finds the default <refspec> by consulting remote.*.pushconfiguration, and if it is not found, honors push.defaultconfiguration to decide what to push (See git-config(1) for themeaning of push.default).When neither the command-line nor the configuration specify whatto push, the default behavior is used, which corresponds to thesimple value for push.default: the current branch is pushed tothe corresponding upstream branch, but as a safety measure, thepush is aborted if the upstream branch does not have the samename as the local one.",
        "name": "git-push - Update remote refs along with associated objects",
        "section": 1
    },
    {
        "command": "git-quiltimport",
        "description": "Applies a quilt patchset onto the current Git branch, preservingthe patch boundaries, patch order, and patch descriptions presentin the quilt patchset.For each patch the code attempts to extract the author from thepatch description. If that fails it falls back to the authorspecified with --author. If the --author flag was not given thepatch description is displayed and the user is asked tointeractively enter the author of the patch.If a subject is not found in the patch description the patch nameis preserved as the 1 line subject in the Git description.",
        "name": "git-quiltimport - Applies a quilt patchset onto the currentbranch",
        "section": 1
    },
    {
        "command": "git-range-diff",
        "description": "This command shows the differences between two versions of apatch series, or more generally, two commit ranges (ignoringmerge commits).In the presence of <path> arguments, these commit ranges arelimited accordingly.To that end, it first finds pairs of commits from both commitranges that correspond with each other. Two commits are said tocorrespond when the diff between their patches (i.e. the authorinformation, the commit message and the commit diff) isreasonably small compared to the patches' size. See ``Algorithm``below for details.Finally, the list of matching commits is shown in the order ofthe second commit range, with unmatched commits being insertedjust after all of their ancestors have been shown.There are three ways to specify the commit ranges:\u2022<range1> <range2>: Either commit range can be of the form<base>..<rev>, <rev>^!or <rev>^-<n>. See SPECIFYING RANGESin gitrevisions(7) for more details.\u2022<rev1>...<rev2>. This is equivalent to <rev2>..<rev1><rev1>..<rev2>.\u2022<base> <rev1> <rev2>: This is equivalent to <base>..<rev1><base>..<rev2>.",
        "name": "git-range-diff - Compare two commit ranges (e.g. two versions ofa branch)",
        "section": 1
    },
    {
        "command": "git-read-tree",
        "description": "Reads the tree information given by <tree-ish> into the index,but does not actually update any of the files it \"caches\". (see:git-checkout-index(1))Optionally, it can merge a tree into the index, perform afast-forward (i.e. 2-way) merge, or a 3-way merge, with the -mflag. When used with -m, the -u flag causes it to also update thefiles in the work tree with the result of the merge.Trivial merges are done by git read-tree itself. Only conflictingpaths will be in unmerged state when git read-tree returns.",
        "name": "git-read-tree - Reads tree information into the index",
        "section": 1
    },
    {
        "command": "git-rebase",
        "description": "If <branch> is specified, git rebase will perform an automaticgit switch <branch> before doing anything else. Otherwise itremains on the current branch.If <upstream> is not specified, the upstream configured inbranch.<name>.remote and branch.<name>.merge options will be used(see git-config(1) for details) and the --fork-point option isassumed. If you are currently not on any branch or if the currentbranch does not have a configured upstream, the rebase willabort.All changes made by commits in the current branch but that arenot in <upstream> are saved to a temporary area. This is the sameset of commits that would be shown by git log <upstream>..HEAD;or by git log 'fork_point'..HEAD, if --fork-point is active (seethe description on --fork-point below); or by git log HEAD, ifthe --root option is specified.The current branch is reset to <upstream> or <newbase> if the--onto option was supplied. This has the exact same effect as gitreset --hard <upstream> (or <newbase>). ORIG_HEAD is set to pointat the tip of the branch before the reset.NoteORIG_HEAD is not guaranteed to still point to the previousbranch tip at the end of the rebase if other commands thatwrite that pseudo-ref (e.g. git reset) are used during therebase. The previous branch tip, however, is accessible usingthe reflog of the current branch (i.e. @{1}, seegitrevisions(7)).The commits that were previously saved into the temporary areaare then reapplied to the current branch, one by one, in order.Note that any commits in HEAD which introduce the same textualchanges as a commit in HEAD..<upstream> are omitted (i.e., apatch already accepted upstream with a different commit messageor timestamp will be skipped).It is possible that a merge failure will prevent this processfrom being completely automatic. You will have to resolve anysuch merge failure and run git rebase --continue. Another optionis to bypass the commit that caused the merge failure with gitrebase --skip. To check out the original <branch> and remove the.git/rebase-apply working files, use the command git rebase--abort instead.Assume the following history exists and the current branch is\"topic\":A---B---C topic/D---E---F---G masterFrom this point, the result of either of the following commands:git rebase mastergit rebase master topicwould be:A'--B'--C' topic/D---E---F---G masterNOTE: The latter form is just a short-hand of git checkout topicfollowed by git rebase master. When rebase exits topic willremain the checked-out branch.If the upstream branch already contains a change you have made(e.g., because you mailed a patch which was applied upstream),then that commit will be skipped and warnings will be issued (ifthe merge backend is used). For example, running git rebasemaster on the following history (in which A' and A introduce thesame set of changes, but have different committer information):A---B---C topic/D---E---A'---F masterwill result in:B'---C' topic/D---E---A'---F masterHere is how you would transplant a topic branch based on onebranch to another, to pretend that you forked the topic branchfrom the latter branch, using rebase --onto.First let\u2019s assume your topic is based on branch next. Forexample, a feature developed in topic depends on somefunctionality which is found in next.o---o---o---o---omaster\\o---o---o---o---onext\\o---o---otopicWe want to make topic forked from branch master; for example,because the functionality on which topic depends was merged intothe more stable master branch. We want our tree to look likethis:o---o---o---o---omaster|\\|o'--o'--o'topic\\o---o---o---o---onextWe can get this using the following command:git rebase --onto master next topicAnother example of --onto option is to rebase part of a branch.If we have the following situation:H---I---J topicB/E---F---GtopicA/A---B---C---Dmasterthen the commandgit rebase --onto master topicA topicBwould result in:H'--I'--J'topicB/| E---F---GtopicA|/A---B---C---DmasterThis is useful when topicB does not depend on topicA.A range of commits could also be removed with rebase. If we havethe following situation:E---F---G---H---I---JtopicAthen the commandgit rebase --onto topicA~5 topicA~3 topicAwould result in the removal of commits F and G:E---H'---I'---J'topicAThis is useful if F and G were flawed in some way, or should notbe part of topicA. Note that the argument to --onto and the<upstream> parameter can be any valid commit-ish.In case of conflict, git rebase will stop at the firstproblematic commit and leave conflict markers in the tree. Youcan use git diff to locate the markers (<<<<<<) and make edits toresolve the conflict. For each file you edit, you need to tellGit that the conflict has been resolved, typically this would bedone withgit add <filename>After resolving the conflict manually and updating the index withthe desired resolution, you can continue the rebasing processwithgit rebase --continueAlternatively, you can undo the git rebase withgit rebase --abort",
        "name": "git-rebase - Reapply commits on top of another base tip",
        "section": 1
    },
    {
        "command": "git-receive-pack",
        "description": "Invoked by git send-pack and updates the repository with theinformation fed from the remote end.This command is usually not invoked directly by the end user. TheUI for the protocol is on the git send-pack side, and the programpair is meant to be used to push updates to remote repository.For pull operations, see git-fetch-pack(1).The command allows for creation and fast-forwarding of sha1 refs(heads/tags) on the remote end (strictly speaking, it is thelocal end git-receive-pack runs, but to the user who is sittingat the send-pack end, it is updating the remote. Confused?)There are other real-world examples of using update andpost-update hooks found in the Documentation/howto directory.git-receive-pack honours the receive.denyNonFastForwards configoption, which tells it if updates to a ref should be denied ifthey are not fast-forwards.A number of other receive.* config options are available to tweakits behavior, see git-config(1).",
        "name": "git-receive-pack - Receive what is pushed into the repository",
        "section": 1
    },
    {
        "command": "git-reflog",
        "description": "This command manages the information recorded in the reflogs.Reference logs, or \"reflogs\", record when the tips of branchesand other references were updated in the local repository.Reflogs are useful in various Git commands, to specify the oldvalue of a reference. For example, HEAD@{2} means \"where HEADused to be two moves ago\", master@{one.week.ago} means \"wheremaster used to point to one week ago in this local repository\",and so on. See gitrevisions(7) for more details.The command takes various subcommands, and different optionsdepending on the subcommand:The \"show\" subcommand (which is also the default, in the absenceof any subcommands) shows the log of the reference provided inthe command-line (or HEAD, by default). The reflog covers allrecent actions, and in addition the HEAD reflog records branchswitching. git reflog show is an alias for git log -g--abbrev-commit --pretty=oneline; see git-log(1) for moreinformation.The \"expire\" subcommand prunes older reflog entries. Entriesolder than expire time, or entries older than expire-unreachabletime and not reachable from the current tip, are removed from thereflog. This is typically not used directly by end users \u2014instead, see git-gc(1).The \"delete\" subcommand deletes single entries from the reflog.Its argument must be an exact entry (e.g. \"git reflog deletemaster@{2}\"). This subcommand is also typically not used directlyby end users.The \"exists\" subcommand checks whether a ref has a reflog. Itexits with zero status if the reflog exists, and non-zero statusif it does not.",
        "name": "git-reflog - Manage reflog information",
        "section": 1
    },
    {
        "command": "git-remote",
        "description": "Manage the set of repositories (\"remotes\") whose branches youtrack.",
        "name": "git-remote - Manage set of tracked repositories",
        "section": 1
    },
    {
        "command": "git-remote-ext",
        "description": "This remote helper uses the specified <command> to connect to aremote Git server.Data written to stdin of the specified <command> is assumed to besent to a git:// server, git-upload-pack, git-receive-pack orgit-upload-archive (depending on situation), and data read fromstdout of <command> is assumed to be received from the sameservice.Command and arguments are separated by an unescaped space.The following sequences have a special meaning:'% 'Literal space in command or argument.%%Literal percent sign.%sReplaced with name (receive-pack, upload-pack, orupload-archive) of the service Git wants to invoke.%SReplaced with long name (git-receive-pack, git-upload-pack,or git-upload-archive) of the service Git wants to invoke.%G (must be the first characters in an argument)This argument will not be passed to <command>. Instead, itwill cause the helper to start by sending git:// servicerequests to the remote side with the service field set to anappropriate value and the repository field set to rest of theargument. Default is not to send such a request.This is useful if remote side is git:// server accessed oversome tunnel.%V (must be first characters in argument)This argument will not be passed to <command>. Instead itsets the vhost field in the git:// service request (to restof the argument). Default is not to send vhost in suchrequest (if sent).",
        "name": "git-remote-ext - Bridge smart transport to external command.",
        "section": 1
    },
    {
        "command": "git-remote-fd",
        "description": "This helper uses specified file descriptors to connect to aremote Git server. This is not meant for end users but forprograms and scripts calling git fetch, push or archive.If only <infd> is given, it is assumed to be a bidirectionalsocket connected to remote Git server (git-upload-pack,git-receive-pack or git-upload-archive). If both <infd> and<outfd> are given, they are assumed to be pipes connected to aremote Git server (<infd> being the inbound pipe and <outfd>being the outbound pipe.It is assumed that any handshaking procedures have already beencompleted (such as sending service request for git://) beforethis helper is started.<anything> can be any string. It is ignored. It is meant forproviding information to user in the URL in case that URL isdisplayed in some context.",
        "name": "git-remote-fd - Reflect smart transport stream back to caller",
        "section": 1
    },
    {
        "command": "git-repack",
        "description": "This command is used to combine all objects that do not currentlyreside in a \"pack\", into a pack. It can also be used tore-organize existing packs into a single, more efficient pack.A pack is a collection of objects, individually compressed, withdelta compression applied, stored in a single file, with anassociated index file.Packs are used to reduce the load on mirror systems, backupengines, disk storage, etc.",
        "name": "git-repack - Pack unpacked objects in a repository",
        "section": 1
    },
    {
        "command": "git-replace",
        "description": "Adds a replace reference in refs/replace/ namespace.The name of the replace reference is the SHA-1 of the object thatis replaced. The content of the replace reference is the SHA-1 ofthe replacement object.The replaced object and the replacement object must be of thesame type. This restriction can be bypassed using -f.Unless -f is given, the replace reference must not yet exist.There is no other restriction on the replaced and replacementobjects. Merge commits can be replaced by non-merge commits andvice versa.Replacement references will be used by default by all Gitcommands except those doing reachability traversal (prune, packtransfer and fsck).It is possible to disable use of replacement references for anycommand using the --no-replace-objects option just after git.For example if commit foo has been replaced by commit bar:$ git --no-replace-objects cat-file commit fooshows information about commit foo, while:$ git cat-file commit fooshows information about commit bar.The GIT_NO_REPLACE_OBJECTS environment variable can be set toachieve the same effect as the --no-replace-objects option.",
        "name": "git-replace - Create, list, delete refs to replace objects",
        "section": 1
    },
    {
        "command": "git-request-pull",
        "description": "Generate a request asking your upstream project to pull changesinto their tree. The request, printed to the standard output,begins with the branch description, summarizes the changes andindicates from where they can be pulled.The upstream project is expected to have the commit named by<start> and the output asks it to integrate the changes you madesince that commit, up to the commit named by <end>, by visitingthe repository named by <URL>.",
        "name": "git-request-pull - Generates a summary of pending changes",
        "section": 1
    },
    {
        "command": "git-rerere",
        "description": "In a workflow employing relatively long lived topic branches, thedeveloper sometimes needs to resolve the same conflicts over andover again until the topic branches are done (either merged tothe \"release\" branch, or sent out and accepted upstream).This command assists the developer in this process by recordingconflicted automerge results and corresponding hand resolveresults on the initial manual merge, and applying previouslyrecorded hand resolutions to their corresponding automergeresults.NoteYou need to set the configuration variable rerere.enabled inorder to enable this command.",
        "name": "git-rerere - Reuse recorded resolution of conflicted merges",
        "section": 1
    },
    {
        "command": "git-reset",
        "description": "In the first three forms, copy entries from <tree-ish> to theindex. In the last form, set the current branch head (HEAD) to<commit>, optionally modifying index and working tree to match.The <tree-ish>/<commit> defaults to HEAD in all forms.git reset [-q] [<tree-ish>] [--] <pathspec>..., git reset [-q][--pathspec-from-file=<file> [--pathspec-file-nul]] [<tree-ish>]These forms reset the index entries for all paths that matchthe <pathspec> to their state at <tree-ish>. (It does notaffect the working tree or the current branch.)This means that git reset <pathspec> is the opposite of gitadd <pathspec>. This command is equivalent to git restore[--source=<tree-ish>] --staged <pathspec>....After running git reset <pathspec> to update the index entry,you can use git-restore(1) to check the contents out of theindex to the working tree. Alternatively, usinggit-restore(1) and specifying a commit with --source, you cancopy the contents of a path out of a commit to the index andto the working tree in one go.git reset (--patch | -p) [<tree-ish>] [--] [<pathspec>...]Interactively select hunks in the difference between theindex and <tree-ish> (defaults to HEAD). The chosen hunks areapplied in reverse to the index.This means that git reset -p is the opposite of git add -p,i.e. you can use it to selectively reset hunks. See the\u201cInteractive Mode\u201d section of git-add(1) to learn how tooperate the --patch mode.git reset [<mode>] [<commit>]This form resets the current branch head to <commit> andpossibly updates the index (resetting it to the tree of<commit>) and the working tree depending on <mode>. Beforethe operation, ORIG_HEAD is set to the tip of the currentbranch. If <mode> is omitted, defaults to --mixed. The <mode>must be one of the following:--softDoes not touch the index file or the working tree at all(but resets the head to <commit>, just like all modesdo). This leaves all your changed files \"Changes to becommitted\", as git status would put it.--mixedResets the index but not the working tree (i.e., thechanged files are preserved but not marked for commit)and reports what has not been updated. This is thedefault action.If -N is specified, removed paths are marked asintent-to-add (see git-add(1)).--hardResets the index and working tree. Any changes to trackedfiles in the working tree since <commit> are discarded.Any untracked files or directories in the way of writingany tracked files are simply deleted.--mergeResets the index and updates the files in the workingtree that are different between <commit> and HEAD, butkeeps those which are different between the index andworking tree (i.e. which have changes which have not beenadded). If a file that is different between <commit> andthe index has unstaged changes, reset is aborted.In other words, --merge does something like a gitread-tree -u -m <commit>, but carries forward unmergedindex entries.--keepResets index entries and updates files in the workingtree that are different between <commit> and HEAD. If afile that is different between <commit> and HEAD haslocal changes, reset is aborted.--[no-]recurse-submodulesWhen the working tree is updated, using--recurse-submodules will also recursively reset theworking tree of all active submodules according to thecommit recorded in the superproject, also setting thesubmodules' HEAD to be detached at that commit.See \"Reset, restore and revert\" in git(1) for the differencesbetween the three commands.",
        "name": "git-reset - Reset current HEAD to the specified state",
        "section": 1
    },
    {
        "command": "git-restore",
        "description": "Restore specified paths in the working tree with some contentsfrom a restore source. If a path is tracked but does not exist inthe restore source, it will be removed to match the source.The command can also be used to restore the content in the indexwith --staged, or restore both the working tree and the indexwith --staged --worktree.By default, if --staged is given, the contents are restored fromHEAD, otherwise from the index. Use --source to restore from adifferent commit.See \"Reset, restore and revert\" in git(1) for the differencesbetween the three commands.THIS COMMAND IS EXPERIMENTAL. THE BEHAVIOR MAY CHANGE.",
        "name": "git-restore - Restore working tree files",
        "section": 1
    },
    {
        "command": "git-rev-list",
        "description": "List commits that are reachable by following the parent linksfrom the given commit(s), but exclude commits that are reachablefrom the one(s) given with a ^ in front of them. The output isgiven in reverse chronological order by default.You can think of this as a set operation. Commits reachable fromany of the commits given on the command line form a set, and thencommits reachable from any of the ones given with ^ in front aresubtracted from that set. The remaining commits are what comesout in the command\u2019s output. Various other options and pathsparameters can be used to further limit the result.Thus, the following command:$ git rev-list foo bar ^bazmeans \"list all the commits which are reachable from foo or bar,but not from baz\".A special notation \"<commit1>..<commit2>\" can be used as ashort-hand for \"^<commit1> <commit2>\". For example, either of thefollowing may be used interchangeably:$ git rev-list origin..HEAD$ git rev-list HEAD ^originAnother special notation is \"<commit1>...<commit2>\" which isuseful for merges. The resulting set of commits is the symmetricdifference between the two operands. The following two commandsare equivalent:$ git rev-list A B --not $(git merge-base --all A B)$ git rev-list A...Brev-list is a very essential Git command, since it provides theability to build and traverse commit ancestry graphs. For thisreason, it has a lot of different options that enables it to beused by commands as different as git bisect and git repack.",
        "name": "git-rev-list - Lists commit objects in reverse chronologicalorder",
        "section": 1
    },
    {
        "command": "git-rev-parse",
        "description": "Many Git porcelainish commands take mixture of flags (i.e.parameters that begin with a dash -) and parameters meant for theunderlying git rev-list command they use internally and flags andparameters for the other commands they use downstream of gitrev-list. This command is used to distinguish between them.",
        "name": "git-rev-parse - Pick out and massage parameters",
        "section": 1
    },
    {
        "command": "git-revert",
        "description": "Given one or more existing commits, revert the changes that therelated patches introduce, and record some new commits thatrecord them. This requires your working tree to be clean (nomodifications from the HEAD commit).Note: git revert is used to record some new commits to reversethe effect of some earlier commits (often only a faulty one). Ifyou want to throw away all uncommitted changes in your workingdirectory, you should see git-reset(1), particularly the --hardoption. If you want to extract specific files as they were inanother commit, you should see git-restore(1), specifically the--source option. Take care with these alternatives as both willdiscard uncommitted changes in your working directory.See \"Reset, restore and revert\" in git(1) for the differencesbetween the three commands.",
        "name": "git-revert - Revert some existing commits",
        "section": 1
    },
    {
        "command": "git-rm",
        "description": "Remove files matching pathspec from the index, or from theworking tree and the index. git rm will not remove a file fromjust your working directory. (There is no option to remove a fileonly from the working tree and yet keep it in the index; use/bin/rm if you want to do that.) The files being removed have tobe identical to the tip of the branch, and no updates to theircontents can be staged in the index, though that default behaviorcan be overridden with the -f option. When --cached is given, thestaged content has to match either the tip of the branch or thefile on disk, allowing the file to be removed from just theindex. When sparse-checkouts are in use (seegit-sparse-checkout(1)), git rm will only remove paths within thesparse-checkout patterns.",
        "name": "git-rm - Remove files from the working tree and from the index",
        "section": 1
    },
    {
        "command": "git-send-email",
        "description": "Takes the patches given on the command line and emails them out.Patches can be specified as files, directories (which will sendall files in the directory), or directly as a revision list. Inthe last case, any format accepted by git-format-patch(1) can bepassed to git send-email, as well as options understood bygit-format-patch(1).The header of the email is configurable via command-line options.If not specified on the command line, the user will be promptedwith a ReadLine enabled interface to provide the necessaryinformation.There are two formats accepted for patch files:1. mbox format filesThis is what git-format-patch(1) generates. Most headers andMIME formatting are ignored.2. The original format used by Greg Kroah-Hartman\u2019ssend_lots_of_email.pl scriptThis format expects the first line of the file to contain the\"Cc:\" value and the \"Subject:\" of the message as the secondline.",
        "name": "git-send-email - Send a collection of patches as emails",
        "section": 1
    },
    {
        "command": "git-send-pack",
        "description": "Usually you would want to use git push, which is a higher-levelwrapper of this command, instead. See git-push(1).Invokes git-receive-pack on a possibly remote repository, andupdates it from the current repository, sending named refs.",
        "name": "git-send-pack - Push objects over Git protocol to anotherrepository",
        "section": 1
    },
    {
        "command": "git-series",
        "description": "git series tracks changes to a patch series over time.gitseries also tracks a cover letter for the patch series, formatsthe series for email, and prepares pull requests.Use git series start seriesname to start a patch seriesseriesname.Use normal git commands to commit changes, and usegit series status to check what has changed.Use git seriescover to add or edit a cover letter.Use git series add and gitseries commit (or git series commit -a) to commit changes to thepatch series.Use git series rebase -i to help rework orreorganize the patch series.Use git series format to preparethe patch series to send via email, or git series req to preparea \"please pull\" mail.Running git series without arguments shows the list of patchseries, marking the current patch series with a '*'.",
        "name": "git-series - track changes to a patch series with git",
        "section": 1
    },
    {
        "command": "git-sh-i18n",
        "description": "This is not a command the end user would want to run. Ever. Thisdocumentation is meant for people who are studying thePorcelain-ish scripts and/or are writing new ones.The 'git sh-i18n scriptlet is designed to be sourced (using .) byGit\u2019s porcelain programs implemented in shell script. It provideswrappers for the GNU gettext and eval_gettext functionsaccessible through the gettext.sh script, and providespass-through fallbacks on systems without GNU gettext.",
        "name": "git-sh-i18n - Git's i18n setup code for shell scripts",
        "section": 1
    },
    {
        "command": "git-sh-i18n--envsubst",
        "description": "This is not a command the end user would want to run. Ever. Thisdocumentation is meant for people who are studying the plumbingscripts and/or are writing new ones.git sh-i18n--envsubst is Git\u2019s stripped-down copy of the GNUenvsubst(1) program that comes with the GNU gettext package. It\u2019sused internally by git-sh-i18n(1) to interpolate the variablespassed to the eval_gettext function.No promises are made about the interface, or that this programwon\u2019t disappear without warning in the next version of Git. Don\u2019tuse it.",
        "name": "git-sh-i18n--envsubst - Git's own envsubst(1) for i18n fallbacks",
        "section": 1
    },
    {
        "command": "git-sh-setup",
        "description": "This is not a command the end user would want to run. Ever. Thisdocumentation is meant for people who are studying thePorcelain-ish scripts and/or are writing new ones.The git sh-setup scriptlet is designed to be sourced (using .) byother shell scripts to set up some variables pointing at thenormal Git directories and a few helper shell functions.Before sourcing it, your script should set up a few variables;USAGE (and LONG_USAGE, if any) is used to define message given byusage() shell function. SUBDIRECTORY_OK can be set if the scriptcan run from a subdirectory of the working tree (some commands donot).The scriptlet sets GIT_DIR and GIT_OBJECT_DIRECTORY shellvariables, but does not export them to the environment.",
        "name": "git-sh-setup - Common Git shell script setup code",
        "section": 1
    },
    {
        "command": "git-shell",
        "description": "This is a login shell for SSH accounts to provide restricted Gitaccess. It permits execution only of server-side Git commandsimplementing the pull/push functionality, plus custom commandspresent in a subdirectory named git-shell-commands in the user\u2019shome directory.",
        "name": "git-shell - Restricted login shell for Git-only SSH access",
        "section": 1
    },
    {
        "command": "git-shortlog",
        "description": "Summarizes git log output in a format suitable for inclusion inrelease announcements. Each commit will be grouped by author andtitle.Additionally, \"[PATCH]\" will be stripped from the commitdescription.If no revisions are passed on the command line and eitherstandard input is not a terminal or there is no current branch,git shortlog will output a summary of the log read from standardinput, without reference to the current repository.",
        "name": "git-shortlog - Summarize 'git log' output",
        "section": 1
    },
    {
        "command": "git-show",
        "description": "Shows one or more objects (blobs, trees, tags and commits).For commits it shows the log message and textual diff. It alsopresents the merge commit in a special format as produced by gitdiff-tree --cc.For tags, it shows the tag message and the referenced objects.For trees, it shows the names (equivalent to git ls-tree with--name-only).For plain blobs, it shows the plain contents.The command takes options applicable to the git diff-tree commandto control how the changes the commit introduces are shown.This manual page describes only the most frequently used options.",
        "name": "git-show - Show various types of objects",
        "section": 1
    },
    {
        "command": "git-show-branch",
        "description": "Shows the commit ancestry graph starting from the commits namedwith <rev>s or <glob>s (or all refs under refs/heads and/orrefs/tags) semi-visually.It cannot show more than 29 branches and commits at a time.It uses showbranch.default multi-valued configuration items if no<rev> or <glob> is given on the command line.",
        "name": "git-show-branch - Show branches and their commits",
        "section": 1
    },
    {
        "command": "git-show-index",
        "description": "Read the .idx file for a Git packfile (created withgit-pack-objects(1) or git-index-pack(1)) from the standardinput, and dump its contents. The output consists of one objectper line, with each line containing two or three space-separatedcolumns:\u2022the first column is the offset in bytes of the object withinthe corresponding packfile\u2022the second column is the object id of the object\u2022if the index version is 2 or higher, the third columncontains the CRC32 of the object dataThe objects are output in the order in which they are found inthe index file, which should be (in a correctly constructed file)sorted by object id.Note that you can get more information on a packfile by callinggit-verify-pack(1). However, as this command considers only theindex file itself, it\u2019s both faster and more flexible.",
        "name": "git-show-index - Show packed archive index",
        "section": 1
    },
    {
        "command": "git-show-ref",
        "description": "Displays references available in a local repository along withthe associated commit IDs. Results can be filtered using apattern and tags can be dereferenced into object IDs.Additionally, it can be used to test whether a particular refexists.By default, shows the tags, heads, and remote refs.The --exclude-existing form is a filter that does the inverse. Itreads refs from stdin, one ref per line, and shows those thatdon\u2019t exist in the local repository.Use of this utility is encouraged in favor of directly accessingfiles under the .git directory.",
        "name": "git-show-ref - List references in a local repository",
        "section": 1
    },
    {
        "command": "git-sparse-checkout",
        "description": "This command is used to create sparse checkouts, which change theworking tree from having all tracked files present to only havinga subset of those files. It can also switch which subset of filesare present, or undo and go back to having all tracked filespresent in the working copy.The subset of files is chosen by providing a list of directoriesin cone mode (the default), or by providing a list of patterns innon-cone mode.When in a sparse-checkout, other Git commands behave a bitdifferently. For example, switching branches will not updatepaths outside the sparse-checkout directories/patterns, and gitcommit -a will not record paths outside the sparse-checkoutdirectories/patterns as deleted.THIS COMMAND IS EXPERIMENTAL. ITS BEHAVIOR, AND THE BEHAVIOR OFOTHER COMMANDS IN THE PRESENCE OF SPARSE-CHECKOUTS, WILL LIKELYCHANGE IN THE FUTURE.",
        "name": "git-sparse-checkout - Reduce your working tree to a subset oftracked files",
        "section": 1
    },
    {
        "command": "git-stage",
        "description": "This is a synonym for git-add(1). Please refer to thedocumentation of that command.",
        "name": "git-stage - Add file contents to the staging area",
        "section": 1
    },
    {
        "command": "git-stash",
        "description": "Use git stash when you want to record the current state of theworking directory and the index, but want to go back to a cleanworking directory. The command saves your local modificationsaway and reverts the working directory to match the HEAD commit.The modifications stashed away by this command can be listed withgit stash list, inspected with git stash show, and restored(potentially on top of a different commit) with git stash apply.Calling git stash without any arguments is equivalent to gitstash push. A stash is by default listed as \"WIP on branchname...\", but you can give a more descriptive message on the commandline when you create one.The latest stash you created is stored in refs/stash; olderstashes are found in the reflog of this reference and can benamed using the usual reflog syntax (e.g. stash@{0} is the mostrecently created stash, stash@{1} is the one before it,stash@{2.hours.ago} is also possible). Stashes may also bereferenced by specifying just the stash index (e.g. the integer nis equivalent to stash@{n}).",
        "name": "git-stash - Stash the changes in a dirty working directory away",
        "section": 1
    },
    {
        "command": "git-status",
        "description": "Displays paths that have differences between the index file andthe current HEAD commit, paths that have differences between theworking tree and the index file, and paths in the working treethat are not tracked by Git (and are not ignored bygitignore(5)). The first are what you would commit by running gitcommit; the second and third are what you could commit by runninggit add before running git commit.",
        "name": "git-status - Show the working tree status",
        "section": 1
    },
    {
        "command": "git-stripspace",
        "description": "Read text, such as commit messages, notes, tags and branchdescriptions, from the standard input and clean it in the mannerused by Git.With no arguments, this will:\u2022remove trailing whitespace from all lines\u2022collapse multiple consecutive empty lines into one empty line\u2022remove empty lines from the beginning and end of the input\u2022add a missing \\n to the last line if necessary.In the case where the input consists entirely of whitespacecharacters, no output will be produced.NOTE: This is intended for cleaning metadata, prefer the--whitespace=fix mode of git-apply(1) for correcting whitespaceof patches or files in the repository.",
        "name": "git-stripspace - Remove unnecessary whitespace",
        "section": 1
    },
    {
        "command": "git-submodule",
        "description": "Inspects, updates and manages submodules.For more information about submodules, see gitsubmodules(7).",
        "name": "git-submodule - Initialize, update or inspect submodules",
        "section": 1
    },
    {
        "command": "git-svn",
        "description": "git svn is a simple conduit for changesets between Subversion andGit. It provides a bidirectional flow of changes between aSubversion and a Git repository.git svn can track a standard Subversion repository, following thecommon \"trunk/branches/tags\" layout, with the --stdlayout option.It can also follow branches and tags in any layout with the-T/-t/-b options (see options to init below, and also the clonecommand).Once tracking a Subversion repository (with any of the abovemethods), the Git repository can be updated from Subversion bythe fetch command and Subversion updated from Git by the dcommitcommand.",
        "name": "git-svn - Bidirectional operation between a Subversion repositoryand Git",
        "section": 1
    },
    {
        "command": "git-switch",
        "description": "Switch to a specified branch. The working tree and the index areupdated to match the branch. All new commits will be added to thetip of this branch.Optionally a new branch could be created with either -c, -C,automatically from a remote branch of same name (see --guess), ordetach the working tree from any branch with --detach, along withswitching.Switching branches does not require a clean index and workingtree (i.e. no differences compared to HEAD). The operation isaborted however if the operation leads to loss of local changes,unless told otherwise with --discard-changes or --merge.THIS COMMAND IS EXPERIMENTAL. THE BEHAVIOR MAY CHANGE.",
        "name": "git-switch - Switch branches",
        "section": 1
    },
    {
        "command": "git-symbolic-ref",
        "description": "Given one argument, reads which branch head the given symbolicref refers to and outputs its path, relative to the .git/directory. Typically you would give HEAD as the <name> argumentto see which branch your working tree is on.Given two arguments, creates or updates a symbolic ref <name> topoint at the given branch <ref>.Given --delete and an additional argument, deletes the givensymbolic ref.A symbolic ref is a regular file that stores a string that beginswith ref: refs/. For example, your .git/HEAD is a regular filewhose contents is ref: refs/heads/master.",
        "name": "git-symbolic-ref - Read, modify and delete symbolic refs",
        "section": 1
    },
    {
        "command": "git-tag",
        "description": "Add a tag reference in refs/tags/, unless -d/-l/-v is given todelete, list or verify tags.Unless -f is given, the named tag must not yet exist.If one of -a, -s, or -u <key-id> is passed, the command creates atag object, and requires a tag message. Unless -m <msg> or -F<file> is given, an editor is started for the user to type in thetag message.If -m <msg> or -F <file> is given and -a, -s, and -u <key-id> areabsent, -a is implied.Otherwise, a tag reference that points directly at the givenobject (i.e., a lightweight tag) is created.A GnuPG signed tag object will be created when -s or -u <key-id>is used. When -u <key-id> is not used, the committer identity forthe current user is used to find the GnuPG key for signing. Theconfiguration variable gpg.program is used to specify customGnuPG binary.Tag objects (created with -a, -s, or -u) are called \"annotated\"tags; they contain a creation date, the tagger name and e-mail, atagging message, and an optional GnuPG signature. Whereas a\"lightweight\" tag is simply a name for an object (usually acommit object).Annotated tags are meant for release while lightweight tags aremeant for private or temporary object labels. For this reason,some git commands for naming objects (like git describe) willignore lightweight tags by default.",
        "name": "git-tag - Create, list, delete or verify a tag object signed withGPG",
        "section": 1
    },
    {
        "command": "git-unpack-file",
        "description": "Creates a file holding the contents of the blob specified bysha1. It returns the name of the temporary file in the followingformat: .merge_file_XXXXX",
        "name": "git-unpack-file - Creates a temporary file with a blob's contents",
        "section": 1
    },
    {
        "command": "git-unpack-objects",
        "description": "Read a packed archive (.pack) from the standard input, expandingthe objects contained within and writing them into the repositoryin \"loose\" (one object per file) format.Objects that already exist in the repository will not be unpackedfrom the packfile. Therefore, nothing will be unpacked if you usethis command on a packfile that exists within the targetrepository.See git-repack(1) for options to generate new packs and replaceexisting ones.",
        "name": "git-unpack-objects - Unpack objects from a packed archive",
        "section": 1
    },
    {
        "command": "git-update-index",
        "description": "Modifies the index. Each file mentioned is updated into the indexand any unmerged or needs updating state is cleared.See also git-add(1) for a more user-friendly way to do some ofthe most common operations on the index.The way git update-index handles files it is told about can bemodified using the various options:",
        "name": "git-update-index - Register file contents in the working tree tothe index",
        "section": 1
    },
    {
        "command": "git-update-ref",
        "description": "Given two arguments, stores the <newvalue> in the <ref>, possiblydereferencing the symbolic refs. E.g. git update-ref HEAD<newvalue> updates the current branch head to the new object.Given three arguments, stores the <newvalue> in the <ref>,possibly dereferencing the symbolic refs, after verifying thatthe current value of the <ref> matches <oldvalue>. E.g. gitupdate-ref refs/heads/master <newvalue> <oldvalue> updates themaster branch head to <newvalue> only if its current value is<oldvalue>. You can specify 40 \"0\" or an empty string as<oldvalue> to make sure that the ref you are creating does notexist.It also allows a \"ref\" file to be a symbolic pointer to anotherref file by starting with the four-byte header sequence of\"ref:\".More importantly, it allows the update of a ref file to followthese symbolic pointers, whether they are symlinks or these\"regular file symbolic refs\". It follows real symlinks only ifthey start with \"refs/\": otherwise it will just try to read themand update them as a regular file (i.e. it will allow thefilesystem to follow them, but will overwrite such a symlink tosomewhere else with a regular filename).If --no-deref is given, <ref> itself is overwritten, rather thanthe result of following the symbolic pointers.In general, usinggit update-ref HEAD \"$head\"should be a lot safer than doingecho \"$head\" > \"$GIT_DIR/HEAD\"both from a symlink following standpoint and an error checkingstandpoint. The \"refs/\" rule for symlinks means that symlinksthat point to \"outside\" the tree are safe: they\u2019ll be followedfor reading but not for writing (so we\u2019ll never write through aref symlink to some other tree, if you have copied a wholearchive by creating a symlink tree).With -d flag, it deletes the named <ref> after verifying it stillcontains <oldvalue>.With --stdin, update-ref reads instructions from standard inputand performs all modifications together. Specify commands of theform:update SP <ref> SP <newvalue> [SP <oldvalue>] LFcreate SP <ref> SP <newvalue> LFdelete SP <ref> [SP <oldvalue>] LFverify SP <ref> [SP <oldvalue>] LFoption SP <opt> LFstart LFprepare LFcommit LFabort LFWith --create-reflog, update-ref will create a reflog for eachref even if one would not ordinarily be created.Quote fields containing whitespace as if they were strings in Csource code; i.e., surrounded by double-quotes and with backslashescapes. Use 40 \"0\" characters or the empty string to specify azero value. To specify a missing value, omit the value and itspreceding SP entirely.Alternatively, use -z to specify in NUL-terminated format,without quoting:update SP <ref> NUL <newvalue> NUL [<oldvalue>] NULcreate SP <ref> NUL <newvalue> NULdelete SP <ref> NUL [<oldvalue>] NULverify SP <ref> NUL [<oldvalue>] NULoption SP <opt> NULstart NULprepare NULcommit NULabort NULIn this format, use 40 \"0\" to specify a zero value, and use theempty string to specify a missing value.In either format, values can be specified in any form that Gitrecognizes as an object name. Commands in any other format or arepeated <ref> produce an error. Command meanings are:updateSet <ref> to <newvalue> after verifying <oldvalue>, if given.Specify a zero <newvalue> to ensure the ref does not existafter the update and/or a zero <oldvalue> to make sure theref does not exist before the update.createCreate <ref> with <newvalue> after verifying it does notexist. The given <newvalue> may not be zero.deleteDelete <ref> after verifying it exists with <oldvalue>, ifgiven. If given, <oldvalue> may not be zero.verifyVerify <ref> against <oldvalue> but do not change it. If<oldvalue> is zero or missing, the ref must not exist.optionModify behavior of the next command naming a <ref>. The onlyvalid option is no-deref to avoid dereferencing a symbolicref.startStart a transaction. In contrast to a non-transactionalsession, a transaction will automatically abort if thesession ends without an explicit commit. This command maycreate a new empty transaction when the current one has beencommitted or aborted already.preparePrepare to commit the transaction. This will create lockfiles for all queued reference updates. If one referencecould not be locked, the transaction will be aborted.commitCommit all reference updates queued for the transaction,ending the transaction.abortAbort the transaction, releasing all locks if the transactionis in prepared state.If all <ref>s can be locked with matching <oldvalue>ssimultaneously, all modifications are performed. Otherwise, nomodifications are performed. Note that while each individual<ref> is updated or deleted atomically, a concurrent reader maystill see a subset of the modifications.",
        "name": "git-update-ref - Update the object name stored in a ref safely",
        "section": 1
    },
    {
        "command": "git-update-server-info",
        "description": "A dumb server that does not do on-the-fly pack generations musthave some auxiliary information files in $GIT_DIR/info and$GIT_OBJECT_DIRECTORY/info directories to help clients discoverwhat references and packs the server has. This command generatessuch auxiliary files.",
        "name": "git-update-server-info - Update auxiliary info file to help dumbservers",
        "section": 1
    },
    {
        "command": "git-upload-archive",
        "description": "Invoked by git archive --remote and sends a generated archive tothe other end over the Git protocol.This command is usually not invoked directly by the end user. TheUI for the protocol is on the git archive side, and the programpair is meant to be used to get an archive from a remoterepository.",
        "name": "git-upload-archive - Send archive back to git-archive",
        "section": 1
    },
    {
        "command": "git-upload-pack",
        "description": "Invoked by git fetch-pack, learns what objects the other side ismissing, and sends them after packing.This command is usually not invoked directly by the end user. TheUI for the protocol is on the git fetch-pack side, and theprogram pair is meant to be used to pull updates from a remoterepository. For push operations, see git send-pack.",
        "name": "git-upload-pack - Send objects packed back to git-fetch-pack",
        "section": 1
    },
    {
        "command": "git-var",
        "description": "Prints a Git logical variable. Exits with code 1 if the variablehas no value.",
        "name": "git-var - Show a Git logical variable",
        "section": 1
    },
    {
        "command": "git-verify-commit",
        "description": "Validates the GPG signature created by git commit -S.",
        "name": "git-verify-commit - Check the GPG signature of commits",
        "section": 1
    },
    {
        "command": "git-verify-pack",
        "description": "Reads given idx file for packed Git archive created with the gitpack-objects command and verifies idx file and the correspondingpack file.",
        "name": "git-verify-pack - Validate packed Git archive files",
        "section": 1
    },
    {
        "command": "git-verify-tag",
        "description": "Validates the gpg signature created by git tag.",
        "name": "git-verify-tag - Check the GPG signature of tags",
        "section": 1
    },
    {
        "command": "git-version",
        "description": "With no options given, the version of git is printed on thestandard output.Note that git --version is identical to git version because theformer is internally converted into the latter.",
        "name": "git-version - Display version information about Git",
        "section": 1
    },
    {
        "command": "git-web--browse",
        "description": "This script tries, as much as possible, to display the URLs andFILEs that are passed as arguments, as HTML pages in new tabs onan already opened web browser.The following browsers (or commands) are currently supported:\u2022firefox (this is the default under X Window when not usingKDE)\u2022iceweasel\u2022seamonkey\u2022iceape\u2022chromium (also supported as chromium-browser)\u2022google-chrome (also supported as chrome)\u2022konqueror (this is the default under KDE, see Note aboutkonqueror below)\u2022opera\u2022w3m (this is the default outside graphical environments)\u2022elinks\u2022links\u2022lynx\u2022dillo\u2022open (this is the default under Mac OS X GUI)\u2022start (this is the default under MinGW)\u2022cygstart (this is the default under Cygwin)\u2022xdg-openCustom commands may also be specified.",
        "name": "git-web--browse - Git helper script to launch a web browser",
        "section": 1
    },
    {
        "command": "git-whatchanged",
        "description": "Shows commit logs and diff output each commit introduces.New users are encouraged to use git-log(1) instead. Thewhatchanged command is essentially the same as git-log(1) butdefaults to show the raw format diff output and to skip merges.The command is kept primarily for historical reasons; fingers ofmany people who learned Git long before git log was invented byreading Linux kernel mailing list are trained to type it.",
        "name": "git-whatchanged - Show logs with difference each commitintroduces",
        "section": 1
    },
    {
        "command": "git-worktree",
        "description": "Manage multiple working trees attached to the same repository.A git repository can support multiple working trees, allowing youto check out more than one branch at a time. With git worktreeadd a new working tree is associated with the repository, alongwith additional metadata that differentiates that working treefrom others in the same repository. The working tree, along withthis metadata, is called a \"worktree\".This new worktree is called a \"linked worktree\" as opposed to the\"main worktree\" prepared by git-init(1) or git-clone(1). Arepository has one main worktree (if it\u2019s not a bare repository)and zero or more linked worktrees. When you are done with alinked worktree, remove it with git worktree remove.In its simplest form, git worktree add <path> automaticallycreates a new branch whose name is the final component of <path>,which is convenient if you plan to work on a new topic. Forinstance, git worktree add ../hotfix creates new branch hotfixand checks it out at path ../hotfix. To instead work on anexisting branch in a new worktree, use git worktree add <path><branch>. On the other hand, if you just plan to make someexperimental changes or do testing without disturbing existingdevelopment, it is often convenient to create a throwawayworktree not associated with any branch. For instance, gitworktree add -d <path> creates a new worktree with a detachedHEAD at the same commit as the current branch.If a working tree is deleted without using git worktree remove,then its associated administrative files, which reside in therepository (see \"DETAILS\" below), will eventually be removedautomatically (see gc.worktreePruneExpire in git-config(1)), oryou can run git worktree prune in the main or any linked worktreeto clean up any stale administrative files.If the working tree for a linked worktree is stored on a portabledevice or network share which is not always mounted, you canprevent its administrative files from being pruned by issuing thegit worktree lock command, optionally specifying --reason toexplain why the worktree is locked.",
        "name": "git-worktree - Manage multiple working trees",
        "section": 1
    },
    {
        "command": "git-write-tree",
        "description": "Creates a tree object using the current index. The name of thenew tree object is printed to standard output.The index must be in a fully merged state.Conceptually, git write-tree sync()s the current index contentsinto a set of tree files. In order to have that match what isactually in your directory right now, you need to have done a gitupdate-index phase before you did the git write-tree.",
        "name": "git-write-tree - Create a tree object from the current index",
        "section": 1
    },
    {
        "command": "gitk",
        "description": "Displays changes in a repository or a selected set of commits.This includes visualizing the commit graph, showing informationrelated to each commit, and the files in the trees of eachrevision.",
        "name": "gitk - The Git repository browser",
        "section": 1
    },
    {
        "command": "gitweb",
        "description": "Gitweb provides a web interface to Git repositories. Its featuresinclude:\u2022Viewing multiple Git repositories with common root.\u2022Browsing every revision of the repository.\u2022Viewing the contents of files in the repository at anyrevision.\u2022Viewing the revision log of branches, history of files anddirectories, see what was changed when, by who.\u2022Viewing the blame/annotation details of any file (ifenabled).\u2022Generating RSS and Atom feeds of commits, for any branch. Thefeeds are auto-discoverable in modern web browsers.\u2022Viewing everything that was changed in a revision, and stepthrough revisions one at a time, viewing the history of therepository.\u2022Finding commits which commit messages matches given searchterm.See http://repo.or.cz/w/git.git/tree/HEAD:/gitweb/ for gitwebsource code, browsed using gitweb itself.",
        "name": "gitweb - Git web interface (web frontend to Git repositories)",
        "section": 1
    },
    {
        "command": "glilypond",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "gnutls-cli",
        "description": "Simple client program to set up a TLS connection to some othercomputer.It sets up a TLS connection and forwards data from thestandard input to the secured socket and vice versa.",
        "name": "gnutls-cli - GnuTLS client",
        "section": 1
    },
    {
        "command": "gnutls-cli-debug",
        "description": "TLS debug client. It sets up multiple TLS connections to a serverand queries its capabilities. It was created to assist indebugging GnuTLS, but it might be useful to extract a TLSserver's capabilities.It connects to a TLS server, performstests and print the server's capabilities. If called with the`-V' parameter more checks will be performed.Can be used tocheck for servers with special needs or bugs.",
        "name": "gnutls-cli-debug - GnuTLS debug client",
        "section": 1
    },
    {
        "command": "gnutls-serv",
        "description": "Server program that listens to incoming TLS connections.",
        "name": "gnutls-serv - GnuTLS server",
        "section": 1
    },
    {
        "command": "gpasswd",
        "description": "The gpasswd command is used to administer /etc/group, and/etc/gshadow. Every group can have administrators, members and apassword.System administrators can use the -A option to define groupadministrator(s) and the -M option to define members. They haveall rights of group administrators and members.gpasswd called by a group administrator with a group name onlyprompts for the new password of the group.If a password is set the members can still use newgrp(1) withouta password, and non-members must supply the password.Notes about group passwordsGroup passwords are an inherent security problem since more thanone person is permitted to know the password. However, groups area useful tool for permitting co-operation between differentusers.",
        "name": "gpasswd - administer /etc/group and /etc/gshadow",
        "section": 1
    },
    {
        "command": "gperl",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "gpinyin",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "gprof",
        "description": "\"gprof\" produces an execution profile of C, Pascal, or Fortran77programs.The effect of called routines is incorporated in theprofile of each caller.The profile data is taken from the callgraph profile file (gmon.out default) which is created byprograms that are compiled with the -pg option of \"cc\", \"pc\", and\"f77\".The -pg option also links in versions of the libraryroutines that are compiled for profiling.\"Gprof\" reads thegiven object file (the default is \"a.out\") and establishes therelation between its symbol table and the call graph profile fromgmon.out.If more than one profile file is specified, the\"gprof\" output shows the sum of the profile information in thegiven profile files.\"Gprof\" calculates the amount of time spent in each routine.Next, these times are propagated along the edges of the callgraph.Cycles are discovered, and calls into a cycle are made toshare the time of the cycle.Several forms of output are available from the analysis.The flat profile shows how much time your program spent in eachfunction, and how many times that function was called.If yousimply want to know which functions burn most of the cycles, itis stated concisely here.The call graph shows, for each function, which functions calledit, which other functions it called, and how many times.Thereis also an estimate of how much time was spent in the subroutinesof each function.This can suggest places where you might try toeliminate function calls that use a lot of time.The annotated source listing is a copy of the program's sourcecode, labeled with the number of times each line of the programwas executed.",
        "name": "gprof - display call graph profile data",
        "section": 1
    },
    {
        "command": "grap2graph",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "grep",
        "description": "grep searches for PATTERNS in each FILE.PATTERNS is one or morepatterns separated by newline characters, and grep prints eachline that matches a pattern.Typically PATTERNS should be quotedwhen grep is used in a shell command.A FILE of \u201c-\u201d stands for standard input.If no FILE is given,recursive searches examine the working directory, andnonrecursive searches read standard input.",
        "name": "grep - print lines that match patterns",
        "section": 1
    },
    {
        "command": "grn",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "grodvi",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "groff",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "grog",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "grohtml",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "grolbp",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "grolj4",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "gropdf",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "grops",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "grotty",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "groups",
        "description": "Print group memberships for each USERNAME or, if no USERNAME isspecified, for the current process (which may differ if thegroups database has changed).--help display this help and exit--versionoutput version information and exit",
        "name": "groups - print the groups a user is in",
        "section": 1
    },
    {
        "command": "guards",
        "description": "The script reads a configuration file that may contain so-calledguards, file names, and comments, and writes those file namesthat satisfy all guards to standard output. The script takes alist of symbols as its arguments. Each line in the configurationfile is processed separately. Lines may start with a number ofguards. The following guards are defined:+xxx Include the file(s) on this line if the symbol xxx isdefined.-xxx Exclude the file(s) on this line if the symbol xxx isdefined.+!xxx Include the file(s) on this line if the symbol xxx isnot defined.-!xxx Exclude the file(s) on this line if the symbol xxx isnot defined.- Exclude this file. Used to avoid spurious --check messages.The guards are processed left to right. The last guard thatmatches determines if the file is included. If no guard isspecified, the --default setting determines if the file isincluded.If no configuration file is specified, the script reads fromstandard input.The --check option is used to compare the specification fileagainst the file system. If files are referenced in thespecification that do not exist, or if files are not enlisted inthe specification file warnings are printed. The --path optioncan be used to specify which directory or directories to scan.Multiple directories are separated by a colon (\":\") character.The --prefix option specifies the location of the files.Alternatively, the --path=@<file> syntax can be used to specify afile from which the file names will be read.Use --list to list all files independent of any rules. Use--invert-match to list only the excluded patches. Use--with-guards to also include all inclusion and exclusion rules.",
        "name": "guards - select from a list of files guarded by conditions",
        "section": 1
    },
    {
        "command": "gxditview",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "hardlink",
        "description": "hardlink is a tool that replaces copies of a file with eitherhardlinks or copy-on-write clones, thus saving space.hardlink first creates a binary tree of file sizes and thencompares the content of files that have the same size. There aretwo basic content comparison methods. The memcmp method directlyreads data blocks from files and compares them. The other methodis based on checksums (like SHA256); in this case for each datablock a checksum is calculated by the Linux kernel crypto API,and this checksum is stored in userspace and used for filecomparisons.For each file also an \"intro\" buffer (32 bytes) is cached. Thisbuffer is used independently from the comparison method andrequested cache-size and io-size. The \"intro\" buffer dramaticallyreduces operations with data content as files are very oftendifferent from the beginning.",
        "name": "hardlink - link multiple copies of a file",
        "section": 1
    },
    {
        "command": "head",
        "description": "Print the first 10 lines of each FILE to standard output.Withmore than one FILE, precede each with a header giving the filename.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-c, --bytes=[-]NUMprint the first NUM bytes of each file; with the leading'-', print all but the last NUM bytes of each file-n, --lines=[-]NUMprint the first NUM lines instead of the first 10; withthe leading '-', print all but the last NUM lines of eachfile-q, --quiet, --silentnever print headers giving file names-v, --verbosealways print headers giving file names-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exitNUM may have a multiplier suffix: b 512, kB 1000, K 1024, MB1000*1000, M 1024*1024, GB 1000*1000*1000, G 1024*1024*1024, andso on for T, P, E, Z, Y, R, Q.Binary prefixes can be used, too:KiB=K, MiB=M, and so on.",
        "name": "head - output the first part of files",
        "section": 1
    },
    {
        "command": "hexdump",
        "description": "The hexdump utility is a filter which displays the specifiedfiles, or standard input if no files are specified, in auser-specified format.",
        "name": "hexdump - display file contents in hexadecimal, decimal, octal,or asciihexdump options file ...hd options file ...",
        "section": 1
    },
    {
        "command": "hg",
        "description": "The hg command provides a command line interface to the Mercurialsystem.",
        "name": "hg - Mercurial source code management system",
        "section": 1
    },
    {
        "command": "hostid",
        "description": "Print the numeric identifier (in hexadecimal) for the currenthost.--help display this help and exit--versionoutput version information and exit",
        "name": "hostid - print the numeric identifier for the current host",
        "section": 1
    },
    {
        "command": "hostname",
        "description": "Hostname is the program that is used to either set or display thecurrent host, domain or node name of the system.These names areused by many of the networking programs to identify the machine.The domain name is also used by NIS/YP.GET NAMEWhen called without any arguments, the program displays thecurrent names:hostname will print the name of the system as returned by thegethostname(2) function.domainname, nisdomainname, ypdomainname will print the name ofthe system as returned by the getdomainname(2) function. This isalso known as the YP/NIS domain name of the system.nodename will print the DECnet node name of the system asreturned by the getnodename(2) function.dnsdomainname will print the domain part of the FQDN (FullyQualified Domain Name). The complete FQDN of the system isreturned with hostname --fqdn.SET NAMEWhen called with one argument or with the --file option, thecommands set the host name, the NIS/YP domain name or the nodename.Note, that only the super-user can change the names.It is not possible to set the FQDN or the DNS domain name withthe dnsdomainname command (see THE FQDN below).The host name is usually set once at system startup by readingthe contents of a file which contains the host name, e.g./etc/hostname).THE FQDNYou can't change the FQDN (as returned by hostname --fqdn) or theDNS domain name (as returned by dnsdomainname) with this command.The FQDN of the system is the name that the resolver(3) returnsfor the host name.Technically: The FQDN is the canonical name returned bygethostbyname2(2) when resolving the result of the gethostname(2)name. The DNS domain name is the part after the first dot.Therefore it depends on the configuration (usually in/etc/host.conf) how you can change it. If hosts is the firstlookup method, you can change the FQDN in /etc/hosts.",
        "name": "hostname - show or set the system's host namednsdomainname - show the system's DNS domain namedomainname - show or set the system's NIS/YP domain namenisdomainname - show or set system's NIS/YP domain namenodename - show or set the system's DECnet node nameypdomainname - show or set the system's NIS/YP domain name",
        "section": 1
    },
    {
        "command": "hostnamectl",
        "description": "hostnamectl may be used to query and change the system hostnameand related settings.systemd-hostnamed.service(8) and this tool distinguish threedifferent hostnames: the high-level \"pretty\" hostname which mightinclude all kinds of special characters (e.g. \"Lennart'sLaptop\"), the \"static\" hostname which is the user-configuredhostname (e.g. \"lennarts-laptop\"), and the transient hostnamewhich is a fallback value received from network configuration(e.g. \"node12345678\"). If a static hostname is set to a validvalue, then the transient hostname is not used.Note that the pretty hostname has little restrictions on thecharacters and length used, while the static and transienthostnames are limited to the usually accepted characters ofInternet domain names, and 64 characters at maximum (the latterbeing a Linux limitation).Use systemd-firstboot(1) to initialize the system hostname formounted (but not booted) system images.",
        "name": "hostnamectl - Control the system hostname",
        "section": 1
    },
    {
        "command": "hpftodit",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "htop",
        "description": "htop is a cross-platform ncurses-based process viewer.It is similar to top, but allows you to scroll vertically andhorizontally, and interact using a pointing device (mouse).Youcan observe all processes running on the system, along with theircommand line arguments, as well as view them in a tree format,select multiple processes and act on them all at once.Tasks related to processes (killing, renicing) can be donewithout entering their PIDs.pcp-htop is a version of htop built using the Performance Co-Pilot (PCP) Metrics API (see PCPIntro(1), PMAPI(3)), allowing toextend htop to display values from arbitrary metrics.See thesection below titled CONFIG FILES for further details.",
        "name": "htop, pcp-htop - interactive process viewer",
        "section": 1
    },
    {
        "command": "ib_acme",
        "description": "ib_acme provides assistance configuring and testing the ibacmservice.The first usage of the service will test that the ibacmis running and operating correctly.The second usage model willautomatically create address and configuration files for theibacm service.",
        "name": "ib_acme - test and configuration utility for the IB ACM",
        "section": 1
    },
    {
        "command": "ibv_asyncwatch",
        "description": "Display asynchronous events forwarded to userspace for an RDMAdevice.",
        "name": "ibv_asyncwatch - display asynchronous events",
        "section": 1
    },
    {
        "command": "ibv_devices",
        "description": "List RDMA devices available for use from userspace.",
        "name": "ibv_devices - list RDMA devices",
        "section": 1
    },
    {
        "command": "ibv_devinfo",
        "description": "Print information about RDMA devices available for use fromuserspace.",
        "name": "ibv_devinfo - query RDMA devices",
        "section": 1
    },
    {
        "command": "ibv_rc_pingpong",
        "description": "Run a simple ping-pong test over InfiniBand via the reliableconnected (RC) transport.",
        "name": "ibv_rc_pingpong - simple InfiniBand RC transport test",
        "section": 1
    },
    {
        "command": "ibv_srq_pingpong",
        "description": "Run a simple ping-pong test over InfiniBand via the reliableconnected (RC) transport, using multiple queue pairs (QPs) and asingle shared receive queue (SRQ).",
        "name": "ibv_srq_pingpong - simple InfiniBand shared receive queue test",
        "section": 1
    },
    {
        "command": "ibv_uc_pingpong",
        "description": "Run a simple ping-pong test over InfiniBand via the unreliableconnected (UC) transport.",
        "name": "ibv_uc_pingpong - simple InfiniBand UC transport test",
        "section": 1
    },
    {
        "command": "ibv_ud_pingpong",
        "description": "Run a simple ping-pong test over InfiniBand via the unreliabledatagram (UD) transport.",
        "name": "ibv_ud_pingpong - simple InfiniBand UD transport test",
        "section": 1
    },
    {
        "command": "ibv_xsrq_pingpong",
        "description": "Run a simple ping-pong test over InfiniBand via the extendedreliable connected (XRC) transport service, using a sharedreceive queue (SRQ).",
        "name": "ibv_xsrq_pingpong - simple InfiniBand shared receive queue test",
        "section": 1
    },
    {
        "command": "iconv",
        "description": "The iconv program reads in text in one encoding and outputs thetext in another encoding.If no input files are given, or if itis given as a dash (-), iconv reads from standard input.If nooutput file is given, iconv writes to standard output.If no from-encoding is given, the default is derived from thecurrent locale's character encoding.If no to-encoding is given,the default is derived from the current locale's characterencoding.",
        "name": "iconv - convert text from one character encoding to another",
        "section": 1
    },
    {
        "command": "icuexportdata",
        "description": null,
        "name": "icuexportdata - Writes text files with Unicode properties datafrom ICU.",
        "section": 1
    },
    {
        "command": "id",
        "description": "Print user and group information for each specified USER, or(when USER omitted) for the current process.-aignore, for compatibility with other versions-Z, --contextprint only the security context of the process-g, --groupprint only the effective group ID-G, --groupsprint all group IDs-n, --nameprint a name instead of a number, for -ugG-r, --realprint the real ID instead of the effective ID, with -ugG-u, --userprint only the effective user ID-z, --zerodelimit entries with NUL characters, not whitespace;not permitted in default format--help display this help and exit--versionoutput version information and exitWithout any OPTION, print some useful set of identifiedinformation.",
        "name": "id - print real and effective user and group IDs",
        "section": 1
    },
    {
        "command": "indent",
        "description": "This man page is generated from the file indent.texinfo.This isEditionof \"The indent Manual\", for Indent Version , lastupdated .The indent program can be used to make code easier to read.Itcan also convert from one style of writing C to another.indent understands a substantial amount about the syntax of C,but it also attempts to cope with incomplete and misformedsyntax.In version 1.2 and more recent versions, the GNU style ofindenting is the default.",
        "name": "indent - changes the appearance of a C program by inserting ordeleting whitespace.",
        "section": 1
    },
    {
        "command": "indxbib",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "init",
        "description": "systemd is a system and service manager for Linux operatingsystems. When run as first process on boot (as PID 1), it acts asinit system that brings up and maintains userspace services.Separate instances are started for logged-in users to start theirservices.systemd is usually not invoked directly by the user, but isinstalled as the /sbin/init symlink and started during earlyboot. The user manager instances are started automaticallythrough the user@.service(5) service.For compatibility with SysV, if the binary is called as init andis not the first process on the machine (PID is not 1), it willexecute telinit and pass all command line arguments unmodified.That means init and telinit are mostly equivalent when invokedfrom normal login sessions. See telinit(8) for more information.When run as a system instance, systemd interprets theconfiguration file system.conf and the files in system.conf.ddirectories; when run as a user instance, systemd interprets theconfiguration file user.conf and the files in user.conf.ddirectories. See systemd-system.conf(5) for more information.",
        "name": "systemd, init - systemd system and service manager",
        "section": 1
    },
    {
        "command": "innochecksum",
        "description": "innochecksum prints checksums for InnoDB files. This tool readsan InnoDB tablespace file, calculates the checksum for each page,compares the calculated checksum to the stored checksum, andreports mismatches, which indicate damaged pages. It wasoriginally developed to speed up verifying the integrity oftablespace files after power outages but can also be used afterfile copies. Because checksum mismatches will cause InnoDB todeliberately shut down a running server, it can be preferable touse this tool rather than waiting for a server in productionusage to encounter the damaged pages.innochecksum cannot be used on tablespace files that the serveralready has open. For such files, you should use CHECK TABLE tocheck tables within the tablespace.If checksum mismatches are found, you would normally restore thetablespace from backup or start the server and attempt to usemysqldump to make a backup of the tables within the tablespace.Invoke innochecksum like this:shell> innochecksum [options] file_nameinnochecksum supports the following options. For options thatrefer to page numbers, the numbers are zero-based.\u2022-?, --helpDisplays help and exits.\u2022-c, --countPrint a count of the number of pages in the file.\u2022-d, --debugDebug mode; prints checksums for each page.\u2022-e num, --end-page=#End at this page number.\u2022-i, --per-page-detailsPrint out per-page detail information.\u2022-I, --infoSynonym for --help.\u2022-l, --leafExamine leaf index pages.\u2022-m num, --merge=#Leaf page count if merge given number of consecutive pages.\u2022-p num, --page-num=#Check only this page number.\u2022-s num, --start-pageStart at this page number.\u2022-u, --skip-corruptSkip corrupt pages.\u2022-v, --verboseVerbose mode; print a progress indicator every five seconds.\u2022-V, --versionDisplays version information and exits.",
        "name": "innochecksum - offline InnoDB file checksum utility",
        "section": 1
    },
    {
        "command": "inotifywait",
        "description": "inotifywait efficiently waits for changes to files using Linux'sinotify(7) interface.It is suitable for waiting for changes tofiles from shell scripts.It can either exit once an eventoccurs, or continually execute and output events as they occur.fsnotifywait is similar to inotifywait but it is using Linux'sfanotify(7) interface by default. If explicitly specified, ituses the inotify(7) interface.",
        "name": "inotifywait, fsnotifywait - wait for changes to files usinginotify or fanotify",
        "section": 1
    },
    {
        "command": "inotifywatch",
        "description": "inotifywatch listens for filesystem events using Linux'sinotify(7) interface, then outputs a summary count of the eventsreceived on each file or directory.fsnotifywatch is similar to inotifywatch but it is using Linux'sfanotify(7) interface by default. If explicitly specified, ituses the inotify(7) interface.",
        "name": "inotifywatch, fsnotifywatch - gather filesystem access statisticsusing inotify or fanotify",
        "section": 1
    },
    {
        "command": "install",
        "description": "This install program copies files (often just compiled) intodestination locations you choose.If you want to download andinstall a ready-to-use package on a GNU/Linux system, you shouldinstead be using a package manager like yum(1) or apt-get(1).In the first three forms, copy SOURCE to DEST or multipleSOURCE(s) to the existing DIRECTORY, while setting permissionmodes and owner/group.In the 4th form, create all components ofthe given DIRECTORY(ies).Mandatory arguments to long options are mandatory for shortoptions too.--backup[=CONTROL]make a backup of each existing destination file-blike --backup but does not accept an argument-c(ignored)-C, --comparecompare content of source and destination files, and if nochange to content, ownership, and permissions, do notmodify the destination at all-d, --directorytreat all arguments as directory names; create allcomponents of the specified directories-Dcreate all leading components of DEST except the last, orall components of --target-directory, then copy SOURCE toDEST--debugexplain how a file is copied.Implies -v-g, --group=GROUPset group ownership, instead of process' current group-m, --mode=MODEset permission mode (as in chmod), instead of rwxr-xr-x-o, --owner=OWNERset ownership (super-user only)-p, --preserve-timestampsapply access/modification times of SOURCE files tocorresponding destination files-s, --stripstrip symbol tables--strip-program=PROGRAMprogram used to strip binaries-S, --suffix=SUFFIXoverride the usual backup suffix-t, --target-directory=DIRECTORYcopy all SOURCE arguments into DIRECTORY-T, --no-target-directorytreat DEST as a normal file-v, --verboseprint the name of each created file or directory--preserve-contextpreserve SELinux security context-Zset SELinux security context of destination file and eachcreated directory to default type--context[=CTX]like -Z, or if CTX is specified then set the SELinux orSMACK security context to CTX--help display this help and exit--versionoutput version information and exitThe backup suffix is '~', unless set with --suffix orSIMPLE_BACKUP_SUFFIX.The version control method may be selectedvia the --backup option or through the VERSION_CONTROLenvironment variable.Here are the values:none, offnever make backups (even if --backup is given)numbered, tmake numbered backupsexisting, nilnumbered if numbered backups exist, simple otherwisesimple, neveralways make simple backups",
        "name": "install - copy files and set attributes",
        "section": 1
    },
    {
        "command": "ionice",
        "description": "This program sets or gets the I/O scheduling class and priorityfor a program. If no arguments or just -p is given, ionice willquery the current I/O scheduling class and priority for thatprocess.When command is given, ionice will run this command with thegiven arguments. If no class is specified, then command will beexecuted with the \"best-effort\" scheduling class. The defaultpriority level is 4.As of this writing, a process can be in one of three schedulingclasses:IdleA program running with idle I/O priority will only get disktime when no other program has asked for disk I/O for adefined grace period. The impact of an idle I/O process onnormal system activity should be zero. This scheduling classdoes not take a priority argument. Presently, this schedulingclass is permitted for an ordinary user (since kernel2.6.25).Best-effortThis is the effective scheduling class for any process thathas not asked for a specific I/O priority. This class takes apriority argument from 0-7, with a lower number being higherpriority. Programs running at the same best-effort priorityare served in a round-robin fashion.Note that before kernel 2.6.26 a process that has not askedfor an I/O priority formally uses \"none\" as scheduling class,but the I/O scheduler will treat such processes as if it werein the best-effort class. The priority within the best-effortclass will be dynamically derived from the CPU nice level ofthe process: io_priority = (cpu_nice + 20) / 5.For kernels after 2.6.26 with the CFQ I/O scheduler, aprocess that has not asked for an I/O priority inherits itsCPU scheduling class. The I/O priority is derived from theCPU nice level of the process (same as before kernel 2.6.26).RealtimeThe RT scheduling class is given first access to the disk,regardless of what else is going on in the system. Thus theRT class needs to be used with some care, as it can starveother processes. As with the best-effort class, 8 prioritylevels are defined denoting how big a time slice a givenprocess will receive on each scheduling window. Thisscheduling class is not permitted for an ordinary (i.e.,non-root) user.",
        "name": "ionice - set or get process I/O scheduling class and priority",
        "section": 1
    },
    {
        "command": "iostat",
        "description": "The iostat command is used for monitoring system input/outputdevice loading by observing the time the devices are active inrelation to their average transfer rates. The iostat commandgenerates reports that can be used to change system configurationto better balance the input/output load between physical disks.The first report generated by the iostat command providesstatistics concerning the time since the system was booted,unless the -y option is used (in this case, this first report isomitted).Each subsequent report covers the time since theprevious report. All statistics are reported each time the iostatcommand is run. The report consists of a CPU header row followedby a row of CPU statistics. On multiprocessor systems, CPUstatistics are calculated system-wide as averages among allprocessors. A device header row is displayed followed by a lineof statistics for each device that is configured.The interval parameter specifies the amount of time in secondsbetween each report. The count parameter can be specified inconjunction with the interval parameter. If the count parameteris specified, the value of count determines the number of reportsgenerated at interval seconds apart. If the interval parameter isspecified without the count parameter, the iostat commandgenerates reports continuously.",
        "name": "iostat - Report Central Processing Unit (CPU) statistics andinput/output statistics for devices and partitions.",
        "section": 1
    },
    {
        "command": "iostat2pcp",
        "description": "iostat2pcp reads a text file created with iostat(1) (infile) andtranslates this into a Performance Co-Pilot (PCP) archive withthe basename outfile.If infile is ``-'' then iostat2pcp readsfrom standard input, allowing easy preprocessing of the iostat(1)output with sed(1) or similar.The resultant PCP archive may be used with all the PCP clienttools to graph subsets of the data using pmchart(1), perform datareduction and reporting, filter with the PCP inference enginepmie(1), etc.A series of physical files will be created with the prefixoutfile.These are outfile.0 (the performance data),outfile.meta (the metadata that describes the performance data)and outfile.index (a temporal index to improve efficiency ofreplay operations for the archive).If any of these files existsalready, then iostat2pcp will not overwrite them and will exitwith an error message.The first output sample from iostat(1) contains a statisticalsummary since boot time and is ignored by iostat2pcp, so thefirst real data set is the second one in the iostat(1) output.The best results are obtained when iostat(1) was run with its own-t flag, so each output sample is prefixed with a timestamp.Even better is -t with $S_TIME_FORMAT=ISO set in environment wheniostat(1) is run, in which case the timestamp includes thetimezone.Note that if $S_TIME_FORMAT=ISO is not used with the -t optionthen iostat(1) may produce a timestamp controlled by LC_TIME fromthe locale that is in a format iostat2pcp cannot parse.Theformats for the timestamp that iostat2pcp accepts are illustratedby these examples:2013-07-06T21:34:39+1000(for the $S_TIME_FORMAT=ISO).2013-07-06 21:34:39(for some of the European formats, e.g. de_AT, de_BE, de_LUand en_DK.utf8).06/07/13 21:34:39(for all of the $LC_TIME settings for English locales outsideNorth America, e.g. en_AU, en_GB, en_IE, en_NZ, en_SG anden_ZA, and all the Spanish locales, e.g. es_ES, es_MX andes_AR).In particular, note that some common North American $LC_TIMEsettings will not work with iostat2pcp (namely, en_US, POSIX andC) because they use the MM/DD format which may be incorrectlyconverted with the assumed DD/MM format.This is another reasonto recommend setting $S_TIME_FORMAT=ISO.If there are no timestamps in the input stream, iostat2pcp willtry and deduce the sample interval if basic Disk data (-d optionfor iostat(1)) is found.If this fails, then the -t option maybe used to specify the sample interval in seconds.This optionis ignored if timestamps are found in the input stream.The -S option may be used to specify as start time for the firstreal sample in infile, where start must have the format HH:MM:SS.This option is ignored if timestamps are found in the inputstream.The -V option specifies the version for the output PCP archive.By default the archive version $PCP_ARCHIVE_VERSION (set to 2 incurrent PCP releases) is used, and the only values currentlysupported for version are 2 or 3.The -Z option may be used to specify a timezone.It must havethe format +HHMM (for hours and minutes East of UTC) or -HHMM(for hours and minutes West of UTC).Note in particular thatneither the zoneinfo (aka Olson) format, e.g. Europe/Paris, northe Posix TZ format, e.g.EST+5 is allowed for the -Z option.This option is ignored if ISO timestamps are found in the inputstream.If the timezone is not specified and cannot be deduced,it defaults to ``UTC''.Some additional diagnostic output is generated with the -voption.iostat2pcp is a Perl script that uses the PCP::LogImport Perlwrapper around the PCP libpcp_import library, and as such couldbe used as an example to develop new tools to import other typesof performance data and create PCP archives.",
        "name": "iostat2pcp - import iostat data and create a PCP archive",
        "section": 1
    },
    {
        "command": "iowatcher",
        "description": "iowatcher graphs the results of a blktrace run.It can graph theresult of an existing blktrace, start a new blktrace, or start anew blktrace and a benchmark run.It can then create an image ormovie of the IO from a given trace.iowatcher can produce eitherSVG files or movies in mp4 format (with ffmpeg) or ogg format(with png2theora).",
        "name": "iowatcher - Create visualizations from blktrace results",
        "section": 1
    },
    {
        "command": "ipcmk",
        "description": "ipcmk allows you to create System V inter-process communication(IPC) objects: shared memory segments, message queues, andsemaphore arrays.",
        "name": "ipcmk - make various IPC resources",
        "section": 1
    },
    {
        "command": "ipcrm",
        "description": "ipcrm removes System V inter-process communication (IPC) objectsand associated data structures from the system. In order todelete such objects, you must be superuser, or the creator orowner of the object.System V IPC objects are of three types: shared memory, messagequeues, and semaphores. Deletion of a message queue or semaphoreobject is immediate (regardless of whether any process stillholds an IPC identifier for the object). A shared memory objectis only removed after all currently attached processes havedetached (shmdt(2)) the object from their virtual address space.Two syntax styles are supported. The old Linux historical syntaxspecifies a three-letter keyword indicating which class of objectis to be deleted, followed by one or more IPC identifiers forobjects of this type.The SUS-compliant syntax allows the specification of zero or moreobjects of all three types in a single command line, with objectsspecified either by key or by identifier (see below). Both keysand identifiers may be specified in decimal, hexadecimal(specified with an initial '0x' or '0X'), or octal (specifiedwith an initial '0').The details of the removes are described in shmctl(2), msgctl(2),and semctl(2). The identifiers and keys can be found by usingipcs(1).",
        "name": "ipcrm - remove certain IPC resources",
        "section": 1
    },
    {
        "command": "ipcs",
        "description": "ipcs shows information on System V inter-process communicationfacilities. By default it shows information about all threeresources: shared memory segments, message queues, and semaphorearrays.",
        "name": "ipcs - show information on IPC facilities",
        "section": 1
    },
    {
        "command": "ippeveprinter",
        "description": "ippeveprinter is a simple Internet Printing Protocol (IPP) serverconforming to the IPP Everywhere (PWG 5100.14) specification. Itcan be used to test client software or act as a very basic printserver that runs a command for every job that is printed.",
        "name": "ippeveprinter - an ipp everywhere printer application for cups",
        "section": 1
    },
    {
        "command": "ippfind",
        "description": "ippfind finds services registered with a DNS server or availablethrough local devices.Its primary purpose is to find IPPprinters and show their URIs, show their current status, or runcommands.REGISTRATION TYPESippfind supports the following registration types:_http._tcpHyperText Transport Protocol (HTTP, RFC 2616)_https._tcpSecure HyperText Transport Protocol (HTTPS, RFC 2818)_ipp._tcpInternet Printing Protocol (IPP, RFC 2911)_ipps._tcpSecure Internet Printing Protocol (IPPS, draft)_printer._tcpLine Printer Daemon (LPD, RFC 1179)EXPRESSIONSippfind supports expressions much like the find(1) utility.However, unlike find(1), ippfind uses POSIX regular expressionsinstead of shell filename matching patterns.If --exec, -l,--ls, -p, --print, --print-name, -q, --quiet, -s, or -x is notspecified, ippfind adds --print to print the service URI ofanything it finds.The following expressions are supported:-d regex--domain regexTrue if the domain matches the given regular expression.--falseAlways false.-h regex--host regexTrue is the hostname matches the given regular expression.-l--ls Lists attributes returned by Get-Printer-Attributes for IPPprinters and traditional find \"-ls\" output for HTTP URLs.The result is true if the URI is accessible, falseotherwise.--localTrue if the service is local to this computer.-N name--literal-name nameTrue if the service instance name matches the given name.-n regex--name regexTrue if the service instance name matches the given regularexpression.--path regexTrue if the URI resource path matches the given regularexpression.-P number[-number]--port number[-number]True if the port matches the given number or range.-p--printPrints the URI if the result of previous expressions istrue.The result is always true.-q--quietQuiet mode - just returns the exit codes below.-r--remoteTrue if the service is not local to this computer.-s--print-namePrints the service instance name if the result of previousexpressions is true.The result is always true.--trueAlways true.-t key--txt keyTrue if the TXT record contains the named key.--txt-key regexTrue if the TXT record contains the named key and matchesthe given regular expression.-u regex--uri regexTrue if the URI matches the given regular expression.-x utility [ argument ... ] ;--exec utility [ argument ... ] ;Executes the specified program if the current result istrue.\"{foo}\" arguments are replaced with the correspondingvalue - see SUBSTITUTIONS below.Expressions may also contain modifiers:( expression )Group the result of expressions.! expression--not expressionUnary NOT of the expression.expression expressionexpression --and expressionLogical AND of expressions.expression --or expressionLogical OR of expressions.SUBSTITUTIONSThe substitutions for \"{foo}\" in -e and --exec are:{service_domain}Domain name, e.g., \"example.com.\", \"local.\", etc.{service_hostname}Fully-qualified domain name, e.g., \"printer.example.com.\",\"printer.local.\", etc.{service_name}Service instance name, e.g., \"My Fine Printer\".{service_port}Port number for server, typically 631 for IPP and 80 forHTTP.{service_regtype}DNS-SD registration type, e.g., \"_ipp._tcp\", \"_http._tcp\",etc.{service_scheme}URI scheme for DNS-SD registration type, e.g., \"ipp\",\"http\", etc.{}{service_uri}URI for service, e.g., \"ipp://printer.local./ipp/print\",\"http://printer.local./\", etc.{txt_key}Value of TXT record key (lowercase).",
        "name": "ippfind - find internet printing protocol printers",
        "section": 1
    },
    {
        "command": "ipptool",
        "description": "ipptool sends IPP requests to the specified printer-uri and testsand/or displays the results.Each named testfile defines one ormore requests, including the expected response status,attributes, and values.Output is either a plain text, formattedtext, CSV, or XML report on the standard output, with a non-zeroexit status indicating that one or more tests have failed.Thetestfile format is described in ipptoolfile(5).",
        "name": "ipptool - perform internet printing protocol requests",
        "section": 1
    },
    {
        "command": "iptables-xml",
        "description": "iptables-xml is used to convert the output of iptables-save intoan easily manipulatable XML format to STDOUT.Use I/O-redirection provided by your shell to write to a file.-c, --combinecombine consecutive rules with the same matches butdifferent targets. iptables does not currently supportmore than one target per match, so this simulates that bycollecting the targets from consecutive iptables rulesinto one action tag, but only when the rule matches areidentical. Terminating actions like RETURN, DROP, ACCEPTand QUEUE are not combined with subsequent targets.-v, --verboseOutput xml comments containing the iptables line fromwhich the XML is derivediptables-xml does a mechanistic conversion to a very expressivexml format; the only semantic considerations are for -g and -jtargets in order to discriminate between <call> <goto> and <nane-of-target> as it helps xml processing scripts if they can tellthe difference between a target like SNAT and another chain.Some sample output is:<iptables-rules><table name=\"mangle\"><chain name=\"PREROUTING\" policy=\"ACCEPT\" packet-count=\"63436\"byte-count=\"7137573\"><rule><conditions><match><p>tcp</p></match><tcp><sport>8443</sport></tcp></conditions><actions><call><check_ip/></call><ACCEPT/></actions></rule></chain></table> </iptables-rules>Conversion from XML to iptables-save format may be done using theiptables.xslt script and xsltproc, or a custom program usinglibxsltproc or similar; in this fashion:xsltproc iptables.xslt my-iptables.xml | iptables-restore",
        "name": "iptables-xml \u2014 Convert iptables-save format to XML",
        "section": 1
    },
    {
        "command": "irqtop",
        "description": "Display kernel interrupt counter information in top(1) styleview.The default output is subject to change. So whenever possible,you should avoid using default outputs in your scripts. Alwaysexplicitly define expected columns by using --output.",
        "name": "irqtop - utility to display kernel interrupt information",
        "section": 1
    },
    {
        "command": "jailcheck",
        "description": "jailcheck attaches itself to all sandboxes started by the userand performs some basic tests on the sandbox filesystem:1. Virtual directoriesjailcheck extracts a list with the main virtualdirectories installed by the sandbox.These directoriesare build by firejail at startup using --private* and--whitelist commands.2. Noexec testjailcheck inserts executable programs in /home/username,/tmp, and /var/tmp directories and tries to run them frominside the sandbox, thus testing if the directory isexecutable or not.3. Read access testjailcheck creates test files in the directories specifiedby the user and tries to read them from inside thesandbox.4. AppArmor test5. Seccomp test6. Networking testThe program is started as root using sudo.",
        "name": "jailcheck - Simple utility program to test running sandboxes",
        "section": 1
    },
    {
        "command": "join",
        "description": "For each pair of input lines with identical join fields, write aline to standard output.The default join field is the first,delimited by blanks.When FILE1 or FILE2 (not both) is -, read standard input.-a FILENUMalso print unpairable lines from file FILENUM, whereFILENUM is 1 or 2, corresponding to FILE1 or FILE2-e STRINGreplace missing (empty) input fields with STRING; I.e.,missing fields specified with '-12jo' options-i, --ignore-caseignore differences in case when comparing fields-j FIELDequivalent to '-1 FIELD -2 FIELD'-o FORMATobey FORMAT while constructing output line-t CHARuse CHAR as input and output field separator-v FILENUMlike -a FILENUM, but suppress joined output lines-1 FIELDjoin on this FIELD of file 1-2 FIELDjoin on this FIELD of file 2--check-ordercheck that the input is correctly sorted, even if allinput lines are pairable--nocheck-orderdo not check that the input is correctly sorted--headertreat the first line in each file as field headers, printthem without trying to pair them-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exitUnless -t CHAR is given, leading blanks separate fields and areignored, else fields are separated by CHAR.Any FIELD is a fieldnumber counted from 1.FORMAT is one or more comma or blankseparated specifications, each being 'FILENUM.FIELD' or '0'.Default FORMAT outputs the join field, the remaining fields fromFILE1, the remaining fields from FILE2, all separated by CHAR.If FORMAT is the keyword 'auto', then the first line of each filedetermines the number of fields output for each line.Important: FILE1 and FILE2 must be sorted on the join fields.E.g., use \"sort -k 1b,1\" if 'join' has no options, or use \"join-t ''\" if 'sort' has no options.Note, comparisons honor therules specified by 'LC_COLLATE'.If the input is not sorted andsome lines cannot be joined, a warning message will be given.",
        "name": "join - join lines of two files on a common field",
        "section": 1
    },
    {
        "command": "journalctl",
        "description": "journalctl is used to print the log entries stored in the journalby systemd-journald.service(8) andsystemd-journal-remote.service(8).If called without parameters, it will show the contents of thejournal accessible to the calling user, starting with the oldestentry collected.If one or more match arguments are passed, the output is filteredaccordingly. A match is in the format \"FIELD=VALUE\", e.g.\"_SYSTEMD_UNIT=httpd.service\", referring to the components of astructured journal entry. See systemd.journal-fields(7) for alist of well-known fields. If multiple matches are specifiedmatching different fields, the log entries are filtered by both,i.e. the resulting output will show only entries matching all thespecified matches of this kind. If two matches apply to the samefield, then they are automatically matched as alternatives, i.e.the resulting output will show entries matching any of thespecified matches for the same field. Finally, the character \"+\"may appear as a separate word between other terms on the commandline. This causes all matches before and after to be combined ina disjunction (i.e. logical OR).It is also possible to filter the entries by specifying anabsolute file path as an argument. The file path may be a file ora symbolic link and the file must exist at the time of the query.If a file path refers to an executable binary, an \"_EXE=\" matchfor the canonicalized binary path is added to the query. If afile path refers to an executable script, a \"_COMM=\" match forthe script name is added to the query. If a file path refers to adevice node, \"_KERNEL_DEVICE=\" matches for the kernel name of thedevice and for each of its ancestor devices is added to thequery. Symbolic links are dereferenced, kernel names aresynthesized, and parent devices are identified from theenvironment at the time of the query. In general, a device nodeis the best proxy for an actual device, as log entries do notusually contain fields that identify an actual device. For theresulting log entries to be correct for the actual device, therelevant parts of the environment at the time the entry waslogged, in particular the actual device corresponding to thedevice node, must have been the same as those at the time of thequery. Because device nodes generally change their correspondingdevices across reboots, specifying a device node path causes theresulting entries to be restricted to those from the currentboot.Additional constraints may be added using options --boot,--unit=, etc., to further limit what entries will be shown(logical AND).Output is interleaved from all accessible journal files, whetherthey are rotated or currently being written, and regardless ofwhether they belong to the system itself or are accessible userjournals. The --header option can be used to identify which filesare being shown.The set of journal files which will be used can be modified usingthe --user, --system, --directory, and --file options, see below.All users are granted access to their private per-user journals.However, by default, only root and users who are members of a fewspecial groups are granted access to the system journal and thejournals of other users. Members of the groups \"systemd-journal\",\"adm\", and \"wheel\" can read all journal files. Note that the twolatter groups traditionally have additional privileges specifiedby the distribution. Members of the \"wheel\" group can oftenperform administrative tasks.The output is paged through less by default, and long lines are\"truncated\" to screen width. The hidden part can be viewed byusing the left-arrow and right-arrow keys. Paging can bedisabled; see the --no-pager option and the \"Environment\" sectionbelow.When outputting to a tty, lines are colored according topriority: lines of level ERROR and higher are colored red; linesof level NOTICE and higher are highlighted; lines of level DEBUGare colored lighter grey; other lines are displayed normally.To write entries to the journal, a few methods may be used. Ingeneral, output from systemd units is automatically connected tothe journal, see systemd-journald.service(8). In addition,systemd-cat(1) may be used to send messages to the journaldirectly.",
        "name": "journalctl - Print log entries from the systemd journal",
        "section": 1
    },
    {
        "command": "kbd_mode",
        "description": "Without argument, kbd_mode prints the current keyboard mode (RAW,MEDIUMRAW or XLATE).With argument, it sets the keyboard mode asindicated:-s: scancode mode (RAW),-k: keycode mode (MEDIUMRAW),-a: ASCII mode (XLATE),-u: UTF-8 mode (UNICODE).Of course the \"-a\" is only traditional, and the code used can beany 8-bit character set.With \"-u\" a 16-bit character set isexpected, and these chars are transmitted to the kernel as 1, 2,or 3 bytes (following the UTF-8 coding).In these latter twomodes the key mapping defined by loadkeys(1) is used.kbd_mode operates on the console specified by the \"-C\" option; ifthere is none, the console associated with stdin is used.Warning: changing the keyboard mode, other than between ASCII andUnicode, will probably make your keyboard unusable. Set the \"-f\"option to force such changes.This command is only meant for use(say via remote login) when some program left your keyboard inthe wrong state.Note that in some obsolete versions of thisprogram the \"-u\" option was a synonym for \"-s\" and older versionsof this program may not recognize the \"-f\" option.",
        "name": "kbd_mode - report or set the keyboard mode",
        "section": 1
    },
    {
        "command": "kbdinfo",
        "description": "The utility allows you to read and check various parameters ofthe keyboard and virtual console.getmodeGet or check virtual console mode.gkbmodeGets current keyboard mode.rawRaw (scancode) mode. These are the raw codesgenerated by the keyboard.mediumrawMedium raw (scancode) mode. This is extended mediumraw mode, with keys above 127 encoded as 0, high 7bits, low 7 bits, with the 0 bearing the 'up' flagif needed. 0 is reserved, so this shouldn'tinterfere with anything else. The two bytes after 0will always have the up flag set not to interferewith older applications. This allows for 16384different keycodes, which should be enough.xlateTranslate keycodes using keymap. These are thecodes generated via the current keysym mapping.unicodeUnicode mode.gkbmetaGets meta key handling mode.escprefixSpecifies if pressing the meta (alt) key generatesan ESC (\\033) prefix followed by the keysym.metabitThe keysym marked with the high bit set.gkbledGet keyboard flags CapsLock, NumLock, ScrollLock (notlights).scrolllockThe scroll lock is down.numlockThe num lock is down.capslockThe caps lock is down.",
        "name": "kbdinfo - read information about keyboard state",
        "section": 1
    },
    {
        "command": "keyctl",
        "description": "This program is used to control the key management facility invarious ways using a variety of subcommands.",
        "name": "keyctl - key management facility control",
        "section": 1
    },
    {
        "command": "kill",
        "description": "The command kill sends the specified signal to the specifiedprocesses or process groups.If no signal is specified, the TERM signal is sent. The defaultaction for this signal is to terminate the process. This signalshould be used in preference to the KILL signal (number 9), sincea process may install a handler for the TERM signal in order toperform clean-up steps before terminating in an orderly fashion.If a process does not terminate after a TERM signal has beensent, then the KILL signal may be used; be aware that the lattersignal cannot be caught, and so does not give the target processthe opportunity to perform any clean-up before terminating.Most modern shells have a builtin kill command, with a usagerather similar to that of the command described here. The --all,--pid, and --queue options, and the possibility to specifyprocesses by command name, are local extensions.If signal is 0, then no actual signal is sent, but error checkingis still performed.",
        "name": "kill - terminate a process",
        "section": 1
    },
    {
        "command": "killall",
        "description": "killall sends a signal to all processes running any of thespecified commands.If no signal name is specified, SIGTERM issent.Signals can be specified either by name (e.g.-HUP or -SIGHUP)or by number (e.g.-1) or by option -s.If the command name is not regular expression (option -r) andcontains a slash (/), processes executing that particular filewill be selected for killing, independent of their name.killall returns a zero return code if at least one process hasbeen killed for each listed command, or no commands were listedand at least one process matched the -u and -Z search criteria.killall returns non-zero otherwise.A killall process never kills itself (but may kill other killallprocesses).",
        "name": "killall - kill processes by name",
        "section": 1
    },
    {
        "command": "kmidiff",
        "description": null,
        "name": "kmidiff - compare KMIs of Linux Kernel treeskmidiff compares the binary Kernel Module Interfaces of two LinuxKernel trees.The binary KMI is the interface that the LinuxKernel exposes to its modules.The trees we are interested inhere are the result of the build of the Linux Kernel source tree.",
        "section": 1
    },
    {
        "command": "last",
        "description": "last looks through the file wtmp (which records alllogins/logouts) and prints information about connect times ofusers. Records are printed from most recent to least recent.Records can be specified by tty and username.tty names can beabbreviated:last 0is equivalent tolast tty0.Multiple arguments can be specified:last root consolewill print all of the entries for the user root and all entrieslogged in on the console tty.The special users reboot and shutdown log in when the systemreboots or (surprise) shuts down.last rebootwill produce a record of reboot times.If last is interrupted by a quit signal, it prints out how farits search in the wtmp file had reached and then quits.",
        "name": "last - list logins on the system",
        "section": 1
    },
    {
        "command": "lastb",
        "description": "last looks through the file wtmp (which records alllogins/logouts) and prints information about connect times ofusers. Records are printed from most recent to least recent.Records can be specified by tty and username.tty names can beabbreviated:last 0is equivalent tolast tty0.Multiple arguments can be specified:last root consolewill print all of the entries for the user root and all entrieslogged in on the console tty.The special users reboot and shutdown log in when the systemreboots or (surprise) shuts down.last rebootwill produce a record of reboot times.If last is interrupted by a quit signal, it prints out how farits search in the wtmp file had reached and then quits.",
        "name": "last - list logins on the system",
        "section": 1
    },
    {
        "command": "lastcomm",
        "description": "lastcomm prints out information about previously executedcommands.If no arguments are specified, lastcomm will printinfo about all of the commands in acct (the record file).Ifcalled with one or more of command-name, user-name, or terminal-name, only records containing those items will be displayed.Forexample, to find out which users used command `a.out' and whichusers were logged into `tty0', type:lastcomm a.out tty0This will print any entry for which `a.out' or `tty0' matches inany of the record's fields (command, name, or terminal).If youwant to find only items that match *all* of the arguments on thecommand line, you must use the '-strict-match' option.Forexample, to list all of the executions of command a.out by userroot on terminal tty0, type:lastcomm --strict-match --command a.out --user root --tty tty0The order of the arguments is not important.For each entry the following information is printed:+ command name of the process+ flags, as recorded by the system accounting routines:S -- command executed by super-userF -- command executed after a fork but without afollowing execC -- command run in PDP-11 compatibility mode (VAX only)D -- command terminated with the generation of a corefileX -- command was terminated with the signal SIGTERM+ the name of the user who ran the process+ time the process started",
        "name": "lastcomm -print out information about previously executedcommands.",
        "section": 1
    },
    {
        "command": "ld",
        "description": "ld combines a number of object and archive files, relocates theirdata and ties up symbol references. Usually the last step incompiling a program is to run ld.ld accepts Linker Command Language files written in a superset ofAT&T's Link Editor Command Language syntax, to provide explicitand total control over the linking process.This man page does not describe the command language; see the ldentry in \"info\" for full details on the command language and onother aspects of the GNU linker.This version of ld uses the general purpose BFD libraries tooperate on object files. This allows ld to read, combine, andwrite object files in many different formats---for example, COFFor \"a.out\".Different formats may be linked together to produceany available kind of object file.Aside from its flexibility, the GNU linker is more helpful thanother linkers in providing diagnostic information.Many linkersabandon execution immediately upon encountering an error;whenever possible, ld continues executing, allowing you toidentify other errors (or, in some cases, to get an output filein spite of the error).The GNU linker ld is meant to cover a broad range of situations,and to be as compatible as possible with other linkers.As aresult, you have many choices to control its behavior.",
        "name": "ld - The GNU linker",
        "section": 1
    },
    {
        "command": "ldapadd",
        "description": "ldapmodify is a shell-accessible interface to theldap_add_ext(3), ldap_modify_ext(3), ldap_delete_ext(3) andldap_rename(3).library calls.ldapadd is implemented as a hardlink to the ldapmodify tool.When invoked as ldapadd the -a (addnew entry) flag is turned on automatically.ldapmodify opens a connection to an LDAP server, binds, andmodifies or adds entries.The entry information is read fromstandard input or from file through the use of the -f option.",
        "name": "ldapmodify, ldapadd - LDAP modify entry and LDAP add entry tools",
        "section": 1
    },
    {
        "command": "ldapcompare",
        "description": "ldapcompare is a shell-accessible interface to theldap_compare_ext(3) library call.ldapcompare opens a connection to an LDAP server, binds, andperforms a compare using specified parameters.The DN should bea distinguished name in the directory.Attr should be a knownattribute.If followed by one colon, the assertion value shouldbe provided as a string.If followed by two colons, the base64encoding of the value is provided.The result code of thecompare is provided as the exit code and, unless ran with -z, theprogram prints TRUE, FALSE, or UNDEFINED on standard output.",
        "name": "ldapcompare - LDAP compare tool",
        "section": 1
    },
    {
        "command": "ldapdelete",
        "description": "ldapdelete is a shell-accessible interface to theldap_delete_ext(3) library call.ldapdelete opens a connection to an LDAP server, binds, anddeletes one or more entries.If one or more DN arguments areprovided, entries with those Distinguished Names are deleted.Each DN should be provided using the LDAPv3 string representationas defined in RFC 4514.If no DN arguments are provided, a listof DNs is read from standard input (or from file if the -f flagis used).",
        "name": "ldapdelete - LDAP delete entry tool",
        "section": 1
    },
    {
        "command": "ldapexop",
        "description": "ldapexop issues the LDAP extended operation specified by oid orone of the special keywords whoami, cancel, or refresh.Additional data for the extended operation can be passed to theserver using data or base-64 encoded as b64data in the case ofoid, or using the additional parameters in the case of thespecially named extended operations above.Please note that ldapexop behaves differently for the sameextended operation when it was given as an OID or as a speciallynamed operation:Calling ldapexop with the OID of the whoami (RFC 4532) extendedoperationldapexop [<options>] 1.3.6.1.4.1.4203.1.11.3yields# extended operation responsedata:: <base64 encoded response data>while calling it with the keyword whoamildapexop [<options>] whoamiresults indn:<client's identity>",
        "name": "ldapexop - issue LDAP extended operations",
        "section": 1
    },
    {
        "command": "ldapmodify",
        "description": "ldapmodify is a shell-accessible interface to theldap_add_ext(3), ldap_modify_ext(3), ldap_delete_ext(3) andldap_rename(3).library calls.ldapadd is implemented as a hardlink to the ldapmodify tool.When invoked as ldapadd the -a (addnew entry) flag is turned on automatically.ldapmodify opens a connection to an LDAP server, binds, andmodifies or adds entries.The entry information is read fromstandard input or from file through the use of the -f option.",
        "name": "ldapmodify, ldapadd - LDAP modify entry and LDAP add entry tools",
        "section": 1
    },
    {
        "command": "ldapmodrdn",
        "description": "ldapmodrdn is a shell-accessible interface to the ldap_rename(3)library call.ldapmodrdn opens a connection to an LDAP server, binds, andmodifies the RDN of entries.The entry information is read fromstandard input, from file through the use of the -f option, orfrom the command-line pair dn and rdn.",
        "name": "ldapmodrdn - LDAP rename entry tool",
        "section": 1
    },
    {
        "command": "ldappasswd",
        "description": "ldappasswd is a tool to set the password of an LDAP user.ldappasswd uses the LDAPv3 Password Modify (RFC 3062) extendedoperation.ldappasswd sets the password of associated with the user [or anoptionally specified user].If the new password is not specifiedon the command line and the user doesn't enable prompting, theserver will be asked to generate a password for the user.ldappasswd is neither designed nor intended to be a replacementfor passwd(1) and should not be installed as such.",
        "name": "ldappasswd - change the password of an LDAP entry",
        "section": 1
    },
    {
        "command": "ldapsearch",
        "description": "ldapsearch is a shell-accessible interface to theldap_search_ext(3) library call.ldapsearch opens a connection to an LDAP server, binds, andperforms a search using specified parameters.The filter shouldconform to the string representation for search filters asdefined in RFC 4515.If not provided, the default filter,(objectClass=*), is used.If ldapsearch finds one or more entries, the attributes specifiedby attrs are returned.If * is listed, all user attributes arereturned.If + is listed, all operational attributes arereturned.If no attrs are listed, all user attributes arereturned.If only 1.1 is listed, no attributes will be returned.The search results are displayed using an extended version ofLDIF.Option -L controls the format of the output.",
        "name": "ldapsearch - LDAP search tool",
        "section": 1
    },
    {
        "command": "ldapurl",
        "description": "ldapurl is a command that allows one to either compose ordecompose LDAP URIs.When invoked with the -H option, ldapurl extracts the componentsof the ldapuri option argument, unescaping hex-escaped chars asrequired.It basically acts as a frontend to theldap_url_parse(3) call.Otherwise, it builds an LDAP URI basedon the components passed with the appropriate options, performingthe inverse operation.Option -H is incompatible with options-a, -b, -E, -f, -H, -h, -p, -S, and -s.",
        "name": "ldapurl - LDAP URL formatting tool",
        "section": 1
    },
    {
        "command": "ldapvc",
        "description": "ldapvc implements the LDAP \"Verify Credentials\" extendedoperation.Verify Credentials operation behaves like LDAP Bind but has noimpact upon the underlying LDAP session.",
        "name": "ldapvc - LDAP verify credentials tool",
        "section": 1
    },
    {
        "command": "ldapwhoami",
        "description": "ldapwhoami implements the LDAP \"Who Am I?\" extended operation.ldapwhoami opens a connection to an LDAP server, binds, andperforms a whoami operation.",
        "name": "ldapwhoami - LDAP who am i? tool",
        "section": 1
    },
    {
        "command": "ldd",
        "description": "ldd prints the shared objects (shared libraries) required by eachprogram or shared object specified on the command line.Anexample of its use and output is the following:$ ldd /bin/lslinux-vdso.so.1 (0x00007ffcc3563000)libselinux.so.1 => /lib64/libselinux.so.1 (0x00007f87e5459000)libcap.so.2 => /lib64/libcap.so.2 (0x00007f87e5254000)libc.so.6 => /lib64/libc.so.6 (0x00007f87e4e92000)libpcre.so.1 => /lib64/libpcre.so.1 (0x00007f87e4c22000)libdl.so.2 => /lib64/libdl.so.2 (0x00007f87e4a1e000)/lib64/ld-linux-x86-64.so.2 (0x00005574bf12e000)libattr.so.1 => /lib64/libattr.so.1 (0x00007f87e4817000)libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f87e45fa000)In the usual case, ldd invokes the standard dynamic linker (seeld.so(8)) with the LD_TRACE_LOADED_OBJECTS environment variableset to 1.This causes the dynamic linker to inspect theprogram's dynamic dependencies, and find (according to the rulesdescribed in ld.so(8)) and load the objects that satisfy thosedependencies.For each dependency, ldd displays the location ofthe matching object and the (hexadecimal) address at which it isloaded.(The linux-vdso and ld-linux shared dependencies arespecial; see vdso(7) and ld.so(8).)SecurityBe aware that in some circumstances (e.g., where the programspecifies an ELF interpreter other than ld-linux.so), someversions of ldd may attempt to obtain the dependency informationby attempting to directly execute the program, which may lead tothe execution of whatever code is defined in the program's ELFinterpreter, and perhaps to execution of the program itself.(Before glibc 2.27, the upstream ldd implementation did this forexample, although most distributions provided a modified versionthat did not.)Thus, you should never employ ldd on an untrusted executable,since this may result in the execution of arbitrary code.Asafer alternative when dealing with untrusted executables is:$ objdump -p /path/to/program | grep NEEDEDNote, however, that this alternative shows only the directdependencies of the executable, while ldd shows the entiredependency tree of the executable.",
        "name": "ldd - print shared object dependencies",
        "section": 1
    },
    {
        "command": "less",
        "description": "Less is a program similar to more(1), but which allows backwardmovement in the file as well as forward movement.Also, lessdoes not have to read the entire input file before starting, sowith large input files it starts up faster than text editors likevi(1).Less uses termcap (or terminfo on some systems), so itcan run on a variety of terminals.There is even limited supportfor hardcopy terminals.(On a hardcopy terminal, lines whichshould be printed at the top of the screen are prefixed with acaret.)Commands are based on both more and vi.Commands may be precededby a decimal number, called N in the descriptions below.Thenumber is used by some commands, as indicated.",
        "name": "less - opposite of more",
        "section": 1
    },
    {
        "command": "lessecho",
        "description": "lessecho is a program that simply echos its arguments on standardoutput.But any metacharacter in the output is preceded by an\"escape\" character, which by default is a backslash.",
        "name": "lessecho - expand metacharacters",
        "section": 1
    },
    {
        "command": "lesskey",
        "description": "A lesskey file specifies a set of key bindings and environmentvariables to be used by subsequent invocations of less.",
        "name": "lesskey - customize key bindings for less",
        "section": 1
    },
    {
        "command": "lexgrog",
        "description": "lexgrog is an implementation of the traditional \u201cgroff guess\u201dutility in lex.It reads the list of files on its command lineas either man page source files or preformatted \u201ccat\u201d pages, anddisplays their name and description as used by apropos andwhatis, the list of preprocessing filters required by the manpage before it is passed to nroff or troff, or both.If its input is badly formatted, lexgrog will print \u201cparsefailed\u201d; this may be useful for external programs that need tocheck man pages for correctness.If one of lexgrog's input filesis \u201c-\u201d, it will read from standard input; if any input file iscompressed, a decompressed version will be read automatically.",
        "name": "lexgrog - parse header information in man pages",
        "section": 1
    },
    {
        "command": "link",
        "description": "Call the link function to create a link named FILE2 to anexisting FILE1.--help display this help and exit--versionoutput version information and exit",
        "name": "link - call the link function to create a link to a file",
        "section": 1
    },
    {
        "command": "lkbib",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "ln",
        "description": "In the 1st form, create a link to TARGET with the name LINK_NAME.In the 2nd form, create a link to TARGET in the currentdirectory.In the 3rd and 4th forms, create links to each TARGETin DIRECTORY.Create hard links by default, symbolic links with--symbolic.By default, each destination (name of new link)should not already exist.When creating hard links, each TARGETmust exist.Symbolic links can hold arbitrary text; if laterresolved, a relative link is interpreted in relation to itsparent directory.Mandatory arguments to long options are mandatory for shortoptions too.--backup[=CONTROL]make a backup of each existing destination file-blike --backup but does not accept an argument-d, -F, --directoryallow the superuser to attempt to hard link directories(note: will probably fail due to system restrictions, evenfor the superuser)-f, --forceremove existing destination files-i, --interactiveprompt whether to remove destinations-L, --logicaldereference TARGETs that are symbolic links-n, --no-dereferencetreat LINK_NAME as a normal file if it is a symbolic linkto a directory-P, --physicalmake hard links directly to symbolic links-r, --relativewith -s, create links relative to link location-s, --symbolicmake symbolic links instead of hard links-S, --suffix=SUFFIXoverride the usual backup suffix-t, --target-directory=DIRECTORYspecify the DIRECTORY in which to create the links-T, --no-target-directorytreat LINK_NAME as a normal file always-v, --verboseprint name of each linked file--help display this help and exit--versionoutput version information and exitThe backup suffix is '~', unless set with --suffix orSIMPLE_BACKUP_SUFFIX.The version control method may be selectedvia the --backup option or through the VERSION_CONTROLenvironment variable.Here are the values:none, offnever make backups (even if --backup is given)numbered, tmake numbered backupsexisting, nilnumbered if numbered backups exist, simple otherwisesimple, neveralways make simple backupsUsing -s ignores -L and -P.Otherwise, the last option specifiedcontrols behavior when a TARGET is a symbolic link, defaulting to-P.",
        "name": "ln - make links between files",
        "section": 1
    },
    {
        "command": "loadkeys",
        "description": "The program loadkeys reads the file or files specified byFILENAME....Its main purpose is to load the kernel keymap forthe console.You can specify console device by the -C (or--console ) option.",
        "name": "loadkeys - load keyboard translation tables",
        "section": 1
    },
    {
        "command": "locale",
        "description": "The locale command displays information about the current locale,or all locales, on standard output.When invoked without arguments, locale displays the currentlocale settings for each locale category (see locale(5)), basedon the settings of the environment variables that control thelocale (see locale(7)).Values for variables set in theenvironment are printed without double quotes, implied values areprinted with double quotes.If either the -a or the -m option (or one of their long-formatequivalents) is specified, the behavior is as follows:-a, --all-localesDisplay a list of all available locales.The -v optioncauses the LC_IDENTIFICATION metadata about each locale tobe included in the output.-m, --charmapsDisplay the available charmaps (character set descriptionfiles).To display the current character set for thelocale, use locale -c charmap.The locale command can also be provided with one or morearguments, which are the names of locale keywords (for example,date_fmt, ctype-class-names, yesexpr, or decimal_point) or localecategories (for example, LC_CTYPE or LC_TIME).For eachargument, the following is displayed:\u2022For a locale keyword, the value of that keyword to bedisplayed.\u2022For a locale category, the values of all keywords in thatcategory are displayed.When arguments are supplied, the following options aremeaningful:-c, --category-nameFor a category name argument, write the name of the localecategory on a separate line preceding the list of keywordvalues for that category.For a keyword name argument, write the name of the localecategory for this keyword on a separate line preceding thekeyword value.This option improves readability when multiple namearguments are specified.It can be combined with the -koption.-k, --keyword-nameFor each keyword whose value is being displayed, includealso the name of that keyword, so that the output has theformat:keyword=\"value\"The locale command also knows about the following options:-v, --verboseDisplay additional information for some command-lineoption and argument combinations.-?, --helpDisplay a summary of command-line options and argumentsand exit.--usageDisplay a short usage message and exit.-V, --versionDisplay the program version and exit.",
        "name": "locale - get locale-specific information",
        "section": 1
    },
    {
        "command": "localectl",
        "description": "localectl may be used to query and change the system locale andkeyboard layout settings. It communicates with systemd-localed(8)to modify files such as /etc/locale.conf and /etc/vconsole.conf.The system locale controls the language settings of systemservices and of the UI before the user logs in, such as thedisplay manager, as well as the default for users after login.The keyboard settings control the keyboard layout used on thetext console and of the graphical UI before the user logs in,such as the display manager, as well as the default for usersafter login.Note that the changes performed using this tool might require theinitrd to be rebuilt to take effect during early system boot. Theinitrd is not rebuilt automatically by localectl, this task hasto be performed manually, usually using a tool like dracut(8).Note that systemd-firstboot(1) may be used to initialize thesystem locale for mounted (but not booted) system images.",
        "name": "localectl - Control the system locale and keyboard layoutsettings",
        "section": 1
    },
    {
        "command": "localedef",
        "description": "The localedef program reads the indicated charmap and inputfiles, compiles them to a binary form quickly usable by thelocale functions in the C library (setlocale(3), localeconv(3),etc.), and places the output in outputpath.The outputpath argument is interpreted as follows:\u2022If outputpath contains a slash character ('/'), it isinterpreted as the name of the directory where the outputdefinitions are to be stored.In this case, there is aseparate output file for each locale category (LC_TIME,LC_NUMERIC, and so on).\u2022If the --no-archive option is used, outputpath is the name ofa subdirectory in /usr/lib/locale where per-category compiledfiles are placed.\u2022Otherwise, outputpath is the name of a locale and the compiledlocale data is added to the archive file/usr/lib/locale/locale-archive.A locale archive is a memory-mapped file which contains all the system-provided locales; itis used by all localized programs when the environmentvariable LOCPATH is not set.In any case, localedef aborts if the directory in which it triesto write locale files has not already been created.If no charmapfile is given, the value ANSI_X3.4-1968 (for ASCII)is used by default.If no inputfile is given, or if it is givenas a dash (-), localedef reads from standard input.",
        "name": "localedef - compile locale definition files",
        "section": 1
    },
    {
        "command": "locate",
        "description": "This manual page documents the GNU version of locate.For eachgiven pattern, locate searches one or more databases of filenames and displays the file names that contain the pattern.Patterns can contain shell-style metacharacters: `*', `?', and`[]'.The metacharacters do not treat `/' or `.'specially.Therefore, a pattern `foo*bar' can match a file name thatcontains `foo3/bar', and a pattern `*duck*' can match a file namethat contains `lake/.ducky'.Patterns that containmetacharacters should be quoted to protect them from expansion bythe shell.If a pattern is a plain string \u2014 it contains no metacharacters \u2014locate displays all file names in the database that contain thatstring anywhere.If a pattern does contain metacharacters,locate only displays file names that match the pattern exactly.As a result, patterns that contain metacharacters should usuallybegin with a `*', and will most often end with one as well.Theexceptions are patterns that are intended to explicitly match thebeginning or end of a file name.The file name databases contain lists of files that were on thesystem when the databases were last updated.The systemadministrator can choose the file name of the default database,the frequency with which the databases are updated, and thedirectories for which they contain entries; see updatedb(1).If locate's output is going to a terminal, unusual characters inthe output are escaped in the same way as for the -print actionof the find command.If the output is not going to a terminal,file names are printed exactly as-is.",
        "name": "locate - list files in databases that match a pattern",
        "section": 1
    },
    {
        "command": "logger",
        "description": "logger makes entries in the system log.When the optional message argument is present, it is written tothe log. If it is not present, and the -f option is not giveneither, then standard input is logged.",
        "name": "logger - enter messages into the system log",
        "section": 1
    },
    {
        "command": "login",
        "description": "login is used when signing onto a system. If no argument isgiven, login prompts for the username.The user is then prompted for a password, where appropriate.Echoing is disabled to prevent revealing the password. Only anumber of password failures are permitted before login exits andthe communications link is severed. See LOGIN_RETRIES in theCONFIG FILE ITEMS section.If password aging has been enabled for the account, the user maybe prompted for a new password before proceeding. In such caseold password must be provided and the new password entered beforecontinuing. Please refer to passwd(1) for more information.The user and group ID will be set according to their values inthe /etc/passwd file. There is one exception if the user ID iszero. In this case, only the primary group ID of the account isset. This should allow the system administrator to login even incase of network problems. The environment variable values for$HOME, $USER, $SHELL, $PATH, $LOGNAME, and $MAIL are setaccording to the appropriate fields in the password entry. $PATHdefaults to /usr/local/bin:/bin:/usr/bin for normal users, and to/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin forroot, if not otherwise configured.The environment variable $TERM will be preserved, if it exists,else it will be initialized to the terminal type on your tty.Other environment variables are preserved if the -p option isgiven.The environment variables defined by PAM are always preserved.Then the user\u2019s shell is started. If no shell is specified forthe user in /etc/passwd, then /bin/sh is used. If there is nohome directory specified in /etc/passwd, then / is used, followedby .hushlogin check as described below.If the file .hushlogin exists, then a \"quiet\" login is performed.This disables the checking of mail and the printing of the lastlogin time and message of the day. Otherwise, if /var/log/lastlogexists, the last login time is printed, and the current login isrecorded.",
        "name": "login - begin session on the system",
        "section": 1
    },
    {
        "command": "loginctl",
        "description": "loginctl may be used to introspect and control the state of thesystemd(1) login manager systemd-logind.service(8).",
        "name": "loginctl - Control the systemd login manager",
        "section": 1
    },
    {
        "command": "logname",
        "description": "Print the user's login name.--help display this help and exit--versionoutput version information and exit",
        "name": "logname - print user\u00b4s login name",
        "section": 1
    },
    {
        "command": "look",
        "description": "The look utility displays any lines in file which contain stringas a prefix. As look performs a binary search, the lines in filemust be sorted (where sort(1) was given the same options -dand/or -f that look is invoked with).If file is not specified, the file /usr/share/dict/words is used,only alphanumeric characters are compared and the case ofalphabetic characters is ignored.",
        "name": "look - display lines beginning with a given string",
        "section": 1
    },
    {
        "command": "lookbib",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "lp",
        "description": "lp submits files for printing or alters a pending job.Use afilename of \"-\" to force printing from the standard input.THE DEFAULT DESTINATIONCUPS provides many ways to set the default destination. TheLPDEST and PRINTER environment variables are consulted first.Ifneither are set, the current default set using the lpoptions(1)command is used, followed by the default set using the lpadmin(8)command.",
        "name": "lp - print files",
        "section": 1
    },
    {
        "command": "lpoptions",
        "description": "lpoptions displays or sets printer options and defaults.If noprinter is specified using the -p option, the default printer isused as described in lp(1).If no -l, -o, or -r options are specified, the current optionsare reported on the standard output.Options set with the lpoptions command are used by the lp(1) andlpr(1) commands when submitting jobs.When run by the root user, lpoptions gets and sets defaultoptions and instances for all users in the /etc/cups/lpoptionsfile.Otherwise, the per-user defaults are managed in the~/.cups/lpoptions file.",
        "name": "lpoptions - display or set printer options and defaults",
        "section": 1
    },
    {
        "command": "lpq",
        "description": "lpq shows the current print queue status on the named printer.Jobs queued on the default destination will be shown if noprinter or class is specified on the command-line.The +interval option allows you to continuously report the jobsin the queue until the queue is empty; the list of jobs is shownonce every interval seconds.",
        "name": "lpq - show printer queue status",
        "section": 1
    },
    {
        "command": "lpr",
        "description": "lpr submits files for printing.Files named on the command lineare sent to the named printer or the default destination if nodestination is specified.If no files are listed on the command-line, lpr reads the print file from the standard input.THE DEFAULT DESTINATIONCUPS provides many ways to set the default destination. TheLPDEST and PRINTER environment variables are consulted first.Ifneither are set, the current default set using the lpoptions(1)command is used, followed by the default set using the lpadmin(8)command.",
        "name": "lpr - print files",
        "section": 1
    },
    {
        "command": "lprm",
        "description": "lprm cancels print jobs that have been queued for printing.Ifno arguments are supplied, the current job on the defaultdestination is canceled.You can specify one or more job IDnumbers to cancel those jobs or use the - option to cancel alljobs.",
        "name": "lprm - cancel print jobs",
        "section": 1
    },
    {
        "command": "lpstat",
        "description": "lpstat displays status information about the current classes,jobs, and printers.When run with no arguments, lpstat will listactive jobs queued by the current user.",
        "name": "lpstat - print cups status information",
        "section": 1
    },
    {
        "command": "ls",
        "description": "List information about the FILEs (the current directory bydefault).Sort entries alphabetically if none of -cftuvSUX nor--sort is specified.Mandatory arguments to long options are mandatory for shortoptions too.-a, --alldo not ignore entries starting with .-A, --almost-alldo not list implied . and ..--authorwith -l, print the author of each file-b, --escapeprint C-style escapes for nongraphic characters--block-size=SIZEwith -l, scale sizes by SIZE when printing them; e.g.,'--block-size=M'; see SIZE format below-B, --ignore-backupsdo not list implied entries ending with ~-cwith -lt: sort by, and show, ctime (time of last change offile status information); with -l: show ctime and sort byname; otherwise: sort by ctime, newest first-Clist entries by columns--color[=WHEN]color the output WHEN; more info below-d, --directorylist directories themselves, not their contents-D, --diredgenerate output designed for Emacs' dired mode-flist all entries in directory order-F, --classify[=WHEN]append indicator (one of */=>@|) to entries WHEN--file-typelikewise, except do not append '*'--format=WORDacross -x, commas -m, horizontal -x, long -l,single-column -1, verbose -l, vertical -C--full-timelike -l --time-style=full-iso-glike -l, but do not list owner--group-directories-firstgroup directories before files; can be augmented with a--sort option, but any use of --sort=none (-U) disablesgrouping-G, --no-groupin a long listing, don't print group names-h, --human-readablewith -l and -s, print sizes like 1K 234M 2G etc.--silikewise, but use powers of 1000 not 1024-H, --dereference-command-linefollow symbolic links listed on the command line--dereference-command-line-symlink-to-dirfollow each command line symbolic link that points to adirectory--hide=PATTERNdo not list implied entries matching shell PATTERN(overridden by -a or -A)--hyperlink[=WHEN]hyperlink file names WHEN--indicator-style=WORDappend indicator with style WORD to entry names: none(default), slash (-p), file-type (--file-type), classify(-F)-i, --inodeprint the index number of each file-I, --ignore=PATTERNdo not list implied entries matching shell PATTERN-k, --kibibytesdefault to 1024-byte blocks for file system usage; usedonly with -s and per directory totals-luse a long listing format-L, --dereferencewhen showing file information for a symbolic link, showinformation for the file the link references rather thanfor the link itself-mfill width with a comma separated list of entries-n, --numeric-uid-gidlike -l, but list numeric user and group IDs-N, --literalprint entry names without quoting-olike -l, but do not list group information-p, --indicator-style=slashappend / indicator to directories-q, --hide-control-charsprint ? instead of nongraphic characters--show-control-charsshow nongraphic characters as-is (the default, unlessprogram is 'ls' and output is a terminal)-Q, --quote-nameenclose entry names in double quotes--quoting-style=WORDuse quoting style WORD for entry names: literal, locale,shell, shell-always, shell-escape, shell-escape-always, c,escape (overrides QUOTING_STYLE environment variable)-r, --reversereverse order while sorting-R, --recursivelist subdirectories recursively-s, --sizeprint the allocated size of each file, in blocks-Ssort by file size, largest first--sort=WORDsort by WORD instead of name: none (-U), size (-S), time(-t), version (-v), extension (-X), width--time=WORDselect which timestamp used to display or sort; accesstime (-u): atime, access, use; metadata change time (-c):ctime, status; modified time (default): mtime,modification; birth time: birth, creation;with -l, WORD determines which time to show; with--sort=time, sort by WORD (newest first)--time-style=TIME_STYLEtime/date format with -l; see TIME_STYLE below-tsort by time, newest first; see --time-T, --tabsize=COLSassume tab stops at each COLS instead of 8-uwith -lt: sort by, and show, access time; with -l: showaccess time and sort by name; otherwise: sort by accesstime, newest first-Udo not sort; list entries in directory order-vnatural sort of (version) numbers within text-w, --width=COLSset output width to COLS.0 means no limit-xlist entries by lines instead of by columns-Xsort alphabetically by entry extension-Z, --contextprint any security context of each file--zero end each output line with NUL, not newline-1list one file per line--help display this help and exit--versionoutput version information and exitThe SIZE argument is an integer and optional unit (example: 10Kis 10*1024).Units are K,M,G,T,P,E,Z,Y,R,Q (powers of 1024) orKB,MB,... (powers of 1000).Binary prefixes can be used, too:KiB=K, MiB=M, and so on.The TIME_STYLE argument can be full-iso, long-iso, iso, locale,or +FORMAT.FORMAT is interpreted like in date(1).If FORMAT isFORMAT1<newline>FORMAT2, then FORMAT1 applies to non-recent filesand FORMAT2 to recent files.TIME_STYLE prefixed with 'posix-'takes effect only outside the POSIX locale.Also the TIME_STYLEenvironment variable sets the default style to use.The WHEN argument defaults to 'always' and can also be 'auto' or'never'.Using color to distinguish file types is disabled both by defaultand with --color=never.With --color=auto, ls emits color codesonly when standard output is connected to a terminal.TheLS_COLORS environment variable can change the settings.Use thedircolors(1) command to set it.Exit status:0if OK,1if minor problems (e.g., cannot access subdirectory),2if serious trouble (e.g., cannot access command-lineargument).",
        "name": "ls - list directory contents",
        "section": 1
    },
    {
        "command": "lsattr",
        "description": "lsattr lists the file attributes on a second extended filesystem.See chattr(1) for a description of the attributes andwhat they mean.",
        "name": "lsattr - list file attributes on a Linux second extended filesystem",
        "section": 1
    },
    {
        "command": "lscpu",
        "description": "lscpu gathers CPU architecture information from sysfs,/proc/cpuinfo and any applicable architecture-specific libraries(e.g. librtas on Powerpc). The command output can be optimizedfor parsing or for easy readability by humans. The informationincludes, for example, the number of CPUs, threads, cores,sockets, and Non-Uniform Memory Access (NUMA) nodes. There isalso information about the CPU caches and cache sharing, family,model, bogoMIPS, byte order, and stepping.The default output formatting on terminal is subject to changeand maybe optimized for better readability. The output fornon-terminals (e.g., pipes) is never affected by thisoptimization and it is always in \"Field: data\\n\" format. Use forexample \"lscpu | less\" to see the default output withoutoptimizations.In virtualized environments, the CPU architecture informationdisplayed reflects the configuration of the guest operatingsystem which is typically different from the physical (host)system. On architectures that support retrieving physicaltopology information, lscpu also displays the number of physicalsockets, chips, cores in the host system.Options that result in an output table have a list argument. Usethis argument to customize the command output. Specify acomma-separated list of column labels to limit the output tableto only the specified columns, arranged in the specified order.See COLUMNS for a list of valid column labels. The column labelsare not case sensitive.Not all columns are supported on all architectures. If anunsupported column is specified, lscpu prints the column but doesnot provide any data for it.The cache sizes are reported as summary from all CPUs. Theversions before v2.34 reported per-core sizes, but this outputwas confusing due to complicated CPUs topology and the way howcaches are shared between CPUs. For more details about caches see--cache. Since version v2.37 lscpu follows cache IDs as providedby Linux kernel and it does not always start from zero.",
        "name": "lscpu - display information about the CPU architecture",
        "section": 1
    },
    {
        "command": "lsfd",
        "description": "lsfd is intended to be a modern replacement for lsof(8) on Linuxsystems. Unlike lsof, lsfd is specialized to Linux kernel; itsupports Linux specific features like namespaces with simplercode. lsfd is not a drop-in replacement for lsof; they aredifferent in the command line interface and output formats.The default output is subject to change. So whenever possible,you should avoid using default outputs in your scripts. Alwaysexplicitly define expected columns by using --output columns-listin environments where a stable output is required.lsfd uses Libsmartcols for output formatting and filtering. Seethe description of --output option for customizing the outputformat, and --filter option for filtering. Use lsfd --help to geta list of all available columns.",
        "name": "lsfd - list file descriptors",
        "section": 1
    },
    {
        "command": "lsinitrd",
        "description": "lsinitrd shows the contents of an initramfs image. if <image> isomitted, then lsinitrd uses the default image/efi/<machine-id>/<kernel-version>/initrd,/boot/<machine-id>/<kernel-version>/initrd,/boot/efi/<machine-id>/<kernel-version>/initrd,/lib/modules/<kernel-version>/initrd or/boot/initramfs-<kernel-version>.img.",
        "name": "lsinitrd - tool to show the contents of an initramfs image",
        "section": 1
    },
    {
        "command": "lsipc",
        "description": "lsipc shows information on the System V inter-processcommunication facilities for which the calling process has readaccess.",
        "name": "lsipc - show information on IPC facilities currently employed inthe system",
        "section": 1
    },
    {
        "command": "lsirq",
        "description": "Display kernel interrupt counter information.The default output is subject to change. So whenever possible,you should avoid using default outputs in your scripts. Alwaysexplicitly define expected columns by using --output.",
        "name": "lsirq - utility to display kernel interrupt information",
        "section": 1
    },
    {
        "command": "lslogins",
        "description": "Examine the wtmp and btmp logs, /etc/shadow (if necessary) and/passwd and output the desired data.The optional argument username forces lslogins to print allavailable details about the specified user only. In this case theoutput format is different than in case of -l or -g and unknownis username reported as an error.The default action is to list info about all the users in thesystem.",
        "name": "lslogins - display information about known users in the system",
        "section": 1
    },
    {
        "command": "lsmem",
        "description": "The lsmem command lists the ranges of available memory with theironline status. The listed memory blocks correspond to the memoryblock representation in sysfs. The command also shows the memoryblock size and the amount of memory in online and offline state.The default output is compatible with original implementationfrom s390-tools, but it\u2019s strongly recommended to avoid usingdefault outputs in your scripts. Always explicitly defineexpected columns by using the --output option together with acolumns list in environments where a stable output is required.The lsmem command lists a new memory range always when thecurrent memory block distinguish from the previous block by someoutput column. This default behavior is possible to override bythe --split option (e.g., lsmem --split=ZONES). The special word\"none\" may be used to ignore all differences between memoryblocks and to create as large as possible continuous ranges. Theopposite semantic is --all to list individual memory blocks.Note that some output columns may provide inaccurate informationif a split policy forces lsmem to ignore differences in someattributes. For example if you merge removable and non-removablememory blocks to the one range than all the range will be markedas non-removable on lsmem output.Not all columns are supported on all systems. If an unsupportedcolumn is specified, lsmem prints the column but does not provideany data for it.Use the --help option to see the columns description.",
        "name": "lsmem - list the ranges of available memory with their onlinestatus",
        "section": 1
    },
    {
        "command": "ltrace",
        "description": "ltrace is a program that simply runs the specified command untilit exits.It intercepts and records the dynamic library callswhich are called by the executed process and the signals whichare received by that process.It can also intercept and printthe system calls executed by the program.Its use is very similar to strace(1).ltrace shows parameters of invoked functions and system calls.To determine what arguments each function has, it needs externaldeclaration of function prototypes.Those are stored in filescalled prototype libraries--see ltrace.conf(5) for details on thesyntax of these files.See the section PROTOTYPE LIBRARYDISCOVERY to learn how ltrace finds prototype libraries.",
        "name": "ltrace - A library call tracer",
        "section": 1
    },
    {
        "command": "lttng",
        "description": "The Linux Trace Toolkit: next generation <https://lttng.org/> isan open source software package used for correlated tracing ofthe Linux kernel, user applications, and user libraries.LTTng consists of Linux kernel modules (for Linux kernel tracing)and dynamically loaded libraries (for user application andlibrary tracing).An LTTng session daemon, lttng-sessiond(8), receives commandsfrom the command-line interface lttng to control the LTTngtracers. All interactions with the LTTng tracers happen throughthe lttng tool or through the liblttng-ctl library shipped withthe LTTng-tools package.A tracing domain is a tracer category. There are five availabledomains. For some commands, the domain needs to be specified witha command-line option. The domain options are:-j, --julApply command to the java.util.logging (JUL) domain.-k, --kernelApply command to the Linux kernel domain.-l, --log4jApply command to the Apache log4j 1.2<https://logging.apache.org/log4j/1.2/> (Java) domain.-p, --pythonApply command to the Python <https://www.python.org/> domain.-u, --userspaceApply command to the user space domain (application usingliblttng-ust directly; see lttng-ust(3)).The LTTng session daemon is a tracing registry which allows theuser to interact with multiple tracers (kernel and user space)within the same container, a tracing session. Traces can begathered from the Linux kernel and/or from instrumentedapplications (see lttng-ust(3)). You can aggregate and read theevents of LTTng traces using babeltrace(1).To trace the Linux kernel, the session daemon needs to be runningas root. LTTng uses a tracing group to allow specific users tointeract with the root session daemon. The default tracing groupname is tracing. You can use the --group option to set thetracing group name to use.Session daemons can coexist. You can have a session daemonrunning as user Alice that can be used to trace her applicationsalongside a root session daemon or a session daemon running asuser Bob.NoteIt is highly recommended to start the session daemon at boottime for stable and long-term tracing.User applications instrumented with LTTng automatically registerto the root session daemon and to user session daemons. Thisallows any session daemon to list the available traceableapplications and event sources (see lttng-list(1)).By default, the lttng-create(1) command automatically spawns auser session daemon if none is currently running. The --no-sessiond general option can be set to avoid this.",
        "name": "lttng - LTTng 2 tracer control command-line tool",
        "section": 1
    },
    {
        "command": "lttng-add-context",
        "description": "The lttng add-context command adds one or more context fields toa channel.Channels are created with the lttng-enable-channel(1) command.When context fields are added to a channel, all the eventsemitted within this channel contain the dynamic values of thosecontext fields.If the --session option is omitted, the current tracing sessionis used. If the --channel option is omitted, the context fieldsare added to all the selected tracing session\u2019s channels.Many context fields can be added to a channel at once byrepeating the --type option.perf counters are available as per-CPU (perf:cpu: prefix) as wellas per-thread (perf:thread: prefix) counters. Currently, per-CPUcounters can only be used in the Linux kernel tracing domain,while per-thread counters can only be used in the user spacetracing domain.It is also possible to enable PMU counters by raw ID using theperf:cpu:raw:rN:NAME (Linux kernel tracing domain) orperf:thread:raw:rN:NAME (user space tracing domain), with:NA hexadecimal event descriptor which is the same format asused by perf-record(1): a concatenation of the event numberand umask value provided by the processor\u2019s manufacturer. Thepossible values for this field are processor-specific.NAMECustom name to easily recognize the counter.Application-specific context fields can be added to a channelusing the following syntax:$app.PROVIDER:TYPEwith:PROVIDERProvider name.TYPEContext type name.NoteMake sure to single-quote the type when running the commandfrom a shell, as $ is a special character for variablesubstitution in most shells.Use the --list option without other arguments to list theavailable context field names.See the LIMITATIONS section below for a list of limitations toconsider.",
        "name": "lttng-add-context - Add context fields to an LTTng channel",
        "section": 1
    },
    {
        "command": "lttng-calibrate",
        "description": "The lttng calibrate commands quantifies the overhead of LTTngtracers.The lttng calibrate command can be used to find out the combinedaverage overhead of the LTTng tracers and the instrumentationmechanisms used. This overhead can be calibrated in terms of timeor using any of the PMU performance counter available on thesystem.For now, the only implemented calibration is the Linux kernelfunction instrumentation (kretprobes).Calibrate Linux kernel function instrumentationAs an example, we use an i7 processor with 4 general-purpose PMUregisters. This information is available by issuing dmesg,looking for generic registers.The following sequence of commands gathers a trace executing akretprobe hooked on an empty function, gathering PMU counters LLC(Last Level Cache) misses information (use lttng add-context--list to get the list of available PMU counters).lttng create calibrate-functionlttng enable-event calibrate --kernel \\--function=lttng_calibrate_kretprobelttng add-context --kernel --type=perf:cpu:LLC-load-misses \\--type=perf:cpu:LLC-store-misses \\--type=perf:cpu:LLC-prefetch-misseslttng startfor a in $(seq 1 10); dolttng calibrate --kernel --functiondonelttng destroybabeltrace $(ls -1drt ~/lttng-traces/calibrate-function-* | tail -n 1)The output from babeltrace(1) can be saved to a text file andopened in a spreadsheet (for example, in LibreOffice) to focus onthe per-PMU counter delta between consecutive calibrate_entry andcalibrate_return events. Note that these counters are per-CPU, soscheduling events would need to be present to account formigration between CPUs. Therefore, for calibration purposes, onlyevents staying on the same CPU must be considered.Here\u2019s an example of the average result, for the i7, on 10samples:\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502PMU counter\u2502 Average \u2502 Standard deviation \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502perf_LLC_load_misses\u2502 5.0\u2502 0.577\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502perf_LLC_store_misses\u2502 1.6\u2502 0.516\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502perf_LLC_prefetch_misses \u2502 9.0\u2502 14.742\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518As we can notice, the load and store misses are relatively stableacross runs (their standard deviation is relatively low) comparedto the prefetch misses. We could conclude from this informationthat LLC load and store misses can be accounted for quiteprecisely, but prefetches within a function seems to behave tooerratically (not much causality link between the code executedand the CPU prefetch activity) to be accounted for.",
        "name": "lttng-calibrate - Quantify LTTng overhead",
        "section": 1
    },
    {
        "command": "lttng-crash",
        "description": "The Linux Trace Toolkit: next generation <https://lttng.org/> isan open source software package used for correlated tracing ofthe Linux kernel, user applications, and user libraries.LTTng consists of Linux kernel modules (for Linux kernel tracing)and dynamically loaded libraries (for user application andlibrary tracing).The lttng-crash command-line tool is used to recover and viewLTTng trace buffers in the event of a system crash.",
        "name": "lttng-crash - Recover and view LTTng 2 trace buffers in the eventof a crash",
        "section": 1
    },
    {
        "command": "lttng-create",
        "description": "The lttng create command creates a new tracing session.A tracing session is a named container of channels, which in turncontain event rules. It is domain-agnostic, in that channels andevent rules can be enabled for the user space tracer and/or theLinux kernel tracer.On execution, an .lttngrc file is created, if it does not exist,in the user\u2019s home directory. This file contains the name of thecurrent tracing session. When creating a new tracing session withlttng create, the current tracing session is set to this newtracing session. The lttng-set-session(1) command can be used toset the current tracing session without manually editing the.lttngrc file.If SESSION is omitted, a session name is automatically createdhaving this form: auto-YYYYmmdd-HHMMSS. SESSION must not containthe character /.The --shm-path option can be used to specify the path to theshared memory holding the ring buffers. Specifying a location onan NVRAM file system makes it possible to retrieve the latestrecorded trace data when the system reboots after a crash. Toview the events of ring buffer files after a system crash, usethe lttng-crash(1) utility.Tracing sessions are destroyed using the lttng-destroy(1)command.Creation modesThere are four tracing session modes:Local modeTraces the local system and writes the trace to the localfile system. The --output option specifies the trace path.Using --set-url=file://PATH is the equivalent of using--output=PATH. The file system output can be disabled usingthe --no-output option.If none of the options mentioned above are used, then thetrace is written locally in the $LTTNG_HOME/lttng-tracesdirectory ($LTTNG_HOME defaults to $HOME).Network streaming modeTraces the local system and sends the trace over the networkto a listening relay daemon (see lttng-relayd(8)). The --set-url, or --ctrl-url and --data-url options set the traceoutput destination (see the URL format section below).Snapshot modeTraces the local system without writing the trace to thelocal file system (implicit --no-output option). Channels areautomatically configured to be snapshot-ready on creation(see lttng-enable-channel(1)). The lttng-snapshot(1) commandis used to take snapshots of the current ring buffers. The--set-url, or --ctrl-url and --data-url options set thedefault snapshot output destination.Live modeTraces the local system, sending trace data to an LTTng relaydaemon over the network (see lttng-relayd(8)). The --set-url,or --ctrl-url and --data-url options set the trace outputdestination. The live output URLs cannot use the file://protocol (see the URL format section below).URL formatThe --set-url, --ctrl-url, and --data-url options' arguments areURLs.The format of those URLs is one of:file://TRACEPATHNETPROTO://(HOST | IPADDR)[:CTRLPORT[:DATAPORT]][/TRACEPATH]The file:// protocol targets the local file system and can onlybe used as the --set-url option\u2019s argument when the session iscreated in local or snapshot mode.TRACEPATHAbsolute path to trace files on the local file system.The other version is available when the session is created innetwork streaming, snapshot, or live mode.NETPROTONetwork protocol, amongst:netTCP over IPv4; the default values of CTRLPORT andDATAPORT are respectively 5342 and 5343.net6TCP over IPv6: same default ports as the net protocol.tcpSame as the net protocol; can only be used with the--ctrl-url and --data-url options together.tcp6Same as the net6 protocol; can only be used with the--ctrl-url and --data-url options together.(HOST | IPADDR)Hostname or IP address (IPv6 address must be enclosed inbrackets ([ and ]); see RFC 2732<https://www.ietf.org/rfc/rfc2732.txt>).CTRLPORTControl port.DATAPORTData port.TRACEPATHPath of trace files on the remote file system. This path isrelative to the base output directory set on the relay daemonside; see lttng-relayd(8).",
        "name": "lttng-create - Create an LTTng tracing session",
        "section": 1
    },
    {
        "command": "lttng-destroy",
        "description": "The lttng destroy command destroys one or more tracing sessions.If no options are specified, the current tracing session isdestroyed (see lttng-create(1) for more information about thecurrent tracing session).If SESSION is specified, the existing tracing session namedSESSION is destroyed. lttng list outputs all the existing tracingsessions (see lttng-list(1)).If the --all option is used, all the tracing sessions, as listedin the output of lttng list, are destroyed.Destroying a tracing session stops any tracing running within thelatter. By default, the implicit lttng-stop(1) command invoked bythe lttng destroy command ensures that the tracing session\u2019strace data is valid before returning. With the --no-wait option,the lttng-stop(1) command finishes immediately, hence a localtrace might not be valid when the command is done. In this case,there is no way to know when the trace becomes valid.Destroying a tracing session does not destroy the recorded tracedata, if any; it frees resources acquired by the session daemonand tracer side, making sure to flush all trace data.If at least one rotation occurred during the chosen tracingsession\u2019s lifetime (see lttng-rotate(1) andlttng-enable-rotation(1)), and without the --no-wait option, allthe tracing session\u2019s output directory\u2019s subdirectories areconsidered trace chunk archives once the command returns: it issafe to read them, modify them, move them, or remove them.",
        "name": "lttng-destroy - Destroy an LTTng tracing session",
        "section": 1
    },
    {
        "command": "lttng-disable-channel",
        "description": "The lttng disable-channel command disables one or more channelspreviously enabled by the lttng-enable-channel(1) command.A channel is always contained in a tracing session (seelttng-create(1) for creating a tracing session). The session inwhich a channel is disabled using lttng disable-channel can bespecified using the --session option. If the --session option isomitted, the current tracing session is targeted.Note that re-enabling a disabled channel once its tracing sessionhas been active at least once is currently not supported.",
        "name": "lttng-disable-channel - Disable LTTng channels",
        "section": 1
    },
    {
        "command": "lttng-disable-event",
        "description": "The lttng disable-event command disables one or more event rulespreviously enabled by the lttng-enable-event(1) command.Event rules are always assigned to a channel when they arecreated. If the --channel option is omitted, the default channelnamed channel0 is used.If the --session option is omitted, the chosen channel is pickedfrom the current tracing session.If the --all-events option is used, all the existing event rulesof the chosen domain are disabled. Otherwise, at least one eventrule to disable named EVENT must be specified.With the --kernel option, the event source type can be specifiedusing one of the --tracepoint, --probe, --function, or --syscalloptions. See lttng-enable-event(1) for more details about eventsource types.Events can be disabled while tracing is active (uselttng-start(1) to make a tracing session active).",
        "name": "lttng-disable-event - Disable LTTng event rules",
        "section": 1
    },
    {
        "command": "lttng-disable-rotation",
        "description": "The lttng disable-rotation command unsets a rotation schedule forthe current tracing session, or for the tracing session namedSESSION if provided, previously set with thelttng-enable-rotation(1) command.",
        "name": "lttng-disable-rotation - Unset a tracing session's rotationschedule",
        "section": 1
    },
    {
        "command": "lttng-enable-channel",
        "description": "The lttng enable-channel command can create a new channel, orenable one or more existing and disabled ones.A channel is the owner of sub-buffers holding recorded events.Event, rules, when created using lttng-enable-event(1), arealways assigned to a channel. When creating a new channel, manyparameters related to those sub-buffers can be fine-tuned. Theyare described in the subsections below.When CHANNEL does not name an existing channel, a channel namedCHANNEL is created. Otherwise, the disabled channel named CHANNELis enabled.Note that the lttng-enable-event(1) command can automaticallycreate default channels when no channel exist.A channel is always contained in a tracing session (seelttng-create(1) for creating a tracing session). The session inwhich a channel is created using lttng enable-channel can bespecified using the --session option. If the --session option isomitted, the current tracing session is targeted.Existing enabled channels can be disabled usinglttng-disable-channel(1). Channels of a given session can belisted using lttng-list(1).See the LIMITATIONS section below for a list of limitations ofthis command to consider.Event loss modesLTTng tracers are non-blocking by default: when no emptysub-buffer exists, losing events is acceptable when thealternative would be to cause substantial delays in theinstrumented application\u2019s execution.LTTng privileges performance over integrity, aiming at perturbingthe traced system as little as possible in order to make tracingof subtle race conditions and rare interrupt cascades possible.You can allow the user space tracer to block with a --blocking-timeout option set to a positive value or to inf, and with anapplication which is instrumented with LTTng-UST started with aset LTTNG_UST_ALLOW_BLOCKING environment variable. Seelttng-ust(3) for more details.When it comes to losing events because no empty sub-buffer isavailable, the channel\u2019s event loss mode, specified by one of the--discard and --overwrite options, determines what to do amongst:DiscardDrop the newest events until a sub-buffer is released.OverwriteClear the sub-buffer containing the oldest recorded eventsand start recording the newest events there. This mode issometimes called flight recorder mode because it behaves likea flight recorder: always keep a fixed amount of the latestdata.Which mechanism to choose depends on the context: prioritize thenewest or the oldest events in the ring buffer?Beware that, in overwrite mode (--overwrite option), a wholesub-buffer is abandoned as soon as a new event doesn\u2019t find anempty sub-buffer, whereas in discard mode (--discard option),only the event that doesn\u2019t fit is discarded.Also note that a count of lost events is incremented and saved inthe trace itself when an event is lost in discard mode, whereasno information is kept when a sub-buffer gets overwritten beforebeing committed.The probability of losing events, if it is experience in a givencontext, can be reduced by fine-tuning the sub-buffers count andsize (see next subsection).Sub-buffers count and sizeThe --num-subbuf and --subbuf-size options respectively set thenumber of sub-buffers and their individual size when creating anew channel.Note that there is a noticeable tracer\u2019s CPU overhead introducedwhen switching sub-buffers (marking a full one as consumable andswitching to an empty one for the following events to berecorded). Knowing this, the following list presents a fewpractical situations along with how to configure sub-buffers forthem when creating a channel in overwrite mode (--overwriteoption):High event throughputIn general, prefer bigger sub-buffers to lower the risk oflosing events. Having bigger sub-buffers also ensures a lowersub-buffer switching frequency. The number of sub-buffers isonly meaningful if the channel is enabled in overwrite mode:in this case, if a sub-buffer overwrite happens, the othersub-buffers are left unaltered.Low event throughputIn general, prefer smaller sub-buffers since the risk oflosing events is already low. Since events happen lessfrequently, the sub-buffer switching frequency should remainlow and thus the tracer\u2019s overhead should not be a problem.Low memory systemIf the target system has a low memory limit, prefer fewerfirst, then smaller sub-buffers. Even if the system islimited in memory, it is recommended to keep the sub-buffersas big as possible to avoid a high sub-buffer switchingfrequency.In discard mode (--discard option), the sub-buffers countparameter is pointless: using two sub-buffers and setting theirsize according to the requirements of the context is fine.Switch timerWhen a channel\u2019s switch timer fires, a sub-buffer switch happens.This timer may be used to ensure that event data is consumed andcommitted to trace files periodically in case of a low eventthroughput.It\u2019s also convenient when big sub-buffers are used to cope withsporadic high event throughput, even if the throughput isnormally lower.Use the --switch-timer option to control the switch timer\u2019speriod of the channel to create.Read timerBy default, an internal notification mechanism is used to signala full sub-buffer so that it can be consumed. When suchnotifications must be avoided, for example in real-timeapplications, the channel\u2019s read timer can be used instead. Whenthe read timer fires, sub-buffers are checked for consumptionwhen they are full.Use the --read-timer option to control the read timer\u2019s period ofthe channel to create.Monitor timerWhen a channel\u2019s monitor timer fires, its registered triggerconditions are evaluated using the current values of itsproperties (for example, the current usage of its sub-buffers).When a trigger condition is true, LTTng executes its associatedaction. The only type of action currently supported is to notifyone or more user applications.See the installed C/C++ headers in lttng/action, lttng/condition,lttng/notification, and lttng/trigger to learn more aboutapplication notifications and triggers.Use the --monitor-timer option to control the monitor timer\u2019speriod of the channel to create.Buffering schemeIn the user space tracing domain, two buffering schemes areavailable when creating a channel:Per-process buffering (--buffers-pid option)Keep one ring buffer per process.Per-user buffering (--buffers-uid option)Keep one ring buffer for all the processes of a single user.The per-process buffering scheme consumes more memory than theper-user option if more than one process is instrumented forLTTng-UST. However, per-process buffering ensures that oneprocess having a high event throughput won\u2019t fill all the sharedsub-buffers, only its own.The Linux kernel tracing domain only has one available bufferingscheme which is to use a single ring buffer for the whole system(--buffers-global option).Trace files limit and sizeBy default, trace files can grow as large as needed. The maximumsize of each trace file written by a channel can be set oncreation using the --tracefile-size option. When such a tracefile\u2019s size reaches the channel\u2019s fixed maximum size, anothertrace file is created to hold the next recorded events. A filecount is appended to each trace file name in this case.If the --tracefile-size option is used, the maximum number ofcreated trace files is unlimited. To limit them, the --tracefile-count option can be used. This option is always used inconjunction with the --tracefile-size option.For example, consider this command:$ lttng enable-channel --kernel --tracefile-size=4096 \\--tracefile-count=32 my-channelHere, for each stream, the maximum size of each trace file is 4kiB and there can be a maximum of 32 different files. When thereis no space left in the last file, trace file rotation happens:the first file is cleared and new sub-buffers containing eventsare written there.LTTng does not guarantee that you can view the trace of an activetracing session (before you run the lttng-stop(1) command), evenwith multiple trace files, because LTTng could overwrite them atany moment, or some of them could be incomplete. You can archivea tracing session\u2019s current trace chunk while the tracing sessionis active to obtain an unmanaged and self-contained LTTng trace:see lttng-rotate(1) and lttng-enable-rotation(1).",
        "name": "lttng-enable-channel - Create or enable LTTng channels",
        "section": 1
    },
    {
        "command": "lttng-enable-event",
        "description": "The lttng enable-event command can create a new event rule, orenable one or more existing and disabled ones.An event rule created by lttng enable-event is a set ofconditions that must be satisfied in order for an actual event tobe emitted by an LTTng tracer when the execution of anapplication or the Linux kernel reaches an event source(tracepoint, system call, dynamic probe). Event sources can belisted with the lttng-list(1) command.The lttng-disable-event(1) command can be used to disableexisting event rules.Event rules are always assigned to a channel when they arecreated. If the --channel option is omitted, a default channelnamed channel0 is used (and created automatically if it does notexist for the specified domain in the selected tracing session).If the --session option is omitted, the chosen channel is pickedfrom the current tracing session.Events can be enabled while tracing is active (use lttng-start(1)to make a tracing session active).Event source typesFour types of event sources are available in the Linux kerneltracing domain (--kernel option):Tracepoint (--tracepoint option; default)A Linux kernel tracepoint, that is, a static instrumentationpoint placed in the kernel source code. Standard tracepointsare designed and placed in the source code by developers andrecord useful payload fields.Dynamic probe (--probe option)A Linux kernel kprobe, that is, an instrumentation pointplaced dynamically in the compiled kernel code. Dynamic probeevents do not record any payload field.Function probe (--function option)A Linux kernel kretprobe, that is, two instrumentation pointsplaced dynamically where a function is entered and where itreturns in the compiled kernel code. Function probe events donot record any payload field.System call (--syscall option)A Linux kernel system call. Two instrumentation points arestatically placed where a system call function is entered andwhere it returns in the compiled kernel code. System callevent sources record useful payload fields.The application tracing domains (--userspace, --jul, --log4j, or--python options) only support tracepoints. In the cases of theJUL, Apache log4j, and Python domains, the event names correspondto logger names.Understanding event rule conditionsWhen creating an event rule with lttng enable-event, conditionsare specified using options. The logical conjunction (logicalAND) of all those conditions must be true when an event source isreached by an application or by the Linux kernel in order for anactual event to be emitted by an LTTng tracer.Any condition that is not explicitly specified on creation isconsidered a don\u2019t care.For example, consider the following commands:$ lttng enable-event --userspace hello:world$ lttng enable-event --userspace hello:world --loglevel=TRACE_INFOHere, two event rules are created. The first one has a singlecondition: the tracepoint name must match hello:world. The secondone has two conditions:\u2022The tracepoint name must match hello:world, and\u2022The tracepoint\u2019s defined log level must be at least as severeas the TRACE_INFO level.In this case, the second event rule is pointless because thefirst one is more general: it does not care about thetracepoint\u2019s log level. If an event source matching both eventrules is reached by the application\u2019s execution, only one eventis emitted.The available conditions for the Linux kernel domain are:\u2022Tracepoint/system call name (EVENT argument with --tracepointor --syscall options) or dynamic probe/function name/address(--probe or --function option\u2019s argument) which must matchevent source\u2019s equivalent.You can use * characters at any place in the tracepoint orsystem call name as wildcards to match zero or morecharacters. To use a literal * character, use \\*.\u2022Filter expression (--filter option) executed against thedynamic values of event fields at execution time that mustevaluate to true. See the Filter expression section below formore information.The available conditions for the application domains are:\u2022Tracepoint name (EVENT with --tracepoint option) which mustmatch event source\u2019s equivalent.You can use * characters at any place in the tracepoint nameas wildcards to match zero or more characters. To use aliteral * character, use \\*. When you create an event rulewith a tracepoint name containing a wildcard, you can excludespecific tracepoint names from the match with the --excludeoption.\u2022Filter expression (--filter option) executed against thedynamic values of event fields at execution time that mustevaluate to true. See the Filter expression section below formore information.\u2022Event\u2019s log level that must be at least as severe as a givenlog level (--loglevel option) or match exactly a given loglevel (--loglevel-only option).When using lttng enable-event with a set of conditions that doesnot currently exist for the chosen tracing session, domain, andchannel, a new event rule is created. Otherwise, the existingevent rule is enabled if it is currently disabled (seelttng-disable-event(1)).The --all option can be used alongside the --tracepoint or--syscall options. When this option is used, no EVENT argumentmust be specified. This option defines a single event rulematching all the possible events of a given tracing domain forthe chosen channel and tracing session. It is the equivalent ofan EVENT argument named * (wildcard).Filter expressionA filter expression can be specified with the --filter optionwhen creating a new event rule. If the filter expressionevaluates to true when executed against the dynamic values of anevent\u2019s fields when tracing, the filtering condition passes.NoteMake sure to single-quote the filter expression when runningthe command from a shell, as filter expressions typicallyinclude characters having a special meaning for most shells.The filter expression syntax is similar to C language conditionalexpressions (expressions that can be evaluated by an ifstatement), albeit with a few differences:\u2022C integer and floating point number constants are supported,as well as literal strings between double quotes (\"). You canuse * characters at any place in a literal string aswildcards to match zero or more characters. To use a literal* character, use \\*.Examples: 32, -0x17, 0755, 12.34, \"a \\\"literal string\\\"\",\"src/*/*.h\".\u2022The dynamic value of an event field is read by using its nameas a C identifier.The dot and square bracket notations are available, like inthe C language, to access nested structure and array/sequencefields. Only a constant, positive integer number can be usedwithin square brackets. If the index is out of bounds, thewhole filter expression evaluates to false (the event isdiscarded).An enumeration field\u2019s value is an integer.When the expression\u2019s field does not exist, the whole filterexpression evaluates to false.Examples: my_field, target_cpu, seq[7],msg.user[1].data[2][17].\u2022The dynamic value of a statically-known context field is readby prefixing its name with $ctx.. Statically-known contextfields are context fields added to channels without the $app.prefix using the lttng-add-context(1) command.When the expression\u2019s statically-known context field does notexist, the whole filter expression evaluates to false.Examples: $ctx.prio, $ctx.preemptible, $ctx.perf:cpu:stalled-cycles-frontend.\u2022The dynamic value of an application-specific context field isread by prefixing its name with $app.(follows the formatused to add such a context field with thelttng-add-context(1) command).When the expression\u2019s application-specific context field doesnot exist, the whole filter expression evaluates to false.Example: $app.server:cur_user.The following precedence table shows the operators which aresupported in a filter expression. In this table, the highestprecedence is 1. Parentheses are supported to bypass the defaultorder.ImportantUnlike the C language, the lttng enable-event filterexpression syntax\u2019s bitwise AND and OR operators (& and |)take precedence over relational operators (<, <=, >, >=, ==,and !=). This means the filter expression 2 & 2 == 2 is truewhile the equivalent C expression is false.\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502Precedence \u2502 Operator \u2502 Description\u2502 Associativity \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25021\u2502 -\u2502 Unary minus\u2502 Right-to-left \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25021\u2502 +\u2502 Unary plus\u2502 Right-to-left \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25021\u2502 !\u2502 Logical NOT\u2502 Right-to-left \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25021\u2502 ~\u2502 Bitwise NOT\u2502 Right-to-left \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25022\u2502 <<\u2502 Bitwise left\u2502 Left-to-right \u2502\u2502\u2502\u2502 shift\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25022\u2502 >>\u2502 Bitwise right \u2502 Left-to-right \u2502\u2502\u2502\u2502 shift\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25023\u2502 &\u2502 Bitwise AND\u2502 Left-to-right \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25024\u2502 ^\u2502 Bitwise XOR\u2502 Left-to-right \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25025\u2502 |\u2502 Bitwise OR\u2502 Left-to-right \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25026\u2502 <\u2502 Less than\u2502 Left-to-right \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25026\u2502 <=\u2502 Less than or\u2502 Left-to-right \u2502\u2502\u2502\u2502 equal to\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25026\u2502 >\u2502 Greater than\u2502 Left-to-right \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25026\u2502 >=\u2502 Greater than\u2502 Left-to-right \u2502\u2502\u2502\u2502 or equal to\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25027\u2502 ==\u2502 Equal to\u2502 Left-to-right \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25027\u2502 !=\u2502 Not equal to\u2502 Left-to-right \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25028\u2502 &&\u2502 Logical AND\u2502 Left-to-right \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502\u2502\u25029\u2502 ||\u2502 Logical OR\u2502 Left-to-right \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518The arithmetic operators are NOT supported.All integer constants and fields are first casted to signed64-bit integers. The representation of negative integers is two\u2019scomplement. This means that, for example, the signed 8-bitinteger field 0xff (-1) becomes 0xffffffffffffffff (still -1)once casted.Before a bitwise operator is applied, all its operands are castedto unsigned 64-bit integers, and the result is casted back to asigned 64-bit integer. For the bitwise NOT operator, it is theequivalent of this C expression:(int64_t) ~((uint64_t) val)For the binary bitwise operators, it is the equivalent of those Cexpressions:(int64_t) ((uint64_t) lhs >> (uint64_t) rhs)(int64_t) ((uint64_t) lhs << (uint64_t) rhs)(int64_t) ((uint64_t) lhs & (uint64_t) rhs)(int64_t) ((uint64_t) lhs ^ (uint64_t) rhs)(int64_t) ((uint64_t) lhs | (uint64_t) rhs)If the right-hand side of a bitwise shift operator (<< and >>) isnot in the [0, 63] range, the whole filter expression evaluatesto false.NoteAlthough it is possible to filter the process ID of an eventwhen the pid context has been added to its channel using, forexample, $ctx.pid == 2832, it is recommended to use the PIDtracker instead, which is much more efficient (seelttng-track(1)).Filter expression examples:msg_id == 23 && size >= 2048$ctx.procname == \"lttng*\" && (!flag || poel < 34)$app.my_provider:my_context == 17.34e9 || some_enum >= 14$ctx.cpu_id == 2 && filename != \"*.log\"eax_reg & 0xff7 == 0x240 && x[4] >> 12 <= 0x1234Log levelsTracepoints and log statements in applications have an attachedlog level. Application event rules can contain a log levelcondition.With the --loglevel option, the event source\u2019s log level must beat least as severe as the option\u2019s argument. With the --loglevel-only option, the event source\u2019s log level must match the option\u2019sargument.The available log levels are:User space domain (--userspace option)Shortcuts such as system are allowed.\u2022TRACE_EMERG (0)\u2022TRACE_ALERT (1)\u2022TRACE_CRIT (2)\u2022TRACE_ERR (3)\u2022TRACE_WARNING (4)\u2022TRACE_NOTICE (5)\u2022TRACE_INFO (6)\u2022TRACE_DEBUG_SYSTEM (7)\u2022TRACE_DEBUG_PROGRAM (8)\u2022TRACE_DEBUG_PROCESS (9)\u2022TRACE_DEBUG_MODULE (10)\u2022TRACE_DEBUG_UNIT (11)\u2022TRACE_DEBUG_FUNCTION (12)\u2022TRACE_DEBUG_LINE (13)\u2022TRACE_DEBUG (14)java.util.logging domain (--jul option)Shortcuts such as severe are allowed.\u2022JUL_OFF (INT32_MAX)\u2022JUL_SEVERE (1000)\u2022JUL_WARNING (900)\u2022JUL_INFO (800)\u2022JUL_CONFIG (700)\u2022JUL_FINE (500)\u2022JUL_FINER (400)\u2022JUL_FINEST (300)\u2022JUL_ALL (INT32_MIN)Apache log4j domain (--log4j option)Shortcuts such as severe are allowed.\u2022LOG4J_OFF (INT32_MAX)\u2022LOG4J_FATAL (50000)\u2022LOG4J_ERROR (40000)\u2022LOG4J_WARN (30000)\u2022LOG4J_INFO (20000)\u2022LOG4J_DEBUG (10000)\u2022LOG4J_TRACE (5000)\u2022LOG4J_ALL (INT32_MIN)Python domain (--python option)Shortcuts such as critical are allowed.\u2022PYTHON_CRITICAL (50)\u2022PYTHON_ERROR (40)\u2022PYTHON_WARNING (30)\u2022PYTHON_INFO (20)\u2022PYTHON_DEBUG (10)\u2022PYTHON_NOTSET (0)",
        "name": "lttng-enable-event - Create or enable LTTng event rules",
        "section": 1
    },
    {
        "command": "lttng-enable-rotation",
        "description": "The lttng enable-rotation command sets a rotation schedule forthe current tracing session, or for the tracing session namedSESSION if provided. See lttng-rotate(1) for more informationabout the concepts of a tracing session rotation and a tracechunk.With the --timer option, the rotation schedule is set so that anautomatic rotation occurs at least every PERIOD (microsecondswithout a unit suffix).With the --size option, the rotation schedule is set so that anautomatic rotation occurs every time the total size of theflushed part of the current trace chunk is at least SIZE (byteswithout a unit suffix).With both the --timer and --size options, LTTng checks theschedule condition periodically using the monitor timers of thetracing session\u2019s channels. This means that, with the --timeroption, the automatic rotation can occur when the elapsed timesince the last automatic rotation is greater than PERIOD, andwith the --size option, the automatic rotation can occur when thesize of the flushed part of the current trace chunk is greaterthan SIZE. See the --monitor-timer option inlttng-enable-channel(1) for more information about the monitortimer.The naming convention of a trace chunk archive which an automaticrotation creates is the same as with the immediate rotationcommand, lttng-rotate(1).You can unset a rotation schedule with thelttng-disable-rotation(1) command.See LIMITATIONS for important limitations regarding this command.",
        "name": "lttng-enable-rotation - Set a tracing session's rotation schedule",
        "section": 1
    },
    {
        "command": "lttng-gen-tp",
        "description": "The lttng-gen-tp tool simplifies the generation of LTTng-USTtracepoint provider files. It takes a simple template file,TEMPLATE, and generates the necessary C code to use the definedtracepoints in your application. See the Template file formatsection below for more information about the format of TEMPLATE.Currently, lttng-gen-tp can generate the .h, .c, and .o filesassociated with your tracepoint provider. The generated .h filecan be included directly in your application. You can let lttng-gen-tp generate the .o file or compile the .c file yourself. Seelttng-ust(3) for more information about compiling LTTng-USTtracepoint providers.By default, lttng-gen-tp generates the .h, .c, and .o files,their basename being the basename of TEMPLATE. You can generateone or more specific file types with the --output option,repeated if needed.Template file formatThe template file, which usually has the .tp extension, containsa list of TRACEPOINT_EVENT() definitions and other optionaldefinition entries, like TRACEPOINT_LOGLEVEL(). See lttng-ust(3)for the complete list of available definitions.The TRACEPOINT_EVENT() definitions are written as you would writethem in an LTTng-UST template provider header file. C commentsare supported (/* */ and //), as well as lines starting with #.NoteThe provider name (the first argument of TRACEPOINT_EVENT())must be the same in all the TRACEPOINT_EVENT() macros ofTEMPLATE.Here\u2019s an example:TRACEPOINT_EVENT(// Tracepoint provider namemy_provider,// Tracepoint/event namemy_event,// Tracepoint arguments (input)TP_ARGS(char *, text),// Tracepoint/event fields (output)TP_FIELDS(ctf_string(message, text)))",
        "name": "lttng-gen-tp - Generate LTTng-UST tracepoint provider code",
        "section": 1
    },
    {
        "command": "lttng-help",
        "description": "The lttng help command displays help information about an LTTngcommand.This command is the equivalent of:$ lttng COMMAND --helpwhere COMMAND is the name of the command about which to get help.If COMMAND is omitted, lttng help shows general help about thelttng(1) command.The lttng help command attempts to launch /usr/bin/man to viewthe command\u2019s man page. The path to the man pager can beoverridden by the LTTNG_MAN_BIN_PATH environment variable.",
        "name": "lttng-help - Display help information about an LTTng command",
        "section": 1
    },
    {
        "command": "lttng-list",
        "description": "The lttng list command lists tracing sessions, tracing domains,channels, and events.Without arguments, lttng list lists the existing tracing sessionsand shows if they are active or not.With one or more of the --kernel, --userspace, --jul, --log4j,and --python domain options, the command lists the availableevent sources of the selected domain on the system. The JUL,log4j, and Python domains list the names of their availableloggers. The --syscall option can be used alongside the --kerneloption to get a list of traceable Linux system calls. The--fields option can be used to show the fields of the listedevent sources.Providing a tracing session name SESSION targets a specifictracing session. If the --domain option is used, domainscontaining at least one channel in the selected tracing sessionare listed. Otherwise, all the domains, channels, and event rulesof the selected tracing session are listed along with its details(trace path, for example), except when the --channel option isused to isolate a specific channel by name.",
        "name": "lttng-list - List LTTng tracing sessions, domains, channels, andevents",
        "section": 1
    },
    {
        "command": "lttng-load",
        "description": "The lttng load command loads the configurations of one or moretracing sessions from files.The lttng load command is used in conjunction with thelttng-save(1) command to save and restore the completeconfigurations of tracing sessions. This includes the enabledchannels and event rules, the context added to channels, thetracing activity, and more.Once one or more tracing session configurations are loaded, theyappear exactly as they were saved from the user\u2019s point of view.The following directories are searched, non-recursively, in thisorder for configuration files:1. $LTTNG_HOME/.lttng/sessions ($LTTNG_HOME defaults to $HOME)2. /usr/local/etc/lttng/sessionsThe input path can be overridden with the --input-path option.When this option is specified, the default directories are NOTsearched for configuration files. When it\u2019s not specified, bothdefault directories are searched for configuration files.If the input path is a directory, then:\u2022If SESSION is specified, the tracing session configurationnamed SESSION is searched for in all the files of thisdirectory and loaded if found.\u2022If SESSION is not specified, the --all option is implicit:all the tracing session configurations found in all the filesin this directory are loaded.If the input path is a file, then:\u2022If SESSION is specified, the tracing session configurationnamed SESSION is searched for in this file and loaded iffound.\u2022If SESSION is not specified, the --all option is implicit:all the tracing session configurations found in this file areloaded.Aspects of the loaded configurations can be overridden at loadtime using the --override-url and --override-name options.By default, existing tracing sessions are not overwritten whenloading: the command fails. The --force option can be used toallow this.",
        "name": "lttng-load - Load LTTng tracing session configurations",
        "section": 1
    },
    {
        "command": "lttng-metadata",
        "description": "WarningThis command is deprecated; it has been replaced by lttngregenerate metadata (see lttng-regenerate(1)).",
        "name": "lttng-metadata - Manage an LTTng tracing session's metadatageneration",
        "section": 1
    },
    {
        "command": "lttng-regenerate",
        "description": "The lttng regenerate command regenerates specific data of atracing session.As of this version, the metadata and statedump actions areavailable.Regenerating a tracing session\u2019s metadataThe lttng regenerate metadata action can be used to resample theoffset between the system\u2019s monotonic clock and the wall-clocktime.This action is meant to be used to resample the wall-clock timefollowing a major NTP<https://en.wikipedia.org/wiki/Network_Time_Protocol> correction.As such, a system booting with an incorrect wall time can betraced before its wall time is NTP-corrected. Regenerating thetracing session\u2019s metadata ensures that trace viewers canaccurately determine the events time relative to Unix Epoch.If you use lttng-rotate(1) or lttng-enable-rotation(1) to maketracing session rotations, this action regenerates the currentand next trace chunks\u2019s metadata files.Regenerating a tracing session\u2019s state dumpThe lttng regenerate statedump action can be used to collectup-to-date state dump information during the tracing session.This is particularly useful in snapshot (see lttng-snapshot(1))or trace file rotation (see lttng-enable-channel(1)) modes wherethe state dump information may be lost.",
        "name": "lttng-regenerate - Manage an LTTng tracing session's dataregeneration",
        "section": 1
    },
    {
        "command": "lttng-rotate",
        "description": "The lttng rotate command archives the current trace chunk of thecurrent tracing session, or of the tracing session named SESSIONif provided, to the file system. This action is called a tracingsession rotation.Once a trace chunk is archived, LTTng does not manage it anymore:you can read it, modify it, move it, or remove it.An archived trace chunk is a collection of metadata and datastream files which form a self-contained trace.The current trace chunk of a given tracing session includes:\u2022The stream files already written to the file system, andwhich are not part of a previously archived trace chunk,since the most recent event amongst:\u2022The first time the tracing session was started withlttng-start(1).\u2022The last rotation, either an immediate one with lttngrotate, or an automatic one from a rotation schedulepreviously set with lttng-enable-rotation(1).\u2022The content of all the non-flushed sub-buffers of the tracingsession\u2019s channels.You can use lttng rotate either at any time when the tracingsession is active (see lttng-start(1)), or a single time once thetracing session becomes inactive (see lttng-stop(1)).By default, the lttng rotate command ensures that the rotation isdone before printing the archived trace chunk\u2019s path andreturning to the prompt. The printed path is absolute when thetracing session was created in normal mode and relative to therelay daemon\u2019s output directory (see the --output option inlttng-relayd(8)) when it was created in network streaming mode(see lttng-create(1)).With the --no-wait option, the command finishes immediately,hence a rotation might not be completed when the command is done.In this case, there is no easy way to know when the current tracechunk is archived, and the command does not print the archivedtrace chunk\u2019s path.Because a rotation causes the tracing session\u2019s currentsub-buffers to be flushed, archived trace chunks are neverredundant, that is, they do not overlap over time like snapshotscan (see lttng-snapshot(1)). Also, a rotation does not directlycause discarded event records or packets.See LIMITATIONS for important limitations regarding this command.Trace chunk archive namingA trace chunk archive is a subdirectory of a tracing session\u2019soutput directory (see the --output option in lttng-create(1))which contains, through tracing domain and possibly UID/PIDsubdirectories, metadata and data stream files.A trace chunk archive is, at the same time:\u2022A self-contained LTTng trace.\u2022A member of a set of trace chunk archives which form thecomplete trace of a tracing session.In other words, an LTTng trace reader can read both the tracingsession output directory (all the trace chunk archives), or asingle trace chunk archive.When a tracing session rotation occurs, the created trace chunkarchive is named:BEGIN-END-IDBEGINDate and time of the beginning of the trace chunk archivewith the ISO 8601-compatible YYYYmmddTHHMMSS\u00b1HHMM form, whereYYYYmmdd is the date and HHMMSS\u00b1HHMM is the time with thetime zone offset from UTC.Example: 20171119T152407-0500ENDDate and time of the end of the trace chunk archive with theISO 8601-compatible YYYYmmddTHHMMSS\u00b1HHMM form, where YYYYmmddis the date and HHMMSS\u00b1HHMM is the time with the time zoneoffset from UTC.Example: 20180118T152407+0930IDUnique numeric identifier of the trace chunk within itstracing session.Trace chunk archive name example:20171119T152407-0500-20171119T151422-0500-3",
        "name": "lttng-rotate - Archive a tracing session's current trace chunk",
        "section": 1
    },
    {
        "command": "lttng-save",
        "description": "The lttng save command saves the configurations of one or moretracing sessions to files.The lttng save command is used in conjunction with thelttng-load(1) command to save and restore the completeconfigurations of tracing sessions. This includes the enabledchannels and event rules, the context added to channels, thetracing activity, and more. lttng save does not save tracingdata, only the tracing session parameters.If SESSION is omitted, all the existing tracing sessionconfigurations are saved (equivalent to using the --all option).Otherwise, SESSION is the name of an existing tracing session.lttng list outputs all the existing tracing sessions (seelttng-list(1)).The default output directory path is $LTTNG_HOME/.lttng/sessions($LTTNG_HOME defaults to $HOME). Each tracing sessionconfiguration file is named SESSION.lttng, where SESSION is theoriginal tracing session name. The default output directory pathcan be overridden with the --output-path option.By default, existing tracing session configuration files are notoverwritten when saving; the command fails. The --force optioncan be used to allow this.",
        "name": "lttng-save - Save LTTng tracing session configurations",
        "section": 1
    },
    {
        "command": "lttng-set-session",
        "description": "The lttng set-session command sets the current tracing session.SESSION is the name of an existing tracing session. lttng listoutputs all the existing tracing sessions (see lttng-list(1)).The current tracing session is used by default when a session canbe specified in other commands. See lttng-create(1) for moreinformation about the current tracing session.",
        "name": "lttng-set-session - Set the current LTTng tracing session",
        "section": 1
    },
    {
        "command": "lttng-snapshot",
        "description": "The lttng snapshot command manages the snapshot outputs and takessnapshots.A snapshot is a dump of the current sub-buffers of all thechannels of a given tracing session. When a snapshot is taken,the memory dump is sent to the registered snapshot outputs.The tracing session should be created in snapshot mode to makesure taking snapshots is allowed. This is done at tracing sessioncreation time using the lttng-create(1) command.Note that, when a snapshot is taken, the sub-buffers are notcleared. This means that different recorded snapshots may containthe same events.Snapshot outputsSnapshot outputs are the destinations of snapshot files when asnapshot is taken using the record action.As of this version, only one snapshot output is allowed.A snapshot output can be added using the add-output action. Theoutput destination URL is set using either the URL positionalargument, or both the --ctrl-url and --data-url options. Seelttng-create(1) to learn more about the URL format.A name can be assigned to an output when adding it using the--name option. This name is part of the names of the snapshotfiles written to this output.By default, the snapshot files can be as big as the sum of thesizes of all the sub-buffers or all the channels of the selectedtracing session. The maximum total size of all the snapshot filescan be configured using the --max-size option.Snapshot outputs can be listed using the list-output action.Snapshot outputs can be removed using the del-output action. Theconfigured name can be used when removing an output, or an ID aslisted by the list-output action.Taking a snapshotTaking a snapshot of the current tracing session is as easy as:$ lttng snapshot recordThis writes the snapshot files to the configured output. It ispossible to use a custom, unregistered output at record timeusing the same options supported by the add-output action.NoteBefore taking a snapshot on a system with a high eventthroughput, it is recommended to first run lttng stop (seelttng-stop(1)). Otherwise, the snapshot could contain\"holes\", the result of the tracers overwriting unconsumedtrace packets during the record operation. After the snapshotis recorded, the tracers can be started again with lttngstart (see lttng-start(1)).",
        "name": "lttng-snapshot - Take LTTng snapshots and configure snapshotoutputs",
        "section": 1
    },
    {
        "command": "lttng-start",
        "description": "The lttng start command starts the various LTTng tracers for agiven inactive tracing session.Starting the LTTng tracers has the effect that all enabled eventrules within enabled channels can make their target event sourcesemit trace events. Whether they are recorded to the local filesystem, sent over the network, or not recorded at all depends onthe specific configuration of the tracing session in whichtracing is started. See lttng-create(1) for different sessionmodes.A tracing session with running tracers is said to be active.Active tracing sessions can return to the inactive state usingthe lttng-stop(1) command.If SESSION is omitted, the LTTng tracers are started for thecurrent tracing session (see lttng-create(1) for more informationabout the current tracing session). Otherwise, they are startedfor the existing tracing session named SESSION. lttng listoutputs all the existing tracing sessions (see lttng-list(1)).",
        "name": "lttng-start - Start LTTng tracers",
        "section": 1
    },
    {
        "command": "lttng-status",
        "description": "The lttng status command shows the status of the current tracingsession.This command is the exact equivalent of:$ lttng list CURSESSIONwhere CURSESSION is the name of the current tracing session. Uselttng-set-session(1) to set the current tracing session.",
        "name": "lttng-status - Get the current LTTng tracing session's status",
        "section": 1
    },
    {
        "command": "lttng-stop",
        "description": "The lttng stop command stops the various LTTng tracers for agiven active tracing session.Stopping the LTTng tracers has the effect that all enabled eventrules within enabled channels cannot make event sources emittrace events anymore.A tracing session with no running tracers is said to be inactive.Inactive tracing sessions can be set active using thelttng-start(1) command.If SESSION is omitted, the LTTng tracers are stopped for thecurrent tracing session (see lttng-create(1) for more informationabout the current tracing session). Otherwise, they are stoppedfor the existing tracing session named SESSION. lttng listoutputs all the existing tracing sessions (see lttng-list(1)).By default, the lttng stop command ensures that the tracingsession\u2019s trace data is valid before returning to the prompt.With the --no-wait option, the command finishes immediately,hence a local trace might not be valid when the command is done.In this case, there is no way to know when the trace becomesvalid.If at least one rotation occurred during the chosen tracingsession\u2019s lifetime (see lttng-rotate(1) andlttng-enable-rotation(1)), the lttng stop command renames thecurrent trace chunk subdirectory and prints the renamed path.Although it is safe to read the content of this renamedsubdirectory while the tracing session remains inactive (untilthe next lttng-start(1)), it is NOT a trace chunk archive: youneed to destroy the tracing session with lttng-destroy(1) or makea rotation with lttng-rotate(1) to archive it.",
        "name": "lttng-stop - Stop LTTng tracers",
        "section": 1
    },
    {
        "command": "lttng-track",
        "description": "The lttng track commands adds one or more entries to a resourcetracker.A resource tracker is a whitelist of resources. Tracked resourcesare allowed to emit events, provided those events are targeted byenabled event rules (see lttng-enable-event(1)).Tracker entries can be removed from the whitelist withlttng-untrack(1).As of this version, the only available tracker is the PIDtracker. The process ID (PID) tracker follows one or more processIDs; only the processes with a tracked PID are allowed to emitevents. By default, all possible PIDs on the system are tracked:any process may emit enabled events (equivalent of lttng track--pid --all for all domains).With the PID tracker, it is possible, for example, to record allsystem calls called by a given process:# lttng enable-event --kernel --all --syscall# lttng track --kernel --pid=2345# lttng startIf all the PIDs are tracked (i.e. lttng track --pid --all, whichis the default state of all domains when creating a tracingsession), then using the track command with one or more specificPIDs has the effect of first removing all the PIDs from thewhitelist, then adding the specified PIDs.ExampleAssume the maximum system PID is 7 for this example.Initial whitelist:[0] [1] [2] [3] [4] [5] [6] [7]Command:$ lttng track --userspace --pid=3,6,7Whitelist:[ ] [ ] [ ] [3] [ ] [ ] [6] [7]Command:$ lttng untrack --userspace --pid=7Whitelist:[ ] [ ] [ ] [3] [ ] [ ] [6] [ ]Command:$ lttng track --userspace --pid=1,5Whitelist:[ ] [1] [ ] [3] [ ] [5] [6] [ ]It should be noted that the PID tracker tracks the numericprocess IDs. Should a process with a given ID exit and anotherprocess be given this ID, then the latter would also be allowedto emit events.See the lttng-untrack(1) for more details about removing entries.",
        "name": "lttng-track - Add one or more entries to an LTTng resourcetracker",
        "section": 1
    },
    {
        "command": "lttng-untrack",
        "description": "The lttng untrack commands removes one or more entries from aresource tracker.See lttng-track(1) to learn more about LTTng trackers.The untrack command removes specific resources from a tracker.The resources to remove must have been precedently added bylttng-track(1). It is also possible to remove all the resourcesfrom the whitelist using the --all option.As of this version, the only available tracker is the PIDtracker.ExampleOne common operation is to create a tracing session (seelttng-create(1)), remove all the entries from the PID trackerwhitelist, start tracing, and then manually track PIDs whiletracing is active.Assume the maximum system PID is 7 for this example.Command:$ lttng createInitial whitelist:[0] [1] [2] [3] [4] [5] [6] [7]Command:$ lttng untrack --userspace --pid --allWhitelist:[ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]Commands:$ lttng enable-event --userspace ...$ lttng start$ # ...$ lttng track --userspace --pid=3,5Whitelist:[ ] [ ] [ ] [3] [ ] [5] [ ] [ ]Command:$ lttng track --userspace --pid=2Whitelist:[ ] [ ] [2] [3] [ ] [5] [ ] [ ]",
        "name": "lttng-untrack - Remove one or more entries from an LTTng resourcetracker",
        "section": 1
    },
    {
        "command": "lttng-version",
        "description": "The lttng version command outputs the version of LTTng-tools.The output of lttng version is broken down into the followingparts:\u2022Major, minor, and patch numbers\u2022Git commit information, if available\u2022Release name with its description\u2022LTTng project\u2019s website URL\u2022License information",
        "name": "lttng-version - Get the version of LTTng-tools",
        "section": 1
    },
    {
        "command": "lttng-view",
        "description": "The lttng view command launches an external trace viewer to viewthe current trace of a tracing session.If SESSION is omitted, the viewer is launched for the currenttracing session (see lttng-create(1) for more information aboutthe current tracing session). Otherwise, it is launched for theexisting tracing session named SESSION. lttng list outputs allthe existing tracing sessions (see lttng-list(1)).By default, the babeltrace(1) trace viewer is launched. Anothertrace viewer command can be specified using the --viewer option.By default, the trace path of the chosen tracing session is givenas the first positional argument to the trace viewer. This pathcan be overridden using the --trace-path option.",
        "name": "lttng-view - View the traces of an LTTng tracing session",
        "section": 1
    },
    {
        "command": "lxc-attach",
        "description": "lxc-attach runs the specified command inside the containerspecified by name. The container has to be running already.If no command is specified, the current default shell of the userrunning lxc-attach will be looked up inside the container andexecuted. This will fail if no such user exists inside thecontainer or the container does not have a working nsswitchmechanism.Previous versions of lxc-attach simply attached to the specifiednamespaces of a container and ran a shell or the specifiedcommand without first allocating a pseudo terminal. This madethem vulnerable to input faking via a TIOCSTI ioctl call afterswitching between userspace execution contexts with differentprivilege levels. Newer versions of lxc-attach will try toallocate a pseudo terminal file descriptor pair on the host andattach any standard file descriptors which refer to a terminal tothe container side of the pseudo terminal before executing ashell or command. Note, that if none of the standard filedescriptors refer to a terminal lxc-attach will not try toallocate a pseudo terminal. Instead it will simply attach to thecontainers namespaces and run a shell or the specified command.",
        "name": "lxc-attach - start a process inside a running container.",
        "section": 1
    },
    {
        "command": "lxc-autostart",
        "description": "lxc-autostart processes containers with lxc.start.auto set. Itlets the user start, shutdown, kill, restart containers in theright order, waiting the right time. Supports filtering bylxc.group or just run against all defined containers. It can alsobe used by external tools in list mode where no action will beperformed and the list of affected containers (and if relevant,delays) will be shown.The [-r], [-s] and [-k] options specify the action to perform.If none is specified, then the containers will be started.[-a]and [-g] are used to specify which containers will be affected.By default only containers without a lxc.group set will beaffected.[-t TIMEOUT] specifies the maximum amount of time towait for the container to complete the shutdown or reboot.",
        "name": "lxc-autostart - start/stop/kill auto-started containers",
        "section": 1
    },
    {
        "command": "lxc-cgroup",
        "description": "lxc-cgroup gets or sets the value of a state-object (e.g.,'cpuset.cpus') in the container's cgroup for the correspondingsubsystem (e.g., 'cpuset'). If no [value] is specified, thecurrent value of the state-object is displayed; otherwise it isset.Note that lxc-cgroup does not check that the state-object isvalid for the running kernel, or that the corresponding subsystemis contained in any mounted cgroup hierarchy.",
        "name": "lxc-cgroup - manage the control group associated with a container",
        "section": 1
    },
    {
        "command": "lxc-checkconfig",
        "description": "lxc-checkconfig check the current kernel for lxc support",
        "name": "lxc-checkconfig - check the current kernel for lxc support",
        "section": 1
    },
    {
        "command": "lxc-checkpoint",
        "description": "lxc-checkpoint checkpoints and restores containers.",
        "name": "lxc-checkpoint - checkpoint a container",
        "section": 1
    },
    {
        "command": "lxc-config",
        "description": "lxc-config queries the lxc system configuration and lets you listall valid keys or query individual keys for their value.",
        "name": "lxc-config - query LXC system configuration",
        "section": 1
    },
    {
        "command": "lxc-console",
        "description": "If the tty service has been configured and is available for thecontainer specified as parameter, this command will launch aconsole allowing to log on the container.The available tty are free slots taken by this command. Thatmeans if the container has four ttys available and the commandhas been launched four times each taking a different tty, thefifth command will fail because no console will be available.The command will connect to a tty. If the connection is lost orbroken, the command can be launched again and regain the tty atthe state it was before the disconnection.A ttynum of 0 may be given to attach to the container's/dev/console instead of its dev/tty<ttynum>.A keyboard escape sequence may be used to disconnect from the ttyand quit lxc-console. The default escape sequence is <Ctrl+a q>.",
        "name": "lxc-console - Launch a console for the specified container",
        "section": 1
    },
    {
        "command": "lxc-copy",
        "description": "lxc-copy creates and optionally starts (ephemeral or non-ephemeral) copies of existing containers.lxc-copy creates copies of existing containers. Copies can becomplete clones of the original container. In this case the wholeroot filesystem of the container is simply copied to the newcontainer. Or they can be snapshots, i.e. small copy-on-writecopies of the original container. In this case the specifiedbacking storage for the copy must support snapshots. Thiscurrently includes btrfs, lvm (lvm devices do not supportsnapshots of snapshots.), overlay, and zfs.The copy's backing storage will be of the same type as theoriginal container. overlay snapshots of directory backedcontainers are exempted from this rule.When the -e flag is specified an ephemeral snapshot of theoriginal container is created and started. Ephemeral containerswill have lxc.ephemeral = 1 set in their config file and will bedestroyed on shutdown. When -e is used in combination with -D anon-ephemeral snapshot of the original container is created andstarted.Ephemeral containers can also be placed on a tmpfs with-t flag. NOTE: If an ephemeral container that is placed on atmpfs is rebooted all changes made to it will currently be lost!When -e is specified and no newname is given via -N a random namefor the snapshot will be chosen.Containers created and started with -e can have custom mounts.These are specified with the -m flag. Currently two types ofmounts are supported: bind, and overlay. Mount types arespecified as suboptions to the -m flag and can be specifiedmultiple times separated by commas. overlay mounts are currentlyspecified in the format -m overlay=/src:/dest. When nodestination dest is specified dest will be identical to src.Read-only bind mounts are specified -m bind=/src:/dest:ro andread-write bind mounts -m bind=/src:/dest:rw. Read-write bindmounts are the default and rw can be missing when a read-writemount is wanted. When dest is missing dest will be identical tosrc. An example for multiple mounts would be -mbind=/src1:/dest1:ro,bind=/src2:ro,overlay=/src3:/dest3.The mounts, their options, and formats supported via the -m flagare subject to change.",
        "name": "lxc-copy - copy an existing container.",
        "section": 1
    },
    {
        "command": "lxc-create",
        "description": "lxc-create creates a system object where is stored theconfiguration information and where can be stored userinformation. The identifier name is used to specify the containerto be used with the different lxc commands.The object is a directory created in /var/lib/lxc and identifiedby its name.The object is the definition of the different resources anapplication can use or can see. The more the configuration filecontains information, the more the container is isolated and themore the application is jailed.If the configuration file config_file is not specified, thecontainer will be created with the default isolation: processes,sysv ipc and mount points.",
        "name": "lxc-create - creates a container",
        "section": 1
    },
    {
        "command": "lxc-destroy",
        "description": "lxc-destroy destroys the system object previously created by thelxc-create command.",
        "name": "lxc-destroy - destroy a container.",
        "section": 1
    },
    {
        "command": "lxc-device",
        "description": "lxc-device manages devices in running container.",
        "name": "lxc-device - manage devices of running containers",
        "section": 1
    },
    {
        "command": "lxc-execute",
        "description": "lxc-execute runs the specified command inside the containerspecified by name.It will setup the container according to the configurationpreviously defined with the lxc-create command or with theconfiguration file parameter.If no configuration is defined,the default isolation is used.This command is mainly used when you want to quickly launch anapplication in an isolated environment.lxc-execute command will run the specified command into thecontainer via an intermediate process, lxc-init.This lxc-initafter launching the specified command, will wait for its end andall other reparented processes.(to support daemons in thecontainer).In other words, in the container, lxc-init has thepid 1 and the first process of the application has the pid 2.The above lxc-init is designed to forward received signals to thestarted command.",
        "name": "lxc-execute - run an application inside a container.",
        "section": 1
    },
    {
        "command": "lxc-freeze",
        "description": "lxc-freeze freezes all the processes running inside thecontainer. The processes will be blocked until they areexplicitly thawed by the lxc-unfreeze command. This command isuseful for batch managers to schedule a group of processes.",
        "name": "lxc-freeze - freeze all the container's processes",
        "section": 1
    },
    {
        "command": "lxc-info",
        "description": "lxc-info queries and shows information about a container.",
        "name": "lxc-info - query information about a container",
        "section": 1
    },
    {
        "command": "lxc-ls",
        "description": "lxc-ls list the containers existing on the system.",
        "name": "lxc-ls - list the containers existing on the system",
        "section": 1
    },
    {
        "command": "lxc-monitor",
        "description": "lxc-monitor monitors the state of containers. The name argumentmay be used to specify which containers to monitor. It is aregular expression, conforming with posix2, so it is possible tomonitor all the containers, several of them or just one. If notspecified, name will default to '.*' which will monitor allcontainers in lxcpath.The -P, --lxcpath=PATH option may be specified multiple times tomonitor more than one container path. Note however thatcontainers with the same name in multiple paths will beindistinguishable in the output.",
        "name": "lxc-monitor - monitor the container state",
        "section": 1
    },
    {
        "command": "lxc-snapshot",
        "description": "lxc-snapshot creates, lists, and restores container snapshots.Snapshots are stored as snapshotted containers under thecontainer's configuration path. For instance, if the container'sconfiguration path is /var/lib/lxc and the container is c1, thenthe first snapshot will be stored as container snap0 under thepath /var/lib/lxc/c1/snaps.If /var/lib/lxcsnaps, as used by LXC1.0, already exists, then it will continue to be used.",
        "name": "lxc-snapshot - Snapshot an existing container.",
        "section": 1
    },
    {
        "command": "lxc-start",
        "description": "lxc-start runs the specified command inside the containerspecified by name.It will setup the container according to the configurationpreviously defined with the lxc-create command or with theconfiguration file parameter.If no configuration is defined,the default isolation is used.If no command is specified, lxc-start will use the commanddefined in lxc.init.cmd or if not set, the default \"/sbin/init\"command to run a system container.",
        "name": "lxc-start - run an application inside a container.",
        "section": 1
    },
    {
        "command": "lxc-stop",
        "description": "lxc-stop reboots, cleanly shuts down, or kills all the processesinside the container. By default, it will request a cleanshutdown of the container by sending lxc.signal.halt (defaults toSIGPWR) to the container's init process, waiting up to 60 secondsfor the container to exit, and then returning. If the containerfails to cleanly exit in 60 seconds, it will be sent thelxc.signal.stop (defaults to SIGKILL) to force it to shut down. Arequest to reboot will send the lxc.signal.reboot (defaults toSIGINT) to the container's init process.The [-W], [-r], [-k] and [--nokill] options specify the action toperform.[-W] indicates that after performing the specifiedaction, lxc-stop should immediately exit, while [-t TIMEOUT]specifies the maximum amount of time to wait for the container tocomplete the shutdown or reboot.",
        "name": "lxc-stop - stop the application running inside a container",
        "section": 1
    },
    {
        "command": "lxc-top",
        "description": "lxc-top displays container statistics. The output is updatedevery delay seconds, and is ordered according to the sortby valuegiven. lxc-top will display as many containers as can fit in yourterminal. Press 'q' to quit. Press one of the sort key letters tosort by that statistic. Pressing a sort key letter a second timereverses the sort order.",
        "name": "lxc-top - monitor container statistics",
        "section": 1
    },
    {
        "command": "lxc-unfreeze",
        "description": "lxc-unfreeze will thaw all the processes previously frozen by thelxc-freeze command.",
        "name": "lxc-unfreeze - thaw all the container's processes",
        "section": 1
    },
    {
        "command": "lxc-unshare",
        "description": "lxc-unshare can be used to run a task in a cloned set ofnamespaces. This command is mainly provided for testing purposes.Despite its name, it always uses clone rather than unshare tocreate the new task with fresh namespaces. Apart from testingkernel regressions this should make no difference.",
        "name": "lxc-unshare - Run a task in a new set of namespaces.",
        "section": 1
    },
    {
        "command": "lxc-update-config",
        "description": "lxc-update-config detects any legacy configuration keys in thegiven config file and will replace them with the appropriate newconfiguration keys.lxc-update-config will first create a backup of the old configfile in the same directory and name it config.backup and thenupdate the original config file in place. In case the updatefails to apply or leads to an invalid config file that cannot beused to start a container users can either compare config withconfig.backup and try to manually repair any the invalidconfiguration keys or simply rollback to the legacy configurationfile by copying config.backup to config.Any failures for lxc-update-config to generate a useable configfile are a bug and should be reported upstream.",
        "name": "lxc-update-config - update a legacy pre LXC 2.1 configurationfile",
        "section": 1
    },
    {
        "command": "lxc-user-nic",
        "description": "lxc-user-nic is a setuid-root program with which unprivilegedusers may manage network interfaces for use by a lxc container.It will consult the configuration file /etc/lxc/lxc-usernet todetermine the number of interfaces which the calling user isallowed to create, and which bridge they may attach them to. Ittracks the number of interfaces each user has created using thefile /run/lxc/nics. It ensures that the calling user isprivileged over the network namespace to which the interface willbe attached.lxc-user-nic also allows one to delete networkdevices.Currently only ovs ports can be deleted.",
        "name": "lxc-user-nic - Manage nics in another network namespace",
        "section": 1
    },
    {
        "command": "lxc-usernsexec",
        "description": "lxc-usernsexec can be used to run a task as root in a new usernamespace.",
        "name": "lxc-usernsexec - Run a task as root in a new user namespace.",
        "section": 1
    },
    {
        "command": "lxc-wait",
        "description": "lxc-wait waits for a specific container state before exiting,this is useful for scripting.",
        "name": "lxc-wait - wait for a specific container state",
        "section": 1
    },
    {
        "command": "machinectl",
        "description": "machinectl may be used to introspect and control the state of thesystemd(1) virtual machine and container registration managersystemd-machined.service(8).machinectl may be used to execute operations on machines andimages. Machines in this sense are considered running instancesof:\u2022Virtual Machines (VMs) that virtualize hardware to run fulloperating system (OS) instances (including their kernels) ina virtualized environment on top of the host OS.\u2022Containers that share the hardware and OS kernel with thehost OS, in order to run OS userspace instances on top thehost OS.\u2022The host system itself.Machines are identified by names that follow the same rules asUNIX and DNS hostnames. For details, see below.Machines are instantiated from disk or file system images thatfrequently \u2014 but not necessarily \u2014 carry the same name asmachines running from them. Images in this sense may be:\u2022Directory trees containing an OS, including the top-leveldirectories /usr/, /etc/, and so on.\u2022btrfs subvolumes containing OS trees, similar to regulardirectory trees.\u2022Binary \"raw\" disk image files containing MBR or GPT partitiontables and Linux file systems.\u2022Similarly, block devices containing MBR or GPT partitiontables and file systems.\u2022The file system tree of the host OS itself.",
        "name": "machinectl - Control the systemd machine manager",
        "section": 1
    },
    {
        "command": "make",
        "description": "The make utility will determine automatically which pieces of alarge program need to be recompiled, and issue the commands torecompile them.The manual describes the GNU implementation ofmake, which was written by Richard Stallman and Roland McGrath,and is currently maintained by Paul Smith.Our examples show Cprograms, since they are very common, but you can use make withany programming language whose compiler can be run with a shellcommand.In fact, make is not limited to programs.You can useit to describe any task where some files must be updatedautomatically from others whenever the others change.To prepare to use make, you must write a file called the makefilethat describes the relationships among files in your program, andprovides commands for updating each file.In a program,typically the executable file is updated from object files, whichare in turn made by compiling source files.Once a suitable makefile exists, each time you change some sourcefiles, this simple shell command:makesuffices to perform all necessary recompilations.The makeprogram uses the makefile description and the last-modificationtimes of the files to decide which of the files need to beupdated.For each of those files, it issues the commandsrecorded in the makefile.make executes commands in the makefile to update one or moretargets, where target is typically a program.If no -f option ispresent, make will look for the makefiles GNUmakefile, makefile,and Makefile, in that order.Normally you should call your makefile either makefile orMakefile.(We recommend Makefile because it appears prominentlynear the beginning of a directory listing, right near otherimportant files such as README.)The first name checked,GNUmakefile, is not recommended for most makefiles.You shoulduse this name if you have a makefile that is specific to GNUmake, and will not be understood by other versions of make.Ifmakefile is '-', the standard input is read.make updates a target if it depends on prerequisite files thathave been modified since the target was last modified, or if thetarget does not exist.",
        "name": "make - GNU make utility to maintain groups of programs",
        "section": 1
    },
    {
        "command": "makeconv",
        "description": "makeconv converts the ICU converter table convertertable into abinary file. The binary file has the same base name asconvertertable but has a .cnv extension (instead of the typical.ucm extension of the convertertable file).This binary file canthen be read directly by ICU, or used by pkgdata(1) forincorporation into a larger archive or library.The convertertable must be in the ICU ucm (Unicode CodepageMapping) format in order to be understood by makeconv.The ICUucm format is similar to the IBM NLTC upmap/tpmap/rpmap files.Comments in the convertertable are handled as follows. If acomment (starting with a `#' sign) that is after some text doescontain the fallback indicator `|' then only the text startingwith the `#' sign, and ending before the `|' sign, is ignored.Otherwise, or if the comment is the first thing on the line, thecomment runs up to the end of the line. This special handling ofcomments is to accommodate the practice of putting fallbackinformation in comments in the strict IBM NLTC ucmap format.Note that new converters will be automatically found by ICU aftertheir installation in ICU's data directory. They do not need tobe listed in the convrtrs.txt(5) converters aliases file in orderto be available to applications using ICU.They do need to belisted there if one wants to give them aliases, or tags, though.",
        "name": "makeconv - compile a converter table",
        "section": 1
    },
    {
        "command": "man",
        "description": "man is the system's manual pager.Each page argument given toman is normally the name of a program, utility or function.Themanual page associated with each of these arguments is then foundand displayed.A section, if provided, will direct man to lookonly in that section of the manual.The default action is tosearch in all of the available sections following a pre-definedorder (see DEFAULTS), and to show only the first page found, evenif page exists in several sections.The table below shows the section numbers of the manual followedby the types of pages they contain.1Executable programs or shell commands2System calls (functions provided by the kernel)3Library calls (functions within program libraries)4Special files (usually found in /dev)5File formats and conventions, e.g. /etc/passwd6Games7Miscellaneous (including macro packages and conventions),e.g. man(7), groff(7), man-pages(7)8System administration commands (usually only for root)9Kernel routines [Non standard]A manual page consists of several sections.Conventional section names include NAME, SYNOPSIS, CONFIGURATION,DESCRIPTION, OPTIONS, EXIT STATUS, RETURN VALUE, ERRORS,ENVIRONMENT, FILES, VERSIONS, CONFORMING TO, NOTES, BUGS,EXAMPLE, AUTHORS, and SEE ALSO.The following conventions apply to the SYNOPSIS section and canbe used as a guide in other sections.bold texttype exactly as shown.italic textreplace with appropriate argument.[-abc]any or all arguments within [ ] are optional.-a|-boptions delimited by | cannot be usedtogether.argument ...argument is repeatable.[expression] ...entire expression within [ ] is repeatable.Exact rendering may vary depending on the output device.Forinstance, man will usually not be able to render italics whenrunning in a terminal, and will typically use underlined orcoloured text instead.The command or function illustration is a pattern that shouldmatch all possible invocations.In some cases it is advisable toillustrate several exclusive invocations as is shown in theSYNOPSIS section of this manual page.",
        "name": "man - an interface to the system reference manuals",
        "section": 1
    },
    {
        "command": "man-recode",
        "description": "man-recode converts multiple manual pages from one encoding toanother, guessing the appropriate input encoding for each one.It is useful when permanently recoding pages written in legacycharacter sets, or in build systems that need to recode a set ofpages to a single common encoding (usually UTF-8) forinstallation.When converting many manual pages, this program ismuch faster than running man --recode or manconv on each page.If an encoding declaration is found on the first line of a manualpage, then that declaration is used as the input encoding forthat page.Failing that, the input encoding is guessed based onthe file name.Encoding declarations have the following form:'\\\" -*- coding: UTF-8 -*-or (if manual page preprocessors are also to be declared):'\\\" t -*- coding: ISO-8859-1 -*-",
        "name": "man-recode - convert manual pages to another encoding",
        "section": 1
    },
    {
        "command": "manconv",
        "description": "manconv converts a manual page from one encoding to another, likeiconv.Unlike iconv, it can try multiple possible inputencodings in sequence.This is useful for manual pages installedin directories without an explicit encoding declaration, sincethey may be in UTF-8 or in a legacy character set.If an encoding declaration is found on the first line of themanual page, that declaration overrides any input encodingsspecified on manconv's command line.Encoding declarations havethe following form:'\\\" -*- coding: UTF-8 -*-or (if manual page preprocessors are also to be declared):'\\\" t -*- coding: ISO-8859-1 -*-",
        "name": "manconv - convert manual page from one encoding to another",
        "section": 1
    },
    {
        "command": "manpath",
        "description": "If $MANPATH is set, manpath will simply display its contents andissue a warning.If not, manpath will determine a suitablemanual page hierarchy search path and display the results.The colon-delimited path is determined using information gainedfrom the man-db configuration file \u2013 (/usr/local/etc/man_db.conf)and the user's environment.",
        "name": "manpath - determine search path for manual pages",
        "section": 1
    },
    {
        "command": "mariabackup",
        "description": "Use mariabackup --help for details on usage.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "mariabackup - Backup tool",
        "section": 1
    },
    {
        "command": "mariadb",
        "description": "mysql is a simple SQL shell (with GNU readline capabilities). Itsupports interactive and non-interactive use. When usedinteractively, query results are presented in an ASCII-tableformat. When used non-interactively (for example, as a filter),the result is presented in tab-separated format. The outputformat can be changed using command options.If you have problems due to insufficient memory for large resultsets, use the --quick option. This forces mysql to retrieveresults from the server a row at a time rather than retrievingthe entire result set and buffering it in memory beforedisplaying it. This is done by returning the result set using themysql_use_result() C API function in the client/server libraryrather than mysql_store_result().Using mysql is very easy. Invoke it from the prompt of yourcommand interpreter as follows:shell> mysql db_nameOr:shell> mysql --user=user_name --password=your_password db_nameThen type an SQL statement, end it with \u201c;\u201d, \\g, or \\G and pressEnter.Typing Control-C causes mysql to attempt to kill the currentstatement. If this cannot be done, or Control-C is typed againbefore the statement is killed, mysql exits.You can execute SQL statements in a script file (batch file) likethis:shell> mysql db_name < script.sql > output.tab",
        "name": "mariadb - the MariaDB command-line tool (mysql is now a symlinkto mariadb)",
        "section": 1
    },
    {
        "command": "mariadb-access",
        "description": "mysqlaccess is a diagnostic tool written by Yves Carlier. Itchecks the access privileges for a host name, user name, anddatabase combination. Note that mysqlaccess checks access usingonly the user, db, and host tables. It does not check table,column, or routine privileges specified in the tables_priv,columns_priv, or procs_priv tables.Invoke mysqlaccess like this:shell> mysqlaccess [host_name [user_name [db_name]]] [options]mysqlaccess supports the following options.\u2022--help, -?Display a help message and exit.\u2022--brief, -bGenerate reports in single-line tabular format.\u2022--commitCopy the new access privileges from the temporary tables tothe original grant tables. The grant tables must be flushedfor the new privileges to take effect. (For example, executea mysqladmin reload command.)\u2022--copyReload the temporary grant tables from original ones.\u2022--db=db_name, -d db_nameSpecify the database name.\u2022--debug=NSpecify the debug level.N can be an integer from 0 to 3.\u2022--host=host_name, -h host_nameThe host name to use in the access privileges.\u2022--howtoDisplay some examples that show how to use mysqlaccess.\u2022--old_serverConnect to a very old MySQL server (before MySQL 3.21) thatdoes not know how to handle full WHERE clauses.\u2022--password[=password], -p[password]The password to use when connecting to the server. If youomit the password value following the --password or -p optionon the command line, mysqlaccess prompts for one.Specifying a password on the command line should beconsidered insecure. See Section 5.3.2.2, \u201cEnd-UserGuidelines for Password Security\u201d.\u2022--planDisplay suggestions and ideas for future releases.\u2022--previewShow the privilege differences after making changes to thetemporary grant tables.\u2022--relnotesDisplay the release notes.\u2022--rhost=host_name, -H host_nameConnect to the MariaDB server on the given host.\u2022--rollbackUndo the most recent changes to the temporary grant tables.\u2022--spassword[=password], -P[password]The password to use when connecting to the server as thesuperuser. If you omit the password value following the--spassword or -p option on the command line, mysqlaccessprompts for one.Specifying a password on the command line should beconsidered insecure. See Section 5.3.2.2, \u201cEnd-UserGuidelines for Password Security\u201d.\u2022--superuser=user_name, -U user_nameSpecify the user name for connecting as the superuser.\u2022--table, -tGenerate reports in table format.\u2022--user=user_name, -u user_nameThe user name to use in the access privileges.\u2022--version, -vDisplay version information and exit.If your MariaDB distribution is installed in some non-standardlocation, you must change the location where mysqlaccess expectsto find the mysql client. Edit the mysqlaccess script atapproximately line 18. Search for a line that looks like this:$MYSQL= \u00b4/usr/local/bin/mysql\u00b4;# path to mysql executableChange the path to reflect the location where mysql actually isstored on your system. If you do not do this, a Broken pipe errorwill occur when you run mysqlaccess.",
        "name": "mariadb-access - client for checking access privileges(mysqlaccess is now a symlink to mariadb-access)",
        "section": 1
    },
    {
        "command": "mariadb-admin",
        "description": "mysqladmin is a client for performing administrative operations.You can use it to check the server\u00b4s configuration and currentstatus, to create and drop databases, and more.Invoke mysqladmin like this:shell> mysqladmin [options] command [command-arg] [command [command-arg]] ...mysqladmin supports the following commands. Some of the commandstake an argument following the command name.\u2022create db_nameCreate a new database named db_name.\u2022debugTell the server to write debug information to the error log.This also includes information about the Event Scheduler.\u2022drop db_nameDelete the database named db_name and all its tables.\u2022extended-statusDisplay the server status variables and their values.\u2022flush-all-statisticsFlush all statistics tables.\u2022flush-all-statusFlush all status and statistics.\u2022flush-binary-logFlush the binary log.\u2022flush-client-statisticsFlush client statistics.\u2022flush-engine-logFlush engine log.\u2022flush-error-logFlush error log.\u2022flush-general-logFlush general query log.\u2022flush-hostsFlush all information in the host cache.\u2022flush-index-statisticsFlush index statistics.\u2022flush-logsFlush all logs.\u2022flush-privilegesReload the grant tables (same as reload).\u2022flush-relay-logFlush relay log.\u2022flush-slow-logFlush slow query log.\u2022flush-sslFlush SSL certificates.\u2022flush-statusClear status variables.\u2022flush-table-statisticsFlush table statistics.\u2022flush-tablesFlush all tables.\u2022flush-threadsFlush the thread cache.\u2022flush-user-resourcesFlush user resources.\u2022kill id,id,...Kill server threads. If multiple thread ID values are given,there must be no spaces in the list.\u2022old-password new-passwordThis is like the password command but stores the passwordusing the old (pre MySQL 4.1) password-hashing format.\u2022password new-passwordSet a new password. This changes the password to new-passwordfor the account that you use with mysqladmin for connectingto the server. Thus, the next time you invoke mysqladmin (orany other client program) using the same account, you willneed to specify the new password.If the new-password value contains spaces or other charactersthat are special to your command interpreter, you need toenclose it within quotes. On Windows, be sure to use doublequotes rather than single quotes; single quotes are notstripped from the password, but rather are interpreted aspart of the password. For example:shell> mysqladmin password \"my new password\"CautionDo not use this command used if the server was startedwith the --skip-grant-tables option. No password changewill be applied. This is true even if you precede thepassword command with flush-privileges on the samecommand line to re-enable the grant tables because theflush operation occurs after you connect. However, youcan use mysqladmin flush-privileges to re-enable thegrant table and then use a separate mysqladmin passwordcommand to change the password.\u2022pingCheck whether the server is alive. The return status frommysqladmin is 0 if the server is running, 1 if it is not.This is 0 even in case of an error such as Access denied,because this means that the server is running but refused theconnection, which is different from the server not running.\u2022processlistShow a list of active server threads. This is like the outputof the SHOW PROCESSLIST statement. If the --verbose option isgiven, the output is like that of SHOW FULL PROCESSLIST.\u2022reloadReload the grant tables.\u2022refreshFlush all tables and close and open log files.\u2022shutdownStop the server.\u2022start-all-slavesStart all slaves.\u2022start-slaveStart replication on a slave server.\u2022statusDisplay a short server status message.\u2022stop-all-slavesStop all slaves.\u2022stop-slaveStop replication on a slave server.\u2022variablesDisplay the server system variables and their values.\u2022versionDisplay version information from the server.All commands can be shortened to any unique prefix. For example:shell> mysqladmin proc stat+----+-------+-----------+----+---------+------+-------+------------------+| Id | User| Host| db | Command | Time | State | Info|+----+-------+-----------+----+---------+------+-------+------------------+| 51 | monty | localhost || Query| 0|| show processlist |+----+-------+-----------+----+---------+------+-------+------------------+Uptime: 1473624Threads: 1Questions: 39487Slow queries: 0Opens: 541Flush tables: 1Open tables: 19Queries per second avg: 0.0268The mysqladmin status command result displays the followingvalues:\u2022UptimeThe number of seconds the MariaDB server has been running.\u2022ThreadsThe number of active threads (clients).\u2022QuestionsThe number of questions (queries) from clients since theserver was started.\u2022Slow queriesThe number of queries that have taken more thanlong_query_time seconds.\u2022OpensThe number of tables the server has opened.\u2022Flush tablesThe number of flush-*, refresh, and reload commands theserver has executed.\u2022Open tablesThe number of tables that currently are open.\u2022Memory in useThe amount of memory allocated directly by mysqld. This valueis displayed only when MariaDB has been compiled with--with-debug=full.\u2022Maximum memory usedThe maximum amount of memory allocated directly by mysqld.This value is displayed only when MariaDB has been compiledwith --with-debug=full.If you execute mysqladmin shutdown when connecting to a localserver using a Unix socket file, mysqladmin waits until theserver\u00b4s process ID file has been removed, to ensure that theserver has stopped properly.mysqladmin supports the following options, which can be specifiedon the command line or in the [mysqladmin] and [client] optionfile groups.\u2022--help, -?Display help and exit.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--connect-timeout=timeoutEquivalent to --connect_timeout, see the end of this section.\u2022--count=N, -c NThe number of iterations to make for repeated commandexecution if the --sleep option is given.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is\u00b4d:t:o,/tmp/mysqladmin.trace\u00b4.\u2022--debug-checkCheck memory and open file usage at exit..\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-authDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files. Must be given as firstoption.\u2022--force, -fDo not ask for confirmation for the drop db_name command.With multiple commands, continue even if an error occurs.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--local, -lSuppress the SQL command(s) from being written to the binarylog by using FLUSH LOCAL or enabling sql_log_bin=0 for thesession.\u2022--no-beep, -bSuppress the warning beep that is emitted by default forerrors such as a failure to connect to the server.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqladmin prompts for one.Specifying a password on the command line should beconsidered insecure.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection or 0 fordefault to, in order of preference, my.cnf, $MYSQL_TCP_PORT,/etc/services, built-in default (3306).Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--print-defaultsPrint the program argument list and exit. This must be givenas the first argument.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--relative, -rShow the difference between the current and previous valueswhen used with the --sleep option. Currently, this optionworks only with the extended-status command.\u2022--shutdown-timeouttimeoutEquivalent of --shutdown_timeout, see the end of thissection.\u2022--silent, -sExit silently if a connection to the server cannot beestablished.\u2022--sleep=delay, -i delayExecute commands repeatedly, sleeping for delay seconds inbetween. The --count option determines the number ofiterations. If --count is not given, mysqladmin executescommands indefinitely until interrupted.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--tls-version=name,Accepts a comma-separated list of TLS protocol versions. ATLS protocol version will only be enabled if it is present inthis list. All other TLS protocol versions will not bepermitted.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.\u2022--version, -VDisplay version information and exit.\u2022--vertical, -EPrint output vertically. This is similar to --relative, butprints output vertically.\u2022--wait[=count], -w[count]If the connection cannot be established, wait and retryinstead of aborting. If a count value is given, it indicatesthe number of times to retry. The default is one time.\u2022--wait-for-all-slavesWait for the last binlog event to be sent to all connectedslaves before shutting down.This option is off by default.You can also set the following variables by using--var_name=value\u2022connect_timeoutThe maximum number of seconds before connection timeout. Thedefault value is 43200 (12 hours).\u2022shutdown_timeoutThe maximum number of seconds to wait for server shutdown.The default value is 3600 (1 hour).",
        "name": "mariadb-admin - client for administering a MariaDB server(mysqladmin is now a symlink to mariadb-admin)",
        "section": 1
    },
    {
        "command": "mariadb-binlog",
        "description": "The server\u00b4s binary log consists of files containing \u201cevents\u201dthat describe modifications to database contents. The serverwrites these files in binary format. To display their contents intext format, use the mysqlbinlog utility. You can also usemysqlbinlog to display the contents of relay log files written bya slave server in a replication setup because relay logs have thesame format as binary logs.Invoke mysqlbinlog like this:shell> mysqlbinlog [options] log_file ...For example, to display the contents of the binary log file namedbinlog.000003, use this command:shell> mysqlbinlog binlog.0000003The output includes events contained in binlog.000003. Forstatement-based logging, event information includes the SQLstatement, the ID of the server on which it was executed, thetimestamp when the statement was executed, how much time it took,and so forth. For row-based logging, the event indicates a rowchange rather than an SQL statement.Events are preceded by header comments that provide additionalinformation. For example:# at 141#1003099:28:36 server id 123end_log_pos 245Query thread_id=3350exec_time=11error_code=0In the first line, the number following at indicates the startingposition of the event in the binary log file.The second line starts with a date and time indicating when thestatement started on the server where the event originated. Forreplication, this timestamp is propagated to slave servers.server id is the server_id value of the server where the eventoriginated.end_log_pos indicates where the next event starts(that is, it is the end position of the current event + 1).thread_id indicates which thread executed the event.exec_timeis the time spent executing the event, on a master server. On aslave, it is the difference of the end execution time on theslave minus the beginning execution time on the master. Thedifference serves as an indicator of how much replication lagsbehind the master.error_code indicates the result fromexecuting the event. Zero means that no error occurred.The output from mysqlbinlog can be re-executed (for example, byusing it as input to mysql) to redo the statements in the log.This is useful for recovery operations after a server crash. Forother usage examples, see the discussion later in this section.Normally, you use mysqlbinlog to read binary log files directlyand apply them to the local MariaDB server. It is also possibleto read binary logs from a remote server by using the--read-from-remote-server option. To read remote binary logs, theconnection parameter options can be given to indicate how toconnect to the server. These options are --host, --password,--port, --protocol, --socket, and --user; they are ignored exceptwhen you also use the --read-from-remote-server option.mysqlbinlog supports the following options, which can bespecified on the command line or in the [mysqlbinlog] and[client] option file groups.\u2022--help, -?Display a help message and exit.\u2022--base64-output=valueThis option determines when events should be displayedencoded as base-64 strings using BINLOG statements. Theoption has these allowable values (not case sensitive):\u2022AUTO (\"automatic\") or UNSPEC (\"unspecified\") displaysBINLOG statements automatically when necessary (that is,for format description events and row events). This isthe default if no --base64-output option is given.NoteAutomatic BINLOG display is the only safe behavior ifyou intend to use the output of mysqlbinlog tore-execute binary log file contents. The other optionvalues are intended only for debugging or testingpurposes because they may produce output that doesnot include all events in executable form.\u2022NEVER causes BINLOG statements not to be displayed.mysqlbinlog exits with an error if a row event is foundthat must be displayed using BINLOG.\u2022DECODE-ROWS specifies to mysqlbinlog that you intend forrow events to be decoded and displayed as commented SQLstatements by also specifying the --verbose option. LikeNEVER, DECODE-ROWS suppresses display of BINLOGstatements, but unlike NEVER, it does not exit with anerror if a row event is found.The --base64-output can be given as --base64-output or--skip-base64-output (with the sense of AUTO or NEVER).For examples that show the effect of --base64-output and--verbose on row event output, see the section called\u201cMYSQLBINLOG ROW EVENT DISPLAY\u201d.\u2022--binlog-row-event-max-size=pathThe directory where character sets are installed.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--database=db_name, -d db_nameThis option causes mysqlbinlog to output entries from thebinary log (local log only) that occur while db_name has beenselected as the default database by USE.The --database option for mysqlbinlog is similar to the--binlog-do-db option for mysqld, but can be used to specifyonly one database. If --database is given multiple times,only the last instance is used.The effects of this option depend on whether thestatement-based or row-based logging format is in use, in thesame way that the effects of --binlog-do-db depend on whetherstatement-based or row-based logging is in use.Statement-based logging. The --database option works asfollows:\u2022While db_name is the default database, statements areoutput whether they modify tables in db_name or adifferent database.\u2022Unless db_name is selected as the default database,statements are not output, even if they modify tables indb_name.\u2022There is an exception for CREATE DATABASE, ALTERDATABASE, and DROP DATABASE. The database being created,altered, or dropped is considered to be the defaultdatabase when determining whether to output thestatement.Suppose that the binary log was created by executingthese statements using statement-based-logging:INSERT INTO test.t1 (i) VALUES(100);INSERT INTO db2.t2 (j)VALUES(200);USE test;INSERT INTO test.t1 (i) VALUES(101);INSERT INTO t1 (i)VALUES(102);INSERT INTO db2.t2 (j)VALUES(201);USE db2;INSERT INTO test.t1 (i) VALUES(103);INSERT INTO db2.t2 (j)VALUES(202);INSERT INTO t2 (j)VALUES(203);mysqlbinlog --database=test does not output the first twoINSERT statements because there is no default database.It outputs the three INSERT statements following USEtest, but not the three INSERT statements following USEdb2.mysqlbinlog --database=db2 does not output the first twoINSERT statements because there is no default database.It does not output the three INSERT statements followingUSE test, but does output the three INSERT statementsfollowing USE db2.Row-based logging.mysqlbinlog outputs only entries thatchange tables belonging to db_name. The default databasehas no effect on this. Suppose that the binary log justdescribed was created using row-based logging rather thanstatement-based logging.mysqlbinlog --database=testoutputs only those entries that modify t1 in the testdatabase, regardless of whether USE was issued or whatthe default database is.If a server is running withbinlog_format set to MIXED and you want it to be possibleto use mysqlbinlog with the --database option, you mustensure that tables that are modified are in the databaseselected by USE. (In particular, no cross-databaseupdates should be used.)NoteThis option did not work correctly for mysqlbinlogwith row-based logging prior to MySQL 5.1.37.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is\u00b4d:t:o,/tmp/mysqlbinlog.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--defaults-extra-file=nameRead this file after the global files are read.\u2022--defaults-file=nameOnly read default options from the given file.\u2022--default-auth=nameDefault authentication client-side plugin to use.\u2022--disable-log-bin, -DDisable binary logging. This is useful for avoiding anendless loop if you use the --to-last-log option and aresending the output to the same MariaDB server. This optionalso is useful when restoring after a crash to avoidduplication of the statements you have logged.This option requires that you have the SUPER privilege. Itcauses mysqlbinlog to include a SET sql_log_bin = 0 statementin its output to disable binary logging of the remainingoutput. The SET statement is ineffective unless you have theSUPER privilege.\u2022--force-if-openForce if binlog was not closed properly. Defaults to on; use--skip-force-if-open to disable.\u2022--force-read, -fWith this option, if mysqlbinlog reads a binary log eventthat it does not recognize, it prints a warning, ignores theevent, and continues. Without this option, mysqlbinlog stopsif it reads such an event.\u2022--hexdump, -HDisplay a hex dump of the log in comments, as described inthe section called \u201cMYSQLBINLOG HEX DUMP FORMAT\u201d. The hexoutput can be helpful for replication debugging.\u2022--host=host_name, -h host_nameGet the binary log from the MariaDB server on the given host.\u2022--local-load=path, -l pathPrepare local temporary files for LOAD DATA INFILE in thespecified directory.\u2022--no-defaultsDon't read default options from any option file.\u2022--offset=N, -o NSkip the first N entries in the log.\u2022--open-files-limit=NUMSets the open_files_limit variable, which is used to reservefile descriptors for mysqlbinlog.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlbinlog prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--print-defaultsPrint the program argument list from all option files andexit.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for connecting to a remoteserver, or 0 for default to, in order of preference, my.cnf,$MYSQL_TCP_PORT, /etc/services, built-in default (3306).Forces --protocol=tcp when specified on the command linewithout other connection properties.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--rawRequires -R. Output raw binlog data instead of SQLstatements. Output files named after server logs.\u2022--read-from-remote-server, -RRead the binary log from a MariaDB server rather than readinga local log file. Any connection parameter options areignored unless this option is given as well. These optionsare --host, --password, --port, --protocol, --socket, and--user.This option requires that the remote server be running. Itworks only for binary log files on the remote server, notrelay log files.\u2022--result-file=name, -r nameDirect output to the given file. With --raw this is a prefixfor the file names.\u2022--rewrite-db=name, -r nameUpdates to a database with a different name than theoriginal.Example: rewrite-db='from->to'. For events thatare binlogged as statements, rewriting the databaseconstitutes changing a statement's default database from db1to db2. There is no statement analysis or rewrite of anykind, that is, if one specifies \"db1.tbl\" in the statementexplicitly, that occurrence won't be changed to \"db2.tbl\".Row-based events are rewritten correctly to use the newdatabase name. Filtering (e.g. with --database=name) happensafter the database rewrites have been performed. If you usethis option on the command line and \">\" has a special meaningto your command interpreter, quote the value (e.g. --rewrite-db=\"oldname->newname\".\u2022--server-id=idDisplay only those events created by the server having thegiven server ID.\u2022--set-charset=charset_nameAdd a SET NAMES charset_name statement to the output tospecify the character set to be used for processing logfiles.\u2022--short-form, -sDisplay only the statements contained in the log, no extrainfo and no row-based events. This is for testing only, andshould not be used in production systems. If you want tosuppress base64-output, consider using --base64-output=neverinstead.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--start-datetime=datetimeStart reading the binary log at the first event having atimestamp equal to or later than the datetime argument. Thedatetime value is relative to the local time zone on themachine where you run mysqlbinlog. The value should be in aformat accepted for the DATETIME or TIMESTAMP data types. Forexample:shell> mysqlbinlog --start-datetime=\"2014-12-25 11:25:56\" binlog.000003This option is useful for point-in-time recovery.\u2022--start-position=N, -j NStart reading the binary log at N. Type can either be apositive integer or a GTID list. When using a positiveinteger, the value only applies to the first binlog passed onthe command line, and the first event that has a positionequal to or greater than N is printed. In GTID mode, multipleGTIDs can be passed as a comma separated list, where eachmust have a unique domain id. The list represents the gtidbinlog state that the client (another \"replica\" server) isaware of. Therefore, each GTID is exclusive; only eventsafter a given sequence number will be printed to allow usersto receive events after their current state.This option is useful for point-in-time recovery.\u2022--gtid-strict-modeProcess binlog according to gtid-strict-mode specification.The start, stop positions are verified to satisfy start <stop comparison condition. Sequence numbers of any gtiddomain must comprise monotically growing sequence.\u2022--stop-datetime=datetimeStop reading the binary log at the first event having atimestamp equal to or later than the datetime argument. Thisoption is useful for point-in-time recovery. See thedescription of the --start-datetime option for informationabout the datetime value.This option is useful for point-in-time recovery.\u2022--stop-neverWait for more data from the server instead of stopping at theend of the last log. Implies --to-last-log.\u2022--stop-never-slave-server-idThe slave server_id used for --read-from-remote-server--stop-never.\u2022--stop-position=NStop reading the binary log at the first event having aposition equal to or greater than N. Type can either be apositive integer or a GTID list. When using a positiveinteger, the value only applies to the last log file named onthe command line. When in GTID mode, multiple GTIDs can bepassed as a comma separated list, where each must have aunique domain id.Each GTID is inclusive; only events up tothe given sequence numbers are printed.This option is useful for point-in-time recovery.\u2022--table, -TList entries for just this table (local log only).\u2022--to-last-log, -tDo not stop at the end of the requested binary log from aMariaDB server, but rather continue printing until the end ofthe last binary log. If you send the output to the sameMariaDB server, this may lead to an endless loop, so thisoption requires --read-from-remote-server.\u2022--user=user_name, -u user_nameThe MariaDB username to use when connecting to a remoteserver.\u2022--verbose, -vReconstruct row events and display them as commented SQLstatements. If this option is given twice, the outputincludes comments to indicate column data types and somemetadata.If this option is given three times, the outputincludes diagnostic warnings about event integrity beforeprogram exit.For examples that show the effect of --base64-output and--verbose on row event output, see the section called\u201cMYSQLBINLOG ROW EVENT DISPLAY\u201d.\u2022--version, -VDisplay version information and exit.You can also set the following variable by using --var_name=valuesyntax:\u2022open_files_limitSpecify the number of open file descriptors to reserve.You can pipe the output of mysqlbinlog into the mysql client toexecute the events contained in the binary log. This technique isused to recover from a crash when you have an old backup. Forexample:shell> mysqlbinlog binlog.000001 | mysql -u root -pOr:shell> mysqlbinlog binlog.[0-9]* | mysql -u root -pYou can also redirect the output of mysqlbinlog to a text fileinstead, if you need to modify the statement log first (forexample, to remove statements that you do not want to execute forsome reason). After editing the file, execute the statements thatit contains by using it as input to the mysql program:shell> mysqlbinlog binlog.000001 > tmpfileshell> ... edit tmpfile ...shell> mysql -u root -p < tmpfileWhen mysqlbinlog is invoked with the --start-position option, itdisplays only those events with an offset in the binary loggreater than or equal to a given position (the given positionmust match the start of one event). It also has options to stopand start when it sees an event with a given date and time. Thisenables you to perform point-in-time recovery using the--stop-datetime option (to be able to say, for example, \u201crollforward my databases to how they were today at 10:30 a.m.\u201d).If you have more than one binary log to execute on the MariaDBserver, the safe method is to process them all using a singleconnection to the server. Here is an example that demonstrateswhat may be unsafe:shell> mysqlbinlog binlog.000001 | mysql -u root -p # DANGER!!shell> mysqlbinlog binlog.000002 | mysql -u root -p # DANGER!!Processing binary logs this way using different connections tothe server causes problems if the first log file contains aCREATE TEMPORARY TABLE statement and the second log contains astatement that uses the temporary table. When the first mysqlprocess terminates, the server drops the temporary table. Whenthe second mysql process attempts to use the table, the serverreports \u201cunknown table.\u201dTo avoid problems like this, use a single mysql process toexecute the contents of all binary logs that you want to process.Here is one way to do so:shell> mysqlbinlog binlog.000001 binlog.000002 | mysql -u root -pAnother approach is to write all the logs to a single file andthen process the file:shell> mysqlbinlog binlog.000001 >/tmp/statements.sqlshell> mysqlbinlog binlog.000002 >> /tmp/statements.sqlshell> mysql -u root -p -e \"source /tmp/statements.sql\"mysqlbinlog can produce output that reproduces a LOAD DATA INFILEoperation without the original data file.mysqlbinlog copies thedata to a temporary file and writes a LOAD DATA LOCAL INFILEstatement that refers to the file. The default location of thedirectory where these files are written is system-specific. Tospecify a directory explicitly, use the --local-load option.Because mysqlbinlog converts LOAD DATA INFILE statements to LOADDATA LOCAL INFILE statements (that is, it adds LOCAL), both theclient and the server that you use to process the statements mustbe configured with the LOCAL capability enabled.WarningThe temporary files created for LOAD DATA LOCAL statementsare not automatically deleted because they are needed untilyou actually execute those statements. You should delete thetemporary files yourself after you no longer need thestatement log. The files can be found in the temporary filedirectory and have names like original_file_name-#-#.",
        "name": "mariadb-binlog - utility for processing binary log files(mysqlbinlog is now a symlink to mariadb-binlog)",
        "section": 1
    },
    {
        "command": "mariadb-check",
        "description": "The mysqlcheck client performs table maintenance: It checks,repairs, optimizes, or analyzes tables.Each table is locked and therefore unavailable to other sessionswhile it is being processed, although for check operations, thetable is locked with a READ lock only. Table maintenanceoperations can be time-consuming, particularly for large tables.If you use the --databases or --all-databases option to processall tables in one or more databases, an invocation of mysqlcheckmight take a long time. (This is also true for mysql_upgradebecause that program invokes mysqlcheck to check all tables andrepair them if necessary.)mysqlcheck is similar in function to myisamchk, but worksdifferently. The main operational difference is that mysqlcheckmust be used when the mysqld server is running, whereas myisamchkshould be used when it is not. The benefit of using mysqlcheck isthat you do not have to stop the server to perform tablemaintenance.mysqlcheck uses the SQL statements CHECK TABLE, REPAIR TABLE,ANALYZE TABLE, and OPTIMIZE TABLE in a convenient way for theuser. It determines which statements to use for the operation youwant to perform, and then sends the statements to the server tobe executed.The MyISAM storage engine supports all four maintenanceoperations, so mysqlcheck can be used to perform any of them onMyISAM tables. Other storage engines do not necessarily supportall operations. In such cases, an error message is displayed. Forexample, if test.t is a MEMORY table, an attempt to check itproduces this result:shell> mysqlcheck test ttest.tnote: The storage engine for the table doesn\u00b4t support checkIf mysqlcheck is unable to repair a table, see the MariaDBKnowledge Base for manual table repair strategies. This will bethe case, for example, for InnoDB tables, which can be checkedwith CHECK TABLE, but not repaired with REPAIR TABLE.The use of mysqlcheck with partitioned tables is not supported.CautionIt is best to make a backup of a table before performing atable repair operation; under some circumstances theoperation might cause data loss. Possible causes include butare not limited to file system errors.There are three general ways to invoke mysqlcheck:shell> mysqlcheck [options] db_name [tbl_name ...]shell> mysqlcheck [options] --databases db_name ...shell> mysqlcheck [options] --all-databasesIf you do not name any tables following db_name or if you use the--databases or --all-databases option, entire databases arechecked.mysqlcheck has a special feature compared to other clientprograms. The default behavior of checking tables (--check) canbe changed by renaming the binary. If you want to have a toolthat repairs tables by default, you should just make a copy ofmysqlcheck named mysqlrepair, or make a symbolic link tomysqlcheck named mysqlrepair. If you invoke mysqlrepair, itrepairs tables.The following names can be used to change mysqlcheck defaultbehavior.\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502mysqlrepair\u2502 The default option is \u2502\u2502\u2502 --repair\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502mysqlanalyze\u2502 The default option is \u2502\u2502\u2502 --analyze\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502mysqloptimize \u2502 The default option is \u2502\u2502\u2502 --optimize\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518mysqlcheck supports the following options, which can be specifiedon the command line or in the [mysqlcheck] and [client] optionfile groups.The -c, -r, -a and -o options are exclusive to eachother.\u2022--help, -?Display a help message and exit.\u2022--all-databases, -ACheck all tables in all databases. This is the same as usingthe --databases option and naming all the databases on thecommand line.\u2022--all-in-1, -1Instead of issuing a statement for each table, execute asingle statement for each database that names all the tablesfrom that database to be processed.\u2022--analyze, -aAnalyze the tables.\u2022--auto-repairIf a checked table is corrupted, automatically fix it. Anynecessary repairs are done after all tables have beenchecked.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--check, -cCheck the tables for errors. This is the default operation.\u2022--check-only-changed, -CCheck only tables that have changed since the last check orthat have not been closed properly.\u2022--check-upgrade, -gInvoke CHECK TABLE with the FOR UPGRADE option to checktables for incompatibilities with the current version of theserver. This option automatically enables the --fix-db-namesand --fix-table-names options.\u2022--compressCompress all information sent between the client and theserver if both support compression.\u2022--databases, -BProcess all tables in the named databases. Normally,mysqlcheck treats the first name argument on the command lineas a database name and following names as table names. Withthis option, it treats all name arguments as database names.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is \u00b4d:t:o\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-auth=nameDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--extended, -eIf you are using this option to check tables, it ensures thatthey are 100% consistent but takes a long time.If you are using this option to repair tables, it will forceusing the old, slow, repair with keycache method, instead ofthe much faster repair by sorting.\u2022--fast, -FCheck only tables that have not been closed properly.\u2022--fix-db-namesConvert database names to the format used since MySQL 5.1.Only database names that contain special characters areaffected.\u2022--fix-table-namesConvert table names (including views) to the format usedsince MySQL 5.1. Only table names that contain specialcharacters are affected.\u2022--flush,Flush each table after check. This is useful if you don'twant to have the checked tables take up space in the cachesafter the check.\u2022--force, -fContinue even if an SQL error occurs.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--medium-check, -mDo a check that is faster than an --extended operation. Thisfinds only 99.99% of all errors, which should be good enoughin most cases.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--optimize, -oOptimize the tables.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlcheck prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--persistent, -ZUsed with ANALYZE TABLE to append the option PERSISENT FORALL.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dir=nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--process-tablesPerform the requested operation on tables. Defaults to on;use --skip-process-tables to disable.\u2022--process-views=valPerform the requested operation (only CHECK VIEW or REPAIRVIEW). Possible values are NO, YES (correct the checksum, ifnecessary, add the mariadb-version field), UPGRADE_FROM_MYSQL(same as YES and toggle the algorithm MERGE<->TEMPTABLE.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--quick, -qIf you are using this option to check tables, it prevents thecheck from scanning the rows to check for incorrect links.This is the fastest check method.If you are using this option to repair tables, it tries torepair only the index tree. This is the fastest repairmethod.\u2022--repair, -rPerform a repair that can fix almost anything except uniquekeys that are not unique.\u2022--silent, -sSilent mode. Print only error messages.\u2022--skip-database=db_nameDon't process the database (case-sensitive) specified asargument.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--tablesOverride the --databases or -B option. All name argumentsfollowing the option are regarded as table names.\u2022--use-frmFor repair operations on MyISAM tables, get the tablestructure from the .frm file so that the table can berepaired even if the .MYI header is corrupted.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print information about the various stages ofprogram operation.Using one --verbose option will give youmore information about what mysqlcheck is doing.Using two --verbose options will also give you connectioninformation.Using it 3 times will print out all CHECK, RENAME and ALTERTABLE during the check phase.\u2022--version, -VDisplay version information and exit.\u2022--write-binlogThis option is enabled by default, so that ANALYZE TABLE,OPTIMIZE TABLE, and REPAIR TABLE statements generated bymysqlcheck are written to the binary log. Use--skip-write-binlog to cause NO_WRITE_TO_BINLOG to be addedto the statements so that they are not logged. Use the--skip-write-binlog when these statements should not be sentto replication slaves or run when using the binary logs forrecovery from backup.",
        "name": "mariadb-check - a table maintenance program (mysqlcheck is now asymlink to mariadb-check)",
        "section": 1
    },
    {
        "command": "mariadb-client-test",
        "description": "The mysql_client_test program is used for testing aspects of theMariaDB client API that cannot be tested using mysqltest and itstest language.mysql_client_test_embedded is similar but usedfor testing the embedded server. Both programs are run as part ofthe test suite.The source code for the programs can be found in intests/mysql_client_test.c in a source distribution. The programserves as a good source of examples illustrating how to usevarious features of the client API.mysql_client_test is used in a test by the same name in the maintests suite of mysql-test-run.pl but may also be run directly.Unlike the other programs listed here, it does not read anexternal description of what tests to run. Instead, all tests arecoded into the program, which is written to cover all aspects ofthe C language API.mysql_client_test supports the following options:\u2022--help, -?Display a help message and exit.\u2022--basedir=dir_name, -b dir_nameThe base directory for the tests.\u2022--count=count, -t countThe number of times to execute the tests.\u2022--database=db_name, -D db_nameThe database to use.\u2022--debug[=debug_options], -#[debug_options]Write a debugging log if MariaDB is built with debuggingsupport. The default debug_options value is\u00b4d:t:o,/tmp/mysql_client_test.trace\u00b4.\u2022--getopt-ll-test=option, -g optionOption to use for testing bugs in the getopt library.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,you are prompted for one.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.\u2022--server-arg=arg, -A argArgument to send to the embedded server.\u2022--show-tests, -TShow all test names.\u2022--silent, -sBe more silent.\u2022--socket=path, -S pathThe socket file to use when connecting to localhost (which isthe default host).\u2022--testcase, -cThe option is used when called from mysql-test-run.pl, sothat mysql_client_test may optionally behave in a differentway than if called manually, for example by skipping sometests. Currently, there is no difference in behavior but theoption is included in order to make this possible.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022-v dir_name, --vardir=dir_nameThe data directory for tests. The default is mysql-test/var.",
        "name": "mariadb-client-test - test client API (mysql_client_test is now asymlink to mariadb-client-test)mariadb-client-test-embedded - test client API for embeddedserver (mysql_client_test_embedded is now a symlink to mariadb-client-test-embedded)",
        "section": 1
    },
    {
        "command": "mariadb-client-test-embedded",
        "description": "The mysql_client_test program is used for testing aspects of theMariaDB client API that cannot be tested using mysqltest and itstest language.mysql_client_test_embedded is similar but usedfor testing the embedded server. Both programs are run as part ofthe test suite.The source code for the programs can be found in intests/mysql_client_test.c in a source distribution. The programserves as a good source of examples illustrating how to usevarious features of the client API.mysql_client_test is used in a test by the same name in the maintests suite of mysql-test-run.pl but may also be run directly.Unlike the other programs listed here, it does not read anexternal description of what tests to run. Instead, all tests arecoded into the program, which is written to cover all aspects ofthe C language API.mysql_client_test supports the following options:\u2022--help, -?Display a help message and exit.\u2022--basedir=dir_name, -b dir_nameThe base directory for the tests.\u2022--count=count, -t countThe number of times to execute the tests.\u2022--database=db_name, -D db_nameThe database to use.\u2022--debug[=debug_options], -#[debug_options]Write a debugging log if MariaDB is built with debuggingsupport. The default debug_options value is\u00b4d:t:o,/tmp/mysql_client_test.trace\u00b4.\u2022--getopt-ll-test=option, -g optionOption to use for testing bugs in the getopt library.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,you are prompted for one.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.\u2022--server-arg=arg, -A argArgument to send to the embedded server.\u2022--show-tests, -TShow all test names.\u2022--silent, -sBe more silent.\u2022--socket=path, -S pathThe socket file to use when connecting to localhost (which isthe default host).\u2022--testcase, -cThe option is used when called from mysql-test-run.pl, sothat mysql_client_test may optionally behave in a differentway than if called manually, for example by skipping sometests. Currently, there is no difference in behavior but theoption is included in order to make this possible.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022-v dir_name, --vardir=dir_nameThe data directory for tests. The default is mysql-test/var.",
        "name": "mariadb-client-test - test client API (mysql_client_test is now asymlink to mariadb-client-test)mariadb-client-test-embedded - test client API for embeddedserver (mysql_client_test_embedded is now a symlink to mariadb-client-test-embedded)",
        "section": 1
    },
    {
        "command": "mariadb-conv",
        "description": "mariadb-conv is a character set conversion utility for MariaDB.mariadb-conv supports the following options.\u2022--from=name, -f nameSpecifies the encoding of the input.\u2022--to=name, -t nameSpecifies the encoding of the output.\u2022--continue, -cSilently ignore conversion errors.\u2022--delimiter=name,Treat the specified characters as delimiters.",
        "name": "mariadb-conv - character set conversion utility for MariaDB",
        "section": 1
    },
    {
        "command": "mariadb-convert-table-format",
        "description": "mysql_convert_table_format converts the tables in a database touse a particular storage engine (MyISAM by default).mysql_convert_table_format is written in Perl and requires thatthe DBI and DBD::MariaDB Perl modules be installed (seeSection 2.15, \u201cPerl Installation Notes\u201d).Invoke mysql_convert_table_format like this:shell> mysql_convert_table_format [options]db_nameThe db_name argument indicates the database containing the tablesto be converted.mysql_convert_table_format supports the options described in thefollowing list.\u2022--helpDisplay a help message and exit.\u2022--forceContinue even if errors occur.\u2022--host=host_nameConnect to the MariaDB server on the given host.\u2022--password=passwordThe password to use when connecting to the server. Note thatthe password value is not optional for this option, unlikefor other MariaDB programs.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--port=port_numThe TCP/IP port number to use for the connection.\u2022--socket=pathFor connections to localhost, the Unix socket file to use.\u2022--type=engine_nameSpecify the storage engine that the tables should beconverted to use. The default is MyISAM if this option is notgiven.\u2022--user=user_nameThe MariaDB user name to use when connecting to the server.\u2022--verboseVerbose mode. Print more information about what the programdoes.\u2022--versionDisplay version information and exit.",
        "name": "mariadb-convert-table-format - convert tables to use a givenstorage engine (mysql_convert_table_format is now a symlink tomariadb-convert-table-format)",
        "section": 1
    },
    {
        "command": "mariadb-dump",
        "description": "The mysqldump client is a backup program originally written byIgor Romanenko. It can be used to dump a database or a collectionof databases for backup or transfer to another SQL server (notnecessarily a MariaDB server). The dump typically contains SQLstatements to create the table, populate it, or both. However,mysqldump can also be used to generate files in CSV, otherdelimited text, or XML format.If you are doing a backup on the server and your tables all areMyISAM tables, consider using the mysqlhotcopy instead because itcan accomplish faster backups and faster restores. Seemysqlhotcopy(1).There are four general ways to invoke mysqldump:shell> mysqldump [options] db_name [tbl_name ...]shell> mysqldump [options] --databases db_name ...shell> mysqldump [options] --all-databasesshell> mysqldump [options] --system={options}If you do not name any tables following db_name or if you use the--databases or --all-databases option, entire databases aredumped.mysqldump does not dump the INFORMATION_SCHEMA orperformance_schema databases by default. To dump these, name themexplicitly on the command line, although you must also use the--skip-lock-tables option.To see a list of the options your version of mysqldump supports,execute mysqldump --help.Some mysqldump options are shorthand for groups of other options:\u2022Use of --opt is the same as specifying --add-drop-table,--add-locks, --create-options, --disable-keys,--extended-insert, --lock-tables, --quick, and --set-charset.All of the options that --opt stands for also are on bydefault because --opt is on by default.\u2022Use of --compact is the same as specifying--skip-add-drop-table, --skip-add-locks, --skip-comments,--skip-disable-keys, and --skip-set-charset options.To reverse the effect of a group option, uses its --skip-xxx form(--skip-opt or --skip-compact). It is also possible to selectonly part of the effect of a group option by following it withoptions that enable or disable specific features. Here are someexamples:\u2022To select the effect of --opt except for some features, usethe --skip option for each feature. To disable extendedinserts and memory buffering, use --opt--skip-extended-insert --skip-quick. (Actually,--skip-extended-insert --skip-quick is sufficient because--opt is on by default.)\u2022To reverse --opt for all features except index disabling andtable locking, use --skip-opt --disable-keys --lock-tables.When you selectively enable or disable the effect of a groupoption, order is important because options are processed first tolast. For example, --disable-keys --lock-tables --skip-opt wouldnot have the intended effect; it is the same as --skip-opt byitself.mysqldump can retrieve and dump table contents row by row, or itcan retrieve the entire content from a table and buffer it inmemory before dumping it. Buffering in memory can be a problem ifyou are dumping large tables. To dump tables row by row, use the--quick option (or --opt, which enables --quick). The --optoption (and hence --quick) is enabled by default, so to enablememory buffering, use --skip-quick.If you are using a recent version of mysqldump to generate a dumpto be reloaded into a very old MySQL server, you should not usethe --opt or --extended-insert option. Use --skip-opt instead.mysqldump supports the following options, which can be specifiedon the command line or in the [mysqldump] and [client] optionfile groups.mysqldump also supports the options for processingoption file.\u2022--help, -?Display a help message and exit.\u2022--add-drop-databaseAdd a DROP DATABASE statement before each CREATE DATABASEstatement. This option is typically used in conjunction withthe --all-databases or --databases option because no CREATEDATABASE statements are written unless one of those optionsis specified.\u2022--add-drop-tableAdd a DROP TABLE statement before each CREATE TABLEstatement.\u2022--add-drop-triggerAdd a DROP TRIGGER statement before each CREATE TRIGGERstatement.\u2022--add-locksSurround each table dump with LOCK TABLES and UNLOCK TABLESstatements. This results in faster inserts when the dump fileis reloaded.\u2022--all-databases, -ADump all tables in all databases. This is the same as usingthe --databases option and naming all the databases on thecommand line.\u2022--all-tablespaces, -YAdds to a table dump all SQL statements needed to create anytablespaces used by an NDBCLUSTER table. This information isnot otherwise included in the output from mysqldump. Thisoption is currently relevant only to MySQL Cluster tables.\u2022--allow-keywordsAllow creation of column names that are keywords. This worksby prefixing each column name with the table name.\u2022--apply-slave-statementsAdds 'STOP SLAVE' prior to 'CHANGE MASTER' and 'START SLAVE'to bottom of dump.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--comments, -iWrite additional information in the dump file such as programversion, server version, and host. This option is enabled bydefault. To suppress this additional information, use--skip-comments.\u2022--compactProduce more compact output. This option enables the--skip-add-drop-table, --skip-add-locks, --skip-comments,--skip-disable-keys, and --skip-set-charset options.\u2022--compatible=nameProduce output that is more compatible with other databasesystems or with older MySQL servers. The value of name can beansi, mysql323, mysql40, postgresql, oracle, mssql, db2,maxdb, no_key_options, no_table_options, or no_field_options.To use several values, separate them by commas. These valueshave the same meaning as the corresponding options forsetting the server SQL mode.This option does not guarantee compatibility with otherservers. It only enables those SQL mode values that arecurrently available for making dump output more compatible.For example, --compatible=oracle does not map data types toOracle types or use Oracle comment syntax.\u2022--complete-insert, -cUse complete INSERT statements that include column names.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--copy-s3-tablesBy default S3 tables are ignored. With this option set, theresult file will contain a CREATE statement for a similarAria table, followed by the table data and ending with anALTER TABLE xxx ENGINE=S3.\u2022--create-options, -aInclude all MariaDB-specific table options in the CREATETABLE statements. Use --skip-create-options to disable.\u2022--databases, -BDump several databases. Normally, mysqldump treats the firstname argument on the command line as a database name andfollowing names as table names. With this option, it treatsall name arguments as database names.CREATE DATABASE andUSE statements are included in the output before each newdatabase.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default value is\u00b4d:t:o,/tmp/mysqldump.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-authDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set. If nocharacter set is specified, mysqldump uses utf8.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--defaults-group-suffix=str,Also read groups with a suffix of str. For example, sincemysqldump normally reads the [client] and [mysqldump] groups,--defaults-group-suffix=x would cause it to also read thegroups [mysqldump_x] and [client_x].\u2022--delayed-insertWrite INSERT DELAYED statements rather than INSERTstatements.\u2022--delete-master-logsOn a master replication server, delete the binary logs bysending a PURGE BINARY LOGS statement to the server afterperforming the dump operation. This option automaticallyenables --master-data.\u2022--disable-keys, -KFor each table, surround the INSERT statements with /*!40000ALTER TABLE tbl_name DISABLE KEYS */; and /*!40000 ALTERTABLE tbl_name ENABLE KEYS */; statements. This makes loadingthe dump file faster because the indexes are created afterall rows are inserted. This option is effective only fornonunique indexes of MyISAM tables.\u2022--dump-dateIf the --comments option is given, mysqldump produces acomment at the end of the dump of the following form:-- Dump completed on DATEHowever, the date causes dump files taken at different timesto appear to be different, even if the data are otherwiseidentical.--dump-date and --skip-dump-date control whetherthe date is added to the comment. The default is --dump-date(include the date in the comment).--skip-dump-datesuppresses date printing\u2022--dump-slave[=value]Used for producing a dump file from a replication slaveserver that can be used to set up another slave server withthe same master. Causes the binary log position and filenameof the master to be appended to the dumped data output.Setting the value to 1 (the default) will print it as aCHANGE MASTER command in the dumped data output; if set to 2,that command will be prefixed with a comment symbol. Thisoption will turn --lock-all-tables on, unless --single-transaction is specified too (in which case a global readlock is only taken a short time at the beginning of the dump- don't forget to read about --single-transaction below). Inall cases any action on logs will happen at the exact momentof the dump. Option automatically turns --lock-tables off.Using this option causes mysqldump to stop the slave SQLthread before beginning the dump, and restart it again aftercompletion.\u2022--events, -EInclude Event Scheduler events for the dumped databases inthe output.\u2022--extended-insert, -eUse multiple-row INSERT syntax that include several VALUESlists. This results in a smaller dump file and speeds upinserts when the file is reloaded.\u2022--fields-terminated-by=..., --fields-enclosed-by=...,--fields-optionally-enclosed-by=..., --fields-escaped-by=...These options are used with the --tab option and have thesame meaning as the corresponding FIELDS clauses for LOADDATA INFILE.\u2022--first-slaveRemoved in MariaDB 5.5. Use --lock-all-tables instead.\u2022--flashback, -BSupport flashback mode.\u2022--flush-logs, -FFlush the MariaDB server log files before starting the dump.This option requires the RELOAD privilege. If you use thisoption in combination with the --all-databases option, thelogs are flushed for each database dumped. The exception iswhen using --lock-all-tables or --master-data: In this case,the logs are flushed only once, corresponding to the momentthat all tables are locked. If you want your dump and the logflush to happen at exactly the same moment, you should use--flush-logs together with either --lock-all-tables or--master-data.\u2022--flush-privilegesSend a FLUSH PRIVILEGES statement to the server after dumpingthe mysql database. This option should be used any time thedump contains the mysql database and any other database thatdepends on the data in the mysql database for properrestoration.\u2022--force, -fContinue even if an SQL error occurs during a table dump.One use for this option is to cause mysqldump to continueexecuting even when it encounters a view that has becomeinvalid because the definition refers to a table that hasbeen dropped. Without --force, mysqldump exits with an errormessage. With --force, mysqldump prints the error message,but it also writes an SQL comment containing the viewdefinition to the dump output and continues executing.\u2022--gtidAvailable from MariaDB 10.0.13, and is used together with--master-data and --dump-slave to more conveniently set up anew GTID slave. It causes those options to output SQLstatements that configure the slave to use the globaltransaction ID to connect to the master instead of old-stylefilename/offset positions. The old-style positions are stillincluded in comments when --gtid is used; likewise the GTIDposition is included in comments even if --gtid is not used.\u2022--hex-blobDump binary columns using hexadecimal notation (for example,\u00b4abc\u00b4 becomes 0x616263). The affected data types are BINARY,VARBINARY, the BLOB types, and BIT.\u2022--host=host_name, -h host_nameDump data from the MariaDB server on the given host. Thedefault host is localhost.\u2022--ignore-table=db_name.tbl_nameDo not dump the given table, which must be specified usingboth the database and table names. To ignore multiple tables,use this option multiple times. This option also can be usedto ignore views.\u2022--include-master-host-portAdd the MASTER_HOST and MASTER_PORT options for the CHANGEMASTER TO statement when using the --dump-slave option for aslave dump.\u2022--insert-ignoreWrite INSERT IGNORE statements rather than INSERT statements.\u2022--lines-terminated-by=...This option is used with the --tab option and has the samemeaning as the corresponding LINES clause for LOAD DATAINFILE.\u2022--lock-all-tables, -xLock all tables across all databases. This is achieved byacquiring a global read lock for the duration of the wholedump. This option automatically turns off--single-transaction and --lock-tables.\u2022--lock-tables, -lFor each dumped database, lock all tables to be dumped beforedumping them. The tables are locked with READ LOCAL to allowconcurrent inserts in the case of MyISAM tables. Fortransactional tables such as InnoDB, --single-transaction isa much better option than --lock-tables because it does notneed to lock the tables at all.Because --lock-tables locks tables for each databaseseparately, this option does not guarantee that the tables inthe dump file are logically consistent between databases.Tables in different databases may be dumped in completelydifferent states.Use --skip-lock-tables to disable.\u2022--log-error=file_nameLog warnings and errors by appending them to the named file.The default is to do no logging.\u2022--log-queriesWhen restoring the dump, the server will, if logging isturned on, log the queries to the general and slow query log.Defaults to on; use --skip-log-queries to disable.\u2022--master-data[=value]Use this option to dump a master replication server toproduce a dump file that can be used to set up another serveras a slave of the master. It causes the dump output toinclude a CHANGE MASTER TO statement that indicates thebinary log coordinates (file name and position) of the dumpedserver. These are the master server coordinates from whichthe slave should start replicating after you load the dumpfile into the slave.If the option value is 2, the CHANGE MASTER TO statement iswritten as an SQL comment, and thus is informative only; ithas no effect when the dump file is reloaded. If the optionvalue is 1, the statement is not written as a comment andtakes effect when the dump file is reloaded. If no optionvalue is specified, the default value is 1.This option requires the RELOAD privilege and the binary logmust be enabled.The --master-data option automatically turns off--lock-tables. It also turns on --lock-all-tables, unless--single-transaction also is specified. In all cases, anyaction on logs happens at the exact moment of the dump.It is also possible to set up a slave by dumping an existingslave of the master. To do this, use the following procedureon the existing slave:1. Stop the slave\u00b4s SQL thread and get its current status:mysql> STOP SLAVE SQL_THREAD;mysql> SHOW SLAVE STATUS;2. From the output of the SHOW SLAVE STATUS statement, thebinary log coordinates of the master server from whichthe new slave should start replicating are the values ofthe Relay_Master_Log_File and Exec_Master_Log_Pos fields.Denote those values as file_name and file_pos.3. Dump the slave server:shell> mysqldump --master-data=2 --all-databases > dumpfile4. Restart the slave:mysql> START SLAVE;5. On the new slave, load the dump file:shell> mysql < dumpfile6. On the new slave, set the replication coordinates tothose of the master server obtained earlier:mysql> CHANGE MASTER TO-> MASTER_LOG_FILE = \u00b4file_name\u00b4, MASTER_LOG_POS = file_pos;The CHANGE MASTER TO statement might also need otherparameters, such as MASTER_HOST to point the slave to thecorrect master server host. Add any such parameters asnecessary.\u2022--max-allowed-packet=lengthSets the maximum packet length to send to or receive fromserver.\u2022--max-statement-time=secondsSets the maximum time any statement can run before beingtimed out by the server. (Default value is 0 (no limit))\u2022--net-buffer-length=lengthSets the buffer size for TCP/IP and socket communication.\u2022--no-autocommitEnclose the INSERT statements for each dumped table withinSET autocommit = 0 and COMMIT statements.\u2022--no-create-db, -nThis option suppresses the CREATE DATABASE statements thatare otherwise included in the output if the --databases or--all-databases option is given.\u2022--no-create-info, -tDo not write CREATE TABLE statements that re-create eachdumped table.\u2022--no-data, -dDo not write any table row information (that is, do not dumptable contents). This is useful if you want to dump only theCREATE TABLE statement for the table (for example, to createan empty copy of the table by loading the dump file).\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--no-set-names, -NThis has the same effect as --skip-set-charset.\u2022--optThis option is shorthand. It is the same as specifying--add-drop-table --add-locks --create-options --disable-keys--extended-insert --lock-tables --quick --set-charset. Itshould give you a fast dump operation and produce a dump filethat can be reloaded into a MariaDB server quickly.The --opt option is enabled by default. Use --skip-opt todisable it.See the discussion at the beginning of thissection for information about selectively enabling ordisabling a subset of the options affected by --opt.\u2022--order-by-primaryDump each table\u00b4s rows sorted by its primary key, or by itsfirst unique index, if such an index exists. This is usefulwhen dumping a MyISAM table to be loaded into an InnoDBtable, but will make the dump operation take considerablylonger.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqldump prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dirDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--quick, -qThis option is useful for dumping large tables. It forcesmysqldump to retrieve rows for a table from the server a rowat a time rather than retrieving the entire row set andbuffering it in memory before writing it out.\u2022--print-defaultsPrint the program argument list and exit. This must be givenas the first argument.\u2022--quote-names, -QQuote identifiers (such as database, table, and column names)within \u201c`\u201d characters. If the ANSI_QUOTES SQL mode isenabled, identifiers are quoted within \u201c\"\u201d characters. Thisoption is enabled by default. It can be disabled with--skip-quote-names, but this option should be given after anyoption such as --compatible that may enable --quote-names.\u2022--replaceWrite REPLACE statements rather than INSERT statements.\u2022--result-file=file_name, -r file_nameDirect output to a given file. This option should be used onWindows to prevent newline \u201c\\n\u201d characters from beingconverted to \u201c\\r\\n\u201d carriage return/newline sequences. Theresult file is created and its previous contents overwritten,even if an error occurs while generating the dump.\u2022--routines, -RIncluded stored routines (procedures and functions) for thedumped databases in the output. Use of this option requiresthe SELECT privilege for the mysql.proc table. The outputgenerated by using --routines contains CREATE PROCEDURE andCREATE FUNCTION statements to re-create the routines.However, these statements do not include attributes such asthe routine creation and modification timestamps. This meansthat when the routines are reloaded, they will be createdwith the timestamps equal to the reload time.If you require routines to be re-created with their originaltimestamp attributes, do not use --routines. Instead, dumpand reload the contents of the mysql.proc table directly,using a MariaDB account that has appropriate privileges forthe mysql database.\u2022--set-charsetAdd SET NAMES default_character_set to the output. Thisoption is enabled by default. To suppress the SET NAMESstatement, use --skip-set-charset.\u2022--single-transactionThis option sends a START TRANSACTION SQL statement to theserver before dumping data. It is useful only withtransactional tables such as InnoDB, because then it dumpsthe consistent state of the database at the time when BEGINwas issued without blocking any applications.When using this option, you should keep in mind that onlyInnoDB tables are dumped in a consistent state. For example,any MyISAM or MEMORY tables dumped while using this optionmay still change state.While a --single-transaction dump is in process, to ensure avalid dump file (correct table contents and binary logcoordinates), no other connection should use the followingstatements: ALTER TABLE, CREATE TABLE, DROP TABLE, RENAMETABLE, TRUNCATE TABLE. A consistent read is not isolated fromthose statements, so use of them on a table to be dumped cancause the SELECT that is performed by mysqldump to retrievethe table contents to obtain incorrect contents or fail.The --single-transaction option and the --lock-tables optionare mutually exclusive because LOCK TABLES causes any pendingtransactions to be committed implicitly.To dump large tables, you should combine the--single-transaction option with --quick.\u2022--skip-add-drop-tableDisable the --add-drop-table option.\u2022--skip-add-locksDisable the --add-locks option.\u2022--skip-commentsDisable the --comments option.\u2022--skip-compactDisable the --compact option.\u2022--skip-disable-keysDisable the --disable-keys option.\u2022--skip-extended-insertDisable the --extended-insert option.\u2022--skip-optDisable the --opt option.\u2022--skip-quickDisable the --quick option.\u2022--skip-quote-namesDisable the --quote-names option.\u2022--skip-set-charsetDisable the --set-charset option.\u2022--skip-triggersDisable the --triggers option.\u2022--skip-tz-utcDisable the --tz-utc option.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--system={all, users, plugins, udfs, servers, stats,timezones}Dump the system tables in the mysql database in a logicalform. This option is an empty set by default.One or more options can be listed in comma separated list.The options here are:\u2022all - an alias to enabling all of the below options.\u2022users - the users, roles and their grants outputed asCREATE USER, CREATE ROLE, GRANT, and SET DEFAULT ROLE(ALTER USER for MySQL-8.0+).\u2022plugins - active plugins of the server outputed asINSTALL PLUGIN.\u2022udfs - user define functions outputed as CREATE FUNCTION.\u2022servers - remote (federated) servers as CREATE SERVER.\u2022stats - statistics tables, InnoDB and Engine IndependentTable Statistics (EITS), are dumped as REPLACE INTO (orINSERT IGNORE if --insert-ignore is specified) statementswithout (re)creating tables.\u2022timezones - timezone related system tables dumped asREPLACE INTO (or INSERT IGNORE if --insert-ignore isspecified) statements without (re)creating tables.The format of the output is affected by --replace and--insert-ignore. The --replace option will output CREATE ORREPLACE forms of SQL, and also DROP IF EXISTS prior toCREATE, if a CREATE OR REPLACE option isn't available.With --system=user (or all), and --replace, SQL is generatedto generate an error if attempting to import the dump with aconnection user that is being replaced within the dump.The --insert-ignore option will cause CREATE IF NOT EXISTforms of SQL to generated if available.For stats, and timezones, --replace and --insert-ignore havethe usual effects.Enabling specific options here will cause the relevant tablesin the mysql database to be ignored when dumping the mysqldatabase or --all-databases.To help in migrating from MySQL to MariaDB, this option isdesigned to be able to dump system information from MySQL-5.7and 8.0 servers. SQL generated is also experimentallycompatible with MySQL-5.7/8.0. Mappings of implementationspecific grants/plugins isn't always one-to-one howeverbetween MariaDB and MySQL and will require manual changes.\u2022--tab=path, -T pathProduce tab-separated text-format data files. For each dumpedtable, mysqldump creates a tbl_name.sql file that containsthe CREATE TABLE statement that creates the table, and theserver writes a tbl_name.txt file that contains its data. Theoption value is the directory in which to write the files.NoteThis option should be used only when mysqldump is run onthe same machine as the mysqld server. You must have theFILE privilege, and the server must have permission towrite files in the directory that you specify.By default, the .txt data files are formatted using tabcharacters between column values and a newline at the end ofeach line. The format can be specified explicitly using the--fields-xxx and --lines-terminated-by options.Column values are converted to the character set specified bythe --default-character-set option.\u2022--tablesOverride the --databases or -B option.mysqldump regards allname arguments following the option as table names.\u2022--triggersInclude triggers for each dumped table in the output. Thisoption is enabled by default; disable it with--skip-triggers.\u2022--tz-utcThis option enables TIMESTAMP columns to be dumped andreloaded between servers in different time zones.mysqldumpsets its connection time zone to UTC and adds SETTIME_ZONE=\u00b4+00:00\u00b4 to the dump file. Without this option,TIMESTAMP columns are dumped and reloaded in the time zoneslocal to the source and destination servers, which can causethe values to change if the servers are in different timezones.--tz-utc also protects against changes due todaylight saving time.--tz-utc is enabled by default. Todisable it, use --skip-tz-utc.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.\u2022--version, -VDisplay version information and exit.\u2022--where=\u00b4where_condition\u00b4, -w \u00b4where_condition\u00b4Dump only rows selected by the given WHERE condition. Quotesaround the condition are mandatory if it contains spaces orother characters that are special to your commandinterpreter.Examples:--where=\"user=\u00b4jimf\u00b4\"-w\"userid>1\"-w\"userid<1\"\u2022--xml, -XWrite dump output as well-formed XML.NULL, \u00b4NULL\u00b4, and Empty Values: For a column namedcolumn_name, the NULL value, an empty string, and the stringvalue \u00b4NULL\u00b4 are distinguished from one another in the outputgenerated by this option as follows.\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502Value:\u2502 XML Representation:\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502NULL (unknown value)\u2502 <field name=\"column_name\"\u2502\u2502\u2502 xsi:nil=\"true\" />\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u00b4\u00b4 (empty string)\u2502 <field name=\"column_name\"></field>\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u00b4NULL\u00b4 (string value) \u2502 <field\u2502\u2502\u2502 name=\"column_name\">NULL</field>\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518The output from the mysql client when run using the --xmloption also follows the preceding rules. (See the sectioncalled \u201cMYSQL OPTIONS\u201d.)XML output from mysqldump includes the XML namespace, asshown here:shell> mysqldump --xml -u root world City<?xml version=\"1.0\"?><mysqldump xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"><database name=\"world\"><table_structure name=\"City\"><field Field=\"ID\" Type=\"int(11)\" Null=\"NO\" Key=\"PRI\" Extra=\"auto_increment\" /><field Field=\"Name\" Type=\"char(35)\" Null=\"NO\" Key=\"\" Default=\"\" Extra=\"\" /><field Field=\"CountryCode\" Type=\"char(3)\" Null=\"NO\" Key=\"\" Default=\"\" Extra=\"\" /><field Field=\"District\" Type=\"char(20)\" Null=\"NO\" Key=\"\" Default=\"\" Extra=\"\" /><field Field=\"Population\" Type=\"int(11)\" Null=\"NO\" Key=\"\" Default=\"0\" Extra=\"\" /><key Table=\"City\" Non_unique=\"0\" Key_name=\"PRIMARY\" Seq_in_index=\"1\" Column_name=\"ID\"Collation=\"A\" Cardinality=\"4079\" Null=\"\" Index_type=\"BTREE\" Comment=\"\" /><options Name=\"City\" Engine=\"MyISAM\" Version=\"10\" Row_format=\"Fixed\" Rows=\"4079\"Avg_row_length=\"67\" Data_length=\"273293\" Max_data_length=\"18858823439613951\"Index_length=\"43008\" Data_free=\"0\" Auto_increment=\"4080\"Create_time=\"2007-03-31 01:47:01\" Update_time=\"2007-03-31 01:47:02\"Collation=\"latin1_swedish_ci\" Create_options=\"\" Comment=\"\" /></table_structure><table_data name=\"City\"><row><field name=\"ID\">1</field><field name=\"Name\">Kabul</field><field name=\"CountryCode\">AFG</field><field name=\"District\">Kabol</field><field name=\"Population\">1780000</field></row>...<row><field name=\"ID\">4079</field><field name=\"Name\">Rafah</field><field name=\"CountryCode\">PSE</field><field name=\"District\">Rafah</field><field name=\"Population\">92020</field></row></table_data></database></mysqldump>You can also set the following variables by using--var_name=value syntax:\u2022max_allowed_packetThe maximum size of the buffer for client/servercommunication. The maximum is 1GB.\u2022max_statement_timeA query that has taken more than max_statement_time secondswill be aborted and the backup will fail. The argument willbe treated as a decimal value with microsecond precision. Avalue of 0 (default) means no timeout. The maximum timeout is31536000 seconds.\u2022net_buffer_lengthThe initial size of the buffer for client/servercommunication. When creating multiple-row INSERT statements(as with the --extended-insert or --opt option), mysqldumpcreates rows up to net_buffer_length length. If you increasethis variable, you should also ensure that thenet_buffer_length variable in the MariaDB server is at leastthis large.A common use of mysqldump is for making a backup of an entiredatabase:shell> mysqldump db_name > backup-file.sqlYou can load the dump file back into the server like this:shell> mysql db_name < backup-file.sqlOr like this:shell> mysql -e \"source /path-to-backup/backup-file.sql\" db_namemysqldump is also very useful for populating databases by copyingdata from one MariaDB server to another:shell> mysqldump --opt db_name | mysql --host=remote_host -C db_nameIt is possible to dump several databases with one command:shell> mysqldump --databases db_name1 [db_name2 ...] > my_databases.sqlTo dump all databases, use the --all-databases option:shell> mysqldump --all-databases > all_databases.sqlFor InnoDB tables, mysqldump provides a way of making an onlinebackup:shell> mysqldump --all-databases --single-transaction > all_databases.sqlThis backup acquires a global read lock on all tables (usingFLUSH TABLES WITH READ LOCK) at the beginning of the dump. Assoon as this lock has been acquired, the binary log coordinatesare read and the lock is released. If long updating statementsare running when the FLUSH statement is issued, the MariaDBserver may get stalled until those statements finish. After that,the dump becomes lock free and does not disturb reads and writeson the tables. If the update statements that the MariaDB serverreceives are short (in terms of execution time), the initial lockperiod should not be noticeable, even with many updates.For point-in-time recovery (also known as \u201croll-forward,\u201d whenyou need to restore an old backup and replay the changes thathappened since that backup), it is often useful to rotate thebinary log or at least know the binary log coordinates to whichthe dump corresponds:shell> mysqldump --all-databases --master-data=2 > all_databases.sqlOr:shell> mysqldump --all-databases --flush-logs --master-data=2> all_databases.sqlThe --master-data and --single-transaction options can be usedsimultaneously, which provides a convenient way to make an onlinebackup suitable for use prior to point-in-time recovery if tablesare stored using the InnoDB storage engine.If you encounter problems backing up views, please read thesection that covers restrictions on views which describes aworkaround for backing up views when this fails due toinsufficient privileges.",
        "name": "mariadb-dump - a database backup program (mysqldump is now asymlink to mariadb-dump)",
        "section": 1
    },
    {
        "command": "mariadb-dumpslow",
        "description": "The MariaDB slow query log contains information about queriesthat take a long time to execute.mysqldumpslow parses MariaDBslow query log files and prints a summary of their contents.Normally, mysqldumpslow groups queries that are similar exceptfor the particular values of number and string data values. It\u201cabstracts\u201d these values to N and \u00b4S\u00b4 when displaying summaryoutput. The -a and -n options can be used to modify valueabstracting behavior.Invoke mysqldumpslow like this:shell> mysqldumpslow [options] [log_file ...]mysqldumpslow supports the following options.\u2022--helpDisplay a help message and exit.\u2022-aDo not abstract all numbers to N and strings to \u00b4S\u00b4.\u2022--debug, -dRun in debug mode.\u2022-g patternConsider only queries that match the (grep-style) pattern.\u2022-h host_nameHost name of MariaDB server for *-slow.log file name. Thevalue can contain a wildcard. The default is * (match all).\u2022-i nameName of server instance (if using mysql.server startupscript).\u2022-lDo not subtract lock time from total time.\u2022-n NAbstract numbers with at least N digits within names.\u2022-rReverse the sort order.\u2022-s sort_typeHow to sort the output. The value of sort_type should bechosen from the following list:\u2022t, aa: Sort by rows affected or average rows affected\u2022l, ae: Sort by rows examined or aggregate rows examined\u2022l, at: Sort by query time or average query time\u2022l, al: Sort by lock time or average lock time\u2022s, as: Sort by rows sent or average rows sent\u2022c: Sort by count\u2022-t NDisplay only the first N queries in the output.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.Example of usage:shell> mysqldumpslowReading mysql slow query log from /usr/local/mysql/data/mysqld51-apple-slow.logCount: 1Time=4.32s (4s)Lock=0.00s (0s)Rows=0.0 (0), root[root]@localhostinsert into t2 select * from t1Count: 3Time=2.53s (7s)Lock=0.00s (0s)Rows=0.0 (0), root[root]@localhostinsert into t2 select * from t1 limit NCount: 3Time=2.13s (6s)Lock=0.00s (0s)Rows=0.0 (0), root[root]@localhostinsert into t1 select * from t1",
        "name": "mariadb-dumpslow - Summarize slow query log files (mysqldumpslowis now a symlink to mariadb-dumpslow)",
        "section": 1
    },
    {
        "command": "mariadb-find-rows",
        "description": "mysql_find_rows reads files containing SQL statements andextracts statements that match a given regular expression or thatcontain USE db_name or SET statements. The utility was writtenfor use with update log files (as used prior to MySQL 5.0) and assuch expects statements to be terminated with semicolon (;)characters. It may be useful with other files that contain SQLstatements as long as statements are terminated with semicolons.Invoke mysql_find_rows like this:shell> mysql_find_rows [options] [file_name ...]Each file_name argument should be the name of file containing SQLstatements. If no file names are given, mysql_find_rows reads thestandard input.Examples:mysql_find_rows --regexp=problem_table --rows=20 < update.logmysql_find_rows --regexp=problem_tableupdate-log.1 update-log.2mysql_find_rows supports the following options:\u2022--help, --InformationDisplay a help message and exit.\u2022--regexp=patternDisplay queries that match the pattern.\u2022--rows=NQuit after displaying N queries.\u2022--skip-use-dbDo not include USE db_name statements in the output.\u2022--start_row=NStart output from this row.",
        "name": "mariadb-find-rows - extract SQL statements from files(mysql_find_rows is now a symlink to mariadb-find-rows)",
        "section": 1
    },
    {
        "command": "mariadb-fix-extensions",
        "description": "mysql_fix_extensions converts the extensions for MyISAM (or ISAM)table files to their canonical forms. It looks for files withextensions matching any lettercase variant of .frm, .myd, .myi,.isd, and .ism and renames them to have extensions of .frm, .MYD,.MYI, .ISD, and .ISM, respectively. This can be useful aftertransferring the files from a system with case-insensitive filenames (such as Windows) to a system with case-sensitive filenames.Invoke mysql_fix_extensions like this, where data_dir is the pathname to the MySQL data directory.shell> mysql_fix_extensions data_dir",
        "name": "mariadb-fix-extensions - normalize table file name extensions(mysql_fix_extensions is now a symlink to mariadb-fix-extensions)",
        "section": 1
    },
    {
        "command": "mariadb-hotcopy",
        "description": "mysqlhotcopy is a Perl script that was originally written andcontributed by Tim Bunce. It uses FLUSH TABLES, LOCK TABLES, andcp or scp to make a database backup. It is a fast way to make abackup of the database or single tables, but it can be run onlyon the same machine where the database directories are located.mysqlhotcopy works only for backing up MyISAM and ARCHIVE tables.It runs on Unix and NetWare.To use mysqlhotcopy, you must have read access to the files forthe tables that you are backing up, the SELECT privilege forthose tables, the RELOAD privilege (to be able to execute FLUSHTABLES), and the LOCK TABLES privilege (to be able to lock thetables).shell> mysqlhotcopy db_name [/path/to/new_directory]shell> mysqlhotcopy db_name_1 ... db_name_n /path/to/new_directoryBack up tables in the given database that match a regularexpression:shell> mysqlhotcopy db_name./regex/The regular expression for the table name can be negated byprefixing it with a tilde (\u201c~\u201d):shell> mysqlhotcopy db_name./~regex/mysqlhotcopy supports the following options, which can bespecified on the command line or in the [mysqlhotcopy] and[client] option file groups.\u2022--help, -?Display a help message and exit.\u2022--addtodestDo not rename target directory (if it exists); merely addfiles to it.\u2022--allowoldDo not abort if a target exists; rename it by adding an _oldsuffix.\u2022--checkpoint=db_name.tbl_nameInsert checkpoint entries into the specified database db_nameand table tbl_name.\u2022--chroot=pathBase directory of the chroot jail in which mysqld operates.The path value should match that of the --chroot option givento mysqld.\u2022--debugEnable debug output.\u2022--dryrun, -nReport actions without performing them.\u2022--flushlogFlush logs after all tables are locked.\u2022--host=host_name, -h host_nameThe host name of the local host to use for making a TCP/IPconnection to the local server. By default, the connection ismade to localhost using a Unix socket file.\u2022--keepoldDo not delete previous (renamed) target when done.\u2022--method=commandThe method for copying files (cp or scp). The default is cp.\u2022--noindicesDo not include full index files for MyISAM tables in thebackup. This makes the backup smaller and faster. The indexesfor reloaded tables can be reconstructed later with myisamchk-rq.\u2022--old-serverConnect to old MySQL-server (before v5.5) which doesn't haveFLUSH TABLES WITH READ LOCK fully implemented..\u2022--password=password, -ppasswordThe password to use when connecting to the server. Thepassword value is not optional for this option, unlike forother MariaDB programs.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--port=port_num, -P port_numThe TCP/IP port number to use when connecting to the localserver.\u2022--quiet, -qBe silent except for errors.\u2022--record_log_pos=db_name.tbl_nameRecord master and slave status in the specified databasedb_name and table tbl_name.\u2022--regexp=exprCopy all databases with names that match the given regularexpression.\u2022--resetmasterReset the binary log after locking all the tables.\u2022--resetslaveReset the master.info file after locking all the tables.\u2022--socket=path, -S pathThe Unix socket file to use for connections to localhost.\u2022--suffix=strThe suffix to use for names of copied databases.\u2022--tmpdir=pathThe temporary directory. The default is /tmp.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.Use perldoc for additional mysqlhotcopy documentation, includinginformation about the structure of the tables needed for the--checkpoint and --record_log_pos options:shell> perldoc mysqlhotcopy",
        "name": "mariadb-hotcopy - a database backup program (mysqlhotcopy is nowa symlink to mariadb-hotcopy)",
        "section": 1
    },
    {
        "command": "mariadb-import",
        "description": "The mysqlimport client provides a command-line interface to theLOAD DATA INFILE SQL statement. Most options to mysqlimportcorrespond directly to clauses of LOAD DATA INFILE syntax.Invoke mysqlimport like this:shell> mysqlimport [options] db_name textfile1 [textfile2 ...]For each text file named on the command line, mysqlimport stripsany extension from the file name and uses the result to determinethe name of the table into which to import the file\u00b4s contents.For example, files named patient.txt, patient.text, and patientall would be imported into a table named patient.mysqlimport supports the following options, which can bespecified on the command line or in the [mysqlimport] and[client] option file groups.mysqlimport also supports theoptions for processing option files.\u2022--help, -?Display a help message and exit.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--columns=column_list, -c column_listThis option takes a comma-separated list of column names asits value. The order of the column names indicates how tomatch data file columns with table columns.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is \u00b4d:t:o\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-auth=plugin_nameDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--delete, -dEmpty the table before importing the text file.\u2022--fields-terminated-by=..., --fields-enclosed-by=...,--fields-optionally-enclosed-by=..., --fields-escaped-by=...These options have the same meaning as the correspondingclauses for LOAD DATA INFILE.\u2022--force, -fIgnore errors. For example, if a table for a text file doesnot exist, continue processing any remaining files. Without--force, mysqlimport exits if a table does not exist.\u2022--host=host_name, -h host_nameImport data to the MariaDB server on the given host. Thedefault host is localhost.\u2022--ignore, -iSee the description for the --replace option.\u2022--ignore-foreign-keys, -kDisable foreign key checks while importing the data.\u2022--ignore-lines=NIgnore the first N lines of the data file.\u2022--lines-terminated-by=...This option has the same meaning as the corresponding clausefor LOAD DATA INFILE. For example, to import Windows filesthat have lines terminated with carriage return/linefeedpairs, use --lines-terminated-by=\"\\r\\n\". (You might have todouble the backslashes, depending on the escaping conventionsof your command interpreter.).\u2022--local, -LRead input files locally from the client host.\u2022--lock-tables, -lLock all tables for writing before processing any text files.This ensures that all tables are synchronized on the server.\u2022--low-priorityUse LOW_PRIORITY when loading the table. This affects onlystorage engines that use only table-level locking (such asMyISAM, MEMORY, and MERGE).\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlimport prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dir=nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--replace, -rThe --replace and --ignore options control handling of inputrows that duplicate existing rows on unique key values. Ifyou specify --replace, new rows replace existing rows thathave the same unique key value. If you specify --ignore,input rows that duplicate an existing row on a unique keyvalue are skipped. If you do not specify either option, anerror occurs when a duplicate key value is found, and therest of the text file is ignored.\u2022--silent, -sSilent mode. Produce output only when errors occur.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--use-threads=NLoad files in parallel using N threads.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.\u2022--version, -VDisplay version information and exit.Here is a sample session that demonstrates use of mysqlimport:shell> mysql -e \u00b4CREATE TABLE imptest(id INT, n VARCHAR(30))\u00b4 testshell> eda100Max Sydow101Count Dracula.w imptest.txt32qshell> od -c imptest.txt0000000100\\tMaxSydow\\n1000000201\\tCountDracula\\n0000040shell> mysqlimport --local test imptest.txttest.imptest: Records: 2Deleted: 0Skipped: 0Warnings: 0shell> mysql -e \u00b4SELECT * FROM imptest\u00b4 test+------+---------------+| id| n|+------+---------------+|100 | Max Sydow||101 | Count Dracula |+------+---------------+",
        "name": "mariadb-import - a data import program (mysqlimport is now asymlink to mariadb-import)",
        "section": 1
    },
    {
        "command": "mariadb-install-db",
        "description": "mysql_install_db initializes the MariaDB data directory andcreates the system tables that it contains, if they do not exist.To invoke mysql_install_db, use the following syntax:shell> mysql_install_db [options]Because the MariaDB server, mysqld, needs to access the datadirectory when it runs later, you should either runmysql_install_db from the same account that will be used forrunning mysqld or run it as root and use the --user option toindicate the user name that mysqld will run as. It might benecessary to specify other options such as --basedir or --datadirif mysql_install_db does not use the correct locations for theinstallation directory or data directory. For example:shell> bin/mysql_install_db --user=mysql \\--basedir=/opt/mysql/mysql \\--datadir=/opt/mysql/mysql/datamysql_install_db needs to invoke mysqld with the --bootstrap and--skip-grant-tables options (see Section 2.3.2, \u201cTypicalconfigure Options\u201d). If MariaDB was configured with the--disable-grant-options option, --bootstrap and--skip-grant-tables will be disabled. To handle this, set theMYSQLD_BOOTSTRAP environment variable to the full path name of aserver that has all options enabled.mysql_install_db will usethat server.mysql_install_db supports the following options, which can bespecified on the command line or in the [mysql_install_db] and(if they are common to mysqld) [mysqld] option file groups.\u2022--basedir=pathThe path to the MariaDB installation directory.\u2022--builddir=pathIf using --srcdir with out-of-directory builds, you will needto set this to the location of the build directory wherebuilt files reside..\u2022--cross-bootstrapFor internal use. Used when building the MariaDB systemtables on a different host than the target..\u2022--datadir=path, --ldata=pathThe path to the MariaDB data directory.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--defaults-group-suffix=nameIn addition to the given groups, also read groups with thissuffix.\u2022--forceCause mysql_install_db to run even if DNS does not work. Inthat case, grant table entries that normally use host nameswill use IP addresses.\u2022--helpDisplay a help message and exit.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--rpmFor internal use. This option is used by RPM files during theMariaDB installation process.\u2022--skip-name-resolveUse IP addresses rather than host names when creating granttable entries. This option can be useful if your DNS does notwork.\u2022--srcdir=pathFor internal use. The directory under which mysql_install_dblooks for support files such as the error message file andthe file for populating the help tables.4.\u2022--user=user_nameThe login user name to use for running mysqld. Files anddirectories created by mysqld will be owned by this user. Youmust be root to use this option. By default, mysqld runsusing your current login name and files and directories thatit creates will be owned by you.\u2022--extra-file=file_pathAdd user defined SQL file, to be executed following regulardatabase initialization.\u2022--verboseVerbose mode. Print more information about what the programdoes.\u2022--windowsFor internal use. This option is used for creating Windowsdistributions.",
        "name": "mariadb-install-db - initialize MariaDB data directory(mysql_install_db is now a symlink to mariadb-install-db)",
        "section": 1
    },
    {
        "command": "mariadb-ldb",
        "description": "Use mysql_ldb --help for details on usage.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "mariadb-ldb - RocksDB tool(mysql_ldb is now a symlink tomariadb-ldb)",
        "section": 1
    },
    {
        "command": "mariadb-plugin",
        "description": "The mysql_plugin utility enables MariaDB administrators to managewhich plugins a MariaDB server loads. It provides an alternativeto manually specifying the --plugin-load option at server startupor using the INSTALL PLUGIN and UNINSTALL PLUGIN statements atruntime.Depending on whether mysql_plugin is invoked to enable or disableplugins, it inserts or deletes rows in the mysql.plugin tablethat serves as a plugin registry. (To perform this operation,mysql_plugin invokes the MariaDB server in bootstrap mode. Thismeans that the server must not already be running.) For normalserver startups, the server loads and enables plugins listed inmysql.plugin automatically. For additional control over pluginactivation, use --plugin_name options named for specific plugins.Each invocation of mysql_plugin reads a configuration file todetermine how to configure the plugins contained in a singleplugin library object file. To invoke mysql_plugin, use thissyntax:mysql_plugin [options] plugin {ENABLE|DISABLE}plugin is the name of the plugin to configure.ENABLE or DISABLE(not case sensitive) specify whether to enable or disablecomponents of the plugin library named in the configuration file.The order of the plugin and ENABLE or DISABLE arguments does notmatter.For example, to configure components of a plugin library filenamed myplugins.so on Linux or myplugins.dll on Windows, specifya plugin value of myplugins. Suppose that this plugin librarycontains three plugins, plugin1, plugin2, and plugin3, all ofwhich should be configured under mysql_plugin control. Byconvention, configuration files have a suffix of .ini and thesame basename as the plugin library, so the default configurationfile name for this plugin library is myplugins.ini. Theconfiguration file contents look like this:mypluginsplugin1plugin2plugin3The first line in the myplugins.ini file is the name of thelibrary object file, without any extension such as .so or .dll.The remaining lines are the names of the components to be enabledor disabled. Each value in the file should be on a separate line.Lines on which the first character is '#' are taken as commentsand ignored.To enable the plugins listed in the configuration file, invokemysql_plugin this way:shell> mysql_plugin myplugins ENABLETo disable the plugins, use DISABLE rather than ENABLE.An error occurs if mysql_plugin cannot find the configurationfile or plugin library file, or if mysql_plugin cannot start theMariaDB server.mysql_plugin supports the following options, which can bespecified on the command line or in the [mysqld] group of anyoption file. For options specified in a [mysqld] group,mysql_plugin recognizes the --basedir, --datadir, and--plugin-dir options and ignores others.mysql_plugin Options\u2022--help, -?Display a help message and exit.\u2022--basedir=path, -b pathThe server base directory.\u2022--datadir=path, -d pathThe server data directory.\u2022--my-print-defaults=path, -b pathThe path to the my_print_defaults program.\u2022--mysqld=path, -b pathThe path to the mysqld server.\u2022--no-defaults, -pDo not read values from the configuration file. This optionenables an administrator to skip reading defaults from theconfiguration file.With mysql_plugin, this option need not be given first on thecommand line, unlike most other MariaDB programs that support--no-defaults.\u2022--plugin-dir=path, -p pathThe server plugin directory.\u2022--plugin-ini=file_name, -i file_nameThe mysql_plugin configuration file. Relative path names areinterpreted relative to the current directory. If this optionis not given, the default is plugin.ini in the plugindirectory, where plugin is the plugin argument on the commandline.\u2022--print-defaults, -PDisplay the default values from the configuration file. Thisoption causes mysql_plugin to print the defaults for--basedir, --datadir, and --plugin-dir if they are found inthe configuration file. If no value for a variable is found,nothing is shown.With mysql_plugin, this option need not be given first on thecommand line, unlike most other MariaDB programs that support--print-defaults.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes. This option can be used multiple times to increase theamount of information.\u2022--version, -VDisplay version information and exit.",
        "name": "mariadb-plugin - configure MariaDB server plugins (mysql_pluginis now a symlink to mariadb-plugin)",
        "section": 1
    },
    {
        "command": "mariadb-secure-installation",
        "description": "This program enables you to improve the security of your MariaDBinstallation in the following ways:\u2022You can set a password for root accounts.\u2022You can remove root accounts that are accessible from outsidethe local host.\u2022You can remove anonymous-user accounts.\u2022You can remove the test database, which by default can beaccessed by anonymous users.mysql_secure_installation can be invoked without arguments:shell> mysql_secure_installationThe script will prompt you to determine which actions to perform.mysql_secure_installation accepts some options:\u2022--basedir=dir_nameBase directory.\u2022--defaults-extra-file=file_nameAdditional option file.\u2022--defaults-file=file_nameOption file.\u2022--no-defaultsDon't read any defaults file.Other unrecognized options will be passed on to the server.",
        "name": "mariadb-secure-installation - improve MariaDB installationsecurity (mysql_secure_installation is now a symlink to mariadb-secure-installation)",
        "section": 1
    },
    {
        "command": "mariadb-service-convert",
        "description": "Use: Generate a mariadb.service file based on the currentmysql/mariadb settings.This is to assist distro maintainers inmigrating to systemd service definations from a user mysqld_safesettings in the my.cnf files.Redirect output to user directory like/etc/systemd/system/mariadb.service.d/migrated-from-my.cnf-settings.confFor more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "mariadb-service-convert - generate a mariadb.service file basedon the current mysql/mariadb settings",
        "section": 1
    },
    {
        "command": "mariadb-setpermission",
        "description": "mysql_setpermission is a Perl script that was originally writtenand contributed by Luuk de Boer. It interactively setspermissions in the MariaDB grant tables.mysql_setpermission iswritten in Perl and requires that the DBI and DBD::MariaDB Perlmodules be installed.Invoke mysql_setpermission like this:shell> mysql_setpermission [options]options should be either --help to display the help message, oroptions that indicate how to connect to the MariaDB server. Theaccount used when you connect determines which permissions youhave when attempting to modify existing permissions in the granttables.mysql_setpermission also reads options from the [client] and[perl] groups in the .my.cnf file in your home directory, if thefile exists.mysql_setpermission supports the following options:\u2022--helpDisplay a help message and exit.\u2022--host=host_nameConnect to the MariaDB server on the given host.\u2022--password=passwordThe password to use when connecting to the server. Note thatthe password value is not optional for this option, unlikefor other MariaDB programs.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--port=port_numThe TCP/IP port number to use for the connection.\u2022--socket=pathFor connections to localhost, the Unix socket file to use.\u2022--user=user_nameThe MariaDB user name to use when connecting to the server.",
        "name": "mariadb-setpermission - interactively set permissions in granttables (mysql_setpermission is now a symlink to mariadb-setpermission)",
        "section": 1
    },
    {
        "command": "mariadb-show",
        "description": "The mysqlshow client can be used to quickly see which databasesexist, their tables, or a table\u00b4s columns or indexes.mysqlshow provides a command-line interface to several SQL SHOWstatements. The same information can be obtained by using thosestatements directly. For example, you can issue them from themysql client program.Invoke mysqlshow like this:shell> mysqlshow [options] [db_name [tbl_name [col_name]]]\u2022If no database is given, a list of database names is shown.\u2022If no table is given, all matching tables in the database areshown.\u2022If no column is given, all matching columns and column typesin the table are shown.The output displays only the names of those databases, tables, orcolumns for which you have some privileges.If the last argument contains shell or SQL wildcard characters(\u201c*\u201d, \u201c?\u201d, \u201c%\u201d, or \u201c_\u201d), only those names that are matched by thewildcard are shown. If a database name contains any underscores,those should be escaped with a backslash (some Unix shellsrequire two) to get a list of the proper tables or columns.\u201c*\u201dand \u201c?\u201dcharacters are converted into SQL \u201c%\u201d and \u201c_\u201d wildcardcharacters. This might cause some confusion when you try todisplay the columns for a table with a \u201c_\u201d in the name, becausein this case, mysqlshow shows you only the table names that matchthe pattern. This is easily fixed by adding an extra \u201c%\u201d last onthe command line as a separate argument.mysqlshow supports the following options, which can be specifiedon the command line or in the [mysqlshow] and [client] optionfile groups.mysqlshow also supports the options for processingoption files described.\u2022--help, -?Display a help message and exit.\u2022--character-sets-dir=path, -c pathThe directory where character sets are installed.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--countShow the number of rows per table. This can be slow fornon-MyISAM tables.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is \u00b4d:t:o\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-auth=nameDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--defaults-group-suffix=suffixIn addition to the groups named on the command line, readgroups that have the given suffix.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--keys, -kShow table indexes.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlshow prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--show-table-type, -tShow a column indicating the table type, as in SHOW FULLTABLES. The type is BASE TABLE or VIEW.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--status, -iDisplay extra information about each table.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes. This option can be used multiple times to increase theamount of information.\u2022--version, -VDisplay version information and exit.",
        "name": "mariadb-show - display database, table, and column information(mysqlshow is now a symlink to mariadb-show)",
        "section": 1
    },
    {
        "command": "mariadb-slap",
        "description": "mysqlslap is a diagnostic program designed to emulate client loadfor a MariaDB server and to report the timing of each stage. Itworks as if multiple clients are accessing the server.Invoke mysqlslap like this:shell> mysqlslap [options]Some options such as --create or --query enable you to specify astring containing an SQL statement or a file containingstatements. If you specify a file, by default it must contain onestatement per line. (That is, the implicit statement delimiter isthe newline character.) Use the --delimiter option to specify adifferent delimiter, which enables you to specify statements thatspan multiple lines or place multiple statements on a singleline. You cannot include comments in a file; mysqlslap does notunderstand them.mysqlslap runs in three stages:1. Create schema, table, and optionally any stored programs ordata you want to using for the test. This stage uses a singleclient connection.2. Run the load test. This stage can use many clientconnections.3. Clean up (disconnect, drop table if specified). This stageuses a single client connection.Examples:Supply your own create and query SQL statements, with 50 clientsquerying and 200 selects for each:mysqlslap --delimiter=\";\" \\--create=\"CREATE TABLE a (b int);INSERT INTO a VALUES (23)\" \\--query=\"SELECT * FROM a\" --concurrency=50 --iterations=200Let mysqlslap build the query SQL statement with a table of twoINT columns and three VARCHAR columns. Use five clients querying20 times each. Do not create the table or insert the data (thatis, use the previous test\u00b4s schema and data):mysqlslap --concurrency=5 --iterations=20 \\--number-int-cols=2 --number-char-cols=3 \\--auto-generate-sqlTell the program to load the create, insert, and query SQLstatements from the specified files, where the create.sql filehas multiple table creation statements delimited by \u00b4;\u00b4 andmultiple insert statements delimited by \u00b4;\u00b4. The --query filewill have multiple queries delimited by \u00b4;\u00b4. Run all the loadstatements, then run all the queries in the query file with fiveclients (five times each):mysqlslap --concurrency=5 \\--iterations=5 --query=query.sql --create=create.sql \\--delimiter=\";\"mysqlslap supports the following options, which can be specifiedon the command line or in the [mysqlslap] and [client] optionfile groups.mysqlslap also supports the options for processingoption files.\u2022--help, -?Display a help message and exit.\u2022--auto-generate-sql, -aGenerate SQL statements automatically when they are notsupplied in files or via command options.\u2022--auto-generate-sql-add-autoincrementAdd an AUTO_INCREMENT column to automatically generatedtables.\u2022--auto-generate-sql-execute-number=NSpecify how many queries to generate automatically.\u2022--auto-generate-sql-guid-primaryAdd a GUID-based primary key to automatically generatedtables.\u2022--auto-generate-sql-load-type=typeSpecify the test load type. The allowable values are read(scan tables), write (insert into tables), key (read primarykeys), update (update primary keys), or mixed (half inserts,half scanning selects). The default is mixed.\u2022--auto-generate-sql-secondary-indexes=NSpecify how many secondary indexes to add to automaticallygenerated tables. By default, none are added.\u2022--auto-generate-sql-unique-query-number=NHow many different queries to generate for automatic tests.For example, if you run a key test that performs 1000selects, you can use this option with a value of 1000 to run1000 unique queries, or with a value of 50 to perform 50different selects. The default is 10.\u2022--auto-generate-sql-unique-write-number=NHow many different queries to generate for--auto-generate-sql-write-number. The default is 10.\u2022--auto-generate-sql-write-number=NHow many row inserts to perform on each thread. The defaultis 100.\u2022--commit=NHow many statements to execute before committing. The defaultis 0 (no commits are done).\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--concurrency=N, -c NThe number of clients to simulate when issuing the SELECTstatement.\u2022--create=valueThe file or string containing the statement to use forcreating the table.\u2022--create-schema=valueThe schema in which to run the tests.\u2022--csv[=file_name]Generate output in comma-separated values format. The outputgoes to the named file, or to the standard output if no fileis given.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is\u00b4d:t:o,/tmp/mysqlslap.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-info, -TPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-auth=nameDefault authentication client-side plugin to use.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--delimiter=str, -F strThe delimiter to use in SQL statements supplied in files orvia command options.\u2022--detach=NDetach (close and reopen) each connection after each Nstatements. The default is 0 (connections are not detached).\u2022--engine=engine_name, -e engine_nameComma separated list of storage engines to use for creatingthe table. The test is run for each engine. You can alsospecify an option for an engine after a colon, for examplememory:max_row=2300.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--init-command=strSQL Command to execute when connecting to MariaDB server.Will automatically be re-executed when reconnecting.\u2022--iterations=N, -i NThe number of times to run the tests.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--no-dropDo not drop any schema created during the test after the testis complete.\u2022--number-char-cols=N, -x NThe number of VARCHAR columns to use if --auto-generate-sqlis specified.\u2022--number-int-cols=N, -y NThe number of INT columns to use if --auto-generate-sql isspecified.\u2022--number-of-queries=NLimit each client to approximately this many queries. Querycounting takes into account the statement delimiter. Forexample, if you invoke mysqlslap as follows, the ; delimiteris recognized so that each instance of the query stringcounts as two queries. As a result, 5 rows (not 10) areinserted.shell> mysqlslap --delimiter=\";\" --number-of-queries=10--query=\"use test;insert into t values(null)\"\u2022--only-printDo not connect to databases.mysqlslap only prints what itwould have done.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlslap prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--post-query=valueThe file or string containing the statement to execute afterthe tests have completed. This execution is not counted fortiming purposes.\u2022--post-system=strThe string to execute via system() after the tests havecompleted. This execution is not counted for timing purposes.\u2022--pre-query=valueThe file or string containing the statement to execute beforerunning the tests. This execution is not counted for timingpurposes.\u2022--pre-system=strThe string to execute via system() before running the tests.This execution is not counted for timing purposes.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--query=value, -q valueThe file or string containing the SELECT statement to use forretrieving data.\u2022--shared-memory-base-name=nameOn Windows, the shared-memory name to use, for connectionsmade via shared memory to a local server. This option appliesonly if the server supports shared-memory connections.\u2022--silent, -sSilent mode. No output.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes. This option can be used multiple times to increase theamount of information.\u2022--version, -VDisplay version information and exit.",
        "name": "mariadb-slap - load emulation client (mysqlslap is now a symlinkto mariadb-slap)",
        "section": 1
    },
    {
        "command": "mariadb-test",
        "description": "The mysqltest program runs a test case against a MariaDB serverand optionally compares the output with a result file. Thisprogram reads input written in a special test language.Typically, you invoke mysqltest via mysql-test-run.pl rather thaninvoking it directly.mysqltest_embedded is similar but is built with support for thelibmysqld embedded server.Features of mysqltest:\u2022Can send SQL statements to MariaDB servers for execution\u2022Can execute external shell commands\u2022Can test whether the result from an SQL statement or shellcommand is as expected\u2022Can connect to one or more standalone mysqld servers andswitch between connections\u2022Can connect to an embedded server (libmysqld), if MariaDB iscompiled with support for libmysqld. (In this case, theexecutable is named mysqltest_embedded rather thanmysqltest.)By default, mysqltest reads the test case on the standard input.To run mysqltest this way, you normally invoke it like this:shell> mysqltest [options] [db_name] < test_fileYou can also name the test case file with a --test-file=file_nameoption.The exit value from mysqltest is 0 for success, 1 for failure,and 62 if it skips the test case (for example, if after checkingsome preconditions it decides not to run the test).mysqltest supports the following options:\u2022--help, -?Display a help message and exit.\u2022--basedir=dir_name, -b dir_nameThe base directory for tests.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--connect-timeout=numThis can be used to set the MYSQL_OPT_CONNECT_TIMEOUTparameter of mysql_options to change the number of secondsbefore an unsuccessful connection attempt times out.\u2022--continue-on-errorContinue test even if we got an error. This is mostly usefulwhen testing a storage engine to see what from a test file itcan execute, or to find all syntax errors in a newly createdbig test file.\u2022--cursor-protocolUse cursors for prepared statements.\u2022--database=db_name, -D db_nameThe default database to use.\u2022--debug[=debug_options], -#[debug_options]Write a debugging log if MariaDB is built with debuggingsupport. The default debug_options value is\u00b4d:t:S:i:O,/tmp/mysqltest.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--logdir=dir_nameThe directory to use for log files.\u2022--mark-progressWrite the line number and elapsed time to test_file.progress.\u2022--max-connect-retries=numThe maximum number of connection attempts when connecting toserver.\u2022--max-connections=numThe maximum number of simultaneous server connections perclient (that is, per test). If not set, the maximum is 128.Minimum allowed limit is 8, maximum is 5120.\u2022--no-defaultsDo not read default options from any option files. If used,this must be the first option.\u2022--non-blocking-apiUse the non-blocking client API for communication.\u2022--overlay-dir=dir_nameOverlay directory.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,you are prompted for one.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection or 0 fordefault to, in order of preference, my.cnf, $MYSQL_TCP_PORT,/etc/services, built-in default (3306).\u2022--prologue=nameInclude the contents of the given file before processing thecontents of the test file. The included file should have thesame format as other mysqltest test files. This option hasthe same effect as putting a --source file_name command asthe first line of the test file.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--ps-protocolUse the prepared-statement protocol for communication.\u2022--quietSuppress all normal output. This is a synonym for --silent.\u2022--record, -rRecord the output that results from running the test fileinto the file named by the --result-file option, if thatoption is given. It is an error to use this option withoutalso using --result-file.\u2022--result-file=file_name, -R file_nameThis option specifies the file for test case expectedresults.--result-file, together with --record, determineshow mysqltest treats the test actual and expected results fora test case:\u2022If the test produces no results, mysqltest exits with anerror message to that effect, unless --result-file isgiven and the named file is an empty file.\u2022Otherwise, if --result-file is not given, mysqltest sendstest results to the standard output.\u2022With --result-file but not --record, mysqltest reads theexpected results from the given file and compares themwith the actual results. If the results do not match,mysqltest writes a .reject file in the same directory asthe result file, outputs a diff of the two files, andexits with an error.\u2022With both --result-file and --record, mysqltest updatesthe given file by writing the actual test results to it.\u2022--result-format-version=#Version of the result file format to use.\u2022--server-arg=value, -A valuePass the argument as an argument to the embedded server. Forexample, --server-arg=--tmpdir=/tmp or --server-arg=--core.Up to 64 arguments can be given.\u2022--server-file=file_name, -F file_nameRead arguments for the embedded server from the given file.The file should contain one argument per line.\u2022--silent, -sSuppress all normal output.\u2022--sleep=num, -T numCause all sleep commands in the test case file to sleep numseconds. This option does not affect real_sleep commands.An option value of 0 can be used, which effectively disablessleep commands in the test case.\u2022--socket=path, -S pathThe socket file to use when connecting to localhost (which isthe default host).\u2022--sp-protocolExecute DML statements within a stored procedure. For everyDML statement, mysqltest creates and invokes a storedprocedure that executes the statement rather than executingthe statement directly.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--suite-dir=dir_nameSuite directory.\u2022--tail-lines=nnSpecify how many lines of the result to include in the outputif the test fails because an SQL statement fails. The defaultis 0, meaning no lines of result printed.\u2022--test-file=file_name, -x file_nameRead test input from this file. The default is to read fromthe standard input.\u2022--timer-file=file_name, -m file_nameIf given, the number of microseconds spent running the testwill be written to this file. This is used bymysql-test-run.pl for its reporting.\u2022--tmpdir=dir_name, -t dir_nameThe temporary directory where socket files are created.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print out more information about what theprogram does.\u2022--version, -VDisplay version information and exit.\u2022--view-protocolEvery SELECT statement is wrapped inside a view.",
        "name": "mariadb-test - program to run test cases (mysqltest is now asymlink to mariadb-test)mysqltest_embedded - program to run embedded test cases",
        "section": 1
    },
    {
        "command": "mariadb-tzinfo-to-sql",
        "description": "The mysql_tzinfo_to_sql program loads the time zone tables in themysql database. It is used on systems that have a zoneinfodatabase (the set of files describing time zones). Examples ofsuch systems are Linux, FreeBSD, Solaris, and Mac OS X. Onelikely location for these files is the /usr/share/zoneinfodirectory (/usr/share/lib/zoneinfo on Solaris).mysql_tzinfo_to_sql can be invoked several ways:shell> mysql_tzinfo_to_sql tz_dirshell> mysql_tzinfo_to_sql tz_file tz_nameshell> mysql_tzinfo_to_sql --leap tz_fileshell> mysql_tzinfo_to_sql --skip-write-binlog tz_dirFor the first invocation syntax, pass the zoneinfo directory pathname to mysql_tzinfo_to_sql and send the output into the mysqlprogram. For example:shell> mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root mysqlmysql_tzinfo_to_sql reads your system\u00b4s time zone files andgenerates SQL statements from them.mysql processes thosestatements to load the time zone tables.The second syntax causes mysql_tzinfo_to_sql to load a singletime zone file tz_file that corresponds to a time zone nametz_name:shell> mysql_tzinfo_to_sql tz_file tz_name | mysql -u root mysqlIf your time zone needs to account for leap seconds, invokemysql_tzinfo_to_sql using the third syntax, which initializes theleap second information.tz_file is the name of your time zonefile:shell> mysql_tzinfo_to_sql --leap tz_file | mysql -u root mysqlUsing the --skip-write-binlog option prevents writing of changesto the binary log or to other Galera cluster members. This can beused with any form of running mysql_tzinfo_to_sql.After running mysql_tzinfo_to_sql, it is best to restart theserver so that it does not continue to use any previously cachedtime zone data.",
        "name": "mariadb-tzinfo-to-sql - load the time zone tables(mysql_tzinfo_to_sql is now a symlink to mariadb-tzinfo-to-sql)",
        "section": 1
    },
    {
        "command": "mariadb-upgrade",
        "description": "mysql_upgrade examines all tables in all databases forincompatibilities with the current version of the MariaDB Server.mysql_upgrade also upgrades the system tables so that you cantake advantage of new privileges or capabilities that might havebeen added.mysql_upgrade should be executed each time you upgrade MariaDB.If a table is found to have a possible incompatibility,mysql_upgrade performs a table check. If any problems are found,a table repair is attempted.NoteOn Windows Server 2008 and Windows Vista, you must runmysql_upgrade with administrator privileges. You can do thisby running a Command Prompt as Administrator and running thecommand. Failure to do so may result in the upgrade failingto execute correctly.CautionYou should always back up your current MariaDB installationbefore performing an upgrade.To use mysql_upgrade, make sure that the server is running, andthen invoke it like this:shell> mysql_upgrade [options]After running mysql_upgrade, stop the server and restart it sothat any changes made to the system tables take effect.mysql_upgrade executes the following commands to check and repairtables and to upgrade the system tables:mysqlcheck --all-databases --check-upgrade --auto-repairmysql < fix_priv_tablesmysqlcheck --all-databases --check-upgrade --fix-db-names --fix-table-namesNotes about the preceding commands:\u2022Because mysql_upgrade invokes mysqlcheck with the--all-databases option, it processes all tables in alldatabases, which might take a long time to complete. Eachtable is locked and therefore unavailable to other sessionswhile it is being processed. Check and repair operations canbe time-consuming, particularly for large tables.\u2022For details about what checks the --check-upgrade optionentails, see the description of the FOR UPGRADE option of theCHECK TABLE statement.\u2022fix_priv_tables represents a script generated internally bymysql_upgrade that contains SQL statements to upgrade thetables in the mysql database.All checked and repaired tables are marked with the currentMariaDB version number. This ensures that next time you runmysql_upgrade with the same version of the server, it can tellwhether there is any need to check or repair the table again.mysql_upgrade also saves the MariaDB version number in a filenamed mysql_upgrade_info in the data directory. This is used toquickly check whether all tables have been checked for thisrelease so that table-checking can be skipped. To ignore thisfile and perform the check regardless, use the --force option.For this reason, mysql_upgrade needs to be run as a user withwrite access to the data directory.If you install MariaDB from RPM packages on Linux, you mustinstall the server and client RPMs.mysql_upgrade is included inthe server RPM but requires the client RPM because the latterincludes mysqlcheck.mysql_upgrade supports the following options, which can bespecified on the command line or in the [mysql_upgrade] and[client] option file groups. Other options are passed tomysqlcheck. For example, it might be necessary to specify the--password[=password] option.mysql_upgrade also supports theoptions for processing option files.\u2022--help, -?Display a short help message and exit.\u2022--basedir=pathOld option accepted for backward compatibility but ignored.\u2022--character-sets-dir=pathOld option accepted for backward compatibility but ignored.\u2022--check-if-upgrade-is-neededExit with a status code indicating if an upgrade is needed.Returns 0 if upgrade needed or current version couldn't bedetermined, 1 when no action required.\u2022--datadir=pathOld option accepted for backward compatibility but ignored.\u2022--debug=path, -# pathFor debug builds, output debug log.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-info, -TPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-character-set=nameOld option accepted for backward compatibility but ignored.\u2022--forceIgnore the mysql_upgrade_info file and force execution ofmysqlcheck even if mysql_upgrade has already been executedfor the current version of MariaDB.\u2022--hostConnect to MariaDB on the given host.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysql_upgrade prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--silentPrint less information.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--tmpdir=path, -t pathThe path name of the directory to use for creating temporaryfiles.\u2022--upgrade-system-tables, -sOnly upgrade the system tables in the mysql database. Tablesin other databases are not checked or touched.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the serverand not using the current login.\u2022--verboseDisplay more output about the process. Using it twice willprint connection arguments; using it 3 times will print outall CHECK, RENAME and ALTER TABLE commands used during thecheck phase; using it 4 times (added in MariaDB 10.0.14) willalso write out all mariadb-check commands used; using it 5times will print all the mariadb commands used and theirresults while running mysql_fix_privilege_tables script.\u2022--version, -VOutput version information and exit.\u2022--version-check, -kRun this program only if its 'server version' matches theversion of the server to which it's connecting. Note: the'server version' of the program is the version of the MariaDBserver with which it was built/distributed. Defaults to on;use --skip-version-check to disable.\u2022--write-binlogCause binary logging to be enabled while mysql_upgrade runs.",
        "name": "mariadb-upgrade - check tables for MariaDB upgrade (mysql_upgradeis now a symlink to mariadb-upgrade)",
        "section": 1
    },
    {
        "command": "mariadb-waitpid",
        "description": "mysql_waitpid signals a process to terminate and waits for theprocess to exit. It uses the kill() system call and Unix signals,so it runs on Unix and Unix-like systems.Invoke mysql_waitpid like this:shell> mysql_waitpid [options] pid wait_timemysql_waitpid sends signal 0 to the process identified by pid andwaits up to wait_time seconds for the process to terminate.pidand wait_time must be positive integers.If process termination occurs within the wait time or the processdoes not exist, mysql_waitpid returns 0. Otherwise, it returns 1.If the kill() system call cannot handle signal 0, mysql_waitpid()uses signal 1 instead.mysql_waitpid supports the following options:\u2022--help, -?, -IDisplay a help message and exit.\u2022--verbose, -vVerbose mode. Display a warning if signal 0 could not be usedand signal 1 is used instead.\u2022--version, -VDisplay version information and exit.",
        "name": "mariadb-waitpid - kill process and wait for its termination(mysql_waitpid is now a symlink to mariadb-waitpid)",
        "section": 1
    },
    {
        "command": "mariadbd-multi",
        "description": "mysqld_multi is designed to manage several mysqld processes thatlisten for connections on different Unix socket files and TCP/IPports. It can start or stop servers, or report their currentstatus.mysqld_multi searches for groups named [mysqldN] in my.cnf (or inthe file named by the --config-file option).N can be anypositive integer. This number is referred to in the followingdiscussion as the option group number, or GNR. Group numbersdistinguish option groups from one another and are used asarguments to mysqld_multi to specify which servers you want tostart, stop, or obtain a status report for. Options listed inthese groups are the same that you would use in the [mysqld]group used for starting mysqld. However, when using multipleservers, it is necessary that each one use its own value foroptions such as the Unix socket file and TCP/IP port number.To invoke mysqld_multi, use the following syntax:shell> mysqld_multi [options] {start|stop|report} [GNR[,GNR] ...]start, stop, and report indicate which operation to perform. Youcan perform the designated operation for a single server ormultiple servers, depending on the GNR list that follows theoption name. If there is no list, mysqld_multi performs theoperation for all servers in the option file.Each GNR value represents an option group number or range ofgroup numbers. The value should be the number at the end of thegroup name in the option file. For example, the GNR for a groupnamed [mysqld17] is 17. To specify a range of numbers, separatethe first and last numbers by a dash. The GNR value 10-13represents groups [mysqld10] through [mysqld13]. Multiple groupsor group ranges can be specified on the command line, separatedby commas. There must be no whitespace characters (spaces ortabs) in the GNR list; anything after a whitespace character isignored.This command starts a single server using option group[mysqld17]:shell> mysqld_multi start 17This command stops several servers, using option groups [mysqld8]and [mysqld10] through [mysqld13]:shell> mysqld_multi stop 8,10-13For an example of how you might set up an option file, use thiscommand:shell> mysqld_multi --examplemysqld_multi searches for option files as follows:\u2022With --no-defaults, no option files are read.\u2022With --defaults-file=file_name, only the named file is read.\u2022Otherwise, option files in the standard list of locations areread, including any file named by the--defaults-extra-file=file_name option, if one is given. (Ifthe option is given multiple times, the last value is used.)Option files read are searched for [mysqld_multi] and [mysqldN]option groups. The [mysqld_multi] group can be used for optionsto mysqld_multi itself.[mysqldN] groups can be used for optionspassed to specific mysqld instances.The [mysqld] or [mysqld_safe] groups can be used for commonoptions read by all instances of mysqld or mysqld_safe. You canspecify a --defaults-file=file_name option to use a differentconfiguration file for that instance, in which case the [mysqld]or [mysqld_safe] groups from that file will be used for thatinstance.mysqld_multi supports the following options.\u2022--helpDisplay a help message and exit.\u2022--exampleDisplay a sample option file.\u2022--log=file_nameSpecify the name of the log file. If the file exists, logoutput is appended to it.\u2022--mysqladmin=prog_nameThe mysqladmin binary to be used to stop servers.\u2022--mysqld=prog_nameThe mysqld binary to be used. Note that you can specifymysqld_safe as the value for this option also. If you usemysqld_safe to start the server, you can include the mysqldor ledir options in the corresponding [mysqldN] option group.These options indicate the name of the server thatmysqld_safe should start and the path name of the directorywhere the server is located. (See the descriptions for theseoptions in mysqld_safe(1).) Example:[mysqld38]mysqld = mysqld-debugledir= /opt/local/mysql/libexec\u2022--no-logPrint log information to stdout rather than to the log file.By default, output goes to the log file.\u2022--password=passwordThe password of the MariaDB account to use when invokingmysqladmin. Note that the password value is not optional forthis option, unlike for other MariaDB programs.\u2022--silentSilent mode; disable warnings.\u2022--tcp-ipConnect to the MariaDB server(s) via the TCP/IP port insteadof the UNIX socket. This affects stopping and reporting. If asocket file is missing, the server may still be running, butcan be accessed only via the TCP/IP port. By defaultconnecting is done via the UNIX socket. This option affectsstop and report operations.\u2022--user=user_nameThe user name of the MariaDB account to use when invokingmysqladmin.\u2022--verboseBe more verbose.\u2022--versionDisplay version information and exit.\u2022--wsrep-new-clusterBootstrap a cluster.Some notes about mysqld_multi:\u2022Most important: Before using mysqld_multi be sure that youunderstand the meanings of the options that are passed to themysqld servers and why you would want to have separate mysqldprocesses. Beware of the dangers of using multiple mysqldservers with the same data directory. Use separate datadirectories, unless you know what you are doing. Startingmultiple servers with the same data directory does not giveyou extra performance in a threaded system.\u2022Important: Make sure that the data directory for each serveris fully accessible to the Unix account that the specificmysqld process is started as.Do not use the Unix rootaccount for this, unless you know what you are doing.\u2022Make sure that the MariaDB account used for stopping themysqld servers (with the mysqladmin program) has the sameuser name and password for each server. Also, make sure thatthe account has the SHUTDOWN privilege. If the servers thatyou want to manage have different user names or passwords forthe administrative accounts, you might want to create anaccount on each server that has the same user name andpassword. For example, you might set up a common multi_adminaccount by executing the following commands for each server:shell> mysql -u root -S /tmp/mysql.sock -pEnter password:mysql> GRANT SHUTDOWN ON *.*-> TO \u00b4multi_admin\u00b4@\u00b4localhost\u00b4 IDENTIFIED BY \u00b4multipass\u00b4;Change the connection parameters appropriately whenconnecting to each one. Note that the host name part of theaccount name must allow you to connect as multi_admin fromthe host where you want to run mysqld_multi.\u2022The Unix socket file and the TCP/IP port number must bedifferent for every mysqld. (Alternatively, if the host hasmultiple network addresses, you can use --bind-address tocause different servers to listen to different interfaces.)\u2022The --pid-file option is very important if you are usingmysqld_safe to start mysqld (for example,--mysqld=mysqld_safe) Every mysqld should have its ownprocess ID file. The advantage of using mysqld_safe insteadof mysqld is that mysqld_safe monitors its mysqld process andrestarts it if the process terminates due to a signal sentusing kill -9 or for other reasons, such as a segmentationfault. Please note that the mysqld_safe script might requirethat you start it from a certain place. This means that youmight have to change location to a certain directory beforerunning mysqld_multi. If you have problems starting, pleasesee the mysqld_safe script. Check especially the lines:----------------------------------------------------------------MY_PWD=`pwd`# Check if we are starting this relative (for the binary release)if test -d $MY_PWD/data/mysql -a \\-f ./share/mysql/english/errmsg.sys -a \\-x ./bin/mysqld----------------------------------------------------------------The test performed by these lines should be successful, oryou might encounter problems. See mysqld_safe(1).\u2022You might want to use the --user option for mysqld, but to dothis you need to run the mysqld_multi script as the Unix rootuser. Having the option in the option file doesn\u00b4t matter;you just get a warning if you are not the superuser and themysqld processes are started under your own Unix account.The following example shows how you might set up an option filefor use with mysqld_multi. The order in which the mysqld programsare started or stopped depends on the order in which they appearin the option file. Group numbers need not form an unbrokensequence. The first and fifth [mysqldN] groups were intentionallyomitted from the example to illustrate that you can have \u201cgaps\u201din the option file. This gives you more flexibility.# This file should probably be in your home dir (~/.my.cnf)# or /etc/my.cnf# Version 2.1 by Jani Tolonen[mysqld_multi]mysqld= /usr/local/bin/mysqld_safemysqladmin = /usr/local/bin/mysqladminuser= multi_adminpassword= multipass[mysqld2]socket= /tmp/mysql.sock2port= 3307pid-file= /usr/local/mysql/var2/hostname.pid2datadir= /usr/local/mysql/var2language= /usr/local/share/mysql/englishuser= john[mysqld3]socket= /tmp/mysql.sock3port= 3308pid-file= /usr/local/mysql/var3/hostname.pid3datadir= /usr/local/mysql/var3language= /usr/local/share/mysql/swedishuser= monty[mysqld4]socket= /tmp/mysql.sock4port= 3309pid-file= /usr/local/mysql/var4/hostname.pid4datadir= /usr/local/mysql/var4language= /usr/local/share/mysql/estoniauser= tonu[mysqld6]socket= /tmp/mysql.sock6port= 3311pid-file= /usr/local/mysql/var6/hostname.pid6datadir= /usr/local/mysql/var6language= /usr/local/share/mysql/japaneseuser= jani",
        "name": "mariadbd-multi - manage multiple MariaDB servers (mysqld_multi isnow a symlink to mariadbd-multi)",
        "section": 1
    },
    {
        "command": "mariadbd-safe",
        "description": "mysqld_safe is the recommended way to start a mysqld server onUnix.mysqld_safe adds some safety features such as restartingthe server when an error occurs and logging runtime informationto an error log file. Descriptions of error logging is givenlater in this section.mysqld_safe tries to start an executable named mysqld. Tooverride the default behavior and specify explicitly the name ofthe server you want to run, specify a --mysqld or--mysqld-version option to mysqld_safe. You can also use --ledirto indicate the directory where mysqld_safe should look for theserver.Many of the options to mysqld_safe are the same as the options tomysqld.Options unknown to mysqld_safe are passed to mysqld if they arespecified on the command line, but ignored if they are specifiedin the [mysqld_safe] or [mariadb_safe] groups of an option file.mysqld_safe reads all options from the [mysqld], [server],[mysqld_safe], and [mariadb_safe] sections in option files. Forexample, if you specify a [mysqld] section like this, mysqld_safewill find and use the --log-error option:[mysqld]log-error=error.logFor backward compatibility, mysqld_safe also reads [safe_mysqld]sections, although you should rename such sections to[mysqld_safe] in current installations.mysqld_safe supports the options in the following list. It alsoreads option files and supports the options for processing them.\u2022--helpDisplay a help message and exit.\u2022--basedir=pathThe path to the MariaDB installation directory.\u2022--core-file-size=sizeThe size of the core file that mysqld should be able tocreate. The option value is passed to ulimit -c.\u2022--crash-script=fileScript to call in the event of mysqld crashing.\u2022--datadir=pathThe path to the data directory.\u2022--defaults-extra-file=pathThe name of an option file to be read in addition to theusual option files. This must be the first option on thecommand line if it is used. If the file does not exist or isotherwise inaccessible, the server will exit with an error.\u2022--defaults-file=file_nameThe name of an option file to be read instead of the usualoption files. This must be the first option on the commandline if it is used.\u2022--flush-cachesFlush and purge buffers/caches before starting the server.\u2022--ledir=pathIf mysqld_safe cannot find the server, use this option toindicate the path name to the directory where the server islocated.\u2022--log-error=file_nameWrite the error log to the given file.\u2022--malloc-lib=libPreload shared library lib if available.\u2022--mysqld=prog_nameThe name of the server program (in the ledir directory) thatyou want to start. This option is needed if you use theMariaDB binary distribution but have the data directoryoutside of the binary distribution. If mysqld_safe cannotfind the server, use the --ledir option to indicate the pathname to the directory where the server is located.\u2022--mysqld-version=suffixThis option is similar to the --mysqld option, but youspecify only the suffix for the server program name. Thebasename is assumed to be mysqld. For example, if you use--mysqld-version=debug, mysqld_safe starts the mysqld-debugprogram in the ledir directory. If the argument to--mysqld-version is empty, mysqld_safe uses mysqld in theledir directory.\u2022--nice=priorityUse the nice program to set the server\u00b4s scheduling priorityto the given value.\u2022--no-auto-restart, --nowatch, --no-watchExit after starting mysqld.\u2022--no-defaultsDo not read any option files. This must be the first optionon the command line if it is used.\u2022--numa-interleaveRun mysqld with its memory interleaved on all NUMA nodes.\u2022--open-files-limit=countThe number of files that mysqld should be able to open. Theoption value is passed to ulimit -n. Note that you need tostart mysqld_safe as root for this to work properly!\u2022--pid-file=file_nameThe path name of the process ID file.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_numThe port number that the server should use when listening forTCP/IP connections. The port number must be 1024 or higherunless the server is started by the root system user.\u2022--skip-kill-mysqldDo not try to kill stray mysqld processes at startup. Thisoption works only on Linux.\u2022--socket=pathThe Unix socket file that the server should use whenlistening for local connections.\u2022--syslog, --skip-syslog--syslog causes error messages to be sent to syslog onsystems that support the logger program.--skip-syslogsuppresses the use of syslog; messages are written to anerror log file.\u2022--syslog-tag=tagFor logging to syslog, messages from mysqld_safe and mysqldare written with a tag of mysqld_safe and mysqld,respectively. To specify a suffix for the tag, use--syslog-tag=tag, which modifies the tags to bemysqld_safe-tag and mysqld-tag.\u2022--timezone=timezoneSet the TZ time zone environment variable to the given optionvalue. Consult your operating system documentation for legaltime zone specification formats.\u2022--user={user_name|user_id}Run the mysqld server as the user having the name user_nameor the numeric user ID user_id. (\u201cUser\u201d in this contextrefers to a system login account, not a MariaDB user listedin the grant tables.)If you execute mysqld_safe with the --defaults-file or--defaults-extra-file option to name an option file, the optionmust be the first one given on the command line or the optionfile will not be used. For example, this command will not use thenamed option file:mysql> mysqld_safe --port=port_num --defaults-file=file_nameInstead, use the following command:mysql> mysqld_safe --defaults-file=file_name --port=port_numThe mysqld_safe script is written so that it normally can start aserver that was installed from either a source or a binarydistribution of MariaDB, even though these types of distributionstypically install the server in slightly different locations.mysqld_safe expects one of the following conditions to be true:\u2022The server and databases can be found relative to the workingdirectory (the directory from which mysqld_safe is invoked).For binary distributions, mysqld_safe looks under its workingdirectory for bin and data directories. For sourcedistributions, it looks for libexec and var directories. Thiscondition should be met if you execute mysqld_safe from yourMariaDB installation directory (for example, /usr/local/mysqlfor a binary distribution).\u2022If the server and databases cannot be found relative to theworking directory, mysqld_safe attempts to locate them byabsolute path names. Typical locations are /usr/local/libexecand /usr/local/var. The actual locations are determined fromthe values configured into the distribution at the time itwas built. They should be correct if MariaDB is installed inthe location specified at configuration time.Because mysqld_safe tries to find the server and databasesrelative to its own working directory, you can install a binarydistribution of MariaDB anywhere, as long as you run mysqld_safefrom the MariaDB installation directory:shell> cd mysql_installation_directoryshell> bin/mysqld_safe &If mysqld_safe fails, even when invoked from the MariaDBinstallation directory, you can specify the --ledir and --datadiroptions to indicate the directories in which the server anddatabases are located on your system.When you use mysqld_safe to start mysqld, mysqld_safe arrangesfor error (and notice) messages from itself and from mysqld to goto the same destination.There are several mysqld_safe options for controlling thedestination of these messages:\u2022--syslog: Write error messages to syslog on systems thatsupport the logger program.\u2022--skip-syslog: Do not write error messages to syslog.Messages are written to the default error log file(host_name.err in the data directory), or to a named file ifthe --log-error option is given.\u2022--log-error=file_name: Write error messages to the namederror file.If none of these options is given, the default is --skip-syslog.NoteIf --syslog and --log-error are both given, a warning is issuedand --log-error takes precedence.When mysqld_safe writes a message, notices go to the loggingdestination (syslog or the error log file) and stdout. Errors goto the logging destination and stderr.Normally, you should not edit the mysqld_safe script. Instead,configure mysqld_safe by using command-line options or options inthe [mysqld_safe] section of a my.cnf option file. In rare cases,it might be necessary to edit mysqld_safe to get it to start theserver properly. However, if you do this, your modified versionof mysqld_safe might be overwritten if you upgrade MariaDB in thefuture, so you should make a copy of your edited version that youcan reinstall.On NetWare, mysqld_safe is a NetWare Loadable Module (NLM) thatis ported from the original Unix shell script. It starts theserver as follows:1. Runs a number of system and option checks.2. Runs a check on MyISAM tables.3. Provides a screen presence for the MariaDB server.4. Starts mysqld, monitors it, and restarts it if it terminatesin error.5. Sends error messages from mysqld to the host_name.err file inthe data directory.6. Sends mysqld_safe screen output to the host_name.safe file inthe data directory.",
        "name": "mariadbd-safe - MariaDB server startup script (mysqld_safe is nowa symlink to mariadbd-safe)",
        "section": 1
    },
    {
        "command": "mariadbd-safe-helper",
        "description": "Use: Helper script.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "mariadbd-safe-helper - helper script (mysqld_safe_helper is now asymlink to mariadbd-safe-helper)",
        "section": 1
    },
    {
        "command": "mbstream",
        "description": "Use mbstream --help for details on usage.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "mbstream - Serialize/deserialize files in the XBSTREAM format",
        "section": 1
    },
    {
        "command": "mckey",
        "description": "Establishes a set of RDMA multicast communication paths betweennodes using the librdmacm, optionally transfers datagrams toreceiving nodes, then tears down the communication.",
        "name": "mckey - RDMA CM multicast setup and simple data transfer test.",
        "section": 1
    },
    {
        "command": "mcookie",
        "description": "mcookie generates a 128-bit random hexadecimal number for usewith the X authority system. Typical usage:xauth add :0 . mcookieThe \"random\" number generated is actually the MD5 message digestof random information coming from one of the sources getrandom(2)system call, /dev/urandom, /dev/random, or the libc pseudo-randomfunctions, in this preference order. See also the option --file.",
        "name": "mcookie - generate magic cookies for xauth",
        "section": 1
    },
    {
        "command": "md5sum",
        "description": "Print or check MD5 (128-bit) checksums.With no FILE, or when FILE is -, read standard input.-b, --binaryread in binary mode-c, --checkread checksums from the FILEs and check them--tagcreate a BSD-style checksum-t, --textread in text mode (default)-z, --zeroend each output line with NUL, not newline, and disablefile name escapingThe following five options are useful only when verifying checksums:--ignore-missingdon't fail or report status for missing files--quietdon't print OK for each successfully verified file--statusdon't output anything, status code shows success--strictexit non-zero for improperly formatted checksum lines-w, --warnwarn about improperly formatted checksum lines--help display this help and exit--versionoutput version information and exitThe sums are computed as described in RFC 1321.When checking,the input should be a former output of this program.The defaultmode is to print a line with: checksum, a space, a characterindicating input mode ('*' for binary, ' ' for text or wherebinary is insignificant), and name for each FILE.Note: There is no difference between binary mode and text mode onGNU systems.",
        "name": "md5sum - compute and check MD5 message digest",
        "section": 1
    },
    {
        "command": "memusage",
        "description": "memusage is a bash script which profiles memory usage of theprogram, program.It preloads the libmemusage.so library intothe caller's environment (via the LD_PRELOAD environmentvariable; see ld.so(8)).The libmemusage.so library tracesmemory allocation by intercepting calls to malloc(3), calloc(3),free(3), and realloc(3); optionally, calls to mmap(2), mremap(2),and munmap(2) can also be intercepted.memusage can output the collected data in textual form, or it canuse memusagestat(1) (see the -p option,below) to create a PNGfile containing graphical representation of the collected data.Memory usage summaryThe \"Memory usage summary\" line output by memusage contains threefields:heap totalSum of size arguments of all malloc(3) calls, productsof arguments (nmemb*size) of all calloc(3) calls, andsum of length arguments of all mmap(2) calls.In thecase of realloc(3) and mremap(2), if the new size ofan allocation is larger than the previous size, thesum of all such differences (new size minus old size)is added.heap peakMaximum of all size arguments of malloc(3), allproducts of nmemb*size of calloc(3), all sizearguments of realloc(3), length arguments of mmap(2),and new_size arguments of mremap(2).stack peakBefore the first call to any monitored function, thestack pointer address (base stack pointer) is saved.After each function call, the actual stack pointeraddress is read and the difference from the base stackpointer computed.The maximum of these differences isthen the stack peak.Immediately following this summary line, a table shows the numbercalls, total memory allocated or deallocated, and number offailed calls for each intercepted function.For realloc(3) andmremap(2), the additional field \"nomove\" shows reallocations thatchanged the address of a block, and the additional \"dec\" fieldshows reallocations that decreased the size of the block.Forrealloc(3), the additional field \"free\" shows reallocations thatcaused a block to be freed (i.e., the reallocated size was 0).The \"realloc/total memory\" of the table output by memusage doesnot reflect cases where realloc(3) is used to reallocate a blockof memory to have a smaller size than previously.This can causesum of all \"total memory\" cells (excluding \"free\") to be largerthan the \"free/total memory\" cell.Histogram for block sizesThe \"Histogram for block sizes\" provides a breakdown of memoryallocations into various bucket sizes.",
        "name": "memusage - profile memory usage of a program",
        "section": 1
    },
    {
        "command": "memusagestat",
        "description": "memusagestat creates a PNG file containing a graphicalrepresentation of the memory profiling data in the file datafile;that file is generated via the -d (or --data) option ofmemusage(1).The red line in the graph shows the heap usage (allocated memory)and the green line shows the stack usage.The x-scale is eitherthe number of memory-handling function calls or (if the -t optionis specified) time.",
        "name": "memusagestat - generate graphic from memory profiling data",
        "section": 1
    },
    {
        "command": "mesg",
        "description": "The mesg utility is invoked by a user to control write accessothers have to the terminal device associated with standard erroroutput. If write access is allowed, then programs such as talk(1)and write(1) may display messages on the terminal.Traditionally, write access is allowed by default. However, asusers become more conscious of various security risks, there is atrend to remove write access by default, at least for the primarylogin shell. To make sure your ttys are set the way you want themto be set, mesg should be executed in your login scripts.The mesg utility silently exits with error status 2 if notexecuted on a terminal. In this case executing mesg is pointless.The command line option --verbose forces mesg to print a warningin this situation. This behaviour has been introduced in version2.33.",
        "name": "mesg - display (or do not display) messages from other users",
        "section": 1
    },
    {
        "command": "minicom",
        "description": "minicom is a communication program which somewhat resembles theshareware program TELIX but is free with source code and runsunder most Unices.Features include dialing directory with auto-redial, support for UUCP-style lock files on serial devices, aseparate script language interpreter, capture to file, multipleusers with individual configurations, and more.",
        "name": "minicom - friendly serial communication program",
        "section": 1
    },
    {
        "command": "mk-ca-bundle",
        "description": "The mk-ca-bundle tool downloads the certdata.txt file fromMozilla's source tree over HTTPS, then parses certdata.txt andextracts certificates into PEM format. By default, only CA rootcertificates trusted to issue SSL server authenticationcertificates are extracted. These are then processed with theOpenSSL command line tool to produce the final ca-bundle file.The default outputfile name is ca-bundle.crt. By setting it to'-' (a single dash) you will get the output sent to STDOUTinstead of a file.The PEM format this scripts uses for output makes the resultreadily available for use by just about all OpenSSL or GnuTLSpowered applications, such as curl and others.",
        "name": "mk-ca-bundle - convert Mozilla's certificate bundle to PEM format",
        "section": 1
    },
    {
        "command": "mkaf",
        "description": "A collection of one or more Performance Co-Pilot (seePCPIntro(1)) archive logs may be combined with mkaf to produce aPCP archive folio and the associated archive folio control file.Some PCP tools use mkaf to create archive folios, e.g. the``record'' facility in the pmchart(1) and pmview(1) tools, tofacilitate playback with pmafm(1).mkaf processes each filename argument, and if this is a componentfile from a PCP archive that archive is added to the folio.If filename is a directory, then this is searched recursivelyusing find(1).Any filename argument beginning with a ``-'' isassumed to be a find(1) command line option (findopts); thedefault is -follow if no findopts are specified.The first named archive in the folio is assumed to be associatedwith the default host for any tool that tries to replay multiplearchives from the folio.The folio control file is written to standard output, and has thefollowing format.1. The first line contains the word PCPFolio.2. The second line contains the tag Version: followed by theformat version number (currently 1).3. For subsequent lines, blank lines and lines beginning with``#'' are ignored.4. The line beginning with the tag Created: documents where andwhen the folio was created.5. The line beginning with the tag Creator: identifies the toolwhich created the folio (and is assumed to know how to replaythe archive folio).If present, the second argument is thename of a configuration file that the creator tool could useto replay the archive folio, e.g. with the replay command forpmafm(1).In the case of mkaf (unlike pmchart(1) orpmview(1)) there is no knowledge of the contents of thearchives, so the ``creator'' cannot replay the archive,however pmchart(1) is able to replay any archive, and so thistool is identified as the Creator: for archive folios createdby mkaf(1).6. This is then followed by one or more lines beginning with thetag Archive: followed by the hostname and base name of thearchive.For example$ mkaf mydir/gonzomight produce the following folio control file.PCPFolioVersion: 1# use pmafm(1) to process this PCP archive folio#Created: on gonzo at Tue Jul2 03:35:54 EST 1996Creator: pmchart#HostBasename#Archive:gonzomydir/gonzo/960627Archive:gonzomydir/gonzo/960628Archive:gonzomydir/gonzo/960629Archive:gonzomydir/gonzo/960630Archive:gonzomydir/gonzo/960701Archive:gonzomydir/gonzo/960701.00.10Archive:gonzomydir/gonzo/960701.05.25Archive:gonzomydir/gonzo/960702.00.10",
        "name": "mkaf - create a Performance Co-Pilot archive folio",
        "section": 1
    },
    {
        "command": "mkdir",
        "description": "Create the DIRECTORY(ies), if they do not already exist.Mandatory arguments to long options are mandatory for shortoptions too.-m, --mode=MODEset file mode (as in chmod), not a=rwx - umask-p, --parentsno error if existing, make parent directories as needed,with their file modes unaffected by any -m option.-v, --verboseprint a message for each created directory-Zset SELinux security context of each created directory tothe default type--context[=CTX]like -Z, or if CTX is specified then set the SELinux orSMACK security context to CTX--help display this help and exit--versionoutput version information and exit",
        "name": "mkdir - make directories",
        "section": 1
    },
    {
        "command": "mkfifo",
        "description": "Create named pipes (FIFOs) with the given NAMEs.Mandatory arguments to long options are mandatory for shortoptions too.-m, --mode=MODEset file permission bits to MODE, not a=rw - umask-Zset the SELinux security context to default type--context[=CTX]like -Z, or if CTX is specified then set the SELinux orSMACK security context to CTX--help display this help and exit--versionoutput version information and exit",
        "name": "mkfifo - make FIFOs (named pipes)",
        "section": 1
    },
    {
        "command": "mknod",
        "description": "Create the special file NAME of the given TYPE.Mandatory arguments to long options are mandatory for shortoptions too.-m, --mode=MODEset file permission bits to MODE, not a=rw - umask-Zset the SELinux security context to default type--context[=CTX]like -Z, or if CTX is specified then set the SELinux orSMACK security context to CTX--help display this help and exit--versionoutput version information and exitBoth MAJOR and MINOR must be specified when TYPE is b, c, or u,and they must be omitted when TYPE is p.If MAJOR or MINORbegins with 0x or 0X, it is interpreted as hexadecimal;otherwise, if it begins with 0, as octal; otherwise, as decimal.TYPE may be:bcreate a block (buffered) special filec, ucreate a character (unbuffered) special filepcreate a FIFONOTE: your shell may have its own version of mknod, which usuallysupersedes the version described here.Please refer to yourshell's documentation for details about the options it supports.",
        "name": "mknod - make block or character special files",
        "section": 1
    },
    {
        "command": "mktemp",
        "description": "Create a temporary file or directory, safely, and print its name.TEMPLATE must contain at least 3 consecutive 'X's in lastcomponent.If TEMPLATE is not specified, use tmp.XXXXXXXXXX, and--tmpdir is implied.Files are created u+rw, and directoriesu+rwx, minus umask restrictions.-d, --directorycreate a directory, not a file-u, --dry-rundo not create anything; merely print a name (unsafe)-q, --quietsuppress diagnostics about file/dir-creation failure--suffix=SUFFappend SUFF to TEMPLATE; SUFF must not contain a slash.This option is implied if TEMPLATE does not end in X-p DIR, --tmpdir[=DIR]interpret TEMPLATE relative to DIR; if DIR is notspecified, use $TMPDIR if set, else /tmp.With thisoption, TEMPLATE must not be an absolute name; unlike with-t, TEMPLATE may contain slashes, but mktemp creates onlythe final component-tinterpret TEMPLATE as a single file name component,relative to a directory: $TMPDIR, if set; else thedirectory specified via -p; else /tmp [deprecated]--help display this help and exit--versionoutput version information and exit",
        "name": "mktemp - create a temporary file or directory",
        "section": 1
    },
    {
        "command": "mmroff",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "more",
        "description": "more is a filter for paging through text one screenful at a time.This version is especially primitive. Users should realize thatless(1) provides more(1) emulation plus extensive enhancements.",
        "name": "more - display the contents of a file in a terminal",
        "section": 1
    },
    {
        "command": "mount.ddi",
        "description": "systemd-dissect is a tool for introspecting and interacting withfile system OS disk images, specifically Discoverable Disk Images(DDIs). It supports four different operations:1. Show general OS image information, including the image'sos-release(5) data, machine ID, partition information andmore.2. Mount an OS image to a local directory. In this mode it willdissect the OS image and mount the included partitionsaccording to their designation onto a directory and possiblysub-directories.3. Unmount an OS image from a local directory. In this mode itwill recursively unmount the mounted partitions and removethe underlying loop device, including all the partitionsub-devices.4. Copy files and directories in and out of an OS image.The tool may operate on three types of OS images:1. OS disk images containing a GPT partition table envelope,with partitions marked according to the DiscoverablePartitions Specification[1].2. OS disk images containing just a plain file-system without anenveloping partition table. (This file system is assumed tobe the root file system of the OS.)3. OS disk images containing a GPT or MBR partition table, witha single partition only. (This partition is assumed tocontain the root file system of the OS.)OS images may use any kind of Linux-supported file systems. Inaddition they may make use of LUKS disk encryption, and containVerity integrity information. Note that qualifying OS images maybe booted with systemd-nspawn(1)'s --image= switch, and be usedas root file system for system service using the RootImage= unitfile setting, see systemd.exec(5).Note that the partition table shown when invoked without commandswitch (as listed below) does not necessarily show all partitionsincluded in the image, but just the partitions that areunderstood and considered part of an OS disk image. Specifically,partitions of unknown types are ignored, as well as duplicatepartitions (i.e. more than one per partition type), as are rootand /usr/ partitions of architectures not compatible with thelocal system. In other words: this tool will display what itoperates with when mounting the image. To display the completelist of partitions use a tool such as fdisk(8).The systemd-dissect command may be invoked as mount.ddi in whichcase it implements the mount(8) \"external helper\" interface. Thisensures disk images compatible with systemd-dissect can bemounted directly by mount and fstab(5). For details see below.",
        "name": "systemd-dissect, mount.ddi - Dissect Discoverable Disk Images(DDIs)",
        "section": 1
    },
    {
        "command": "mountpoint",
        "description": "mountpoint checks whether the given directory or file ismentioned in the /proc/self/mountinfo file.",
        "name": "mountpoint - see if a directory or file is a mountpoint",
        "section": 1
    },
    {
        "command": "mpstat",
        "description": "The mpstat command writes to standard output activities for eachavailable processor, processor 0 being the first one.Globalaverage activities among all processors are also reported.Thempstat command can be used on both SMP and UP machines, but inthe latter, only global average activities will be printed. If noactivity has been selected, then the default report is the CPUutilization report.The interval parameter specifies the amount of time in secondsbetween each report.A value of 0 (or no parameters at all)indicates that processors statistics are to be reported for thetime since system startup (boot). The count parameter can bespecified in conjunction with the interval parameter if this oneis not set to zero. The value of count determines the number ofreports generated at interval seconds apart. If the intervalparameter is specified without the count parameter, the mpstatcommand generates reports continuously.",
        "name": "mpstat - Report processors related statistics.",
        "section": 1
    },
    {
        "command": "mrtg2pcp",
        "description": "mrtg2pcp is intended to read an MRTG log file as created bymrtg(1) and translate this into a Performance Co-Pilot (PCP)archive with the basename outfile.The hostname, devname, andtimezone arguments specify information about the system for whichthe statistics were gathered.The resultant PCP achive may be used with all PCP client tools tograph subsets of the data using pmchart(1), perform datareduction and reporting, filter with the PCP inference enginepmie(1), etc.A series of physical files will be created with the prefixoutfile.These are outfile.0 (the performance data),outfile.meta (the metadata that describes the performance data)and outfile.index (a temporal index to improve efficiency ofreplay operations for the archive).If any of these files existsalready, then mrtg2pcp will not overwrite them and will exit withan error message of the form__pmLogNewFile: ``blah.0'' already exists, not over-writtenmrtg2pcp is a Perl script that uses the PCP::LogImport Perlwrapper around the PCP libpcp_import library, and as such couldbe used as an example to develop new tools to import other typesof performance data and create PCP archives.",
        "name": "mrtg2pcp - import MRTG data and create a PCP archive",
        "section": 1
    },
    {
        "command": "ms_print",
        "description": "ms_print takes an output file produced by the Valgrind toolMassif and prints the information in an easy-to-read form.",
        "name": "ms_print - post-processing tool for Massif",
        "section": 1
    },
    {
        "command": "msgattrib",
        "description": "Filters the messages of a translation catalog according to theirattributes, and manipulates the attributes.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:INPUTFILEinput PO file-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf no input file is given or if it is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Message selection:--translatedkeep translated, remove untranslated messages--untranslatedkeep untranslated, remove translated messages--no-fuzzyremove 'fuzzy' marked messages--only-fuzzykeep 'fuzzy' marked messages--no-obsoleteremove obsolete #~ messages--only-obsoletekeep obsolete #~ messagesAttribute manipulation:--set-fuzzyset all messages 'fuzzy'--clear-fuzzyset all messages non-'fuzzy'--set-obsoleteset all messages obsolete--clear-obsoleteset all messages non-obsolete--previouswhen setting 'fuzzy', keep previous msgids of translatedmessages.--clear-previousremove the \"previous msgid\" from all messages--emptywhen removing 'fuzzy', also set msgstr empty--only-file=FILE.pomanipulate only entries listed in FILE.po--ignore-file=FILE.pomanipulate only entries not listed in FILE.po--fuzzysynonym for --only-fuzzy --clear-fuzzy--obsoletesynonym for --only-obsolete --clear-obsoleteInput file syntax:-P, --properties-inputinput file is in Java .properties syntax--stringtable-inputinput file is in NeXTstep/GNUstep .strings syntaxOutput details:--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentwrite the .po file using indented style--no-locationdo not write '#: filename:line' lines-n, --add-locationgenerate '#: filename:line' lines (default)--strictwrite out strict Uniforum conforming .po file-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output-F, --sort-by-filesort output by file locationInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msgattrib - attribute matching and manipulation on messagecatalog",
        "section": 1
    },
    {
        "command": "msgcat",
        "description": "Concatenates and merges the specified PO files.Find messageswhich are common to two or more of the specified PO files.Byusing the --more-than option, greater commonality may berequested before messages are printed.Conversely, the--less-than option may be used to specify less commonality beforemessages are printed (i.e.--less-than=2 will only print theunique messages).Translations, comments, extracted comments,and file positions will be cumulated, except that if --use-firstis specified, they will be taken from the first PO file to definethem.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:INPUTFILE ...input files-f, --files-from=FILEget list of input files from FILE-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf input file is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Message selection:-<, --less-than=NUMBERprint messages with less than this many definitions,defaults to infinite if not set->, --more-than=NUMBERprint messages with more than this many definitions,defaults to 0 if not set-u, --uniqueshorthand for --less-than=2, requests that only uniquemessages be printedInput file syntax:-P, --properties-inputinput files are in Java .properties syntax--stringtable-inputinput files are in NeXTstep/GNUstep .strings syntaxOutput details:-t, --to-code=NAMEencoding for output--use-firstuse first available translation for each message, don'tmerge several translations--lang=CATALOGNAMEset 'Language' field in the header entry--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentwrite the .po file using indented style--no-locationdo not write '#: filename:line' lines-n, --add-locationgenerate '#: filename:line' lines (default)--strictwrite out strict Uniforum conforming .po file-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output-F, --sort-by-filesort output by file locationInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msgcat - combines several message catalogs",
        "section": 1
    },
    {
        "command": "msgcmp",
        "description": "Compare two Uniforum style .po files to check that both containthe same set of msgid strings.The def.po file is an existing POfile with the translations.The ref.pot file is the last createdPO file, or a PO Template file (generally created by xgettext).This is useful for checking that you have translated each andevery message in your program.Where an exact match cannot befound, fuzzy matching is used to produce better diagnostics.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:def.po translationsref.potreferences to the sources-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchOperation modifiers:-m, --multi-domainapply ref.pot to each of the domains in def.po-N, --no-fuzzy-matchingdo not use fuzzy matching--use-fuzzyconsider fuzzy entries--use-untranslatedconsider untranslated entriesInput file syntax:-P, --properties-inputinput files are in Java .properties syntax--stringtable-inputinput files are in NeXTstep/GNUstep .strings syntaxInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msgcmp - compare message catalog and template",
        "section": 1
    },
    {
        "command": "msgcomm",
        "description": "Find messages which are common to two or more of the specified POfiles.By using the --more-than option, greater commonality maybe requested before messages are printed.Conversely, the--less-than option may be used to specify less commonality beforemessages are printed (i.e.--less-than=2 will only print theunique messages).Translations, comments and extracted commentswill be preserved, but only from the first PO file to definethem.File positions from all PO files will be cumulated.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:INPUTFILE ...input files-f, --files-from=FILEget list of input files from FILE-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf input file is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Message selection:-<, --less-than=NUMBERprint messages with less than this many definitions,defaults to infinite if not set->, --more-than=NUMBERprint messages with more than this many definitions,defaults to 1 if not set-u, --uniqueshorthand for --less-than=2, requests that only uniquemessages be printedInput file syntax:-P, --properties-inputinput files are in Java .properties syntax--stringtable-inputinput files are in NeXTstep/GNUstep .strings syntaxOutput details:--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentwrite the .po file using indented style--no-locationdo not write '#: filename:line' lines-n, --add-locationgenerate '#: filename:line' lines (default)--strictwrite out strict Uniforum conforming .po file-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output-F, --sort-by-filesort output by file location--omit-headerdon't write header with 'msgid \"\"' entryInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msgcomm - match two message catalogs",
        "section": 1
    },
    {
        "command": "msgconv",
        "description": "Converts a translation catalog to a different character encoding.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:INPUTFILEinput PO file-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf no input file is given or if it is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Conversion target:-t, --to-code=NAMEencoding for outputThe default encoding is the current locale's encoding.Input file syntax:-P, --properties-inputinput file is in Java .properties syntax--stringtable-inputinput file is in NeXTstep/GNUstep .strings syntaxOutput details:--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentindented output style--no-locationsuppress '#: filename:line' lines-n, --add-locationpreserve '#: filename:line' lines (default)--strictstrict Uniforum output style-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output-F, --sort-by-filesort output by file locationInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msgconv - character set conversion for message catalog",
        "section": 1
    },
    {
        "command": "msgen",
        "description": "Creates an English translation catalog.The input file is thelast created English PO file, or a PO Template file (generallycreated by xgettext).Untranslated entries are assigned atranslation that is identical to the msgid.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:INPUTFILEinput PO or POT file-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf input file is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Input file syntax:-P, --properties-inputinput file is in Java .properties syntax--stringtable-inputinput file is in NeXTstep/GNUstep .strings syntaxOutput details:--lang=CATALOGNAMEset 'Language' field in the header entry--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentindented output style--no-locationsuppress '#: filename:line' lines-n, --add-locationpreserve '#: filename:line' lines (default)--strictstrict Uniforum output style-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output-F, --sort-by-filesort output by file locationInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msgen - create English message catalog",
        "section": 1
    },
    {
        "command": "msgexec",
        "description": "Applies a command to all translations of a translation catalog.The COMMAND can be any program that reads a translation fromstandard input.It is invoked once for each translation.Itsoutput becomes msgexec's output.msgexec's return code is themaximum return code across all invocations.A special builtin command called '0' outputs the translation,followed by a null byte.The output of \"msgexec 0\" is suitableas input for \"xargs -0\".Command input:--newlineadd newline at the end of inputMandatory arguments to long options are mandatory for shortoptions too.Input file location:-i, --input=INPUTFILEinput PO file-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf no input file is given or if it is -, standard input is read.Input file syntax:-P, --properties-inputinput file is in Java .properties syntax--stringtable-inputinput file is in NeXTstep/GNUstep .strings syntaxInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msgexec - process translations of message catalog",
        "section": 1
    },
    {
        "command": "msgfilter",
        "description": "Applies a filter to all translations of a translation catalog.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:-i, --input=INPUTFILEinput PO file-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf no input file is given or if it is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.The FILTER can be any program that reads a translation fromstandard input and writes a modified translation to standardoutput.Filter input and output:--newlineadd a newline at the end of input and remove a newlinefrom the end of outputUseful FILTER-OPTIONs when the FILTER is 'sed':-e, --expression=SCRIPTadd SCRIPT to the commands to be executed-f, --file=SCRIPTFILEadd the contents of SCRIPTFILE to the commands to beexecuted-n, --quiet, --silentsuppress automatic printing of pattern spaceInput file syntax:-P, --properties-inputinput file is in Java .properties syntax--stringtable-inputinput file is in NeXTstep/GNUstep .strings syntaxOutput details:--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color--no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty--indentindented output style--keep-headerkeep header entry unmodified, don't filter it--no-locationsuppress '#: filename:line' lines-n, --add-locationpreserve '#: filename:line' lines (default)--strictstrict Uniforum output style-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output-F, --sort-by-filesort output by file locationInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msgfilter - edit translations of message catalog",
        "section": 1
    },
    {
        "command": "msgfmt",
        "description": "Generate binary message catalog from textual translationdescription.Mandatory arguments to long options are mandatory for shortoptions too.Similarly for optional arguments.Input file location:filename.po ...input files-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf input file is -, standard input is read.Operation mode:-j, --javaJava mode: generate a Java ResourceBundle class--java2like --java, and assume Java2 (JDK 1.2 or higher)--csharpC# mode: generate a .NET .dll file--csharp-resourcesC# resources mode: generate a .NET .resources file--tclTcl mode: generate a tcl/msgcat .msg file--qtQt mode: generate a Qt .qm file--desktopDesktop Entry mode: generate a .desktop file--xmlXML mode: generate XML fileOutput file location:-o, --output-file=FILEwrite output to specified file--strictenable strict Uniforum modeIf output file is -, output is written to standard output.Output file location in Java mode:-r, --resource=RESOURCEresource name-l, --locale=LOCALElocale name, either language or language_COUNTRY--sourceproduce a .java file, instead of a .class file-d DIRECTORYbase directory of classes directory hierarchyThe class name is determined by appending the locale name to theresource name, separated with an underscore.The -d option ismandatory.The class is written under the specified directory.Output file location in C# mode:-r, --resource=RESOURCEresource name-l, --locale=LOCALElocale name, either language or language_COUNTRY-d DIRECTORYbase directory for locale dependent .dll filesThe -l and -d options are mandatory.The .dll file is written ina subdirectory of the specified directory whose name depends onthe locale.Output file location in Tcl mode:-l, --locale=LOCALElocale name, either language or language_COUNTRY-d DIRECTORYbase directory of .msg message catalogsThe -l and -d options are mandatory.The .msg file is written inthe specified directory.Desktop Entry mode options:-l, --locale=LOCALElocale name, either language or language_COUNTRY-o, --output-file=FILEwrite output to specified file--template=TEMPLATEa .desktop file used as a template-d DIRECTORYbase directory of .po files-kWORD, --keyword=WORDlook for WORD as an additional keyword-k, --keyworddo not to use default keywordsThe -l, -o, and --template options are mandatory.If -D isspecified, input files are read from the directory instead of thecommand line arguments.XML mode options:-l, --locale=LOCALElocale name, either language or language_COUNTRY-L, --language=NAMErecognise the specified XML language-o, --output-file=FILEwrite output to specified file--template=TEMPLATEan XML file used as a template-d DIRECTORYbase directory of .po filesThe -l, -o, and --template options are mandatory.If -D isspecified, input files are read from the directory instead of thecommand line arguments.Input file syntax:-P, --properties-inputinput files are in Java .properties syntax--stringtable-inputinput files are in NeXTstep/GNUstep .strings syntaxInput file interpretation:-c, --checkperform all the checks implied by --check-format,--check-header, --check-domain--check-formatcheck language dependent format strings--check-headerverify presence and contents of the header entry--check-domaincheck for conflicts between domain directives and the--output-file option-C, --check-compatibilitycheck that GNU msgfmt behaves like X/Open msgfmt--check-accelerators[=CHAR]check presence of keyboard accelerators for menu items-f, --use-fuzzyuse fuzzy entries in outputOutput details:--no-convertdon't convert the messages to UTF-8 encoding--no-redundancydon't pre-expand ISO C 99 <inttypes.h> format stringdirective macros-a, --alignment=NUMBERalign strings to NUMBER bytes (default: 1)--endianness=BYTEORDERwrite out 32-bit numbers in the given byte order (big orlittle, default depends on platform)--no-hashbinary file will not include the hash tableInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit--statisticsprint statistics about translations-v, --verboseincrease verbosity level",
        "name": "msgfmt - compile message catalog to binary format",
        "section": 1
    },
    {
        "command": "msggrep",
        "description": "Extracts all messages of a translation catalog that match a givenpattern or belong to some given source files.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:INPUTFILEinput PO file-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf no input file is given or if it is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Message selection:[-N SOURCEFILE]... [-M DOMAINNAME]...[-JMSGCTXT-PATTERN] [-K MSGID-PATTERN] [-T MSGSTR-PATTERN][-C COMMENT-PATTERN] [-X EXTRACTED-COMMENT-PATTERN]A message is selected if it comes from one of the specifiedsource files, or if it comes from one of the specified domains,or if -J is given and its context (msgctxt) matchesMSGCTXT-PATTERN, or if -K is given and its key (msgid ormsgid_plural) matches MSGID-PATTERN, or if -T is given and itstranslation (msgstr) matches MSGSTR-PATTERN, or if -C is givenand the translator's comment matches COMMENT-PATTERN, or if -X isgiven and the extracted comment matchesEXTRACTED-COMMENT-PATTERN.When more than one selection criterion is specified, the set ofselected messages is the union of the selected messages of eachcriterion.MSGCTXT-PATTERN or MSGID-PATTERN or MSGSTR-PATTERN orCOMMENT-PATTERN or EXTRACTED-COMMENT-PATTERN syntax:[-E | -F] [-e PATTERN | -f FILE]...PATTERNs are basic regular expressions by default, or extendedregular expressions if -E is given, or fixed strings if -F isgiven.-N, --location=SOURCEFILEselect messages extracted from SOURCEFILE-M, --domain=DOMAINNAMEselect messages belonging to domain DOMAINNAME-J, --msgctxtstart of patterns for the msgctxt-K, --msgidstart of patterns for the msgid-T, --msgstrstart of patterns for the msgstr-C, --commentstart of patterns for the translator's comment-X, --extracted-commentstart of patterns for the extracted comment-E, --extended-regexpPATTERN is an extended regular expression-F, --fixed-stringsPATTERN is a set of newline-separated strings-e, --regexp=PATTERNuse PATTERN as a regular expression-f, --file=FILEobtain PATTERN from FILE-i, --ignore-caseignore case distinctions-v, --invert-matchoutput only the messages that do not match any selectioncriterionInput file syntax:-P, --properties-inputinput file is in Java .properties syntax--stringtable-inputinput file is in NeXTstep/GNUstep .strings syntaxOutput details:--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color--no-escapedo not use C escapes in output (default)--escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty--indentindented output style--no-locationsuppress '#: filename:line' lines-n, --add-locationpreserve '#: filename:line' lines (default)--strictstrict Uniforum output style-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines--sort-outputgenerate sorted output--sort-by-filesort output by file locationInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msggrep - pattern matching on message catalog",
        "section": 1
    },
    {
        "command": "msginit",
        "description": "Creates a new PO file, initializing the meta information withvalues from the user's environment.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:-i, --input=INPUTFILEinput POT fileIf no input file is given, the current directory is searched forthe POT file.If it is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified PO fileIf no output file is given, it depends on the --locale option orthe user's locale setting.If it is -, the results are writtento standard output.Input file syntax:-P, --properties-inputinput file is in Java .properties syntax--stringtable-inputinput file is in NeXTstep/GNUstep .strings syntaxOutput details:-l, --locale=LL_CC[.ENCODING]set target locale--no-translatorassume the PO file is automatically generated--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several linesInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msginit - initialize a message catalog",
        "section": 1
    },
    {
        "command": "msgmerge",
        "description": "Merges two Uniforum style .po files together.The def.po file isan existing PO file with translations which will be taken over tothe newly created file as long as they still match; comments willbe preserved, but extracted comments and file positions will bediscarded.The ref.pot file is the last created PO file withup-to-date source references but old translations, or a POTemplate file (generally created by xgettext); any translationsor comments in the file will be discarded, however dot commentsand file positions will be preserved.Where an exact matchcannot be found, fuzzy matching is used to produce betterresults.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:def.po translations referring to old sourcesref.potreferences to new sources-D, --directory=DIRECTORYadd DIRECTORY to list for input files search-C, --compendium=FILEadditional library of message translations, may bespecified more than onceOperation mode:-U, --updateupdate def.po, do nothing if def.po already up to dateOutput file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Output file location in update mode: The result is written backto def.po.--backup=CONTROLmake a backup of def.po--suffix=SUFFIXoverride the usual backup suffixThe version control method may be selected via the --backupoption or through the VERSION_CONTROL environment variable.Hereare the values:none, offnever make backups (even if --backup is given)numbered, tmake numbered backupsexisting, nilnumbered if numbered backups exist, simple otherwisesimple, neveralways make simple backupsThe backup suffix is '~', unless set with --suffix or theSIMPLE_BACKUP_SUFFIX environment variable.Operation modifiers:-m, --multi-domainapply ref.pot to each of the domains in def.po--for-msgfmtproduce output for 'msgfmt', not for a translator-N, --no-fuzzy-matchingdo not use fuzzy matching--previouskeep previous msgids of translated messagesInput file syntax:-P, --properties-inputinput files are in Java .properties syntax--stringtable-inputinput files are in NeXTstep/GNUstep .strings syntaxOutput details:--lang=CATALOGNAMEset 'Language' field in the header entry--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentindented output style--no-locationsuppress '#: filename:line' lines-n, --add-locationpreserve '#: filename:line' lines (default)--strictstrict Uniforum output style-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output-F, --sort-by-filesort output by file locationInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit-v, --verboseincrease verbosity level-q, --quiet, --silentsuppress progress indicators",
        "name": "msgmerge - merge message catalog and template",
        "section": 1
    },
    {
        "command": "msgunfmt",
        "description": "Convert binary message catalog to Uniforum style .po file.Mandatory arguments to long options are mandatory for shortoptions too.Operation mode:-j, --javaJava mode: input is a Java ResourceBundle class--csharpC# mode: input is a .NET .dll file--csharp-resourcesC# resources mode: input is a .NET .resources file--tclTcl mode: input is a tcl/msgcat .msg fileInput file location:FILE ...input .mo filesIf no input file is given or if it is -, standard input is read.Input file location in Java mode:-r, --resource=RESOURCEresource name-l, --locale=LOCALElocale name, either language or language_COUNTRYThe class name is determined by appending the locale name to theresource name, separated with an underscore.The class islocated using the CLASSPATH.Input file location in C# mode:-r, --resource=RESOURCEresource name-l, --locale=LOCALElocale name, either language or language_COUNTRY-d DIRECTORYbase directory for locale dependent .dll filesThe -l and -d options are mandatory.The .dll file is located ina subdirectory of the specified directory whose name depends onthe locale.Input file location in Tcl mode:-l, --locale=LOCALElocale name, either language or language_COUNTRY-d DIRECTORYbase directory of .msg message catalogsThe -l and -d options are mandatory.The .msg file is located inthe specified directory.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Output details:--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentwrite indented output style--strictwrite strict uniforum style-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted outputInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit-v, --verboseincrease verbosity level",
        "name": "msgunfmt - uncompile message catalog from binary format",
        "section": 1
    },
    {
        "command": "msguniq",
        "description": "Unifies duplicate translations in a translation catalog.Findsduplicate translations of the same message ID.Such duplicatesare invalid input for other programs like msgfmt, msgmerge ormsgcat.By default, duplicates are merged together.When usingthe --repeated option, only duplicates are output, and all othermessages are discarded.Comments and extracted comments will becumulated, except that if --use-first is specified, they will betaken from the first translation.File positions will becumulated.When using the --unique option, duplicates arediscarded.Mandatory arguments to long options are mandatory for shortoptions too.Input file location:INPUTFILEinput PO file-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf no input file is given or if it is -, standard input is read.Output file location:-o, --output-file=FILEwrite output to specified fileThe results are written to standard output if no output file isspecified or if it is -.Message selection:-d, --repeatedprint only duplicates-u, --uniqueprint only unique messages, discard duplicatesInput file syntax:-P, --properties-inputinput file is in Java .properties syntax--stringtable-inputinput file is in NeXTstep/GNUstep .strings syntaxOutput details:-t, --to-code=NAMEencoding for output--use-firstuse first available translation for each message, don'tmerge several translations--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentwrite the .po file using indented style--no-locationdo not write '#: filename:line' lines-n, --add-locationgenerate '#: filename:line' lines (default)--strictwrite out strict Uniforum conforming .po file-p, --properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output-F, --sort-by-filesort output by file locationInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "msguniq - unify duplicate translations in message catalog",
        "section": 1
    },
    {
        "command": "msql2mysql",
        "description": "Initially, the MySQL C API was developed to be very similar tothat for the mSQL database system. Because of this, mSQL programsoften can be converted relatively easily for use with MySQL bychanging the names of the C API functions.The msql2mysql utility performs the conversion of mSQL C APIfunction calls to their MySQL equivalents.msql2mysql convertsthe input file in place, so make a copy of the original beforeconverting it. For example, use msql2mysql like this:shell> cp client-prog.c client-prog.c.origshell> msql2mysql client-prog.cclient-prog.c convertedThen examine client-prog.c and make any post-conversion revisionsthat may be necessary.msql2mysql uses the replace utility to make the function namesubstitutions. See replace(1).",
        "name": "msql2mysql - convert mSQL programs for use with MySQL",
        "section": 1
    },
    {
        "command": "mtrace",
        "description": "mtrace is a Perl script used to interpret and provide humanreadable output of the trace log contained in the filemtracedata, whose contents were produced by mtrace(3).If binaryis provided, the output of mtrace also contains the source filename with line number information for problem locations (assumingthat binary was compiled with debugging information).For more information about the mtrace(3) function and mtracescript usage, see mtrace(3).",
        "name": "mtrace - interpret the malloc trace log",
        "section": 1
    },
    {
        "command": "mv",
        "description": "Rename SOURCE to DEST, or move SOURCE(s) to DIRECTORY.Mandatory arguments to long options are mandatory for shortoptions too.--backup[=CONTROL]make a backup of each existing destination file-blike --backup but does not accept an argument--debugexplain how a file is copied.Implies -v-f, --forcedo not prompt before overwriting-i, --interactiveprompt before overwrite-n, --no-clobberdo not overwrite an existing fileIf you specify more than one of -i, -f, -n, only the final onetakes effect.--no-copydo not copy if renaming fails--strip-trailing-slashesremove any trailing slashes from each SOURCE argument-S, --suffix=SUFFIXoverride the usual backup suffix-t, --target-directory=DIRECTORYmove all SOURCE arguments into DIRECTORY-T, --no-target-directorytreat DEST as a normal file--update[=UPDATE]control which existing files are updated;UPDATE={all,none,older(default)}.See below-uequivalent to --update[=older]-v, --verboseexplain what is being done-Z, --contextset SELinux security context of destination file todefault type--help display this help and exit--versionoutput version information and exitUPDATE controls which existing files in the destination arereplaced.'all' is the default operation when an --update optionis not specified, and results in all existing files in thedestination being replaced.'none' is similar to the--no-clobber option, in that no files in the destination arereplaced, but also skipped files do not induce a failure.'older' is the default operation when --update is specified, andresults in files being replaced if they're older than thecorresponding source file.The backup suffix is '~', unless set with --suffix orSIMPLE_BACKUP_SUFFIX.The version control method may be selectedvia the --backup option or through the VERSION_CONTROLenvironment variable.Here are the values:none, offnever make backups (even if --backup is given)numbered, tmake numbered backupsexisting, nilnumbered if numbered backups exist, simple otherwisesimple, neveralways make simple backups",
        "name": "mv - move (rename) files",
        "section": 1
    },
    {
        "command": "my_print_defaults",
        "description": "my_print_defaults displays the options that are present in optiongroups of option files. The output indicates what options will beused by programs that read the specified option groups. Forexample, the mysqlcheck program reads the [mysqlcheck] and[client] option groups. To see what options are present in thosegroups in the standard option files, invoke my_print_defaultslike this:shell> my_print_defaults mysqlcheck client--user=myusername--password=secret--host=localhostThe output consists of options, one per line, in the form thatthey would be specified on the command line.my_print_defaults supports the following options.\u2022--help, -?Display a help message and exit.\u2022--defaults-file=file_name, -c file_nameRead only the given option file. If no extension is given,default extension(.ini or .cnf) will be used. If--defaults-file is the first option, then read this fileonly, do not read global or per-user config files; should bethe first option.\u2022--debug=debug_options, -# debug_optionsWrite a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is\u00b4d:t:o,/tmp/my_print_defaults.trace\u00b4.\u2022--defaults-extra-file=file_name, -e file_nameRead this option file after the global option file but (onUnix) before the user option file. Should be the firstoption.\u2022--defaults-group-suffix=suffix, -g suffixIn addition to the groups named on the command line, readgroups that have the given suffix.\u2022--mysqldRead the same set of groups that the mysqld binary does.\u2022--no-defaults, -nReturn an empty string (useful for scripts).\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.\u2022--version, -VDisplay version information and exit.",
        "name": "my_print_defaults - display options from option files",
        "section": 1
    },
    {
        "command": "my_safe_process",
        "description": "Use: safe_process [options to safe_process] -- progname arg1 ...argn.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "my_safe_process - Utility program that encapsulates processcreation, monitoring and bulletproof process cleanup",
        "section": 1
    },
    {
        "command": "myisam_ftdump",
        "description": "myisam_ftdump displays information about FULLTEXT indexes inMyISAM tables. It reads the MyISAM index file directly, so itmust be run on the server host where the table is located. Beforeusing myisam_ftdump, be sure to issue a FLUSH TABLES statementfirst if the server is running.myisam_ftdump scans and dumps the entire index, which is notparticularly fast. On the other hand, the distribution of wordschanges infrequently, so it need not be run often.Invoke myisam_ftdump like this:shell> myisam_ftdump [options] tbl_name index_numThe tbl_name argument should be the name of a MyISAM table. Youcan also specify a table by naming its index file (the file withthe .MYI suffix). If you do not invoke myisam_ftdump in thedirectory where the table files are located, the table or indexfile name must be preceded by the path name to the table\u00b4sdatabase directory. Index numbers begin with 0.Example: Suppose that the test database contains a table namedmytexttablel that has the following definition:CREATE TABLE mytexttable(idINT NOT NULL,txtTEXT NOT NULL,PRIMARY KEY (id),FULLTEXT (txt));The index on id is index 0 and the FULLTEXT index on txt is index1. If your working directory is the test database directory,invoke myisam_ftdump as follows:shell> myisam_ftdump mytexttable 1If the path name to the test database directory is/usr/local/mysql/data/test, you can also specify the table nameargument using that path name. This is useful if you do notinvoke myisam_ftdump in the database directory:shell> myisam_ftdump /usr/local/mysql/data/test/mytexttable 1You can use myisam_ftdump to generate a list of index entries inorder of frequency of occurrence like this:shell> myisam_ftdump -c mytexttable 1 | sort -rmyisam_ftdump supports the following options:\u2022--help, -h -?Display a help message and exit.\u2022--count, -cCalculate per-word statistics (counts and global weights).\u2022--dump, -dDump the index, including data offsets and word weights.\u2022--length, -lReport the length distribution.\u2022--stats, -sReport global index statistics. This is the default operationif no other operation is specified.\u2022--verbose, -vVerbose mode. Print more output about what the program does.",
        "name": "myisam_ftdump - display full-text index information",
        "section": 1
    },
    {
        "command": "myisamchk",
        "description": "The myisamchk utility gets information about your database tablesor checks, repairs, or optimizes them.myisamchk works withMyISAM tables (tables that have .MYD and .MYI files for storingdata and indexes).The use of myisamchk with partitioned tables is not supported.CautionIt is best to make a backup of a table before performing atable repair operation; under some circumstances theoperation might cause data loss. Possible causes include butare not limited to file system errors.Invoke myisamchk like this:shell> myisamchk [options] tbl_name ...The options specify what you want myisamchk to do. They aredescribed in the following sections. You can also get a list ofoptions by invoking myisamchk --help.With no options, myisamchk simply checks your table as thedefault operation. To get more information or to tell myisamchkto take corrective action, specify options as described in thefollowing discussion.tbl_name is the database table you want to check or repair. Ifyou run myisamchk somewhere other than in the database directory,you must specify the path to the database directory, becausemyisamchk has no idea where the database is located. In fact,myisamchk does not actually care whether the files you areworking on are located in a database directory. You can copy thefiles that correspond to a database table into some otherlocation and perform recovery operations on them there.You can name several tables on the myisamchk command line if youwish. You can also specify a table by naming its index file (thefile with the .MYI suffix). This allows you to specify all tablesin a directory by using the pattern *.MYI. For example, if youare in a database directory, you can check all the MyISAM tablesin that directory like this:shell> myisamchk *.MYIIf you are not in the database directory, you can check all thetables there by specifying the path to the directory:shell> myisamchk /path/to/database_dir/*.MYIYou can even check all tables in all databases by specifying awildcard with the path to the MariaDB data directory:shell> myisamchk /path/to/datadir/*/*.MYIThe recommended way to quickly check all MyISAM tables is:shell> myisamchk --silent --fast /path/to/datadir/*/*.MYIIf you want to check all MyISAM tables and repair any that arecorrupted, you can use the following command:shell> myisamchk --silent --force --fast --update-state \\--key_buffer_size=64M --sort_buffer_size=64M \\--read_buffer_size=1M --write_buffer_size=1M \\/path/to/datadir/*/*.MYIThis command assumes that you have more than 64MB free. For moreinformation about memory allocation with myisamchk, see thesection called \u201cMYISAMCHK MEMORY USAGE\u201d.ImportantYou must ensure that no other program is using the tableswhile you are running myisamchk. The most effective means ofdoing so is to shut down the MariaDB server while runningmyisamchk, or to lock all tables that myisamchk is being usedon.Otherwise, when you run myisamchk, it may display thefollowing error message:warning: clients are using or haven\u00b4t closed the table properlyThis means that you are trying to check a table that has beenupdated by another program (such as the mysqld server) thathasn\u00b4t yet closed the file or that has died without closingthe file properly, which can sometimes lead to the corruptionof one or more MyISAM tables.If mysqld is running, you must force it to flush any tablemodifications that are still buffered in memory by usingFLUSH TABLES. You should then ensure that no one is using thetables while you are running myisamchkHowever, the easiest way to avoid this problem is to useCHECK TABLE instead of myisamchk to check tables.myisamchk supports the following options, which can be specifiedon the command line or in the [myisamchk] option file group.",
        "name": "myisamchk - MyISAM table-maintenance utility",
        "section": 1
    },
    {
        "command": "myisamlog",
        "description": "myisamlog processes the contents of a MyISAM log file.Invoke myisamlog like this:shell> myisamlog [options] [log_file [tbl_name] ...]shell> isamlog [options] [log_file [tbl_name] ...]The default operation is update (-u). If a recovery is done (-r),all writes and possibly updates and deletes are done and errorsare only counted. The default log file name is myisam.log formyisamlog and isam.log for isamlog if no log_file argument isgiven. If tables are named on the command line, only those tablesare updated.myisamlog supports the following options:\u2022-?, -IDisplay a help message and exit.\u2022-c NExecute only N commands.\u2022-f NSpecify the maximum number of open files.\u2022-iDisplay extra information before exiting.\u2022-o offsetSpecify the starting offset.\u2022-p NRemove N components from path.\u2022-rPerform a recovery operation.\u2022-R record_pos_file record_posSpecify record position file and record position.\u2022-uPerform an update operation.\u2022-vVerbose mode. Print more output about what the program does.This option can be given multiple times to produce more andmore output.\u2022-w write_fileSpecify the write file.\u2022-VDisplay version information.",
        "name": "myisamlog - display MyISAM log file contents",
        "section": 1
    },
    {
        "command": "myisampack",
        "description": "The myisampack utility compresses MyISAM tables.myisampackworks by compressing each column in the table separately.Usually, myisampack packs the data file 40%\u201370%.When the table is used later, the server reads into memory theinformation needed to decompress columns. This results in muchbetter performance when accessing individual rows, because youonly have to uncompress exactly one row.MySQL uses mmap() when possible to perform memory mapping oncompressed tables. If mmap() does not work, MySQL falls back tonormal read/write file operations.Please note the following:\u2022If the mysqld server was invoked with external lockingdisabled, it is not a good idea to invoke myisampack if thetable might be updated by the server during the packingprocess. It is safest to compress tables with the serverstopped.\u2022After packing a table, it becomes read only. This isgenerally intended (such as when accessing packed tables on aCD).Invoke myisampack like this:shell> myisampack [options] file_name ...Each file name argument should be the name of an index (.MYI)file. If you are not in the database directory, you shouldspecify the path name to the file. It is permissible to omit the.MYI extension.After you compress a table with myisampack, you should usemyisamchk -rq to rebuild its indexes.myisamchk(1).myisampack supports the following options. It also reads optionfiles and supports the options for processing them described atSection 4.2.3.3.1, \u201cCommand-Line Options that Affect Option-FileHandling\u201d.\u2022--help, -?Display a help message and exit.\u2022--backup, -bMake a backup of each table\u00b4s data file using the nametbl_name.OLD.\u2022--character-sets-dir=pathThe directory where character sets are installed. SeeSection 9.5, \u201cCharacter Set Configuration\u201d.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is \u00b4d:t:o\u00b4.\u2022--force, -fProduce a packed table even if it becomes larger than theoriginal or if the intermediate file from an earlierinvocation of myisampack exists. (myisampack creates anintermediate file named tbl_name.TMD in the databasedirectory while it compresses the table. If you killmyisampack, the .TMD file might not be deleted.) Normally,myisampack exits with an error if it finds that tbl_name.TMDexists. With --force, myisampack packs the table anyway.\u2022--join=big_tbl_name, -j big_tbl_nameJoin all tables named on the command line into a singlepacked table big_tbl_name. All tables that are to be combinedmust have identical structure (same column names and types,same indexes, and so forth).big_tbl_name must not exist prior to the join operation. Allsource tables named on the command line to be merged intobig_tbl_name must exist. The source tables are read for thejoin operation but not modified. The join operation does notcreate a .frm file for big_tbl_name, so after the joinoperation finishes, copy the .frm file from one of the sourcetables and name it big_tbl_name.frm.\u2022--silent, -sSilent mode. Write output only when errors occur.\u2022--test, -tDo not actually pack the table, just test packing it.\u2022--tmpdir=path, -T pathUse the named directory as the location where myisampackcreates temporary files.\u2022--verbose, -vVerbose mode. Write information about the progress of thepacking operation and its result.\u2022--version, -VDisplay version information and exit.\u2022--wait, -wWait and retry if the table is in use. If the mysqld serverwas invoked with external locking disabled, it is not a goodidea to invoke myisampack if the table might be updated bythe server during the packing process.The following sequence of commands illustrates a typical tablecompression session:shell> ls -l station.*-rw-rw-r--1 montymy994128 Apr 17 19:00 station.MYD-rw-rw-r--1 montymy53248 Apr 17 19:00 station.MYI-rw-rw-r--1 montymy5767 Apr 17 19:00 station.frmshell> myisamchk -dvv stationMyISAM file:stationIsam-version:2Creation time: 1996-03-13 10:08:58Recover time:1997-02-023:06:43Data records:1192Deleted blocks:0Datafile parts:1192Deleted data:0Datafile pointer (bytes):2Keyfile pointer (bytes):2Max datafile length:54657023Max keyfile length:33554431Recordlength:834Record format: Fixed lengthtable description:Key Start Len IndexTypeRootBlocksizeRec/key124uniqueunsigned long10241024123230multip. text1024010241Field Start Length Type1112243644101511206311732308623599735101323511167412171161318735142224152261616242201726220182822019302302033242133642234012334182434982535782636522736722836942937343037713137823238083338843439243539643640043740413840543940944041344141744242144342544442920454493046479147480148481794956079506397951718795279785380515480615580720568274578314shell> myisampack station.MYICompressing station.MYI: (1192 records)- Calculating statisticsnormal:20empty-space:16empty-zero:12empty-fill:11pre-space:0end-space:12table-lookups:5zero:7Original trees:57After join: 17- Compressing file87.14%Remember to run myisamchk -rq on compressed tablesshell> ls -l station.*-rw-rw-r--1 montymy127874 Apr 17 19:00 station.MYD-rw-rw-r--1 montymy55296 Apr 17 19:04 station.MYI-rw-rw-r--1 montymy5767 Apr 17 19:00 station.frmshell> myisamchk -dvv stationMyISAM file:stationIsam-version:2Creation time: 1996-03-13 10:08:58Recover time:1997-04-17 19:04:26Data records:1192Deleted blocks:0Datafile parts:1192Deleted data:0Datafile pointer (bytes):3Keyfile pointer (bytes):1Max datafile length:16777215Max keyfile length:131071Recordlength:834Record format: Compressedtable description:Key Start Len IndexTypeRootBlocksizeRec/key124uniqueunsigned long102401024123230multip. text5427210241Field Start Length TypeHuff treeBits111constant10224zerofill(1)29364no zeros, zerofill(1)2941013951120table-lookup4063113973230no endspace, not_always5986235no endspace, not_always, no empty6999735no empty791013235no endspace, not_always, no empty69111674zerofill(1)291217116no endspace, not_always, no empty591318735no endspace, not_always, no empty69142224zerofill(1)291522616no endspace, not_always, no empty591624220no endspace, not_always891726220no endspace, no empty891828220no endspace, no empty591930230no endspace, no empty69203324always zero29213364always zero2922340139233418table-lookup90243498table-lookup100253578always zero2926365229273672no zeros, zerofill(1)29283694no zeros, zerofill(1)29293734table-lookup11030377139313782no zeros, zerofill(1)29323808no zeros29333884always zero29343924table-lookup120353964no zeros, zerofill(1)139364004no zeros, zerofill(1)2937404129384054no zeros29394094always zero29404134no zeros29414174always zero29424214no zeros29434254always zero294442920no empty394544930no empty394647911444748011444848179no endspace, no empty1594956079no empty295063979no empty295171879no endspace169527978no empty29538051171548061395580720no empty39568274no zeros, zerofill(2)29578314no zeros, zerofill(1)29myisampack displays the following kinds of information:\u2022normalThe number of columns for which no extra packing is used.\u2022empty-spaceThe number of columns containing values that are only spaces.These occupy one bit.\u2022empty-zeroThe number of columns containing values that are only binaryzeros. These occupy one bit.\u2022empty-fillThe number of integer columns that do not occupy the fullbyte range of their type. These are changed to a smallertype. For example, a BIGINT column (eight bytes) can bestored as a TINYINT column (one byte) if all its values arein the range from -128 to 127.\u2022pre-spaceThe number of decimal columns that are stored with leadingspaces. In this case, each value contains a count for thenumber of leading spaces.\u2022end-spaceThe number of columns that have a lot of trailing spaces. Inthis case, each value contains a count for the number oftrailing spaces.\u2022table-lookupThe column had only a small number of different values, whichwere converted to an ENUM before Huffman compression.\u2022zeroThe number of columns for which all values are zero.\u2022Original treesThe initial number of Huffman trees.\u2022After joinThe number of distinct Huffman trees left after joining treesto save some header space.After a table has been compressed, the Field lines displayed bymyisamchk -dvv include additional information about each column:\u2022TypeThe data type. The value may contain any of the followingdescriptors:\u2022constantAll rows have the same value.\u2022no endspaceDo not store endspace.\u2022no endspace, not_alwaysDo not store endspace and do not do endspace compressionfor all values.\u2022no endspace, no emptyDo not store endspace. Do not store empty values.\u2022table-lookupThe column was converted to an ENUM.\u2022zerofill(N)The most significant N bytes in the value are always 0and are not stored.\u2022no zerosDo not store zeros.\u2022always zeroZero values are stored using one bit.\u2022Huff treeThe number of the Huffman tree associated with the column.\u2022BitsThe number of bits used in the Huffman tree.After you run myisampack, you must run myisamchk to re-create anyindexes. At this time, you can also sort the index blocks andcreate statistics needed for the MySQL optimizer to work moreefficiently:shell> myisamchk -rq --sort-index --analyze tbl_name.MYIAfter you have installed the packed table into the MySQL databasedirectory, you should execute mysqladmin flush-tables to forcemysqld to start using the new table.To unpack a packed table, use the --unpack option to myisamchk.",
        "name": "myisampack - generate compressed, read-only MyISAM tables",
        "section": 1
    },
    {
        "command": "myrocks_hotbackup",
        "description": "Usage:Backup: set -o pipefail; myrocks_hotbackup --user=root--password=pw --port=3306 --checkpoint_dir=<directory wheretemporary backup hard links are created> | ssh -o NoneEnabled=yesremote_server 'tar -xi -C <directory on remote server wherebackups will be sent>' . You need to execute backup command on aserver where you take backups.Backup using WDT: myrocks_hotbackup --user=root --password=pw--stream=wdt --checkpoint_dir=<directory where temporary backuphard links are created> --destination=<remote host name>--backup_dir=<remote directory name>. This has to be executed atthe src host.Move-Back: myrocks_hotbackup --move_back --datadir=<dest mysqldatadir> --rocksdb_datadir=<dest rocksdb datadir>--rocksdb_waldir=<dest rocksdb wal dir> --backup_dir=<wherebackup files are stored> . You need to execute move-back commandon a server where backup files are sent.",
        "name": "myrocks_hotbackup - streaming backup for MariaDB MyRocks",
        "section": 1
    },
    {
        "command": "mysql",
        "description": "mysql is a simple SQL shell (with GNU readline capabilities). Itsupports interactive and non-interactive use. When usedinteractively, query results are presented in an ASCII-tableformat. When used non-interactively (for example, as a filter),the result is presented in tab-separated format. The outputformat can be changed using command options.If you have problems due to insufficient memory for large resultsets, use the --quick option. This forces mysql to retrieveresults from the server a row at a time rather than retrievingthe entire result set and buffering it in memory beforedisplaying it. This is done by returning the result set using themysql_use_result() C API function in the client/server libraryrather than mysql_store_result().Using mysql is very easy. Invoke it from the prompt of yourcommand interpreter as follows:shell> mysql db_nameOr:shell> mysql --user=user_name --password=your_password db_nameThen type an SQL statement, end it with \u201c;\u201d, \\g, or \\G and pressEnter.Typing Control-C causes mysql to attempt to kill the currentstatement. If this cannot be done, or Control-C is typed againbefore the statement is killed, mysql exits.You can execute SQL statements in a script file (batch file) likethis:shell> mysql db_name < script.sql > output.tab",
        "name": "mariadb - the MariaDB command-line tool (mysql is now a symlinkto mariadb)",
        "section": 1
    },
    {
        "command": "mysql-stress-test.pl",
        "description": "The mysql-stress-test.pl Perl script performs stress-testing ofthe MariaDB server.mysql-stress-test.pl requires a version of Perl that has beenbuilt with threads support.Invoke mysql-stress-test.pl like this:shell> mysql-stress-test.pl [options]mysql-stress-test.pl supports the following options:\u2022--helpDisplay a help message and exit.\u2022--abort-on-error=NCauses the program to abort if an error with severity lessthan or equal to N was encountered. Set to 1 to abort on anyerror.\u2022--check-tests-filePeriodically check the file that lists the tests to be run.If it has been modified, reread the file. This can be usefulif you update the list of tests to be run during a stresstest.\u2022--cleanupForce cleanup of the working directory.\u2022--log-error-detailsLog error details in the global error log file.\u2022--loop-count=NIn sequential test mode, the number of loops to executebefore exiting.\u2022--mysqltest=pathThe path name to the mysqltest program.\u2022--server-database=db_nameThe database to use for the tests. The default is test.\u2022--server-host=host_nameThe host name of the local host to use for making a TCP/IPconnection to the local server. By default, the connection ismade to localhost using a Unix socket file.\u2022--server-logs-dir=pathThis option is required.path is the directory where allclient session logs will be stored. Usually this is theshared directory that is associated with the server used fortesting.\u2022--server-password=passwordThe password to use when connecting to the server.\u2022--server-port=port_numThe TCP/IP port number to use for connecting to the server.The default is 3306.\u2022--server-socket=file_nameFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use. Thedefault is /tmp/mysql.sock.\u2022--server-user=user_nameThe MariaDB user name to use when connecting to the server.The default is root.\u2022--sleep-time=NThe delay in seconds between test executions.\u2022--stress-basedir=pathThis option is required.path is the working directory forthe test run. It is used as the temporary location for resulttracking during testing.\u2022--stress-datadir=pathThe directory of data files to be used during testing. Thedefault location is the data directory under the locationgiven by the --stress-suite-basedir option.\u2022--stress-init-file[=path]file_name is the location of the file that contains the listof tests to be run once to initialize the database for thetesting. If missing, the default file is stress_init.txt inthe test suite directory.\u2022--stress-mode=modeThis option indicates the test order in stress-test mode. Themode value is either random to select tests in random orderor seq to run tests in each thread in the order specified inthe test list file. The default mode is random.\u2022--stress-suite-basedir=pathThis option is required.path is the directory that has thet and r subdirectories containing the test case and resultfiles. This directory is also the default location of thestress-test.txt file that contains the list of tests. (Adifferent location can be specified with the--stress-tests-file option.)\u2022--stress-tests-file[=file_name]Use this option to run the stress tests.file_name is thelocation of the file that contains the list of tests. Iffile_name is omitted, the default file is stress-test.txt inthe stress suite directory. (See --stress-suite-basedir.)\u2022--suite=suite_nameRun the named test suite. The default name is main (theregular test suite located in the mysql-test directory).\u2022--test-count=NThe number of tests to execute before exiting.\u2022--test-duration=NThe duration of stress testing in seconds.\u2022--threads=NThe number of threads. The default is 1.\u2022--verboseVerbose mode. Print more information about what the programdoes.",
        "name": "mysql-stress-test.pl - server stress test program",
        "section": 1
    },
    {
        "command": "mysql-test-run.pl",
        "description": "The mysql-test-run.pl Perl script is the main application used torun the MariaDB test suite. It invokes mysqltest to runindividual test cases.Invoke mysql-test-run.pl in the mysql-test directory like this:shell> mysql-test-run.pl [options] [test_name] ...Each test_name argument names a test case. The test case filethat corresponds to the test name is t/test_name.test.For each test_name argument, mysql-test-run.pl runs the namedtest case. With no test_name arguments, mysql-test-run.pl runsall .test files in the t subdirectory.If no suffix is given for the test name, a suffix of .test isassumed. Any leading path name is ignored. These commands areequivalent:shell> mysql-test-run.pl mytestshell> mysql-test-run.pl mytest.testshell> mysql-test-run.pl t/mytest.testA suite name can be given as part of the test name. That is, thesyntax for naming a test is:[suite_name.]test_name[.suffix]If a suite name is given, mysql-test-run.pl looks in that suitefor the test. The test file corresponding to a test namedsuite_name.test_name is found insuite/suite_name/t/test_name.test. There is also an implicitsuite name main for the tests in the top t directory. With nosuite name, mysql-test-run.pl looks in the default list of suitesfor a match and runs the test in any suites where it finds thetest. Suppose that the default suite list is main, binlog, rpl,and that a test mytest.test exists in the main and rpl suites.With an argument of mytest or mytest.test, mysql-test-run.pl willrun mytest.test from the main and rpl suites.To run a family of test cases for which the names share a commonprefix, use the --do-test=prefix option. For example,--do-test=rpl runs the replication tests (test cases that havenames beginning with rpl).--skip-test has the opposite effectof skipping test cases for which the names share a common prefix.The argument for the --do-test and --skip-test options alsoallows more flexible specification of which tests to perform orskip. If the argument contains a pattern metacharacter other thana lone period, it is interpreted as a Perl regular expression andapplies to test names that match the pattern. If the argumentcontains a lone period or does not contain any patternmetacharacters, it is interpreted the same way as previously andmatches test names that begin with the argument value. Forexample, --do-test=testa matches tests that begin with testa,--do-test=main.testa matches tests in the main test suite thatbegin with testa, and --do-test=main.*testa matches test namesthat contain main followed by testa with anything in between. Inthe latter case, the pattern match is not anchored to thebeginning of the test name, so it also matches names such asxmainytesta.To perform setup prior to running tests, mysql-test-run.pl needsto invoke mysqld with the --bootstrap and --skip-grant-tablesoptions. If MySQL was configured with the --disable-grant-optionsoption, --bootstrap, --skip-grant-tables, and --init-file will bedisabled. To handle this, set the MYSQLD_BOOTSTRAP environmentvariable to the full path name of a server that has all optionsenabled.mysql-test-run.pl will use that server to performsetup; it is not used to run the tests.The init_file test will fail if --init-file is disabled. This isan expected failure that can be handled as follows:shell> export MYSQLD_BOOTSTRAPshell> MYSQLD_BOOTSTRAP=/full/path/to/mysqldshell> make test force=\"--skip-test=init_file\"To run mysql-test-run.pl on Windows, you\u00b4ll need either Cygwin orActiveState Perl to run it. You may also need to install themodules required by the script. To run the test script, changelocation into the mysql-test directory, set the MTR_VS_CONFIGenvironment variable to the configuration you selected earlier(or use the --vs-config option), and invoke mysql-test-run.pl.For example (using Cygwin and the bash shell):shell> cd mysql-testshell> export MTR_VS_CONFIG=debugshell> ./mysqltest-run.pl --force --timershell> ./mysqltest-run.pl --force --timer --ps-protocolmysql-test-run.pl uses several environment variables. Some ofthem are listed in the following table. Some of these are setfrom the outside and used by mysql-test-run.pl, others are set bymysql-test-run.pl instead, and may be referred to in tests.\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502Variable\u2502 Meaning\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MTR_VERSION\u2502 If set to 1, will run\u2502\u2502\u2502 the older version 1 of\u2502\u2502\u2502 mysql-test-run.pl. This\u2502\u2502\u2502 will affect what\u2502\u2502\u2502 functionailty is\u2502\u2502\u2502 available and what\u2502\u2502\u2502 command line options are\u2502\u2502\u2502 supported.\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MTR_MEM\u2502 If set to anything, will\u2502\u2502\u2502 run tests with files in\u2502\u2502\u2502 \"memory\" using tmpfs or\u2502\u2502\u2502ramdisk.\u2502\u2502\u2502 Not available on\u2502\u2502\u2502 Windows. Same as\u2502\u2502\u2502--mem\u2502\u2502\u2502 option\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MTR_PARALLEL\u2502 If set, defines number\u2502\u2502\u2502 of parallel threads\u2502\u2502\u2502 executing tests. Same as\u2502\u2502\u2502--parallel \u2502\u2502\u2502 option\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MTR_BUILD_THREAD \u2502 If set, defines which port \u2502\u2502\u2502 number range is used for\u2502\u2502\u2502 the server\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MTR_PORT_BASE\u2502 If set, defines which port \u2502\u2502\u2502 number range is used for\u2502\u2502\u2502 the server\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MTR_NAME_TIMEOUT \u2502 Setting of a timeout in\u2502\u2502\u2502 minutes or seconds,\u2502\u2502\u2502 corresponding to command\u2502\u2502\u2502 line option\u2502\u2502\u2502 --name-timeout. Available\u2502\u2502\u2502 timeout names are\u2502\u2502\u2502 TESTCASE, SUITE (both in\u2502\u2502\u2502 minutes) and START,\u2502\u2502\u2502 SHUTDOWN (both in\u2502\u2502\u2502 seconds). These variables\u2502\u2502\u2502 are supported from MySQL\u2502\u2502\u2502 5.1.44.\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MYSQL_TEST\u2502 Path name to mysqltest\u2502\u2502\u2502 binary\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MYSQLD_BOOTSTRAP \u2502 Full path name to mysqld\u2502\u2502\u2502 that has all options\u2502\u2502\u2502 enabled\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MYSQLTEST_VARDIR \u2502 Path name to the var\u2502\u2502\u2502 directory that is used for \u2502\u2502\u2502logs,\u2502\u2502\u2502 temporary files, and so\u2502\u2502\u2502 forth\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MYSQL_TEST_DIR\u2502 Full path to the\u2502\u2502\u2502 mysql-test directory where \u2502\u2502\u2502 tests\u2502\u2502\u2502are being\u2502\u2502\u2502 run from\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502MYSQL_TMP_DIR\u2502 Path to temp directory\u2502\u2502\u2502 used for temporary files\u2502\u2502\u2502 during tests\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518The variable MTR_PORT_BASE was added in MySQL 5.1.45 as a morelogical replacement for MTR_BUILD_THREAD. It gives the actualport number directly (will be rounded down to a multiple of 10).If you use MTR_BUILD_THREAD, the port number is found bymultiplying this by 10 and adding 10000.Tests sometimes rely on certain environment variables beingdefined. For example, certain tests assume that MYSQL_TEST isdefined so that mysqltest can invoke itself with exec$MYSQL_TEST.Other tests may refer to the last three variables listed in thepreceding table, to locate files to read or write. For example,tests that need to create files will typically put them in$MYSQL_TMP_DIR/file_name.If you are running mysql-test-run.pl version 1 by settingMTR_VERSION, note that this only affects the test driver, not thetest client (and its language) or the tests themselves.A few tests might not run with version 1 because they depend onsome feature of version 2. You may have those tests skipped byadding the test name to the file lib/v1/incompatible.tests. Thisfeature is available from MySQL 5.1.40.mysql-test-run.pl supports the options in the following list. Anargument of -- tells mysql-test-run.pl not to process anyfollowing arguments as options.\u2022--help, -hDisplay a help message and exit.\u2022--big-testAllow tests marked as \"big\" to run. Tests can be thus markedby including the line --source include/big_test.inc, and theywill only be run if this option is given, or if theenvironment variable BIG_TEST is set to 1. Repeat this optiontwice to run only \"big\" tests.This is typically used for tests that take a very long torun, or that use many resources, so that they are notsuitable for running as part of a normal test suite run.\u2022--boot-dbxRun the mysqld server used for bootstrapping the databasethrough the dbx debugger.\u2022--boot-dddRun the mysqld server used for bootstrapping the databasethrough the ddd debugger.\u2022--boot-gdbRun the mysqld server used for bootstrapping the databasethrough the gdb debugger.\u2022--[mtr-]build-thread=numberSpecify a number to calculate port numbers from. The formulais 10 * build_thread + 10000. Instead of a number, it can beset to auto, which is also the default value, in which casemysql-test-run.pl will allocate a number unique to this host.The value (number or auto) can also be set with theMTR_BUILD_THREAD environment variable.The more logical --port-base is supported as an alternative.\u2022--callgrindInstructs valgrind to use callgrind.\u2022--check-testcasesCheck test cases for side effects. This is done by checkingsystem state before and after each test case; if there is anydifference, a warning to that effect will be written, but thetest case will not be marked as failed because of it. Thischeck is enabled by default.\u2022--client-bindir=pathThe path to the directory where client binaries are located.\u2022--client-dbxStart mysqltest in the dbx debugger.\u2022--client-dddStart mysqltest in the ddd debugger.\u2022--client-debugger=debuggerStart mysqltest in the named debugger.\u2022--client-gdbStart mysqltest in the gdb debugger.\u2022--client-libdir=pathThe path to the directory where client libraries are located.\u2022--combination=valueExtra options to pass to mysqld. The value should consist ofone or more comma-separated mysqld options. This option issimilar to --mysqld but should be given two or more times.mysql-test-run.pl executes multiple test runs, using theoptions for each instance of --combination in successiveruns. If --combination is given only once, it has no effect.For test runs specific to a given test suite, an alternativeto the use of --combination is to create a combinations filein the suite directory. The file should contain a section ofoptions for each test run.\u2022--comment=strWrite str to the output within lines filled with #, as a formof banner.\u2022--compressCompress all information sent between the client and theserver if both support compression.\u2022--cursor-protocolUse the cursor protocol between client and server (implies--ps-protocol).\u2022--dbxStart the mysqld(s) in the dbx debugger.\u2022--dddStart the mysqld(s) in the ddd debugger.\u2022--debugDump trace output for all clients and servers.\u2022--debug-commonSame as --debug, but sets the 'd' debug flags to\"query,info,error,enter,exit\".\u2022--debug-serverUse debug version of server, but without turning on tracing.\u2022--debugger=debuggerStart mysqld using the named debugger.\u2022--debug-sync-timeout=NControls whether the Debug Sync facility for testing anddebugging is enabled. The option value is a timeout inseconds. The default value is 300. A value of 0 disablesDebug Sync. The value of this option also becomes the defaulttimeout for individual synchronization points.mysql-test-run.pl passes --loose-debug-sync-timeout=N tomysqld. The --loose prefix is used so that mysqld does notfail if Debug Sync is not compiled in.\u2022--defaults-file=file_nameUse the named file as fixed config file template for alltests.\u2022--defaults_extra_file=file_nameAdd setting from the named file to all generated configs.\u2022--do-test=prefix|regexRun all test cases having a name that begins with the givenprefix value, or fulfils the regex. This option provides aconvenient way to run a family of similarly named tests.The argument for the --do-test option also allows moreflexible specification of which tests to perform. If theargument contains a pattern metacharacter other than a loneperiod, it is interpreted as a Perl regular expression andapplies to test names that match the pattern. If the argumentcontains a lone period or does not contain any patternmetacharacters, it is interpreted the same way as previouslyand matches test names that begin with the argument value.For example, --do-test=testa matches tests that begin withtesta, --do-test=main.testa matches tests in the main testsuite that begin with testa, and --do-test=main.*testamatches test names that contain main followed by testa withanything in between. In the latter case, the pattern match isnot anchored to the beginning of the test name, so it alsomatches names such as xmainytestz.\u2022--dry-runDon't run any tests, print the list of tests that wereselected for execution.\u2022--embedded-serverUse a version of mysqltest built with the embedded server.\u2022--enable-disabledIgnore any disabled.def file, and also run tests marked asdisabled. Success or failure of those tests will be reportedthe same way as other tests.\u2022--experimental=file_nameSpecify a file that contains a list of test cases that shouldbe displayed with the [ exp-fail ] code rather than [ fail ]if they fail.For an example of a file that might be specified via thisoption, see mysql-test/collections/default.experimental.\u2022--extern option=valueUse an already running server. The option/value pair is whatis needed by the mysql client to connect to the server. Each--extern option can only take one option/value pair as anargument, so you need to repeat --extern for each pairneeded. Example:./mysql-test-run.pl --extern socket=var/tmp/mysqld.1.sock aliasNote: If a test case has an .opt file that requires theserver to be restarted with specific options, the file willnot be used. The test case likely will fail as a result.\u2022--fastDo not perform controlled shutdown when servers need to berestarted or at the end of the test run. This is equivalentto using --shutdown-timeout=0.\u2022--force-restartAlways restart servers between tests.\u2022--forceNormally, mysql-test-run.pl exits if a test case fails.--force causes execution to continue regardless of test casefailure.\u2022--gcovCollect coverage information after the test. The result is agcov file per source and header file.\u2022--gcov-src-dirColllect coverage only within the given subdirectory. Forexample, if you're only developing the SQL layer, it makessense to use --gcov-src-dir=sql.\u2022--gdbStart the mysqld(s) in the gdb debugger.\u2022--gprofCollect profiling information using the gprof profiling tool.\u2022--manual-dbxUse a server that has already been started by the user in thedbx debugger.\u2022--manual-dddUse a server that has already been started by the user in theddd debugger.\u2022--manual-debugUse a server that has already been started by the user in adebugger.\u2022--manual-gdbUse a server that has already been started by the user in thegdb debugger.\u2022--manual-lldbUse a server that has already been started by the user in thelldb debugger.\u2022--mark-progressMarks progress with timing (in milliseconds) and line numberin var/log/testname.progress.\u2022--max-connections=numThe maximum number of simultaneous server connections thatmay be used per test. If not set, the maximum is 128. Minimumallowed limit is 8, maximum is 5120. Corresponds to the sameoption for mysqltest.\u2022--max-save-core=NLimit the number of core files saved, to avoid filling updisks in case of a frequently crashing server. Defaults to 5,set to 0 for no limit. May also be set with the environmentvariable MTR_MAX_SAVE_CORE\u2022--max-save-datadir=NLimit the number of data directories saved after failedtests, to avoid filling up disks in case of frequentfailures. Defaults to 20, set to 0 for no limit. May also beset with the environment variable MTR_MAX_SAVE_DATADIR\u2022--max-test-fail=NStop execution after the specified number of tests havefailed, to avoid using up resources (and time) in case ofmassive failures. retries are not counted, nor are failuresof tests marked experimental. Defaults to 10, set to 0 for nolimit. May also be set with the environment variableMTR_MAX_TEST_FAIL\u2022--memThis option is not supported on Windows.Run the test suite in memory, using tmpfs or ramdisk. Thiscan decrease test times significantly, in particular if youwould otherwise be running over a remote file system.mysql-test-run.pl attempts to find a suitable location usinga built-in list of standard locations for tmpfs and puts thevar directory there. This option also affects placement oftemporary files, which are created in var/tmp.The default list includes /dev/shm. You can also enable thisoption by setting the environment variableMTR_MEM[=dir_name]. If dir_name is given, it is added to thebeginning of the list of locations to search, so it takesprecedence over any built-in locations.Once you have run tests with --mem within amysql-testdirectory, a soflink var will have been set up tothe temporary directory, and this will be re-used the nexttime, until the soflink is deleted. Thus, you do not have torepeat the --mem option next time.\u2022--mysqld=valueExtra options to pass to mysqld. The value should consist ofone or more comma-separated mysqld options.\u2022--mysqld-env=VAR=VALSpecify additional environment settings for \"mysqld\". Useadditional --mysqld-env options to set more than onevariable.\u2022--nocheck-testcasesDisable the check for test case side effects; see--check-testcases for a description.\u2022--noreorderDo not reorder tests to reduce number of restarts, but runthem in exactly the order given. If a whole suite is to berun, the tests are run in alphabetical order, though similarcombinations will be grouped together. If more than one suiteis listed, the tests are run one suite at a time, in theorder listed.\u2022--notimerCause mysqltest not to generate a timing file. The effect ofthis is that the report from each test case does not includethe timing in milliseconds as it normally does.\u2022--nowarningsDo not look for and report errors and warning in the serverlogs.\u2022--parallel={N|auto}Run tests using N parallel threads. By default, 1 thread isused. Use --parallel=auto for auto-setting of N.\u2022--[mtr-]port-base=PSpecify base of port numbers to be used; a block of 10 willbe allocated.P should be divisible by 10; if it is not, itwill be rounded down. If running with more than one paralleltest thread, thread 2 will use the next block of 10 and soon.If the port number is given as auto, which is also thedefault, mysql-test-run.pl will allocate a number unique tothis host. The value may also be given with the environmentvariable MTR_PORT_BASE.If both --build-thread and --port-base are used, --port-basetakes precedence.\u2022--print-testcasesDo not run any tests, but print details about all tests, inthe order they would have been run.\u2022--ps-protocolUse the binary protocol between client and server.\u2022--recordPass the --record option to mysqltest. This option requires aspecific test case to be named on the command line.\u2022--reorderReorder tests to minimize the number of server restartsneeded. This is the default behavior. There is no guaranteethat a particular set of tests will always end up in the sameorder.\u2022--repeat=NRun each test N number of times.\u2022--report-featuresFirst run a \"test\" that reports MariaDB features, displayingthe output of SHOW ENGINES and SHOW VARIABLES. This can beused to verify that binaries are built with all requiredfeatures.\u2022--report-timesReport how much time has been spent on different phases oftest execution.\u2022--retry=NIf a test fails, it is retried up to a maximum of N runs(default 1). Retries are also limited by the maximum numberof failures before stopping, set with the --retry-failureoption. This option has no effect unless --force is alsoused; without it, test execution will terminate after thefirst failure.The --retry and --retry-failure options do not affect howmany times a test repeated with --repeat may fail in total,as each repetition is considered a new test case, which mayin turn be retried if it fails.\u2022--retry-failure=NWhen using the --retry option to retry failed tests, stopwhen N failures have occurred (default 2). Setting it to 0 or1 effectively turns off retries.\u2022--shutdown-timeout=SECONDSMax number of seconds to wait for servers to do controlledshutdown before killing them. Default is 10.\u2022--skip-combinationsDo not apply combinations; ignore combinations file oroption.\u2022--skip-rplSkip replication test cases.\u2022--skip-sslDo not start mysqld with support for SSL connections.\u2022--skip-test=regex|regexSpecify a regular expression to be applied to test casenames. Cases with names that match the expression areskipped. tests to skip.The argument for the --skip-test option allows more flexiblespecification of which tests to skip. If the argumentcontains a pattern metacharacter other than a lone period, itis interpreted as a Perl regular expression and applies totest names that match the pattern. See the description of the--do-test option for details.\u2022--skip-test-list=FILESkip the tests listed in FILE. Each line in the file is anentry and should be formatted as: <TESTNAME> : <COMMENT>\u2022--skip-*--skip-* options not otherwise recognized bymysql-test-run.pl are passed to the master server.\u2022--sleep=NPass --sleep=N to mysqltest.\u2022--sp-protocolCreate a stored procedure to execute all queries.\u2022--sslIf mysql-test-run.pl is started with the --ssl option, itsets up a secure connection for all test cases. In this case,if mysqld does not support SSL, mysql-test-run.pl exits withan error message: Couldn\u00b4t find support for SSL\u2022--staging-runRun a limited number of tests (no slow tests). Used forrunning staging trees with valgrind.\u2022--startInitialize and start servers with the startup settings forthe specified test case. You can use this option to start aserver to which you can connect later. For example, afterbuilding a source distribution you can start a server andconnect to it with the mysql client like this:shell> cd mysql-testshell> ./mysql-test-run.pl --start alias &shell> ../mysql -S ./var/tmp/master.sock -h localhost -u rootIf no tests are named on the command line, the server(s) willbe started with settings for the first test that would havebeen run without the --start option.mysql-test-run.pl will stop once the server has been started,but will terminate if the server dies. If killed, it willalso shut down the server.\u2022--start-and-exitSame --start, but mysql-test-run terminates and leaves justthe server running.\u2022--start-dirtyThis is similar to --start, but will skip the databaseinitialization phase and assume that database files arealready available. Usually this means you must have runanother test first.\u2022--start-from=test_namemysql-test-run.pl sorts the list of names of the test casesto be run, and then begins with test_name.\u2022--straceRun the \"mysqld\" executables using strace. Default optionsare -f -o var/log/'mysqld-name'.strace.\u2022--strace-clientCreate strace output for mysqltest, optionally specifyingname and path to the trace program to use.Example: ./mysql-test-run.pl --strace-client=ktrace\u2022--strace-option=ARGSOption to give strace, replaces default option(s).\u2022--stress=ARGSRun stress test, providing options to mysql-stress-test.pl.Options are separated by comma.\u2022--suite[s]=suite_name...Comma separated list of suite names to run. The default is:\"main-,archive-,binlog-,csv-,federated-,funcs_1-,funcs_2-,handler-,heap-,innodb-,innodb_fts-,innodb_zip-,maria-,multi_source-,optimizer_unfixed_bugs-,parts-,perfschema-,plugins-,roles-,rpl-,sys_vars-,unit-,vcol-\".\u2022--stop-file=fileIf this file is detected, mysqltest will not start new testsuntil the file is removed (also MTR_STOP_FILE environmentvariable).\u2022--stop-keep-alive=secWorks with --stop-file, print messages every sec seconds whenmysqltest is waiting to remove the file (for buildbot) (alsoMTR_STOP_KEEP_ALIVE environment variable).\u2022--suite-timeout=minutesSpecify the maximum test suite runtime in minutes. Thedefault is 360.\u2022--testcase-timeoutSpecify the maximum test case runtime in minutes. The defaultis 15.\u2022--timediffUsed with --timestamp, also print time passed since theprevious test started.\u2022--timerCause mysqltest to generate a timing file. The default fileis named ./var/log/timer.\u2022--timestampPrints a timestamp before the test case name in each testreport line, showing when the test ended.\u2022--tmpdir=pathThe directory where temporary file are stored. The defaultlocation is ./var/tmp. The environment variable MYSQL_TMP_DIRwill be set to the path for this directory, whether it hasthe default value or has been set explicitly. This may bereferred to in tests.\u2022--user=user_nameThe MariaDB user name to use when connecting to the server(default root).\u2022--user-argsIn combination with start* and no test name, drops argumentsto mysqld except those specified with --mysqld (if any).\u2022--valgrind[-all]Run mysqltest and mysqld with valgrind. This and thefollowing --valgrind options require that the executableshave been built with valgrind support.\u2022--valgrind-mysqldRun the mysqld server with valgrind.\u2022--valgrind-mysqltestRun the mysqltest and mysql_client_test executables withvalgrind.\u2022--valgrind-option=strOption to give valgrind. Replaces default option(s). Can bespecified more then once&.\u2022--valgrind-path=pathPath to the valgrind executable.\u2022--vardir=pathSpecify the path where files generated during the test runare stored. The default location is ./var. The environmentvariable MYSQLTEST_VARDIR will be set to the path for thisdirectory, whether it has the default value or has been setexplicitly. This may be referred to in tests.\u2022--verboseGive more verbose output regarding test execution. Use theoption twice to get even more output. Note that the outputgenerated within each test case is not affected.\u2022--verbose-restartWrite when and why servers are restarted between test cases.\u2022--view-protocolCreate a view to execute all non updating queries.\u2022--vs-config=config_valVisual Studio configuration used to create executables(default: MTR_VS_CONFIG environment variable) This option isfor Windows only.\u2022--wait-allIf --start or --start-dirty is used, wait for all servers toexit before termination. Otherwise, it will terminate if one(of several) servers is restarted.\u2022--warningsSearch the server log for errors or warning after each testand report any suspicious ones; if any are found, the testwill be marked as failed. This is the default behavior, itmay be turned off with --nowarnings.",
        "name": "mysql-test-run.pl - run MariaDB test suite",
        "section": 1
    },
    {
        "command": "mysql.server",
        "description": "MariaDB distributions on Unix include a script namedmysql.server. It can be used on systems such as Linux and Solaristhat use System V-style run directories to start and stop systemservices. It is also used by the Mac OS X Startup Item forMariaDB.mysql.server can be found in the support-files directory underyour MariaDB installation directory or in a MariaDB sourcedistribution.If you use the Linux server RPM package(MySQL-server-VERSION.rpm), the mysql.server script will beinstalled in the /etc/init.d directory with the name mysql. Youneed not install it manually.Some vendors provide RPM packages that install a startup scriptunder a different name such as mysqld.If you install MariaDB from a source distribution or using abinary distribution format that does not install mysql.serverautomatically, you can install it manually.mysql.server reads options from the [mysql.server] and [mysqld]sections of option files. For backward compatibility, it alsoreads [mysql_server] sections, although you should rename suchsections to [mysql.server]&.mysql.server supports the following options.\u2022--basedir=pathThe path to the MariaDB installation directory.\u2022--datadir=pathThe path to the MariaDB data directory.\u2022--pid-file=file_nameThe path name of the file in which the server should writeits process ID. If not provided, the default, \"host_name.pid\"is used.\u2022--service-startup-timeout=file_nameHow long in seconds to wait for confirmation of serverstartup. If the server does not start within this time,mysql.server exits with an error. The default value is 900. Avalue of 0 means not to wait at all for startup. Negativevalues mean to wait forever (no timeout).\u2022--use-mysqld_safeUse mysqld_safe to start the server. This is the default.\u2022--use-managerUse Instance Manager to start the server.\u2022--user=user_nameThe login user name to use for running mysqld.",
        "name": "mysql.server - MariaDB server startup script",
        "section": 1
    },
    {
        "command": "mysql_client_test",
        "description": "The mysql_client_test program is used for testing aspects of theMariaDB client API that cannot be tested using mysqltest and itstest language.mysql_client_test_embedded is similar but usedfor testing the embedded server. Both programs are run as part ofthe test suite.The source code for the programs can be found in intests/mysql_client_test.c in a source distribution. The programserves as a good source of examples illustrating how to usevarious features of the client API.mysql_client_test is used in a test by the same name in the maintests suite of mysql-test-run.pl but may also be run directly.Unlike the other programs listed here, it does not read anexternal description of what tests to run. Instead, all tests arecoded into the program, which is written to cover all aspects ofthe C language API.mysql_client_test supports the following options:\u2022--help, -?Display a help message and exit.\u2022--basedir=dir_name, -b dir_nameThe base directory for the tests.\u2022--count=count, -t countThe number of times to execute the tests.\u2022--database=db_name, -D db_nameThe database to use.\u2022--debug[=debug_options], -#[debug_options]Write a debugging log if MariaDB is built with debuggingsupport. The default debug_options value is\u00b4d:t:o,/tmp/mysql_client_test.trace\u00b4.\u2022--getopt-ll-test=option, -g optionOption to use for testing bugs in the getopt library.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,you are prompted for one.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.\u2022--server-arg=arg, -A argArgument to send to the embedded server.\u2022--show-tests, -TShow all test names.\u2022--silent, -sBe more silent.\u2022--socket=path, -S pathThe socket file to use when connecting to localhost (which isthe default host).\u2022--testcase, -cThe option is used when called from mysql-test-run.pl, sothat mysql_client_test may optionally behave in a differentway than if called manually, for example by skipping sometests. Currently, there is no difference in behavior but theoption is included in order to make this possible.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022-v dir_name, --vardir=dir_nameThe data directory for tests. The default is mysql-test/var.",
        "name": "mariadb-client-test - test client API (mysql_client_test is now asymlink to mariadb-client-test)mariadb-client-test-embedded - test client API for embeddedserver (mysql_client_test_embedded is now a symlink to mariadb-client-test-embedded)",
        "section": 1
    },
    {
        "command": "mysql_client_test_embedded",
        "description": "The mysql_client_test program is used for testing aspects of theMariaDB client API that cannot be tested using mysqltest and itstest language.mysql_client_test_embedded is similar but usedfor testing the embedded server. Both programs are run as part ofthe test suite.The source code for the programs can be found in intests/mysql_client_test.c in a source distribution. The programserves as a good source of examples illustrating how to usevarious features of the client API.mysql_client_test is used in a test by the same name in the maintests suite of mysql-test-run.pl but may also be run directly.Unlike the other programs listed here, it does not read anexternal description of what tests to run. Instead, all tests arecoded into the program, which is written to cover all aspects ofthe C language API.mysql_client_test supports the following options:\u2022--help, -?Display a help message and exit.\u2022--basedir=dir_name, -b dir_nameThe base directory for the tests.\u2022--count=count, -t countThe number of times to execute the tests.\u2022--database=db_name, -D db_nameThe database to use.\u2022--debug[=debug_options], -#[debug_options]Write a debugging log if MariaDB is built with debuggingsupport. The default debug_options value is\u00b4d:t:o,/tmp/mysql_client_test.trace\u00b4.\u2022--getopt-ll-test=option, -g optionOption to use for testing bugs in the getopt library.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,you are prompted for one.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.\u2022--server-arg=arg, -A argArgument to send to the embedded server.\u2022--show-tests, -TShow all test names.\u2022--silent, -sBe more silent.\u2022--socket=path, -S pathThe socket file to use when connecting to localhost (which isthe default host).\u2022--testcase, -cThe option is used when called from mysql-test-run.pl, sothat mysql_client_test may optionally behave in a differentway than if called manually, for example by skipping sometests. Currently, there is no difference in behavior but theoption is included in order to make this possible.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022-v dir_name, --vardir=dir_nameThe data directory for tests. The default is mysql-test/var.",
        "name": "mariadb-client-test - test client API (mysql_client_test is now asymlink to mariadb-client-test)mariadb-client-test-embedded - test client API for embeddedserver (mysql_client_test_embedded is now a symlink to mariadb-client-test-embedded)",
        "section": 1
    },
    {
        "command": "mysql_config",
        "description": "mysql_config provides you with useful information for compilingyour MariaDB client and connecting it to MariaDB.mysql_config supports the following options.\u2022--cflagsCompiler flags to find include files and critical compilerflags and defines used when compiling the libmysqlclientlibrary. The options returned are tied to the specificcompiler that was used when the library was created and mightclash with the settings for your own compiler. Use --includefor more portable options that contain only include paths.\u2022--includeCompiler options to find MariaDB include files.\u2022--libmysqld-libs, --embeddedLibraries and options required to link with the MariaDBembedded server.\u2022--libsLibraries and options required to link with the MariaDBclient library.\u2022--libs_rLibraries and options required to link with the thread-safeMariaDB client library.\u2022--plugindirThe default plugin directory path name, defined whenconfiguring MariaDB.\u2022--portThe default TCP/IP port number, defined when configuringMariaDB.\u2022--socketThe default Unix socket file, defined when configuringMariaDB.\u2022--variable=VARPath to MariaDB include, library and plugin directories. VARis one of `pkgincludedir`, `pkglibdir` and `plugindir`,respectively.\u2022--versionVersion number for the MariaDB distribution.If you invoke mysql_config with no options, it displays a list ofall options that it supports, and their values:shell> mysql_configUsage: /usr/local/mysql/bin/mysql_config [options]Options:--cflags[-I/usr/local/mysql/include/mysql -mcpu=pentiumpro]--include[-I/usr/local/mysql/include/mysql]--libs[-L/usr/local/mysql/lib/mysql -lmysqlclient -lz-lcrypt -lnsl -lm -L/usr/lib -lssl -lcrypto]--libs_r[-L/usr/local/mysql/lib/mysql -lmysqlclient_r-lpthread -lz -lcrypt -lnsl -lm -lpthread]--socket[/tmp/mysql.sock]--port[3306]--version[4.0.16]--libmysqld-libs [-L/usr/local/mysql/lib/mysql -lmysqld -lpthread -lz-lcrypt -lnsl -lm -lpthread -lrt]You can use mysql_config within a command line to include thevalue that it displays for a particular option. For example, tocompile a MariaDB client program, use mysql_config as follows:shell> CFG=/usr/local/mysql/bin/mysql_configshell> sh -c \"gcc -o progname `$CFG --include` progname.c `$CFG --libs`\"When you use mysql_config this way, be sure to invoke it withinbacktick (\u201c`\u201d) characters. That tells the shell to execute it andsubstitute its output into the surrounding command.",
        "name": "mysql_config - get compile options for compiling clients",
        "section": 1
    },
    {
        "command": "mysql_convert_table_format",
        "description": "mysql_convert_table_format converts the tables in a database touse a particular storage engine (MyISAM by default).mysql_convert_table_format is written in Perl and requires thatthe DBI and DBD::MariaDB Perl modules be installed (seeSection 2.15, \u201cPerl Installation Notes\u201d).Invoke mysql_convert_table_format like this:shell> mysql_convert_table_format [options]db_nameThe db_name argument indicates the database containing the tablesto be converted.mysql_convert_table_format supports the options described in thefollowing list.\u2022--helpDisplay a help message and exit.\u2022--forceContinue even if errors occur.\u2022--host=host_nameConnect to the MariaDB server on the given host.\u2022--password=passwordThe password to use when connecting to the server. Note thatthe password value is not optional for this option, unlikefor other MariaDB programs.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--port=port_numThe TCP/IP port number to use for the connection.\u2022--socket=pathFor connections to localhost, the Unix socket file to use.\u2022--type=engine_nameSpecify the storage engine that the tables should beconverted to use. The default is MyISAM if this option is notgiven.\u2022--user=user_nameThe MariaDB user name to use when connecting to the server.\u2022--verboseVerbose mode. Print more information about what the programdoes.\u2022--versionDisplay version information and exit.",
        "name": "mariadb-convert-table-format - convert tables to use a givenstorage engine (mysql_convert_table_format is now a symlink tomariadb-convert-table-format)",
        "section": 1
    },
    {
        "command": "mysql_embedded",
        "description": "mysql is a simple SQL shell (with GNU readline capabilities). Itsupports interactive and non-interactive use. When usedinteractively, query results are presented in an ASCII-tableformat. When used non-interactively (for example, as a filter),the result is presented in tab-separated format. The outputformat can be changed using command options.If you have problems due to insufficient memory for large resultsets, use the --quick option. This forces mysql to retrieveresults from the server a row at a time rather than retrievingthe entire result set and buffering it in memory beforedisplaying it. This is done by returning the result set using themysql_use_result() C API function in the client/server libraryrather than mysql_store_result().Using mysql is very easy. Invoke it from the prompt of yourcommand interpreter as follows:shell> mysql db_nameOr:shell> mysql --user=user_name --password=your_password db_nameThen type an SQL statement, end it with \u201c;\u201d, \\g, or \\G and pressEnter.Typing Control-C causes mysql to attempt to kill the currentstatement. If this cannot be done, or Control-C is typed againbefore the statement is killed, mysql exits.You can execute SQL statements in a script file (batch file) likethis:shell> mysql db_name < script.sql > output.tab",
        "name": "mariadb - the MariaDB command-line tool (mysql is now a symlinkto mariadb)",
        "section": 1
    },
    {
        "command": "mysql_find_rows",
        "description": "mysql_find_rows reads files containing SQL statements andextracts statements that match a given regular expression or thatcontain USE db_name or SET statements. The utility was writtenfor use with update log files (as used prior to MySQL 5.0) and assuch expects statements to be terminated with semicolon (;)characters. It may be useful with other files that contain SQLstatements as long as statements are terminated with semicolons.Invoke mysql_find_rows like this:shell> mysql_find_rows [options] [file_name ...]Each file_name argument should be the name of file containing SQLstatements. If no file names are given, mysql_find_rows reads thestandard input.Examples:mysql_find_rows --regexp=problem_table --rows=20 < update.logmysql_find_rows --regexp=problem_tableupdate-log.1 update-log.2mysql_find_rows supports the following options:\u2022--help, --InformationDisplay a help message and exit.\u2022--regexp=patternDisplay queries that match the pattern.\u2022--rows=NQuit after displaying N queries.\u2022--skip-use-dbDo not include USE db_name statements in the output.\u2022--start_row=NStart output from this row.",
        "name": "mariadb-find-rows - extract SQL statements from files(mysql_find_rows is now a symlink to mariadb-find-rows)",
        "section": 1
    },
    {
        "command": "mysql_fix_extensions",
        "description": "mysql_fix_extensions converts the extensions for MyISAM (or ISAM)table files to their canonical forms. It looks for files withextensions matching any lettercase variant of .frm, .myd, .myi,.isd, and .ism and renames them to have extensions of .frm, .MYD,.MYI, .ISD, and .ISM, respectively. This can be useful aftertransferring the files from a system with case-insensitive filenames (such as Windows) to a system with case-sensitive filenames.Invoke mysql_fix_extensions like this, where data_dir is the pathname to the MySQL data directory.shell> mysql_fix_extensions data_dir",
        "name": "mariadb-fix-extensions - normalize table file name extensions(mysql_fix_extensions is now a symlink to mariadb-fix-extensions)",
        "section": 1
    },
    {
        "command": "mysql_install_db",
        "description": "mysql_install_db initializes the MariaDB data directory andcreates the system tables that it contains, if they do not exist.To invoke mysql_install_db, use the following syntax:shell> mysql_install_db [options]Because the MariaDB server, mysqld, needs to access the datadirectory when it runs later, you should either runmysql_install_db from the same account that will be used forrunning mysqld or run it as root and use the --user option toindicate the user name that mysqld will run as. It might benecessary to specify other options such as --basedir or --datadirif mysql_install_db does not use the correct locations for theinstallation directory or data directory. For example:shell> bin/mysql_install_db --user=mysql \\--basedir=/opt/mysql/mysql \\--datadir=/opt/mysql/mysql/datamysql_install_db needs to invoke mysqld with the --bootstrap and--skip-grant-tables options (see Section 2.3.2, \u201cTypicalconfigure Options\u201d). If MariaDB was configured with the--disable-grant-options option, --bootstrap and--skip-grant-tables will be disabled. To handle this, set theMYSQLD_BOOTSTRAP environment variable to the full path name of aserver that has all options enabled.mysql_install_db will usethat server.mysql_install_db supports the following options, which can bespecified on the command line or in the [mysql_install_db] and(if they are common to mysqld) [mysqld] option file groups.\u2022--basedir=pathThe path to the MariaDB installation directory.\u2022--builddir=pathIf using --srcdir with out-of-directory builds, you will needto set this to the location of the build directory wherebuilt files reside..\u2022--cross-bootstrapFor internal use. Used when building the MariaDB systemtables on a different host than the target..\u2022--datadir=path, --ldata=pathThe path to the MariaDB data directory.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--defaults-group-suffix=nameIn addition to the given groups, also read groups with thissuffix.\u2022--forceCause mysql_install_db to run even if DNS does not work. Inthat case, grant table entries that normally use host nameswill use IP addresses.\u2022--helpDisplay a help message and exit.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--rpmFor internal use. This option is used by RPM files during theMariaDB installation process.\u2022--skip-name-resolveUse IP addresses rather than host names when creating granttable entries. This option can be useful if your DNS does notwork.\u2022--srcdir=pathFor internal use. The directory under which mysql_install_dblooks for support files such as the error message file andthe file for populating the help tables.4.\u2022--user=user_nameThe login user name to use for running mysqld. Files anddirectories created by mysqld will be owned by this user. Youmust be root to use this option. By default, mysqld runsusing your current login name and files and directories thatit creates will be owned by you.\u2022--extra-file=file_pathAdd user defined SQL file, to be executed following regulardatabase initialization.\u2022--verboseVerbose mode. Print more information about what the programdoes.\u2022--windowsFor internal use. This option is used for creating Windowsdistributions.",
        "name": "mariadb-install-db - initialize MariaDB data directory(mysql_install_db is now a symlink to mariadb-install-db)",
        "section": 1
    },
    {
        "command": "mysql_ldb",
        "description": "Use mysql_ldb --help for details on usage.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "mariadb-ldb - RocksDB tool(mysql_ldb is now a symlink tomariadb-ldb)",
        "section": 1
    },
    {
        "command": "mysql_plugin",
        "description": "The mysql_plugin utility enables MariaDB administrators to managewhich plugins a MariaDB server loads. It provides an alternativeto manually specifying the --plugin-load option at server startupor using the INSTALL PLUGIN and UNINSTALL PLUGIN statements atruntime.Depending on whether mysql_plugin is invoked to enable or disableplugins, it inserts or deletes rows in the mysql.plugin tablethat serves as a plugin registry. (To perform this operation,mysql_plugin invokes the MariaDB server in bootstrap mode. Thismeans that the server must not already be running.) For normalserver startups, the server loads and enables plugins listed inmysql.plugin automatically. For additional control over pluginactivation, use --plugin_name options named for specific plugins.Each invocation of mysql_plugin reads a configuration file todetermine how to configure the plugins contained in a singleplugin library object file. To invoke mysql_plugin, use thissyntax:mysql_plugin [options] plugin {ENABLE|DISABLE}plugin is the name of the plugin to configure.ENABLE or DISABLE(not case sensitive) specify whether to enable or disablecomponents of the plugin library named in the configuration file.The order of the plugin and ENABLE or DISABLE arguments does notmatter.For example, to configure components of a plugin library filenamed myplugins.so on Linux or myplugins.dll on Windows, specifya plugin value of myplugins. Suppose that this plugin librarycontains three plugins, plugin1, plugin2, and plugin3, all ofwhich should be configured under mysql_plugin control. Byconvention, configuration files have a suffix of .ini and thesame basename as the plugin library, so the default configurationfile name for this plugin library is myplugins.ini. Theconfiguration file contents look like this:mypluginsplugin1plugin2plugin3The first line in the myplugins.ini file is the name of thelibrary object file, without any extension such as .so or .dll.The remaining lines are the names of the components to be enabledor disabled. Each value in the file should be on a separate line.Lines on which the first character is '#' are taken as commentsand ignored.To enable the plugins listed in the configuration file, invokemysql_plugin this way:shell> mysql_plugin myplugins ENABLETo disable the plugins, use DISABLE rather than ENABLE.An error occurs if mysql_plugin cannot find the configurationfile or plugin library file, or if mysql_plugin cannot start theMariaDB server.mysql_plugin supports the following options, which can bespecified on the command line or in the [mysqld] group of anyoption file. For options specified in a [mysqld] group,mysql_plugin recognizes the --basedir, --datadir, and--plugin-dir options and ignores others.mysql_plugin Options\u2022--help, -?Display a help message and exit.\u2022--basedir=path, -b pathThe server base directory.\u2022--datadir=path, -d pathThe server data directory.\u2022--my-print-defaults=path, -b pathThe path to the my_print_defaults program.\u2022--mysqld=path, -b pathThe path to the mysqld server.\u2022--no-defaults, -pDo not read values from the configuration file. This optionenables an administrator to skip reading defaults from theconfiguration file.With mysql_plugin, this option need not be given first on thecommand line, unlike most other MariaDB programs that support--no-defaults.\u2022--plugin-dir=path, -p pathThe server plugin directory.\u2022--plugin-ini=file_name, -i file_nameThe mysql_plugin configuration file. Relative path names areinterpreted relative to the current directory. If this optionis not given, the default is plugin.ini in the plugindirectory, where plugin is the plugin argument on the commandline.\u2022--print-defaults, -PDisplay the default values from the configuration file. Thisoption causes mysql_plugin to print the defaults for--basedir, --datadir, and --plugin-dir if they are found inthe configuration file. If no value for a variable is found,nothing is shown.With mysql_plugin, this option need not be given first on thecommand line, unlike most other MariaDB programs that support--print-defaults.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes. This option can be used multiple times to increase theamount of information.\u2022--version, -VDisplay version information and exit.",
        "name": "mariadb-plugin - configure MariaDB server plugins (mysql_pluginis now a symlink to mariadb-plugin)",
        "section": 1
    },
    {
        "command": "mysql_secure_installation",
        "description": "This program enables you to improve the security of your MariaDBinstallation in the following ways:\u2022You can set a password for root accounts.\u2022You can remove root accounts that are accessible from outsidethe local host.\u2022You can remove anonymous-user accounts.\u2022You can remove the test database, which by default can beaccessed by anonymous users.mysql_secure_installation can be invoked without arguments:shell> mysql_secure_installationThe script will prompt you to determine which actions to perform.mysql_secure_installation accepts some options:\u2022--basedir=dir_nameBase directory.\u2022--defaults-extra-file=file_nameAdditional option file.\u2022--defaults-file=file_nameOption file.\u2022--no-defaultsDon't read any defaults file.Other unrecognized options will be passed on to the server.",
        "name": "mariadb-secure-installation - improve MariaDB installationsecurity (mysql_secure_installation is now a symlink to mariadb-secure-installation)",
        "section": 1
    },
    {
        "command": "mysql_setpermission",
        "description": "mysql_setpermission is a Perl script that was originally writtenand contributed by Luuk de Boer. It interactively setspermissions in the MariaDB grant tables.mysql_setpermission iswritten in Perl and requires that the DBI and DBD::MariaDB Perlmodules be installed.Invoke mysql_setpermission like this:shell> mysql_setpermission [options]options should be either --help to display the help message, oroptions that indicate how to connect to the MariaDB server. Theaccount used when you connect determines which permissions youhave when attempting to modify existing permissions in the granttables.mysql_setpermission also reads options from the [client] and[perl] groups in the .my.cnf file in your home directory, if thefile exists.mysql_setpermission supports the following options:\u2022--helpDisplay a help message and exit.\u2022--host=host_nameConnect to the MariaDB server on the given host.\u2022--password=passwordThe password to use when connecting to the server. Note thatthe password value is not optional for this option, unlikefor other MariaDB programs.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--port=port_numThe TCP/IP port number to use for the connection.\u2022--socket=pathFor connections to localhost, the Unix socket file to use.\u2022--user=user_nameThe MariaDB user name to use when connecting to the server.",
        "name": "mariadb-setpermission - interactively set permissions in granttables (mysql_setpermission is now a symlink to mariadb-setpermission)",
        "section": 1
    },
    {
        "command": "mysql_tzinfo_to_sql",
        "description": "The mysql_tzinfo_to_sql program loads the time zone tables in themysql database. It is used on systems that have a zoneinfodatabase (the set of files describing time zones). Examples ofsuch systems are Linux, FreeBSD, Solaris, and Mac OS X. Onelikely location for these files is the /usr/share/zoneinfodirectory (/usr/share/lib/zoneinfo on Solaris).mysql_tzinfo_to_sql can be invoked several ways:shell> mysql_tzinfo_to_sql tz_dirshell> mysql_tzinfo_to_sql tz_file tz_nameshell> mysql_tzinfo_to_sql --leap tz_fileshell> mysql_tzinfo_to_sql --skip-write-binlog tz_dirFor the first invocation syntax, pass the zoneinfo directory pathname to mysql_tzinfo_to_sql and send the output into the mysqlprogram. For example:shell> mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root mysqlmysql_tzinfo_to_sql reads your system\u00b4s time zone files andgenerates SQL statements from them.mysql processes thosestatements to load the time zone tables.The second syntax causes mysql_tzinfo_to_sql to load a singletime zone file tz_file that corresponds to a time zone nametz_name:shell> mysql_tzinfo_to_sql tz_file tz_name | mysql -u root mysqlIf your time zone needs to account for leap seconds, invokemysql_tzinfo_to_sql using the third syntax, which initializes theleap second information.tz_file is the name of your time zonefile:shell> mysql_tzinfo_to_sql --leap tz_file | mysql -u root mysqlUsing the --skip-write-binlog option prevents writing of changesto the binary log or to other Galera cluster members. This can beused with any form of running mysql_tzinfo_to_sql.After running mysql_tzinfo_to_sql, it is best to restart theserver so that it does not continue to use any previously cachedtime zone data.",
        "name": "mariadb-tzinfo-to-sql - load the time zone tables(mysql_tzinfo_to_sql is now a symlink to mariadb-tzinfo-to-sql)",
        "section": 1
    },
    {
        "command": "mysql_upgrade",
        "description": "mysql_upgrade examines all tables in all databases forincompatibilities with the current version of the MariaDB Server.mysql_upgrade also upgrades the system tables so that you cantake advantage of new privileges or capabilities that might havebeen added.mysql_upgrade should be executed each time you upgrade MariaDB.If a table is found to have a possible incompatibility,mysql_upgrade performs a table check. If any problems are found,a table repair is attempted.NoteOn Windows Server 2008 and Windows Vista, you must runmysql_upgrade with administrator privileges. You can do thisby running a Command Prompt as Administrator and running thecommand. Failure to do so may result in the upgrade failingto execute correctly.CautionYou should always back up your current MariaDB installationbefore performing an upgrade.To use mysql_upgrade, make sure that the server is running, andthen invoke it like this:shell> mysql_upgrade [options]After running mysql_upgrade, stop the server and restart it sothat any changes made to the system tables take effect.mysql_upgrade executes the following commands to check and repairtables and to upgrade the system tables:mysqlcheck --all-databases --check-upgrade --auto-repairmysql < fix_priv_tablesmysqlcheck --all-databases --check-upgrade --fix-db-names --fix-table-namesNotes about the preceding commands:\u2022Because mysql_upgrade invokes mysqlcheck with the--all-databases option, it processes all tables in alldatabases, which might take a long time to complete. Eachtable is locked and therefore unavailable to other sessionswhile it is being processed. Check and repair operations canbe time-consuming, particularly for large tables.\u2022For details about what checks the --check-upgrade optionentails, see the description of the FOR UPGRADE option of theCHECK TABLE statement.\u2022fix_priv_tables represents a script generated internally bymysql_upgrade that contains SQL statements to upgrade thetables in the mysql database.All checked and repaired tables are marked with the currentMariaDB version number. This ensures that next time you runmysql_upgrade with the same version of the server, it can tellwhether there is any need to check or repair the table again.mysql_upgrade also saves the MariaDB version number in a filenamed mysql_upgrade_info in the data directory. This is used toquickly check whether all tables have been checked for thisrelease so that table-checking can be skipped. To ignore thisfile and perform the check regardless, use the --force option.For this reason, mysql_upgrade needs to be run as a user withwrite access to the data directory.If you install MariaDB from RPM packages on Linux, you mustinstall the server and client RPMs.mysql_upgrade is included inthe server RPM but requires the client RPM because the latterincludes mysqlcheck.mysql_upgrade supports the following options, which can bespecified on the command line or in the [mysql_upgrade] and[client] option file groups. Other options are passed tomysqlcheck. For example, it might be necessary to specify the--password[=password] option.mysql_upgrade also supports theoptions for processing option files.\u2022--help, -?Display a short help message and exit.\u2022--basedir=pathOld option accepted for backward compatibility but ignored.\u2022--character-sets-dir=pathOld option accepted for backward compatibility but ignored.\u2022--check-if-upgrade-is-neededExit with a status code indicating if an upgrade is needed.Returns 0 if upgrade needed or current version couldn't bedetermined, 1 when no action required.\u2022--datadir=pathOld option accepted for backward compatibility but ignored.\u2022--debug=path, -# pathFor debug builds, output debug log.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-info, -TPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-character-set=nameOld option accepted for backward compatibility but ignored.\u2022--forceIgnore the mysql_upgrade_info file and force execution ofmysqlcheck even if mysql_upgrade has already been executedfor the current version of MariaDB.\u2022--hostConnect to MariaDB on the given host.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysql_upgrade prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--silentPrint less information.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--tmpdir=path, -t pathThe path name of the directory to use for creating temporaryfiles.\u2022--upgrade-system-tables, -sOnly upgrade the system tables in the mysql database. Tablesin other databases are not checked or touched.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the serverand not using the current login.\u2022--verboseDisplay more output about the process. Using it twice willprint connection arguments; using it 3 times will print outall CHECK, RENAME and ALTER TABLE commands used during thecheck phase; using it 4 times (added in MariaDB 10.0.14) willalso write out all mariadb-check commands used; using it 5times will print all the mariadb commands used and theirresults while running mysql_fix_privilege_tables script.\u2022--version, -VOutput version information and exit.\u2022--version-check, -kRun this program only if its 'server version' matches theversion of the server to which it's connecting. Note: the'server version' of the program is the version of the MariaDBserver with which it was built/distributed. Defaults to on;use --skip-version-check to disable.\u2022--write-binlogCause binary logging to be enabled while mysql_upgrade runs.",
        "name": "mariadb-upgrade - check tables for MariaDB upgrade (mysql_upgradeis now a symlink to mariadb-upgrade)",
        "section": 1
    },
    {
        "command": "mysql_waitpid",
        "description": "mysql_waitpid signals a process to terminate and waits for theprocess to exit. It uses the kill() system call and Unix signals,so it runs on Unix and Unix-like systems.Invoke mysql_waitpid like this:shell> mysql_waitpid [options] pid wait_timemysql_waitpid sends signal 0 to the process identified by pid andwaits up to wait_time seconds for the process to terminate.pidand wait_time must be positive integers.If process termination occurs within the wait time or the processdoes not exist, mysql_waitpid returns 0. Otherwise, it returns 1.If the kill() system call cannot handle signal 0, mysql_waitpid()uses signal 1 instead.mysql_waitpid supports the following options:\u2022--help, -?, -IDisplay a help message and exit.\u2022--verbose, -vVerbose mode. Display a warning if signal 0 could not be usedand signal 1 is used instead.\u2022--version, -VDisplay version information and exit.",
        "name": "mariadb-waitpid - kill process and wait for its termination(mysql_waitpid is now a symlink to mariadb-waitpid)",
        "section": 1
    },
    {
        "command": "mysqlaccess",
        "description": "mysqlaccess is a diagnostic tool written by Yves Carlier. Itchecks the access privileges for a host name, user name, anddatabase combination. Note that mysqlaccess checks access usingonly the user, db, and host tables. It does not check table,column, or routine privileges specified in the tables_priv,columns_priv, or procs_priv tables.Invoke mysqlaccess like this:shell> mysqlaccess [host_name [user_name [db_name]]] [options]mysqlaccess supports the following options.\u2022--help, -?Display a help message and exit.\u2022--brief, -bGenerate reports in single-line tabular format.\u2022--commitCopy the new access privileges from the temporary tables tothe original grant tables. The grant tables must be flushedfor the new privileges to take effect. (For example, executea mysqladmin reload command.)\u2022--copyReload the temporary grant tables from original ones.\u2022--db=db_name, -d db_nameSpecify the database name.\u2022--debug=NSpecify the debug level.N can be an integer from 0 to 3.\u2022--host=host_name, -h host_nameThe host name to use in the access privileges.\u2022--howtoDisplay some examples that show how to use mysqlaccess.\u2022--old_serverConnect to a very old MySQL server (before MySQL 3.21) thatdoes not know how to handle full WHERE clauses.\u2022--password[=password], -p[password]The password to use when connecting to the server. If youomit the password value following the --password or -p optionon the command line, mysqlaccess prompts for one.Specifying a password on the command line should beconsidered insecure. See Section 5.3.2.2, \u201cEnd-UserGuidelines for Password Security\u201d.\u2022--planDisplay suggestions and ideas for future releases.\u2022--previewShow the privilege differences after making changes to thetemporary grant tables.\u2022--relnotesDisplay the release notes.\u2022--rhost=host_name, -H host_nameConnect to the MariaDB server on the given host.\u2022--rollbackUndo the most recent changes to the temporary grant tables.\u2022--spassword[=password], -P[password]The password to use when connecting to the server as thesuperuser. If you omit the password value following the--spassword or -p option on the command line, mysqlaccessprompts for one.Specifying a password on the command line should beconsidered insecure. See Section 5.3.2.2, \u201cEnd-UserGuidelines for Password Security\u201d.\u2022--superuser=user_name, -U user_nameSpecify the user name for connecting as the superuser.\u2022--table, -tGenerate reports in table format.\u2022--user=user_name, -u user_nameThe user name to use in the access privileges.\u2022--version, -vDisplay version information and exit.If your MariaDB distribution is installed in some non-standardlocation, you must change the location where mysqlaccess expectsto find the mysql client. Edit the mysqlaccess script atapproximately line 18. Search for a line that looks like this:$MYSQL= \u00b4/usr/local/bin/mysql\u00b4;# path to mysql executableChange the path to reflect the location where mysql actually isstored on your system. If you do not do this, a Broken pipe errorwill occur when you run mysqlaccess.",
        "name": "mariadb-access - client for checking access privileges(mysqlaccess is now a symlink to mariadb-access)",
        "section": 1
    },
    {
        "command": "mysqladmin",
        "description": "mysqladmin is a client for performing administrative operations.You can use it to check the server\u00b4s configuration and currentstatus, to create and drop databases, and more.Invoke mysqladmin like this:shell> mysqladmin [options] command [command-arg] [command [command-arg]] ...mysqladmin supports the following commands. Some of the commandstake an argument following the command name.\u2022create db_nameCreate a new database named db_name.\u2022debugTell the server to write debug information to the error log.This also includes information about the Event Scheduler.\u2022drop db_nameDelete the database named db_name and all its tables.\u2022extended-statusDisplay the server status variables and their values.\u2022flush-all-statisticsFlush all statistics tables.\u2022flush-all-statusFlush all status and statistics.\u2022flush-binary-logFlush the binary log.\u2022flush-client-statisticsFlush client statistics.\u2022flush-engine-logFlush engine log.\u2022flush-error-logFlush error log.\u2022flush-general-logFlush general query log.\u2022flush-hostsFlush all information in the host cache.\u2022flush-index-statisticsFlush index statistics.\u2022flush-logsFlush all logs.\u2022flush-privilegesReload the grant tables (same as reload).\u2022flush-relay-logFlush relay log.\u2022flush-slow-logFlush slow query log.\u2022flush-sslFlush SSL certificates.\u2022flush-statusClear status variables.\u2022flush-table-statisticsFlush table statistics.\u2022flush-tablesFlush all tables.\u2022flush-threadsFlush the thread cache.\u2022flush-user-resourcesFlush user resources.\u2022kill id,id,...Kill server threads. If multiple thread ID values are given,there must be no spaces in the list.\u2022old-password new-passwordThis is like the password command but stores the passwordusing the old (pre MySQL 4.1) password-hashing format.\u2022password new-passwordSet a new password. This changes the password to new-passwordfor the account that you use with mysqladmin for connectingto the server. Thus, the next time you invoke mysqladmin (orany other client program) using the same account, you willneed to specify the new password.If the new-password value contains spaces or other charactersthat are special to your command interpreter, you need toenclose it within quotes. On Windows, be sure to use doublequotes rather than single quotes; single quotes are notstripped from the password, but rather are interpreted aspart of the password. For example:shell> mysqladmin password \"my new password\"CautionDo not use this command used if the server was startedwith the --skip-grant-tables option. No password changewill be applied. This is true even if you precede thepassword command with flush-privileges on the samecommand line to re-enable the grant tables because theflush operation occurs after you connect. However, youcan use mysqladmin flush-privileges to re-enable thegrant table and then use a separate mysqladmin passwordcommand to change the password.\u2022pingCheck whether the server is alive. The return status frommysqladmin is 0 if the server is running, 1 if it is not.This is 0 even in case of an error such as Access denied,because this means that the server is running but refused theconnection, which is different from the server not running.\u2022processlistShow a list of active server threads. This is like the outputof the SHOW PROCESSLIST statement. If the --verbose option isgiven, the output is like that of SHOW FULL PROCESSLIST.\u2022reloadReload the grant tables.\u2022refreshFlush all tables and close and open log files.\u2022shutdownStop the server.\u2022start-all-slavesStart all slaves.\u2022start-slaveStart replication on a slave server.\u2022statusDisplay a short server status message.\u2022stop-all-slavesStop all slaves.\u2022stop-slaveStop replication on a slave server.\u2022variablesDisplay the server system variables and their values.\u2022versionDisplay version information from the server.All commands can be shortened to any unique prefix. For example:shell> mysqladmin proc stat+----+-------+-----------+----+---------+------+-------+------------------+| Id | User| Host| db | Command | Time | State | Info|+----+-------+-----------+----+---------+------+-------+------------------+| 51 | monty | localhost || Query| 0|| show processlist |+----+-------+-----------+----+---------+------+-------+------------------+Uptime: 1473624Threads: 1Questions: 39487Slow queries: 0Opens: 541Flush tables: 1Open tables: 19Queries per second avg: 0.0268The mysqladmin status command result displays the followingvalues:\u2022UptimeThe number of seconds the MariaDB server has been running.\u2022ThreadsThe number of active threads (clients).\u2022QuestionsThe number of questions (queries) from clients since theserver was started.\u2022Slow queriesThe number of queries that have taken more thanlong_query_time seconds.\u2022OpensThe number of tables the server has opened.\u2022Flush tablesThe number of flush-*, refresh, and reload commands theserver has executed.\u2022Open tablesThe number of tables that currently are open.\u2022Memory in useThe amount of memory allocated directly by mysqld. This valueis displayed only when MariaDB has been compiled with--with-debug=full.\u2022Maximum memory usedThe maximum amount of memory allocated directly by mysqld.This value is displayed only when MariaDB has been compiledwith --with-debug=full.If you execute mysqladmin shutdown when connecting to a localserver using a Unix socket file, mysqladmin waits until theserver\u00b4s process ID file has been removed, to ensure that theserver has stopped properly.mysqladmin supports the following options, which can be specifiedon the command line or in the [mysqladmin] and [client] optionfile groups.\u2022--help, -?Display help and exit.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--connect-timeout=timeoutEquivalent to --connect_timeout, see the end of this section.\u2022--count=N, -c NThe number of iterations to make for repeated commandexecution if the --sleep option is given.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is\u00b4d:t:o,/tmp/mysqladmin.trace\u00b4.\u2022--debug-checkCheck memory and open file usage at exit..\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-authDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files. Must be given as firstoption.\u2022--force, -fDo not ask for confirmation for the drop db_name command.With multiple commands, continue even if an error occurs.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--local, -lSuppress the SQL command(s) from being written to the binarylog by using FLUSH LOCAL or enabling sql_log_bin=0 for thesession.\u2022--no-beep, -bSuppress the warning beep that is emitted by default forerrors such as a failure to connect to the server.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqladmin prompts for one.Specifying a password on the command line should beconsidered insecure.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection or 0 fordefault to, in order of preference, my.cnf, $MYSQL_TCP_PORT,/etc/services, built-in default (3306).Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--print-defaultsPrint the program argument list and exit. This must be givenas the first argument.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--relative, -rShow the difference between the current and previous valueswhen used with the --sleep option. Currently, this optionworks only with the extended-status command.\u2022--shutdown-timeouttimeoutEquivalent of --shutdown_timeout, see the end of thissection.\u2022--silent, -sExit silently if a connection to the server cannot beestablished.\u2022--sleep=delay, -i delayExecute commands repeatedly, sleeping for delay seconds inbetween. The --count option determines the number ofiterations. If --count is not given, mysqladmin executescommands indefinitely until interrupted.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--tls-version=name,Accepts a comma-separated list of TLS protocol versions. ATLS protocol version will only be enabled if it is present inthis list. All other TLS protocol versions will not bepermitted.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.\u2022--version, -VDisplay version information and exit.\u2022--vertical, -EPrint output vertically. This is similar to --relative, butprints output vertically.\u2022--wait[=count], -w[count]If the connection cannot be established, wait and retryinstead of aborting. If a count value is given, it indicatesthe number of times to retry. The default is one time.\u2022--wait-for-all-slavesWait for the last binlog event to be sent to all connectedslaves before shutting down.This option is off by default.You can also set the following variables by using--var_name=value\u2022connect_timeoutThe maximum number of seconds before connection timeout. Thedefault value is 43200 (12 hours).\u2022shutdown_timeoutThe maximum number of seconds to wait for server shutdown.The default value is 3600 (1 hour).",
        "name": "mariadb-admin - client for administering a MariaDB server(mysqladmin is now a symlink to mariadb-admin)",
        "section": 1
    },
    {
        "command": "mysqlbinlog",
        "description": "The server\u00b4s binary log consists of files containing \u201cevents\u201dthat describe modifications to database contents. The serverwrites these files in binary format. To display their contents intext format, use the mysqlbinlog utility. You can also usemysqlbinlog to display the contents of relay log files written bya slave server in a replication setup because relay logs have thesame format as binary logs.Invoke mysqlbinlog like this:shell> mysqlbinlog [options] log_file ...For example, to display the contents of the binary log file namedbinlog.000003, use this command:shell> mysqlbinlog binlog.0000003The output includes events contained in binlog.000003. Forstatement-based logging, event information includes the SQLstatement, the ID of the server on which it was executed, thetimestamp when the statement was executed, how much time it took,and so forth. For row-based logging, the event indicates a rowchange rather than an SQL statement.Events are preceded by header comments that provide additionalinformation. For example:# at 141#1003099:28:36 server id 123end_log_pos 245Query thread_id=3350exec_time=11error_code=0In the first line, the number following at indicates the startingposition of the event in the binary log file.The second line starts with a date and time indicating when thestatement started on the server where the event originated. Forreplication, this timestamp is propagated to slave servers.server id is the server_id value of the server where the eventoriginated.end_log_pos indicates where the next event starts(that is, it is the end position of the current event + 1).thread_id indicates which thread executed the event.exec_timeis the time spent executing the event, on a master server. On aslave, it is the difference of the end execution time on theslave minus the beginning execution time on the master. Thedifference serves as an indicator of how much replication lagsbehind the master.error_code indicates the result fromexecuting the event. Zero means that no error occurred.The output from mysqlbinlog can be re-executed (for example, byusing it as input to mysql) to redo the statements in the log.This is useful for recovery operations after a server crash. Forother usage examples, see the discussion later in this section.Normally, you use mysqlbinlog to read binary log files directlyand apply them to the local MariaDB server. It is also possibleto read binary logs from a remote server by using the--read-from-remote-server option. To read remote binary logs, theconnection parameter options can be given to indicate how toconnect to the server. These options are --host, --password,--port, --protocol, --socket, and --user; they are ignored exceptwhen you also use the --read-from-remote-server option.mysqlbinlog supports the following options, which can bespecified on the command line or in the [mysqlbinlog] and[client] option file groups.\u2022--help, -?Display a help message and exit.\u2022--base64-output=valueThis option determines when events should be displayedencoded as base-64 strings using BINLOG statements. Theoption has these allowable values (not case sensitive):\u2022AUTO (\"automatic\") or UNSPEC (\"unspecified\") displaysBINLOG statements automatically when necessary (that is,for format description events and row events). This isthe default if no --base64-output option is given.NoteAutomatic BINLOG display is the only safe behavior ifyou intend to use the output of mysqlbinlog tore-execute binary log file contents. The other optionvalues are intended only for debugging or testingpurposes because they may produce output that doesnot include all events in executable form.\u2022NEVER causes BINLOG statements not to be displayed.mysqlbinlog exits with an error if a row event is foundthat must be displayed using BINLOG.\u2022DECODE-ROWS specifies to mysqlbinlog that you intend forrow events to be decoded and displayed as commented SQLstatements by also specifying the --verbose option. LikeNEVER, DECODE-ROWS suppresses display of BINLOGstatements, but unlike NEVER, it does not exit with anerror if a row event is found.The --base64-output can be given as --base64-output or--skip-base64-output (with the sense of AUTO or NEVER).For examples that show the effect of --base64-output and--verbose on row event output, see the section called\u201cMYSQLBINLOG ROW EVENT DISPLAY\u201d.\u2022--binlog-row-event-max-size=pathThe directory where character sets are installed.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--database=db_name, -d db_nameThis option causes mysqlbinlog to output entries from thebinary log (local log only) that occur while db_name has beenselected as the default database by USE.The --database option for mysqlbinlog is similar to the--binlog-do-db option for mysqld, but can be used to specifyonly one database. If --database is given multiple times,only the last instance is used.The effects of this option depend on whether thestatement-based or row-based logging format is in use, in thesame way that the effects of --binlog-do-db depend on whetherstatement-based or row-based logging is in use.Statement-based logging. The --database option works asfollows:\u2022While db_name is the default database, statements areoutput whether they modify tables in db_name or adifferent database.\u2022Unless db_name is selected as the default database,statements are not output, even if they modify tables indb_name.\u2022There is an exception for CREATE DATABASE, ALTERDATABASE, and DROP DATABASE. The database being created,altered, or dropped is considered to be the defaultdatabase when determining whether to output thestatement.Suppose that the binary log was created by executingthese statements using statement-based-logging:INSERT INTO test.t1 (i) VALUES(100);INSERT INTO db2.t2 (j)VALUES(200);USE test;INSERT INTO test.t1 (i) VALUES(101);INSERT INTO t1 (i)VALUES(102);INSERT INTO db2.t2 (j)VALUES(201);USE db2;INSERT INTO test.t1 (i) VALUES(103);INSERT INTO db2.t2 (j)VALUES(202);INSERT INTO t2 (j)VALUES(203);mysqlbinlog --database=test does not output the first twoINSERT statements because there is no default database.It outputs the three INSERT statements following USEtest, but not the three INSERT statements following USEdb2.mysqlbinlog --database=db2 does not output the first twoINSERT statements because there is no default database.It does not output the three INSERT statements followingUSE test, but does output the three INSERT statementsfollowing USE db2.Row-based logging.mysqlbinlog outputs only entries thatchange tables belonging to db_name. The default databasehas no effect on this. Suppose that the binary log justdescribed was created using row-based logging rather thanstatement-based logging.mysqlbinlog --database=testoutputs only those entries that modify t1 in the testdatabase, regardless of whether USE was issued or whatthe default database is.If a server is running withbinlog_format set to MIXED and you want it to be possibleto use mysqlbinlog with the --database option, you mustensure that tables that are modified are in the databaseselected by USE. (In particular, no cross-databaseupdates should be used.)NoteThis option did not work correctly for mysqlbinlogwith row-based logging prior to MySQL 5.1.37.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is\u00b4d:t:o,/tmp/mysqlbinlog.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--defaults-extra-file=nameRead this file after the global files are read.\u2022--defaults-file=nameOnly read default options from the given file.\u2022--default-auth=nameDefault authentication client-side plugin to use.\u2022--disable-log-bin, -DDisable binary logging. This is useful for avoiding anendless loop if you use the --to-last-log option and aresending the output to the same MariaDB server. This optionalso is useful when restoring after a crash to avoidduplication of the statements you have logged.This option requires that you have the SUPER privilege. Itcauses mysqlbinlog to include a SET sql_log_bin = 0 statementin its output to disable binary logging of the remainingoutput. The SET statement is ineffective unless you have theSUPER privilege.\u2022--force-if-openForce if binlog was not closed properly. Defaults to on; use--skip-force-if-open to disable.\u2022--force-read, -fWith this option, if mysqlbinlog reads a binary log eventthat it does not recognize, it prints a warning, ignores theevent, and continues. Without this option, mysqlbinlog stopsif it reads such an event.\u2022--hexdump, -HDisplay a hex dump of the log in comments, as described inthe section called \u201cMYSQLBINLOG HEX DUMP FORMAT\u201d. The hexoutput can be helpful for replication debugging.\u2022--host=host_name, -h host_nameGet the binary log from the MariaDB server on the given host.\u2022--local-load=path, -l pathPrepare local temporary files for LOAD DATA INFILE in thespecified directory.\u2022--no-defaultsDon't read default options from any option file.\u2022--offset=N, -o NSkip the first N entries in the log.\u2022--open-files-limit=NUMSets the open_files_limit variable, which is used to reservefile descriptors for mysqlbinlog.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlbinlog prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--print-defaultsPrint the program argument list from all option files andexit.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for connecting to a remoteserver, or 0 for default to, in order of preference, my.cnf,$MYSQL_TCP_PORT, /etc/services, built-in default (3306).Forces --protocol=tcp when specified on the command linewithout other connection properties.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--rawRequires -R. Output raw binlog data instead of SQLstatements. Output files named after server logs.\u2022--read-from-remote-server, -RRead the binary log from a MariaDB server rather than readinga local log file. Any connection parameter options areignored unless this option is given as well. These optionsare --host, --password, --port, --protocol, --socket, and--user.This option requires that the remote server be running. Itworks only for binary log files on the remote server, notrelay log files.\u2022--result-file=name, -r nameDirect output to the given file. With --raw this is a prefixfor the file names.\u2022--rewrite-db=name, -r nameUpdates to a database with a different name than theoriginal.Example: rewrite-db='from->to'. For events thatare binlogged as statements, rewriting the databaseconstitutes changing a statement's default database from db1to db2. There is no statement analysis or rewrite of anykind, that is, if one specifies \"db1.tbl\" in the statementexplicitly, that occurrence won't be changed to \"db2.tbl\".Row-based events are rewritten correctly to use the newdatabase name. Filtering (e.g. with --database=name) happensafter the database rewrites have been performed. If you usethis option on the command line and \">\" has a special meaningto your command interpreter, quote the value (e.g. --rewrite-db=\"oldname->newname\".\u2022--server-id=idDisplay only those events created by the server having thegiven server ID.\u2022--set-charset=charset_nameAdd a SET NAMES charset_name statement to the output tospecify the character set to be used for processing logfiles.\u2022--short-form, -sDisplay only the statements contained in the log, no extrainfo and no row-based events. This is for testing only, andshould not be used in production systems. If you want tosuppress base64-output, consider using --base64-output=neverinstead.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--start-datetime=datetimeStart reading the binary log at the first event having atimestamp equal to or later than the datetime argument. Thedatetime value is relative to the local time zone on themachine where you run mysqlbinlog. The value should be in aformat accepted for the DATETIME or TIMESTAMP data types. Forexample:shell> mysqlbinlog --start-datetime=\"2014-12-25 11:25:56\" binlog.000003This option is useful for point-in-time recovery.\u2022--start-position=N, -j NStart reading the binary log at N. Type can either be apositive integer or a GTID list. When using a positiveinteger, the value only applies to the first binlog passed onthe command line, and the first event that has a positionequal to or greater than N is printed. In GTID mode, multipleGTIDs can be passed as a comma separated list, where eachmust have a unique domain id. The list represents the gtidbinlog state that the client (another \"replica\" server) isaware of. Therefore, each GTID is exclusive; only eventsafter a given sequence number will be printed to allow usersto receive events after their current state.This option is useful for point-in-time recovery.\u2022--gtid-strict-modeProcess binlog according to gtid-strict-mode specification.The start, stop positions are verified to satisfy start <stop comparison condition. Sequence numbers of any gtiddomain must comprise monotically growing sequence.\u2022--stop-datetime=datetimeStop reading the binary log at the first event having atimestamp equal to or later than the datetime argument. Thisoption is useful for point-in-time recovery. See thedescription of the --start-datetime option for informationabout the datetime value.This option is useful for point-in-time recovery.\u2022--stop-neverWait for more data from the server instead of stopping at theend of the last log. Implies --to-last-log.\u2022--stop-never-slave-server-idThe slave server_id used for --read-from-remote-server--stop-never.\u2022--stop-position=NStop reading the binary log at the first event having aposition equal to or greater than N. Type can either be apositive integer or a GTID list. When using a positiveinteger, the value only applies to the last log file named onthe command line. When in GTID mode, multiple GTIDs can bepassed as a comma separated list, where each must have aunique domain id.Each GTID is inclusive; only events up tothe given sequence numbers are printed.This option is useful for point-in-time recovery.\u2022--table, -TList entries for just this table (local log only).\u2022--to-last-log, -tDo not stop at the end of the requested binary log from aMariaDB server, but rather continue printing until the end ofthe last binary log. If you send the output to the sameMariaDB server, this may lead to an endless loop, so thisoption requires --read-from-remote-server.\u2022--user=user_name, -u user_nameThe MariaDB username to use when connecting to a remoteserver.\u2022--verbose, -vReconstruct row events and display them as commented SQLstatements. If this option is given twice, the outputincludes comments to indicate column data types and somemetadata.If this option is given three times, the outputincludes diagnostic warnings about event integrity beforeprogram exit.For examples that show the effect of --base64-output and--verbose on row event output, see the section called\u201cMYSQLBINLOG ROW EVENT DISPLAY\u201d.\u2022--version, -VDisplay version information and exit.You can also set the following variable by using --var_name=valuesyntax:\u2022open_files_limitSpecify the number of open file descriptors to reserve.You can pipe the output of mysqlbinlog into the mysql client toexecute the events contained in the binary log. This technique isused to recover from a crash when you have an old backup. Forexample:shell> mysqlbinlog binlog.000001 | mysql -u root -pOr:shell> mysqlbinlog binlog.[0-9]* | mysql -u root -pYou can also redirect the output of mysqlbinlog to a text fileinstead, if you need to modify the statement log first (forexample, to remove statements that you do not want to execute forsome reason). After editing the file, execute the statements thatit contains by using it as input to the mysql program:shell> mysqlbinlog binlog.000001 > tmpfileshell> ... edit tmpfile ...shell> mysql -u root -p < tmpfileWhen mysqlbinlog is invoked with the --start-position option, itdisplays only those events with an offset in the binary loggreater than or equal to a given position (the given positionmust match the start of one event). It also has options to stopand start when it sees an event with a given date and time. Thisenables you to perform point-in-time recovery using the--stop-datetime option (to be able to say, for example, \u201crollforward my databases to how they were today at 10:30 a.m.\u201d).If you have more than one binary log to execute on the MariaDBserver, the safe method is to process them all using a singleconnection to the server. Here is an example that demonstrateswhat may be unsafe:shell> mysqlbinlog binlog.000001 | mysql -u root -p # DANGER!!shell> mysqlbinlog binlog.000002 | mysql -u root -p # DANGER!!Processing binary logs this way using different connections tothe server causes problems if the first log file contains aCREATE TEMPORARY TABLE statement and the second log contains astatement that uses the temporary table. When the first mysqlprocess terminates, the server drops the temporary table. Whenthe second mysql process attempts to use the table, the serverreports \u201cunknown table.\u201dTo avoid problems like this, use a single mysql process toexecute the contents of all binary logs that you want to process.Here is one way to do so:shell> mysqlbinlog binlog.000001 binlog.000002 | mysql -u root -pAnother approach is to write all the logs to a single file andthen process the file:shell> mysqlbinlog binlog.000001 >/tmp/statements.sqlshell> mysqlbinlog binlog.000002 >> /tmp/statements.sqlshell> mysql -u root -p -e \"source /tmp/statements.sql\"mysqlbinlog can produce output that reproduces a LOAD DATA INFILEoperation without the original data file.mysqlbinlog copies thedata to a temporary file and writes a LOAD DATA LOCAL INFILEstatement that refers to the file. The default location of thedirectory where these files are written is system-specific. Tospecify a directory explicitly, use the --local-load option.Because mysqlbinlog converts LOAD DATA INFILE statements to LOADDATA LOCAL INFILE statements (that is, it adds LOCAL), both theclient and the server that you use to process the statements mustbe configured with the LOCAL capability enabled.WarningThe temporary files created for LOAD DATA LOCAL statementsare not automatically deleted because they are needed untilyou actually execute those statements. You should delete thetemporary files yourself after you no longer need thestatement log. The files can be found in the temporary filedirectory and have names like original_file_name-#-#.",
        "name": "mariadb-binlog - utility for processing binary log files(mysqlbinlog is now a symlink to mariadb-binlog)",
        "section": 1
    },
    {
        "command": "mysqlcheck",
        "description": "The mysqlcheck client performs table maintenance: It checks,repairs, optimizes, or analyzes tables.Each table is locked and therefore unavailable to other sessionswhile it is being processed, although for check operations, thetable is locked with a READ lock only. Table maintenanceoperations can be time-consuming, particularly for large tables.If you use the --databases or --all-databases option to processall tables in one or more databases, an invocation of mysqlcheckmight take a long time. (This is also true for mysql_upgradebecause that program invokes mysqlcheck to check all tables andrepair them if necessary.)mysqlcheck is similar in function to myisamchk, but worksdifferently. The main operational difference is that mysqlcheckmust be used when the mysqld server is running, whereas myisamchkshould be used when it is not. The benefit of using mysqlcheck isthat you do not have to stop the server to perform tablemaintenance.mysqlcheck uses the SQL statements CHECK TABLE, REPAIR TABLE,ANALYZE TABLE, and OPTIMIZE TABLE in a convenient way for theuser. It determines which statements to use for the operation youwant to perform, and then sends the statements to the server tobe executed.The MyISAM storage engine supports all four maintenanceoperations, so mysqlcheck can be used to perform any of them onMyISAM tables. Other storage engines do not necessarily supportall operations. In such cases, an error message is displayed. Forexample, if test.t is a MEMORY table, an attempt to check itproduces this result:shell> mysqlcheck test ttest.tnote: The storage engine for the table doesn\u00b4t support checkIf mysqlcheck is unable to repair a table, see the MariaDBKnowledge Base for manual table repair strategies. This will bethe case, for example, for InnoDB tables, which can be checkedwith CHECK TABLE, but not repaired with REPAIR TABLE.The use of mysqlcheck with partitioned tables is not supported.CautionIt is best to make a backup of a table before performing atable repair operation; under some circumstances theoperation might cause data loss. Possible causes include butare not limited to file system errors.There are three general ways to invoke mysqlcheck:shell> mysqlcheck [options] db_name [tbl_name ...]shell> mysqlcheck [options] --databases db_name ...shell> mysqlcheck [options] --all-databasesIf you do not name any tables following db_name or if you use the--databases or --all-databases option, entire databases arechecked.mysqlcheck has a special feature compared to other clientprograms. The default behavior of checking tables (--check) canbe changed by renaming the binary. If you want to have a toolthat repairs tables by default, you should just make a copy ofmysqlcheck named mysqlrepair, or make a symbolic link tomysqlcheck named mysqlrepair. If you invoke mysqlrepair, itrepairs tables.The following names can be used to change mysqlcheck defaultbehavior.\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502mysqlrepair\u2502 The default option is \u2502\u2502\u2502 --repair\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502mysqlanalyze\u2502 The default option is \u2502\u2502\u2502 --analyze\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502mysqloptimize \u2502 The default option is \u2502\u2502\u2502 --optimize\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518mysqlcheck supports the following options, which can be specifiedon the command line or in the [mysqlcheck] and [client] optionfile groups.The -c, -r, -a and -o options are exclusive to eachother.\u2022--help, -?Display a help message and exit.\u2022--all-databases, -ACheck all tables in all databases. This is the same as usingthe --databases option and naming all the databases on thecommand line.\u2022--all-in-1, -1Instead of issuing a statement for each table, execute asingle statement for each database that names all the tablesfrom that database to be processed.\u2022--analyze, -aAnalyze the tables.\u2022--auto-repairIf a checked table is corrupted, automatically fix it. Anynecessary repairs are done after all tables have beenchecked.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--check, -cCheck the tables for errors. This is the default operation.\u2022--check-only-changed, -CCheck only tables that have changed since the last check orthat have not been closed properly.\u2022--check-upgrade, -gInvoke CHECK TABLE with the FOR UPGRADE option to checktables for incompatibilities with the current version of theserver. This option automatically enables the --fix-db-namesand --fix-table-names options.\u2022--compressCompress all information sent between the client and theserver if both support compression.\u2022--databases, -BProcess all tables in the named databases. Normally,mysqlcheck treats the first name argument on the command lineas a database name and following names as table names. Withthis option, it treats all name arguments as database names.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is \u00b4d:t:o\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-auth=nameDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--extended, -eIf you are using this option to check tables, it ensures thatthey are 100% consistent but takes a long time.If you are using this option to repair tables, it will forceusing the old, slow, repair with keycache method, instead ofthe much faster repair by sorting.\u2022--fast, -FCheck only tables that have not been closed properly.\u2022--fix-db-namesConvert database names to the format used since MySQL 5.1.Only database names that contain special characters areaffected.\u2022--fix-table-namesConvert table names (including views) to the format usedsince MySQL 5.1. Only table names that contain specialcharacters are affected.\u2022--flush,Flush each table after check. This is useful if you don'twant to have the checked tables take up space in the cachesafter the check.\u2022--force, -fContinue even if an SQL error occurs.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--medium-check, -mDo a check that is faster than an --extended operation. Thisfinds only 99.99% of all errors, which should be good enoughin most cases.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--optimize, -oOptimize the tables.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlcheck prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--persistent, -ZUsed with ANALYZE TABLE to append the option PERSISENT FORALL.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dir=nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--process-tablesPerform the requested operation on tables. Defaults to on;use --skip-process-tables to disable.\u2022--process-views=valPerform the requested operation (only CHECK VIEW or REPAIRVIEW). Possible values are NO, YES (correct the checksum, ifnecessary, add the mariadb-version field), UPGRADE_FROM_MYSQL(same as YES and toggle the algorithm MERGE<->TEMPTABLE.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--quick, -qIf you are using this option to check tables, it prevents thecheck from scanning the rows to check for incorrect links.This is the fastest check method.If you are using this option to repair tables, it tries torepair only the index tree. This is the fastest repairmethod.\u2022--repair, -rPerform a repair that can fix almost anything except uniquekeys that are not unique.\u2022--silent, -sSilent mode. Print only error messages.\u2022--skip-database=db_nameDon't process the database (case-sensitive) specified asargument.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--tablesOverride the --databases or -B option. All name argumentsfollowing the option are regarded as table names.\u2022--use-frmFor repair operations on MyISAM tables, get the tablestructure from the .frm file so that the table can berepaired even if the .MYI header is corrupted.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print information about the various stages ofprogram operation.Using one --verbose option will give youmore information about what mysqlcheck is doing.Using two --verbose options will also give you connectioninformation.Using it 3 times will print out all CHECK, RENAME and ALTERTABLE during the check phase.\u2022--version, -VDisplay version information and exit.\u2022--write-binlogThis option is enabled by default, so that ANALYZE TABLE,OPTIMIZE TABLE, and REPAIR TABLE statements generated bymysqlcheck are written to the binary log. Use--skip-write-binlog to cause NO_WRITE_TO_BINLOG to be addedto the statements so that they are not logged. Use the--skip-write-binlog when these statements should not be sentto replication slaves or run when using the binary logs forrecovery from backup.",
        "name": "mariadb-check - a table maintenance program (mysqlcheck is now asymlink to mariadb-check)",
        "section": 1
    },
    {
        "command": "mysqld_multi",
        "description": "mysqld_multi is designed to manage several mysqld processes thatlisten for connections on different Unix socket files and TCP/IPports. It can start or stop servers, or report their currentstatus.mysqld_multi searches for groups named [mysqldN] in my.cnf (or inthe file named by the --config-file option).N can be anypositive integer. This number is referred to in the followingdiscussion as the option group number, or GNR. Group numbersdistinguish option groups from one another and are used asarguments to mysqld_multi to specify which servers you want tostart, stop, or obtain a status report for. Options listed inthese groups are the same that you would use in the [mysqld]group used for starting mysqld. However, when using multipleservers, it is necessary that each one use its own value foroptions such as the Unix socket file and TCP/IP port number.To invoke mysqld_multi, use the following syntax:shell> mysqld_multi [options] {start|stop|report} [GNR[,GNR] ...]start, stop, and report indicate which operation to perform. Youcan perform the designated operation for a single server ormultiple servers, depending on the GNR list that follows theoption name. If there is no list, mysqld_multi performs theoperation for all servers in the option file.Each GNR value represents an option group number or range ofgroup numbers. The value should be the number at the end of thegroup name in the option file. For example, the GNR for a groupnamed [mysqld17] is 17. To specify a range of numbers, separatethe first and last numbers by a dash. The GNR value 10-13represents groups [mysqld10] through [mysqld13]. Multiple groupsor group ranges can be specified on the command line, separatedby commas. There must be no whitespace characters (spaces ortabs) in the GNR list; anything after a whitespace character isignored.This command starts a single server using option group[mysqld17]:shell> mysqld_multi start 17This command stops several servers, using option groups [mysqld8]and [mysqld10] through [mysqld13]:shell> mysqld_multi stop 8,10-13For an example of how you might set up an option file, use thiscommand:shell> mysqld_multi --examplemysqld_multi searches for option files as follows:\u2022With --no-defaults, no option files are read.\u2022With --defaults-file=file_name, only the named file is read.\u2022Otherwise, option files in the standard list of locations areread, including any file named by the--defaults-extra-file=file_name option, if one is given. (Ifthe option is given multiple times, the last value is used.)Option files read are searched for [mysqld_multi] and [mysqldN]option groups. The [mysqld_multi] group can be used for optionsto mysqld_multi itself.[mysqldN] groups can be used for optionspassed to specific mysqld instances.The [mysqld] or [mysqld_safe] groups can be used for commonoptions read by all instances of mysqld or mysqld_safe. You canspecify a --defaults-file=file_name option to use a differentconfiguration file for that instance, in which case the [mysqld]or [mysqld_safe] groups from that file will be used for thatinstance.mysqld_multi supports the following options.\u2022--helpDisplay a help message and exit.\u2022--exampleDisplay a sample option file.\u2022--log=file_nameSpecify the name of the log file. If the file exists, logoutput is appended to it.\u2022--mysqladmin=prog_nameThe mysqladmin binary to be used to stop servers.\u2022--mysqld=prog_nameThe mysqld binary to be used. Note that you can specifymysqld_safe as the value for this option also. If you usemysqld_safe to start the server, you can include the mysqldor ledir options in the corresponding [mysqldN] option group.These options indicate the name of the server thatmysqld_safe should start and the path name of the directorywhere the server is located. (See the descriptions for theseoptions in mysqld_safe(1).) Example:[mysqld38]mysqld = mysqld-debugledir= /opt/local/mysql/libexec\u2022--no-logPrint log information to stdout rather than to the log file.By default, output goes to the log file.\u2022--password=passwordThe password of the MariaDB account to use when invokingmysqladmin. Note that the password value is not optional forthis option, unlike for other MariaDB programs.\u2022--silentSilent mode; disable warnings.\u2022--tcp-ipConnect to the MariaDB server(s) via the TCP/IP port insteadof the UNIX socket. This affects stopping and reporting. If asocket file is missing, the server may still be running, butcan be accessed only via the TCP/IP port. By defaultconnecting is done via the UNIX socket. This option affectsstop and report operations.\u2022--user=user_nameThe user name of the MariaDB account to use when invokingmysqladmin.\u2022--verboseBe more verbose.\u2022--versionDisplay version information and exit.\u2022--wsrep-new-clusterBootstrap a cluster.Some notes about mysqld_multi:\u2022Most important: Before using mysqld_multi be sure that youunderstand the meanings of the options that are passed to themysqld servers and why you would want to have separate mysqldprocesses. Beware of the dangers of using multiple mysqldservers with the same data directory. Use separate datadirectories, unless you know what you are doing. Startingmultiple servers with the same data directory does not giveyou extra performance in a threaded system.\u2022Important: Make sure that the data directory for each serveris fully accessible to the Unix account that the specificmysqld process is started as.Do not use the Unix rootaccount for this, unless you know what you are doing.\u2022Make sure that the MariaDB account used for stopping themysqld servers (with the mysqladmin program) has the sameuser name and password for each server. Also, make sure thatthe account has the SHUTDOWN privilege. If the servers thatyou want to manage have different user names or passwords forthe administrative accounts, you might want to create anaccount on each server that has the same user name andpassword. For example, you might set up a common multi_adminaccount by executing the following commands for each server:shell> mysql -u root -S /tmp/mysql.sock -pEnter password:mysql> GRANT SHUTDOWN ON *.*-> TO \u00b4multi_admin\u00b4@\u00b4localhost\u00b4 IDENTIFIED BY \u00b4multipass\u00b4;Change the connection parameters appropriately whenconnecting to each one. Note that the host name part of theaccount name must allow you to connect as multi_admin fromthe host where you want to run mysqld_multi.\u2022The Unix socket file and the TCP/IP port number must bedifferent for every mysqld. (Alternatively, if the host hasmultiple network addresses, you can use --bind-address tocause different servers to listen to different interfaces.)\u2022The --pid-file option is very important if you are usingmysqld_safe to start mysqld (for example,--mysqld=mysqld_safe) Every mysqld should have its ownprocess ID file. The advantage of using mysqld_safe insteadof mysqld is that mysqld_safe monitors its mysqld process andrestarts it if the process terminates due to a signal sentusing kill -9 or for other reasons, such as a segmentationfault. Please note that the mysqld_safe script might requirethat you start it from a certain place. This means that youmight have to change location to a certain directory beforerunning mysqld_multi. If you have problems starting, pleasesee the mysqld_safe script. Check especially the lines:----------------------------------------------------------------MY_PWD=`pwd`# Check if we are starting this relative (for the binary release)if test -d $MY_PWD/data/mysql -a \\-f ./share/mysql/english/errmsg.sys -a \\-x ./bin/mysqld----------------------------------------------------------------The test performed by these lines should be successful, oryou might encounter problems. See mysqld_safe(1).\u2022You might want to use the --user option for mysqld, but to dothis you need to run the mysqld_multi script as the Unix rootuser. Having the option in the option file doesn\u00b4t matter;you just get a warning if you are not the superuser and themysqld processes are started under your own Unix account.The following example shows how you might set up an option filefor use with mysqld_multi. The order in which the mysqld programsare started or stopped depends on the order in which they appearin the option file. Group numbers need not form an unbrokensequence. The first and fifth [mysqldN] groups were intentionallyomitted from the example to illustrate that you can have \u201cgaps\u201din the option file. This gives you more flexibility.# This file should probably be in your home dir (~/.my.cnf)# or /etc/my.cnf# Version 2.1 by Jani Tolonen[mysqld_multi]mysqld= /usr/local/bin/mysqld_safemysqladmin = /usr/local/bin/mysqladminuser= multi_adminpassword= multipass[mysqld2]socket= /tmp/mysql.sock2port= 3307pid-file= /usr/local/mysql/var2/hostname.pid2datadir= /usr/local/mysql/var2language= /usr/local/share/mysql/englishuser= john[mysqld3]socket= /tmp/mysql.sock3port= 3308pid-file= /usr/local/mysql/var3/hostname.pid3datadir= /usr/local/mysql/var3language= /usr/local/share/mysql/swedishuser= monty[mysqld4]socket= /tmp/mysql.sock4port= 3309pid-file= /usr/local/mysql/var4/hostname.pid4datadir= /usr/local/mysql/var4language= /usr/local/share/mysql/estoniauser= tonu[mysqld6]socket= /tmp/mysql.sock6port= 3311pid-file= /usr/local/mysql/var6/hostname.pid6datadir= /usr/local/mysql/var6language= /usr/local/share/mysql/japaneseuser= jani",
        "name": "mariadbd-multi - manage multiple MariaDB servers (mysqld_multi isnow a symlink to mariadbd-multi)",
        "section": 1
    },
    {
        "command": "mysqld_safe",
        "description": "mysqld_safe is the recommended way to start a mysqld server onUnix.mysqld_safe adds some safety features such as restartingthe server when an error occurs and logging runtime informationto an error log file. Descriptions of error logging is givenlater in this section.mysqld_safe tries to start an executable named mysqld. Tooverride the default behavior and specify explicitly the name ofthe server you want to run, specify a --mysqld or--mysqld-version option to mysqld_safe. You can also use --ledirto indicate the directory where mysqld_safe should look for theserver.Many of the options to mysqld_safe are the same as the options tomysqld.Options unknown to mysqld_safe are passed to mysqld if they arespecified on the command line, but ignored if they are specifiedin the [mysqld_safe] or [mariadb_safe] groups of an option file.mysqld_safe reads all options from the [mysqld], [server],[mysqld_safe], and [mariadb_safe] sections in option files. Forexample, if you specify a [mysqld] section like this, mysqld_safewill find and use the --log-error option:[mysqld]log-error=error.logFor backward compatibility, mysqld_safe also reads [safe_mysqld]sections, although you should rename such sections to[mysqld_safe] in current installations.mysqld_safe supports the options in the following list. It alsoreads option files and supports the options for processing them.\u2022--helpDisplay a help message and exit.\u2022--basedir=pathThe path to the MariaDB installation directory.\u2022--core-file-size=sizeThe size of the core file that mysqld should be able tocreate. The option value is passed to ulimit -c.\u2022--crash-script=fileScript to call in the event of mysqld crashing.\u2022--datadir=pathThe path to the data directory.\u2022--defaults-extra-file=pathThe name of an option file to be read in addition to theusual option files. This must be the first option on thecommand line if it is used. If the file does not exist or isotherwise inaccessible, the server will exit with an error.\u2022--defaults-file=file_nameThe name of an option file to be read instead of the usualoption files. This must be the first option on the commandline if it is used.\u2022--flush-cachesFlush and purge buffers/caches before starting the server.\u2022--ledir=pathIf mysqld_safe cannot find the server, use this option toindicate the path name to the directory where the server islocated.\u2022--log-error=file_nameWrite the error log to the given file.\u2022--malloc-lib=libPreload shared library lib if available.\u2022--mysqld=prog_nameThe name of the server program (in the ledir directory) thatyou want to start. This option is needed if you use theMariaDB binary distribution but have the data directoryoutside of the binary distribution. If mysqld_safe cannotfind the server, use the --ledir option to indicate the pathname to the directory where the server is located.\u2022--mysqld-version=suffixThis option is similar to the --mysqld option, but youspecify only the suffix for the server program name. Thebasename is assumed to be mysqld. For example, if you use--mysqld-version=debug, mysqld_safe starts the mysqld-debugprogram in the ledir directory. If the argument to--mysqld-version is empty, mysqld_safe uses mysqld in theledir directory.\u2022--nice=priorityUse the nice program to set the server\u00b4s scheduling priorityto the given value.\u2022--no-auto-restart, --nowatch, --no-watchExit after starting mysqld.\u2022--no-defaultsDo not read any option files. This must be the first optionon the command line if it is used.\u2022--numa-interleaveRun mysqld with its memory interleaved on all NUMA nodes.\u2022--open-files-limit=countThe number of files that mysqld should be able to open. Theoption value is passed to ulimit -n. Note that you need tostart mysqld_safe as root for this to work properly!\u2022--pid-file=file_nameThe path name of the process ID file.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_numThe port number that the server should use when listening forTCP/IP connections. The port number must be 1024 or higherunless the server is started by the root system user.\u2022--skip-kill-mysqldDo not try to kill stray mysqld processes at startup. Thisoption works only on Linux.\u2022--socket=pathThe Unix socket file that the server should use whenlistening for local connections.\u2022--syslog, --skip-syslog--syslog causes error messages to be sent to syslog onsystems that support the logger program.--skip-syslogsuppresses the use of syslog; messages are written to anerror log file.\u2022--syslog-tag=tagFor logging to syslog, messages from mysqld_safe and mysqldare written with a tag of mysqld_safe and mysqld,respectively. To specify a suffix for the tag, use--syslog-tag=tag, which modifies the tags to bemysqld_safe-tag and mysqld-tag.\u2022--timezone=timezoneSet the TZ time zone environment variable to the given optionvalue. Consult your operating system documentation for legaltime zone specification formats.\u2022--user={user_name|user_id}Run the mysqld server as the user having the name user_nameor the numeric user ID user_id. (\u201cUser\u201d in this contextrefers to a system login account, not a MariaDB user listedin the grant tables.)If you execute mysqld_safe with the --defaults-file or--defaults-extra-file option to name an option file, the optionmust be the first one given on the command line or the optionfile will not be used. For example, this command will not use thenamed option file:mysql> mysqld_safe --port=port_num --defaults-file=file_nameInstead, use the following command:mysql> mysqld_safe --defaults-file=file_name --port=port_numThe mysqld_safe script is written so that it normally can start aserver that was installed from either a source or a binarydistribution of MariaDB, even though these types of distributionstypically install the server in slightly different locations.mysqld_safe expects one of the following conditions to be true:\u2022The server and databases can be found relative to the workingdirectory (the directory from which mysqld_safe is invoked).For binary distributions, mysqld_safe looks under its workingdirectory for bin and data directories. For sourcedistributions, it looks for libexec and var directories. Thiscondition should be met if you execute mysqld_safe from yourMariaDB installation directory (for example, /usr/local/mysqlfor a binary distribution).\u2022If the server and databases cannot be found relative to theworking directory, mysqld_safe attempts to locate them byabsolute path names. Typical locations are /usr/local/libexecand /usr/local/var. The actual locations are determined fromthe values configured into the distribution at the time itwas built. They should be correct if MariaDB is installed inthe location specified at configuration time.Because mysqld_safe tries to find the server and databasesrelative to its own working directory, you can install a binarydistribution of MariaDB anywhere, as long as you run mysqld_safefrom the MariaDB installation directory:shell> cd mysql_installation_directoryshell> bin/mysqld_safe &If mysqld_safe fails, even when invoked from the MariaDBinstallation directory, you can specify the --ledir and --datadiroptions to indicate the directories in which the server anddatabases are located on your system.When you use mysqld_safe to start mysqld, mysqld_safe arrangesfor error (and notice) messages from itself and from mysqld to goto the same destination.There are several mysqld_safe options for controlling thedestination of these messages:\u2022--syslog: Write error messages to syslog on systems thatsupport the logger program.\u2022--skip-syslog: Do not write error messages to syslog.Messages are written to the default error log file(host_name.err in the data directory), or to a named file ifthe --log-error option is given.\u2022--log-error=file_name: Write error messages to the namederror file.If none of these options is given, the default is --skip-syslog.NoteIf --syslog and --log-error are both given, a warning is issuedand --log-error takes precedence.When mysqld_safe writes a message, notices go to the loggingdestination (syslog or the error log file) and stdout. Errors goto the logging destination and stderr.Normally, you should not edit the mysqld_safe script. Instead,configure mysqld_safe by using command-line options or options inthe [mysqld_safe] section of a my.cnf option file. In rare cases,it might be necessary to edit mysqld_safe to get it to start theserver properly. However, if you do this, your modified versionof mysqld_safe might be overwritten if you upgrade MariaDB in thefuture, so you should make a copy of your edited version that youcan reinstall.On NetWare, mysqld_safe is a NetWare Loadable Module (NLM) thatis ported from the original Unix shell script. It starts theserver as follows:1. Runs a number of system and option checks.2. Runs a check on MyISAM tables.3. Provides a screen presence for the MariaDB server.4. Starts mysqld, monitors it, and restarts it if it terminatesin error.5. Sends error messages from mysqld to the host_name.err file inthe data directory.6. Sends mysqld_safe screen output to the host_name.safe file inthe data directory.",
        "name": "mariadbd-safe - MariaDB server startup script (mysqld_safe is nowa symlink to mariadbd-safe)",
        "section": 1
    },
    {
        "command": "mysqld_safe_helper",
        "description": "Use: Helper script.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "mariadbd-safe-helper - helper script (mysqld_safe_helper is now asymlink to mariadbd-safe-helper)",
        "section": 1
    },
    {
        "command": "mysqldump",
        "description": "The mysqldump client is a backup program originally written byIgor Romanenko. It can be used to dump a database or a collectionof databases for backup or transfer to another SQL server (notnecessarily a MariaDB server). The dump typically contains SQLstatements to create the table, populate it, or both. However,mysqldump can also be used to generate files in CSV, otherdelimited text, or XML format.If you are doing a backup on the server and your tables all areMyISAM tables, consider using the mysqlhotcopy instead because itcan accomplish faster backups and faster restores. Seemysqlhotcopy(1).There are four general ways to invoke mysqldump:shell> mysqldump [options] db_name [tbl_name ...]shell> mysqldump [options] --databases db_name ...shell> mysqldump [options] --all-databasesshell> mysqldump [options] --system={options}If you do not name any tables following db_name or if you use the--databases or --all-databases option, entire databases aredumped.mysqldump does not dump the INFORMATION_SCHEMA orperformance_schema databases by default. To dump these, name themexplicitly on the command line, although you must also use the--skip-lock-tables option.To see a list of the options your version of mysqldump supports,execute mysqldump --help.Some mysqldump options are shorthand for groups of other options:\u2022Use of --opt is the same as specifying --add-drop-table,--add-locks, --create-options, --disable-keys,--extended-insert, --lock-tables, --quick, and --set-charset.All of the options that --opt stands for also are on bydefault because --opt is on by default.\u2022Use of --compact is the same as specifying--skip-add-drop-table, --skip-add-locks, --skip-comments,--skip-disable-keys, and --skip-set-charset options.To reverse the effect of a group option, uses its --skip-xxx form(--skip-opt or --skip-compact). It is also possible to selectonly part of the effect of a group option by following it withoptions that enable or disable specific features. Here are someexamples:\u2022To select the effect of --opt except for some features, usethe --skip option for each feature. To disable extendedinserts and memory buffering, use --opt--skip-extended-insert --skip-quick. (Actually,--skip-extended-insert --skip-quick is sufficient because--opt is on by default.)\u2022To reverse --opt for all features except index disabling andtable locking, use --skip-opt --disable-keys --lock-tables.When you selectively enable or disable the effect of a groupoption, order is important because options are processed first tolast. For example, --disable-keys --lock-tables --skip-opt wouldnot have the intended effect; it is the same as --skip-opt byitself.mysqldump can retrieve and dump table contents row by row, or itcan retrieve the entire content from a table and buffer it inmemory before dumping it. Buffering in memory can be a problem ifyou are dumping large tables. To dump tables row by row, use the--quick option (or --opt, which enables --quick). The --optoption (and hence --quick) is enabled by default, so to enablememory buffering, use --skip-quick.If you are using a recent version of mysqldump to generate a dumpto be reloaded into a very old MySQL server, you should not usethe --opt or --extended-insert option. Use --skip-opt instead.mysqldump supports the following options, which can be specifiedon the command line or in the [mysqldump] and [client] optionfile groups.mysqldump also supports the options for processingoption file.\u2022--help, -?Display a help message and exit.\u2022--add-drop-databaseAdd a DROP DATABASE statement before each CREATE DATABASEstatement. This option is typically used in conjunction withthe --all-databases or --databases option because no CREATEDATABASE statements are written unless one of those optionsis specified.\u2022--add-drop-tableAdd a DROP TABLE statement before each CREATE TABLEstatement.\u2022--add-drop-triggerAdd a DROP TRIGGER statement before each CREATE TRIGGERstatement.\u2022--add-locksSurround each table dump with LOCK TABLES and UNLOCK TABLESstatements. This results in faster inserts when the dump fileis reloaded.\u2022--all-databases, -ADump all tables in all databases. This is the same as usingthe --databases option and naming all the databases on thecommand line.\u2022--all-tablespaces, -YAdds to a table dump all SQL statements needed to create anytablespaces used by an NDBCLUSTER table. This information isnot otherwise included in the output from mysqldump. Thisoption is currently relevant only to MySQL Cluster tables.\u2022--allow-keywordsAllow creation of column names that are keywords. This worksby prefixing each column name with the table name.\u2022--apply-slave-statementsAdds 'STOP SLAVE' prior to 'CHANGE MASTER' and 'START SLAVE'to bottom of dump.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--comments, -iWrite additional information in the dump file such as programversion, server version, and host. This option is enabled bydefault. To suppress this additional information, use--skip-comments.\u2022--compactProduce more compact output. This option enables the--skip-add-drop-table, --skip-add-locks, --skip-comments,--skip-disable-keys, and --skip-set-charset options.\u2022--compatible=nameProduce output that is more compatible with other databasesystems or with older MySQL servers. The value of name can beansi, mysql323, mysql40, postgresql, oracle, mssql, db2,maxdb, no_key_options, no_table_options, or no_field_options.To use several values, separate them by commas. These valueshave the same meaning as the corresponding options forsetting the server SQL mode.This option does not guarantee compatibility with otherservers. It only enables those SQL mode values that arecurrently available for making dump output more compatible.For example, --compatible=oracle does not map data types toOracle types or use Oracle comment syntax.\u2022--complete-insert, -cUse complete INSERT statements that include column names.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--copy-s3-tablesBy default S3 tables are ignored. With this option set, theresult file will contain a CREATE statement for a similarAria table, followed by the table data and ending with anALTER TABLE xxx ENGINE=S3.\u2022--create-options, -aInclude all MariaDB-specific table options in the CREATETABLE statements. Use --skip-create-options to disable.\u2022--databases, -BDump several databases. Normally, mysqldump treats the firstname argument on the command line as a database name andfollowing names as table names. With this option, it treatsall name arguments as database names.CREATE DATABASE andUSE statements are included in the output before each newdatabase.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default value is\u00b4d:t:o,/tmp/mysqldump.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-authDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set. If nocharacter set is specified, mysqldump uses utf8.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--defaults-group-suffix=str,Also read groups with a suffix of str. For example, sincemysqldump normally reads the [client] and [mysqldump] groups,--defaults-group-suffix=x would cause it to also read thegroups [mysqldump_x] and [client_x].\u2022--delayed-insertWrite INSERT DELAYED statements rather than INSERTstatements.\u2022--delete-master-logsOn a master replication server, delete the binary logs bysending a PURGE BINARY LOGS statement to the server afterperforming the dump operation. This option automaticallyenables --master-data.\u2022--disable-keys, -KFor each table, surround the INSERT statements with /*!40000ALTER TABLE tbl_name DISABLE KEYS */; and /*!40000 ALTERTABLE tbl_name ENABLE KEYS */; statements. This makes loadingthe dump file faster because the indexes are created afterall rows are inserted. This option is effective only fornonunique indexes of MyISAM tables.\u2022--dump-dateIf the --comments option is given, mysqldump produces acomment at the end of the dump of the following form:-- Dump completed on DATEHowever, the date causes dump files taken at different timesto appear to be different, even if the data are otherwiseidentical.--dump-date and --skip-dump-date control whetherthe date is added to the comment. The default is --dump-date(include the date in the comment).--skip-dump-datesuppresses date printing\u2022--dump-slave[=value]Used for producing a dump file from a replication slaveserver that can be used to set up another slave server withthe same master. Causes the binary log position and filenameof the master to be appended to the dumped data output.Setting the value to 1 (the default) will print it as aCHANGE MASTER command in the dumped data output; if set to 2,that command will be prefixed with a comment symbol. Thisoption will turn --lock-all-tables on, unless --single-transaction is specified too (in which case a global readlock is only taken a short time at the beginning of the dump- don't forget to read about --single-transaction below). Inall cases any action on logs will happen at the exact momentof the dump. Option automatically turns --lock-tables off.Using this option causes mysqldump to stop the slave SQLthread before beginning the dump, and restart it again aftercompletion.\u2022--events, -EInclude Event Scheduler events for the dumped databases inthe output.\u2022--extended-insert, -eUse multiple-row INSERT syntax that include several VALUESlists. This results in a smaller dump file and speeds upinserts when the file is reloaded.\u2022--fields-terminated-by=..., --fields-enclosed-by=...,--fields-optionally-enclosed-by=..., --fields-escaped-by=...These options are used with the --tab option and have thesame meaning as the corresponding FIELDS clauses for LOADDATA INFILE.\u2022--first-slaveRemoved in MariaDB 5.5. Use --lock-all-tables instead.\u2022--flashback, -BSupport flashback mode.\u2022--flush-logs, -FFlush the MariaDB server log files before starting the dump.This option requires the RELOAD privilege. If you use thisoption in combination with the --all-databases option, thelogs are flushed for each database dumped. The exception iswhen using --lock-all-tables or --master-data: In this case,the logs are flushed only once, corresponding to the momentthat all tables are locked. If you want your dump and the logflush to happen at exactly the same moment, you should use--flush-logs together with either --lock-all-tables or--master-data.\u2022--flush-privilegesSend a FLUSH PRIVILEGES statement to the server after dumpingthe mysql database. This option should be used any time thedump contains the mysql database and any other database thatdepends on the data in the mysql database for properrestoration.\u2022--force, -fContinue even if an SQL error occurs during a table dump.One use for this option is to cause mysqldump to continueexecuting even when it encounters a view that has becomeinvalid because the definition refers to a table that hasbeen dropped. Without --force, mysqldump exits with an errormessage. With --force, mysqldump prints the error message,but it also writes an SQL comment containing the viewdefinition to the dump output and continues executing.\u2022--gtidAvailable from MariaDB 10.0.13, and is used together with--master-data and --dump-slave to more conveniently set up anew GTID slave. It causes those options to output SQLstatements that configure the slave to use the globaltransaction ID to connect to the master instead of old-stylefilename/offset positions. The old-style positions are stillincluded in comments when --gtid is used; likewise the GTIDposition is included in comments even if --gtid is not used.\u2022--hex-blobDump binary columns using hexadecimal notation (for example,\u00b4abc\u00b4 becomes 0x616263). The affected data types are BINARY,VARBINARY, the BLOB types, and BIT.\u2022--host=host_name, -h host_nameDump data from the MariaDB server on the given host. Thedefault host is localhost.\u2022--ignore-table=db_name.tbl_nameDo not dump the given table, which must be specified usingboth the database and table names. To ignore multiple tables,use this option multiple times. This option also can be usedto ignore views.\u2022--include-master-host-portAdd the MASTER_HOST and MASTER_PORT options for the CHANGEMASTER TO statement when using the --dump-slave option for aslave dump.\u2022--insert-ignoreWrite INSERT IGNORE statements rather than INSERT statements.\u2022--lines-terminated-by=...This option is used with the --tab option and has the samemeaning as the corresponding LINES clause for LOAD DATAINFILE.\u2022--lock-all-tables, -xLock all tables across all databases. This is achieved byacquiring a global read lock for the duration of the wholedump. This option automatically turns off--single-transaction and --lock-tables.\u2022--lock-tables, -lFor each dumped database, lock all tables to be dumped beforedumping them. The tables are locked with READ LOCAL to allowconcurrent inserts in the case of MyISAM tables. Fortransactional tables such as InnoDB, --single-transaction isa much better option than --lock-tables because it does notneed to lock the tables at all.Because --lock-tables locks tables for each databaseseparately, this option does not guarantee that the tables inthe dump file are logically consistent between databases.Tables in different databases may be dumped in completelydifferent states.Use --skip-lock-tables to disable.\u2022--log-error=file_nameLog warnings and errors by appending them to the named file.The default is to do no logging.\u2022--log-queriesWhen restoring the dump, the server will, if logging isturned on, log the queries to the general and slow query log.Defaults to on; use --skip-log-queries to disable.\u2022--master-data[=value]Use this option to dump a master replication server toproduce a dump file that can be used to set up another serveras a slave of the master. It causes the dump output toinclude a CHANGE MASTER TO statement that indicates thebinary log coordinates (file name and position) of the dumpedserver. These are the master server coordinates from whichthe slave should start replicating after you load the dumpfile into the slave.If the option value is 2, the CHANGE MASTER TO statement iswritten as an SQL comment, and thus is informative only; ithas no effect when the dump file is reloaded. If the optionvalue is 1, the statement is not written as a comment andtakes effect when the dump file is reloaded. If no optionvalue is specified, the default value is 1.This option requires the RELOAD privilege and the binary logmust be enabled.The --master-data option automatically turns off--lock-tables. It also turns on --lock-all-tables, unless--single-transaction also is specified. In all cases, anyaction on logs happens at the exact moment of the dump.It is also possible to set up a slave by dumping an existingslave of the master. To do this, use the following procedureon the existing slave:1. Stop the slave\u00b4s SQL thread and get its current status:mysql> STOP SLAVE SQL_THREAD;mysql> SHOW SLAVE STATUS;2. From the output of the SHOW SLAVE STATUS statement, thebinary log coordinates of the master server from whichthe new slave should start replicating are the values ofthe Relay_Master_Log_File and Exec_Master_Log_Pos fields.Denote those values as file_name and file_pos.3. Dump the slave server:shell> mysqldump --master-data=2 --all-databases > dumpfile4. Restart the slave:mysql> START SLAVE;5. On the new slave, load the dump file:shell> mysql < dumpfile6. On the new slave, set the replication coordinates tothose of the master server obtained earlier:mysql> CHANGE MASTER TO-> MASTER_LOG_FILE = \u00b4file_name\u00b4, MASTER_LOG_POS = file_pos;The CHANGE MASTER TO statement might also need otherparameters, such as MASTER_HOST to point the slave to thecorrect master server host. Add any such parameters asnecessary.\u2022--max-allowed-packet=lengthSets the maximum packet length to send to or receive fromserver.\u2022--max-statement-time=secondsSets the maximum time any statement can run before beingtimed out by the server. (Default value is 0 (no limit))\u2022--net-buffer-length=lengthSets the buffer size for TCP/IP and socket communication.\u2022--no-autocommitEnclose the INSERT statements for each dumped table withinSET autocommit = 0 and COMMIT statements.\u2022--no-create-db, -nThis option suppresses the CREATE DATABASE statements thatare otherwise included in the output if the --databases or--all-databases option is given.\u2022--no-create-info, -tDo not write CREATE TABLE statements that re-create eachdumped table.\u2022--no-data, -dDo not write any table row information (that is, do not dumptable contents). This is useful if you want to dump only theCREATE TABLE statement for the table (for example, to createan empty copy of the table by loading the dump file).\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--no-set-names, -NThis has the same effect as --skip-set-charset.\u2022--optThis option is shorthand. It is the same as specifying--add-drop-table --add-locks --create-options --disable-keys--extended-insert --lock-tables --quick --set-charset. Itshould give you a fast dump operation and produce a dump filethat can be reloaded into a MariaDB server quickly.The --opt option is enabled by default. Use --skip-opt todisable it.See the discussion at the beginning of thissection for information about selectively enabling ordisabling a subset of the options affected by --opt.\u2022--order-by-primaryDump each table\u00b4s rows sorted by its primary key, or by itsfirst unique index, if such an index exists. This is usefulwhen dumping a MyISAM table to be loaded into an InnoDBtable, but will make the dump operation take considerablylonger.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqldump prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dirDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--quick, -qThis option is useful for dumping large tables. It forcesmysqldump to retrieve rows for a table from the server a rowat a time rather than retrieving the entire row set andbuffering it in memory before writing it out.\u2022--print-defaultsPrint the program argument list and exit. This must be givenas the first argument.\u2022--quote-names, -QQuote identifiers (such as database, table, and column names)within \u201c`\u201d characters. If the ANSI_QUOTES SQL mode isenabled, identifiers are quoted within \u201c\"\u201d characters. Thisoption is enabled by default. It can be disabled with--skip-quote-names, but this option should be given after anyoption such as --compatible that may enable --quote-names.\u2022--replaceWrite REPLACE statements rather than INSERT statements.\u2022--result-file=file_name, -r file_nameDirect output to a given file. This option should be used onWindows to prevent newline \u201c\\n\u201d characters from beingconverted to \u201c\\r\\n\u201d carriage return/newline sequences. Theresult file is created and its previous contents overwritten,even if an error occurs while generating the dump.\u2022--routines, -RIncluded stored routines (procedures and functions) for thedumped databases in the output. Use of this option requiresthe SELECT privilege for the mysql.proc table. The outputgenerated by using --routines contains CREATE PROCEDURE andCREATE FUNCTION statements to re-create the routines.However, these statements do not include attributes such asthe routine creation and modification timestamps. This meansthat when the routines are reloaded, they will be createdwith the timestamps equal to the reload time.If you require routines to be re-created with their originaltimestamp attributes, do not use --routines. Instead, dumpand reload the contents of the mysql.proc table directly,using a MariaDB account that has appropriate privileges forthe mysql database.\u2022--set-charsetAdd SET NAMES default_character_set to the output. Thisoption is enabled by default. To suppress the SET NAMESstatement, use --skip-set-charset.\u2022--single-transactionThis option sends a START TRANSACTION SQL statement to theserver before dumping data. It is useful only withtransactional tables such as InnoDB, because then it dumpsthe consistent state of the database at the time when BEGINwas issued without blocking any applications.When using this option, you should keep in mind that onlyInnoDB tables are dumped in a consistent state. For example,any MyISAM or MEMORY tables dumped while using this optionmay still change state.While a --single-transaction dump is in process, to ensure avalid dump file (correct table contents and binary logcoordinates), no other connection should use the followingstatements: ALTER TABLE, CREATE TABLE, DROP TABLE, RENAMETABLE, TRUNCATE TABLE. A consistent read is not isolated fromthose statements, so use of them on a table to be dumped cancause the SELECT that is performed by mysqldump to retrievethe table contents to obtain incorrect contents or fail.The --single-transaction option and the --lock-tables optionare mutually exclusive because LOCK TABLES causes any pendingtransactions to be committed implicitly.To dump large tables, you should combine the--single-transaction option with --quick.\u2022--skip-add-drop-tableDisable the --add-drop-table option.\u2022--skip-add-locksDisable the --add-locks option.\u2022--skip-commentsDisable the --comments option.\u2022--skip-compactDisable the --compact option.\u2022--skip-disable-keysDisable the --disable-keys option.\u2022--skip-extended-insertDisable the --extended-insert option.\u2022--skip-optDisable the --opt option.\u2022--skip-quickDisable the --quick option.\u2022--skip-quote-namesDisable the --quote-names option.\u2022--skip-set-charsetDisable the --set-charset option.\u2022--skip-triggersDisable the --triggers option.\u2022--skip-tz-utcDisable the --tz-utc option.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--system={all, users, plugins, udfs, servers, stats,timezones}Dump the system tables in the mysql database in a logicalform. This option is an empty set by default.One or more options can be listed in comma separated list.The options here are:\u2022all - an alias to enabling all of the below options.\u2022users - the users, roles and their grants outputed asCREATE USER, CREATE ROLE, GRANT, and SET DEFAULT ROLE(ALTER USER for MySQL-8.0+).\u2022plugins - active plugins of the server outputed asINSTALL PLUGIN.\u2022udfs - user define functions outputed as CREATE FUNCTION.\u2022servers - remote (federated) servers as CREATE SERVER.\u2022stats - statistics tables, InnoDB and Engine IndependentTable Statistics (EITS), are dumped as REPLACE INTO (orINSERT IGNORE if --insert-ignore is specified) statementswithout (re)creating tables.\u2022timezones - timezone related system tables dumped asREPLACE INTO (or INSERT IGNORE if --insert-ignore isspecified) statements without (re)creating tables.The format of the output is affected by --replace and--insert-ignore. The --replace option will output CREATE ORREPLACE forms of SQL, and also DROP IF EXISTS prior toCREATE, if a CREATE OR REPLACE option isn't available.With --system=user (or all), and --replace, SQL is generatedto generate an error if attempting to import the dump with aconnection user that is being replaced within the dump.The --insert-ignore option will cause CREATE IF NOT EXISTforms of SQL to generated if available.For stats, and timezones, --replace and --insert-ignore havethe usual effects.Enabling specific options here will cause the relevant tablesin the mysql database to be ignored when dumping the mysqldatabase or --all-databases.To help in migrating from MySQL to MariaDB, this option isdesigned to be able to dump system information from MySQL-5.7and 8.0 servers. SQL generated is also experimentallycompatible with MySQL-5.7/8.0. Mappings of implementationspecific grants/plugins isn't always one-to-one howeverbetween MariaDB and MySQL and will require manual changes.\u2022--tab=path, -T pathProduce tab-separated text-format data files. For each dumpedtable, mysqldump creates a tbl_name.sql file that containsthe CREATE TABLE statement that creates the table, and theserver writes a tbl_name.txt file that contains its data. Theoption value is the directory in which to write the files.NoteThis option should be used only when mysqldump is run onthe same machine as the mysqld server. You must have theFILE privilege, and the server must have permission towrite files in the directory that you specify.By default, the .txt data files are formatted using tabcharacters between column values and a newline at the end ofeach line. The format can be specified explicitly using the--fields-xxx and --lines-terminated-by options.Column values are converted to the character set specified bythe --default-character-set option.\u2022--tablesOverride the --databases or -B option.mysqldump regards allname arguments following the option as table names.\u2022--triggersInclude triggers for each dumped table in the output. Thisoption is enabled by default; disable it with--skip-triggers.\u2022--tz-utcThis option enables TIMESTAMP columns to be dumped andreloaded between servers in different time zones.mysqldumpsets its connection time zone to UTC and adds SETTIME_ZONE=\u00b4+00:00\u00b4 to the dump file. Without this option,TIMESTAMP columns are dumped and reloaded in the time zoneslocal to the source and destination servers, which can causethe values to change if the servers are in different timezones.--tz-utc also protects against changes due todaylight saving time.--tz-utc is enabled by default. Todisable it, use --skip-tz-utc.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.\u2022--version, -VDisplay version information and exit.\u2022--where=\u00b4where_condition\u00b4, -w \u00b4where_condition\u00b4Dump only rows selected by the given WHERE condition. Quotesaround the condition are mandatory if it contains spaces orother characters that are special to your commandinterpreter.Examples:--where=\"user=\u00b4jimf\u00b4\"-w\"userid>1\"-w\"userid<1\"\u2022--xml, -XWrite dump output as well-formed XML.NULL, \u00b4NULL\u00b4, and Empty Values: For a column namedcolumn_name, the NULL value, an empty string, and the stringvalue \u00b4NULL\u00b4 are distinguished from one another in the outputgenerated by this option as follows.\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502Value:\u2502 XML Representation:\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502NULL (unknown value)\u2502 <field name=\"column_name\"\u2502\u2502\u2502 xsi:nil=\"true\" />\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u00b4\u00b4 (empty string)\u2502 <field name=\"column_name\"></field>\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u00b4NULL\u00b4 (string value) \u2502 <field\u2502\u2502\u2502 name=\"column_name\">NULL</field>\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518The output from the mysql client when run using the --xmloption also follows the preceding rules. (See the sectioncalled \u201cMYSQL OPTIONS\u201d.)XML output from mysqldump includes the XML namespace, asshown here:shell> mysqldump --xml -u root world City<?xml version=\"1.0\"?><mysqldump xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"><database name=\"world\"><table_structure name=\"City\"><field Field=\"ID\" Type=\"int(11)\" Null=\"NO\" Key=\"PRI\" Extra=\"auto_increment\" /><field Field=\"Name\" Type=\"char(35)\" Null=\"NO\" Key=\"\" Default=\"\" Extra=\"\" /><field Field=\"CountryCode\" Type=\"char(3)\" Null=\"NO\" Key=\"\" Default=\"\" Extra=\"\" /><field Field=\"District\" Type=\"char(20)\" Null=\"NO\" Key=\"\" Default=\"\" Extra=\"\" /><field Field=\"Population\" Type=\"int(11)\" Null=\"NO\" Key=\"\" Default=\"0\" Extra=\"\" /><key Table=\"City\" Non_unique=\"0\" Key_name=\"PRIMARY\" Seq_in_index=\"1\" Column_name=\"ID\"Collation=\"A\" Cardinality=\"4079\" Null=\"\" Index_type=\"BTREE\" Comment=\"\" /><options Name=\"City\" Engine=\"MyISAM\" Version=\"10\" Row_format=\"Fixed\" Rows=\"4079\"Avg_row_length=\"67\" Data_length=\"273293\" Max_data_length=\"18858823439613951\"Index_length=\"43008\" Data_free=\"0\" Auto_increment=\"4080\"Create_time=\"2007-03-31 01:47:01\" Update_time=\"2007-03-31 01:47:02\"Collation=\"latin1_swedish_ci\" Create_options=\"\" Comment=\"\" /></table_structure><table_data name=\"City\"><row><field name=\"ID\">1</field><field name=\"Name\">Kabul</field><field name=\"CountryCode\">AFG</field><field name=\"District\">Kabol</field><field name=\"Population\">1780000</field></row>...<row><field name=\"ID\">4079</field><field name=\"Name\">Rafah</field><field name=\"CountryCode\">PSE</field><field name=\"District\">Rafah</field><field name=\"Population\">92020</field></row></table_data></database></mysqldump>You can also set the following variables by using--var_name=value syntax:\u2022max_allowed_packetThe maximum size of the buffer for client/servercommunication. The maximum is 1GB.\u2022max_statement_timeA query that has taken more than max_statement_time secondswill be aborted and the backup will fail. The argument willbe treated as a decimal value with microsecond precision. Avalue of 0 (default) means no timeout. The maximum timeout is31536000 seconds.\u2022net_buffer_lengthThe initial size of the buffer for client/servercommunication. When creating multiple-row INSERT statements(as with the --extended-insert or --opt option), mysqldumpcreates rows up to net_buffer_length length. If you increasethis variable, you should also ensure that thenet_buffer_length variable in the MariaDB server is at leastthis large.A common use of mysqldump is for making a backup of an entiredatabase:shell> mysqldump db_name > backup-file.sqlYou can load the dump file back into the server like this:shell> mysql db_name < backup-file.sqlOr like this:shell> mysql -e \"source /path-to-backup/backup-file.sql\" db_namemysqldump is also very useful for populating databases by copyingdata from one MariaDB server to another:shell> mysqldump --opt db_name | mysql --host=remote_host -C db_nameIt is possible to dump several databases with one command:shell> mysqldump --databases db_name1 [db_name2 ...] > my_databases.sqlTo dump all databases, use the --all-databases option:shell> mysqldump --all-databases > all_databases.sqlFor InnoDB tables, mysqldump provides a way of making an onlinebackup:shell> mysqldump --all-databases --single-transaction > all_databases.sqlThis backup acquires a global read lock on all tables (usingFLUSH TABLES WITH READ LOCK) at the beginning of the dump. Assoon as this lock has been acquired, the binary log coordinatesare read and the lock is released. If long updating statementsare running when the FLUSH statement is issued, the MariaDBserver may get stalled until those statements finish. After that,the dump becomes lock free and does not disturb reads and writeson the tables. If the update statements that the MariaDB serverreceives are short (in terms of execution time), the initial lockperiod should not be noticeable, even with many updates.For point-in-time recovery (also known as \u201croll-forward,\u201d whenyou need to restore an old backup and replay the changes thathappened since that backup), it is often useful to rotate thebinary log or at least know the binary log coordinates to whichthe dump corresponds:shell> mysqldump --all-databases --master-data=2 > all_databases.sqlOr:shell> mysqldump --all-databases --flush-logs --master-data=2> all_databases.sqlThe --master-data and --single-transaction options can be usedsimultaneously, which provides a convenient way to make an onlinebackup suitable for use prior to point-in-time recovery if tablesare stored using the InnoDB storage engine.If you encounter problems backing up views, please read thesection that covers restrictions on views which describes aworkaround for backing up views when this fails due toinsufficient privileges.",
        "name": "mariadb-dump - a database backup program (mysqldump is now asymlink to mariadb-dump)",
        "section": 1
    },
    {
        "command": "mysqldumpslow",
        "description": "The MariaDB slow query log contains information about queriesthat take a long time to execute.mysqldumpslow parses MariaDBslow query log files and prints a summary of their contents.Normally, mysqldumpslow groups queries that are similar exceptfor the particular values of number and string data values. It\u201cabstracts\u201d these values to N and \u00b4S\u00b4 when displaying summaryoutput. The -a and -n options can be used to modify valueabstracting behavior.Invoke mysqldumpslow like this:shell> mysqldumpslow [options] [log_file ...]mysqldumpslow supports the following options.\u2022--helpDisplay a help message and exit.\u2022-aDo not abstract all numbers to N and strings to \u00b4S\u00b4.\u2022--debug, -dRun in debug mode.\u2022-g patternConsider only queries that match the (grep-style) pattern.\u2022-h host_nameHost name of MariaDB server for *-slow.log file name. Thevalue can contain a wildcard. The default is * (match all).\u2022-i nameName of server instance (if using mysql.server startupscript).\u2022-lDo not subtract lock time from total time.\u2022-n NAbstract numbers with at least N digits within names.\u2022-rReverse the sort order.\u2022-s sort_typeHow to sort the output. The value of sort_type should bechosen from the following list:\u2022t, aa: Sort by rows affected or average rows affected\u2022l, ae: Sort by rows examined or aggregate rows examined\u2022l, at: Sort by query time or average query time\u2022l, al: Sort by lock time or average lock time\u2022s, as: Sort by rows sent or average rows sent\u2022c: Sort by count\u2022-t NDisplay only the first N queries in the output.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.Example of usage:shell> mysqldumpslowReading mysql slow query log from /usr/local/mysql/data/mysqld51-apple-slow.logCount: 1Time=4.32s (4s)Lock=0.00s (0s)Rows=0.0 (0), root[root]@localhostinsert into t2 select * from t1Count: 3Time=2.53s (7s)Lock=0.00s (0s)Rows=0.0 (0), root[root]@localhostinsert into t2 select * from t1 limit NCount: 3Time=2.13s (6s)Lock=0.00s (0s)Rows=0.0 (0), root[root]@localhostinsert into t1 select * from t1",
        "name": "mariadb-dumpslow - Summarize slow query log files (mysqldumpslowis now a symlink to mariadb-dumpslow)",
        "section": 1
    },
    {
        "command": "mysqlhotcopy",
        "description": "mysqlhotcopy is a Perl script that was originally written andcontributed by Tim Bunce. It uses FLUSH TABLES, LOCK TABLES, andcp or scp to make a database backup. It is a fast way to make abackup of the database or single tables, but it can be run onlyon the same machine where the database directories are located.mysqlhotcopy works only for backing up MyISAM and ARCHIVE tables.It runs on Unix and NetWare.To use mysqlhotcopy, you must have read access to the files forthe tables that you are backing up, the SELECT privilege forthose tables, the RELOAD privilege (to be able to execute FLUSHTABLES), and the LOCK TABLES privilege (to be able to lock thetables).shell> mysqlhotcopy db_name [/path/to/new_directory]shell> mysqlhotcopy db_name_1 ... db_name_n /path/to/new_directoryBack up tables in the given database that match a regularexpression:shell> mysqlhotcopy db_name./regex/The regular expression for the table name can be negated byprefixing it with a tilde (\u201c~\u201d):shell> mysqlhotcopy db_name./~regex/mysqlhotcopy supports the following options, which can bespecified on the command line or in the [mysqlhotcopy] and[client] option file groups.\u2022--help, -?Display a help message and exit.\u2022--addtodestDo not rename target directory (if it exists); merely addfiles to it.\u2022--allowoldDo not abort if a target exists; rename it by adding an _oldsuffix.\u2022--checkpoint=db_name.tbl_nameInsert checkpoint entries into the specified database db_nameand table tbl_name.\u2022--chroot=pathBase directory of the chroot jail in which mysqld operates.The path value should match that of the --chroot option givento mysqld.\u2022--debugEnable debug output.\u2022--dryrun, -nReport actions without performing them.\u2022--flushlogFlush logs after all tables are locked.\u2022--host=host_name, -h host_nameThe host name of the local host to use for making a TCP/IPconnection to the local server. By default, the connection ismade to localhost using a Unix socket file.\u2022--keepoldDo not delete previous (renamed) target when done.\u2022--method=commandThe method for copying files (cp or scp). The default is cp.\u2022--noindicesDo not include full index files for MyISAM tables in thebackup. This makes the backup smaller and faster. The indexesfor reloaded tables can be reconstructed later with myisamchk-rq.\u2022--old-serverConnect to old MySQL-server (before v5.5) which doesn't haveFLUSH TABLES WITH READ LOCK fully implemented..\u2022--password=password, -ppasswordThe password to use when connecting to the server. Thepassword value is not optional for this option, unlike forother MariaDB programs.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--port=port_num, -P port_numThe TCP/IP port number to use when connecting to the localserver.\u2022--quiet, -qBe silent except for errors.\u2022--record_log_pos=db_name.tbl_nameRecord master and slave status in the specified databasedb_name and table tbl_name.\u2022--regexp=exprCopy all databases with names that match the given regularexpression.\u2022--resetmasterReset the binary log after locking all the tables.\u2022--resetslaveReset the master.info file after locking all the tables.\u2022--socket=path, -S pathThe Unix socket file to use for connections to localhost.\u2022--suffix=strThe suffix to use for names of copied databases.\u2022--tmpdir=pathThe temporary directory. The default is /tmp.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.Use perldoc for additional mysqlhotcopy documentation, includinginformation about the structure of the tables needed for the--checkpoint and --record_log_pos options:shell> perldoc mysqlhotcopy",
        "name": "mariadb-hotcopy - a database backup program (mysqlhotcopy is nowa symlink to mariadb-hotcopy)",
        "section": 1
    },
    {
        "command": "mysqlimport",
        "description": "The mysqlimport client provides a command-line interface to theLOAD DATA INFILE SQL statement. Most options to mysqlimportcorrespond directly to clauses of LOAD DATA INFILE syntax.Invoke mysqlimport like this:shell> mysqlimport [options] db_name textfile1 [textfile2 ...]For each text file named on the command line, mysqlimport stripsany extension from the file name and uses the result to determinethe name of the table into which to import the file\u00b4s contents.For example, files named patient.txt, patient.text, and patientall would be imported into a table named patient.mysqlimport supports the following options, which can bespecified on the command line or in the [mysqlimport] and[client] option file groups.mysqlimport also supports theoptions for processing option files.\u2022--help, -?Display a help message and exit.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--columns=column_list, -c column_listThis option takes a comma-separated list of column names asits value. The order of the column names indicates how tomatch data file columns with table columns.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is \u00b4d:t:o\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-auth=plugin_nameDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--delete, -dEmpty the table before importing the text file.\u2022--fields-terminated-by=..., --fields-enclosed-by=...,--fields-optionally-enclosed-by=..., --fields-escaped-by=...These options have the same meaning as the correspondingclauses for LOAD DATA INFILE.\u2022--force, -fIgnore errors. For example, if a table for a text file doesnot exist, continue processing any remaining files. Without--force, mysqlimport exits if a table does not exist.\u2022--host=host_name, -h host_nameImport data to the MariaDB server on the given host. Thedefault host is localhost.\u2022--ignore, -iSee the description for the --replace option.\u2022--ignore-foreign-keys, -kDisable foreign key checks while importing the data.\u2022--ignore-lines=NIgnore the first N lines of the data file.\u2022--lines-terminated-by=...This option has the same meaning as the corresponding clausefor LOAD DATA INFILE. For example, to import Windows filesthat have lines terminated with carriage return/linefeedpairs, use --lines-terminated-by=\"\\r\\n\". (You might have todouble the backslashes, depending on the escaping conventionsof your command interpreter.).\u2022--local, -LRead input files locally from the client host.\u2022--lock-tables, -lLock all tables for writing before processing any text files.This ensures that all tables are synchronized on the server.\u2022--low-priorityUse LOW_PRIORITY when loading the table. This affects onlystorage engines that use only table-level locking (such asMyISAM, MEMORY, and MERGE).\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlimport prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dir=nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--replace, -rThe --replace and --ignore options control handling of inputrows that duplicate existing rows on unique key values. Ifyou specify --replace, new rows replace existing rows thathave the same unique key value. If you specify --ignore,input rows that duplicate an existing row on a unique keyvalue are skipped. If you do not specify either option, anerror occurs when a duplicate key value is found, and therest of the text file is ignored.\u2022--silent, -sSilent mode. Produce output only when errors occur.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--use-threads=NLoad files in parallel using N threads.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes.\u2022--version, -VDisplay version information and exit.Here is a sample session that demonstrates use of mysqlimport:shell> mysql -e \u00b4CREATE TABLE imptest(id INT, n VARCHAR(30))\u00b4 testshell> eda100Max Sydow101Count Dracula.w imptest.txt32qshell> od -c imptest.txt0000000100\\tMaxSydow\\n1000000201\\tCountDracula\\n0000040shell> mysqlimport --local test imptest.txttest.imptest: Records: 2Deleted: 0Skipped: 0Warnings: 0shell> mysql -e \u00b4SELECT * FROM imptest\u00b4 test+------+---------------+| id| n|+------+---------------+|100 | Max Sydow||101 | Count Dracula |+------+---------------+",
        "name": "mariadb-import - a data import program (mysqlimport is now asymlink to mariadb-import)",
        "section": 1
    },
    {
        "command": "mysqlshow",
        "description": "The mysqlshow client can be used to quickly see which databasesexist, their tables, or a table\u00b4s columns or indexes.mysqlshow provides a command-line interface to several SQL SHOWstatements. The same information can be obtained by using thosestatements directly. For example, you can issue them from themysql client program.Invoke mysqlshow like this:shell> mysqlshow [options] [db_name [tbl_name [col_name]]]\u2022If no database is given, a list of database names is shown.\u2022If no table is given, all matching tables in the database areshown.\u2022If no column is given, all matching columns and column typesin the table are shown.The output displays only the names of those databases, tables, orcolumns for which you have some privileges.If the last argument contains shell or SQL wildcard characters(\u201c*\u201d, \u201c?\u201d, \u201c%\u201d, or \u201c_\u201d), only those names that are matched by thewildcard are shown. If a database name contains any underscores,those should be escaped with a backslash (some Unix shellsrequire two) to get a list of the proper tables or columns.\u201c*\u201dand \u201c?\u201dcharacters are converted into SQL \u201c%\u201d and \u201c_\u201d wildcardcharacters. This might cause some confusion when you try todisplay the columns for a table with a \u201c_\u201d in the name, becausein this case, mysqlshow shows you only the table names that matchthe pattern. This is easily fixed by adding an extra \u201c%\u201d last onthe command line as a separate argument.mysqlshow supports the following options, which can be specifiedon the command line or in the [mysqlshow] and [client] optionfile groups.mysqlshow also supports the options for processingoption files described.\u2022--help, -?Display a help message and exit.\u2022--character-sets-dir=path, -c pathThe directory where character sets are installed.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--countShow the number of rows per table. This can be slow fornon-MyISAM tables.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is \u00b4d:t:o\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-auth=nameDefault authentication client-side plugin to use.\u2022--default-character-set=charset_nameUse charset_name as the default character set.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--defaults-group-suffix=suffixIn addition to the groups named on the command line, readgroups that have the given suffix.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--keys, -kShow table indexes.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlshow prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--show-table-type, -tShow a column indicating the table type, as in SHOW FULLTABLES. The type is BASE TABLE or VIEW.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--status, -iDisplay extra information about each table.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes. This option can be used multiple times to increase theamount of information.\u2022--version, -VDisplay version information and exit.",
        "name": "mariadb-show - display database, table, and column information(mysqlshow is now a symlink to mariadb-show)",
        "section": 1
    },
    {
        "command": "mysqlslap",
        "description": "mysqlslap is a diagnostic program designed to emulate client loadfor a MariaDB server and to report the timing of each stage. Itworks as if multiple clients are accessing the server.Invoke mysqlslap like this:shell> mysqlslap [options]Some options such as --create or --query enable you to specify astring containing an SQL statement or a file containingstatements. If you specify a file, by default it must contain onestatement per line. (That is, the implicit statement delimiter isthe newline character.) Use the --delimiter option to specify adifferent delimiter, which enables you to specify statements thatspan multiple lines or place multiple statements on a singleline. You cannot include comments in a file; mysqlslap does notunderstand them.mysqlslap runs in three stages:1. Create schema, table, and optionally any stored programs ordata you want to using for the test. This stage uses a singleclient connection.2. Run the load test. This stage can use many clientconnections.3. Clean up (disconnect, drop table if specified). This stageuses a single client connection.Examples:Supply your own create and query SQL statements, with 50 clientsquerying and 200 selects for each:mysqlslap --delimiter=\";\" \\--create=\"CREATE TABLE a (b int);INSERT INTO a VALUES (23)\" \\--query=\"SELECT * FROM a\" --concurrency=50 --iterations=200Let mysqlslap build the query SQL statement with a table of twoINT columns and three VARCHAR columns. Use five clients querying20 times each. Do not create the table or insert the data (thatis, use the previous test\u00b4s schema and data):mysqlslap --concurrency=5 --iterations=20 \\--number-int-cols=2 --number-char-cols=3 \\--auto-generate-sqlTell the program to load the create, insert, and query SQLstatements from the specified files, where the create.sql filehas multiple table creation statements delimited by \u00b4;\u00b4 andmultiple insert statements delimited by \u00b4;\u00b4. The --query filewill have multiple queries delimited by \u00b4;\u00b4. Run all the loadstatements, then run all the queries in the query file with fiveclients (five times each):mysqlslap --concurrency=5 \\--iterations=5 --query=query.sql --create=create.sql \\--delimiter=\";\"mysqlslap supports the following options, which can be specifiedon the command line or in the [mysqlslap] and [client] optionfile groups.mysqlslap also supports the options for processingoption files.\u2022--help, -?Display a help message and exit.\u2022--auto-generate-sql, -aGenerate SQL statements automatically when they are notsupplied in files or via command options.\u2022--auto-generate-sql-add-autoincrementAdd an AUTO_INCREMENT column to automatically generatedtables.\u2022--auto-generate-sql-execute-number=NSpecify how many queries to generate automatically.\u2022--auto-generate-sql-guid-primaryAdd a GUID-based primary key to automatically generatedtables.\u2022--auto-generate-sql-load-type=typeSpecify the test load type. The allowable values are read(scan tables), write (insert into tables), key (read primarykeys), update (update primary keys), or mixed (half inserts,half scanning selects). The default is mixed.\u2022--auto-generate-sql-secondary-indexes=NSpecify how many secondary indexes to add to automaticallygenerated tables. By default, none are added.\u2022--auto-generate-sql-unique-query-number=NHow many different queries to generate for automatic tests.For example, if you run a key test that performs 1000selects, you can use this option with a value of 1000 to run1000 unique queries, or with a value of 50 to perform 50different selects. The default is 10.\u2022--auto-generate-sql-unique-write-number=NHow many different queries to generate for--auto-generate-sql-write-number. The default is 10.\u2022--auto-generate-sql-write-number=NHow many row inserts to perform on each thread. The defaultis 100.\u2022--commit=NHow many statements to execute before committing. The defaultis 0 (no commits are done).\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--concurrency=N, -c NThe number of clients to simulate when issuing the SELECTstatement.\u2022--create=valueThe file or string containing the statement to use forcreating the table.\u2022--create-schema=valueThe schema in which to run the tests.\u2022--csv[=file_name]Generate output in comma-separated values format. The outputgoes to the named file, or to the standard output if no fileis given.\u2022--debug[=debug_options], -# [debug_options]Write a debugging log. A typical debug_options string is\u00b4d:t:o,file_name\u00b4. The default is\u00b4d:t:o,/tmp/mysqlslap.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-info, -TPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--default-auth=nameDefault authentication client-side plugin to use.\u2022--defaults-extra-file=filenameSet filename as the file to read default options from afterthe global defaults files has been read.Must be given asfirst option.\u2022--defaults-file=filenameSet filename as the file to read default options from,override global defaults files.Must be given as firstoption.\u2022--delimiter=str, -F strThe delimiter to use in SQL statements supplied in files orvia command options.\u2022--detach=NDetach (close and reopen) each connection after each Nstatements. The default is 0 (connections are not detached).\u2022--engine=engine_name, -e engine_nameComma separated list of storage engines to use for creatingthe table. The test is run for each engine. You can alsospecify an option for an engine after a colon, for examplememory:max_row=2300.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--init-command=strSQL Command to execute when connecting to MariaDB server.Will automatically be re-executed when reconnecting.\u2022--iterations=N, -i NThe number of times to run the tests.\u2022--no-defaultsDo not read default options from any option file. This mustbe given as the first argument.\u2022--no-dropDo not drop any schema created during the test after the testis complete.\u2022--number-char-cols=N, -x NThe number of VARCHAR columns to use if --auto-generate-sqlis specified.\u2022--number-int-cols=N, -y NThe number of INT columns to use if --auto-generate-sql isspecified.\u2022--number-of-queries=NLimit each client to approximately this many queries. Querycounting takes into account the statement delimiter. Forexample, if you invoke mysqlslap as follows, the ; delimiteris recognized so that each instance of the query stringcounts as two queries. As a result, 5 rows (not 10) areinserted.shell> mysqlslap --delimiter=\";\" --number-of-queries=10--query=\"use test;insert into t values(null)\"\u2022--only-printDo not connect to databases.mysqlslap only prints what itwould have done.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,mysqlslap prompts for one.Specifying a password on the command line should beconsidered insecure. You can use an option file to avoidgiving the password on the command line.\u2022--pipe, -WOn Windows, connect to the server via a named pipe. Thisoption applies only if the server supports named-pipeconnections.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection.Forces--protocol=tcp when specified on the command line withoutother connection properties.\u2022--post-query=valueThe file or string containing the statement to execute afterthe tests have completed. This execution is not counted fortiming purposes.\u2022--post-system=strThe string to execute via system() after the tests havecompleted. This execution is not counted for timing purposes.\u2022--pre-query=valueThe file or string containing the statement to execute beforerunning the tests. This execution is not counted for timingpurposes.\u2022--pre-system=strThe string to execute via system() before running the tests.This execution is not counted for timing purposes.\u2022--print-defaultsPrint the program argument list and exit.This must be givenas the first argument.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--query=value, -q valueThe file or string containing the SELECT statement to use forretrieving data.\u2022--shared-memory-base-name=nameOn Windows, the shared-memory name to use, for connectionsmade via shared memory to a local server. This option appliesonly if the server supports shared-memory connections.\u2022--silent, -sSilent mode. No output.\u2022--socket=path, -S pathFor connections to localhost, the Unix socket file to use,or, on Windows, the name of the named pipe to use.Forces--protocol=socket when specified on the command line withoutother connection properties; on Windows, forces--protocol=pipe.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print more information about what the programdoes. This option can be used multiple times to increase theamount of information.\u2022--version, -VDisplay version information and exit.",
        "name": "mariadb-slap - load emulation client (mysqlslap is now a symlinkto mariadb-slap)",
        "section": 1
    },
    {
        "command": "mysqltest",
        "description": "The mysqltest program runs a test case against a MariaDB serverand optionally compares the output with a result file. Thisprogram reads input written in a special test language.Typically, you invoke mysqltest via mysql-test-run.pl rather thaninvoking it directly.mysqltest_embedded is similar but is built with support for thelibmysqld embedded server.Features of mysqltest:\u2022Can send SQL statements to MariaDB servers for execution\u2022Can execute external shell commands\u2022Can test whether the result from an SQL statement or shellcommand is as expected\u2022Can connect to one or more standalone mysqld servers andswitch between connections\u2022Can connect to an embedded server (libmysqld), if MariaDB iscompiled with support for libmysqld. (In this case, theexecutable is named mysqltest_embedded rather thanmysqltest.)By default, mysqltest reads the test case on the standard input.To run mysqltest this way, you normally invoke it like this:shell> mysqltest [options] [db_name] < test_fileYou can also name the test case file with a --test-file=file_nameoption.The exit value from mysqltest is 0 for success, 1 for failure,and 62 if it skips the test case (for example, if after checkingsome preconditions it decides not to run the test).mysqltest supports the following options:\u2022--help, -?Display a help message and exit.\u2022--basedir=dir_name, -b dir_nameThe base directory for tests.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--connect-timeout=numThis can be used to set the MYSQL_OPT_CONNECT_TIMEOUTparameter of mysql_options to change the number of secondsbefore an unsuccessful connection attempt times out.\u2022--continue-on-errorContinue test even if we got an error. This is mostly usefulwhen testing a storage engine to see what from a test file itcan execute, or to find all syntax errors in a newly createdbig test file.\u2022--cursor-protocolUse cursors for prepared statements.\u2022--database=db_name, -D db_nameThe default database to use.\u2022--debug[=debug_options], -#[debug_options]Write a debugging log if MariaDB is built with debuggingsupport. The default debug_options value is\u00b4d:t:S:i:O,/tmp/mysqltest.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--logdir=dir_nameThe directory to use for log files.\u2022--mark-progressWrite the line number and elapsed time to test_file.progress.\u2022--max-connect-retries=numThe maximum number of connection attempts when connecting toserver.\u2022--max-connections=numThe maximum number of simultaneous server connections perclient (that is, per test). If not set, the maximum is 128.Minimum allowed limit is 8, maximum is 5120.\u2022--no-defaultsDo not read default options from any option files. If used,this must be the first option.\u2022--non-blocking-apiUse the non-blocking client API for communication.\u2022--overlay-dir=dir_nameOverlay directory.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,you are prompted for one.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection or 0 fordefault to, in order of preference, my.cnf, $MYSQL_TCP_PORT,/etc/services, built-in default (3306).\u2022--prologue=nameInclude the contents of the given file before processing thecontents of the test file. The included file should have thesame format as other mysqltest test files. This option hasthe same effect as putting a --source file_name command asthe first line of the test file.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--ps-protocolUse the prepared-statement protocol for communication.\u2022--quietSuppress all normal output. This is a synonym for --silent.\u2022--record, -rRecord the output that results from running the test fileinto the file named by the --result-file option, if thatoption is given. It is an error to use this option withoutalso using --result-file.\u2022--result-file=file_name, -R file_nameThis option specifies the file for test case expectedresults.--result-file, together with --record, determineshow mysqltest treats the test actual and expected results fora test case:\u2022If the test produces no results, mysqltest exits with anerror message to that effect, unless --result-file isgiven and the named file is an empty file.\u2022Otherwise, if --result-file is not given, mysqltest sendstest results to the standard output.\u2022With --result-file but not --record, mysqltest reads theexpected results from the given file and compares themwith the actual results. If the results do not match,mysqltest writes a .reject file in the same directory asthe result file, outputs a diff of the two files, andexits with an error.\u2022With both --result-file and --record, mysqltest updatesthe given file by writing the actual test results to it.\u2022--result-format-version=#Version of the result file format to use.\u2022--server-arg=value, -A valuePass the argument as an argument to the embedded server. Forexample, --server-arg=--tmpdir=/tmp or --server-arg=--core.Up to 64 arguments can be given.\u2022--server-file=file_name, -F file_nameRead arguments for the embedded server from the given file.The file should contain one argument per line.\u2022--silent, -sSuppress all normal output.\u2022--sleep=num, -T numCause all sleep commands in the test case file to sleep numseconds. This option does not affect real_sleep commands.An option value of 0 can be used, which effectively disablessleep commands in the test case.\u2022--socket=path, -S pathThe socket file to use when connecting to localhost (which isthe default host).\u2022--sp-protocolExecute DML statements within a stored procedure. For everyDML statement, mysqltest creates and invokes a storedprocedure that executes the statement rather than executingthe statement directly.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--suite-dir=dir_nameSuite directory.\u2022--tail-lines=nnSpecify how many lines of the result to include in the outputif the test fails because an SQL statement fails. The defaultis 0, meaning no lines of result printed.\u2022--test-file=file_name, -x file_nameRead test input from this file. The default is to read fromthe standard input.\u2022--timer-file=file_name, -m file_nameIf given, the number of microseconds spent running the testwill be written to this file. This is used bymysql-test-run.pl for its reporting.\u2022--tmpdir=dir_name, -t dir_nameThe temporary directory where socket files are created.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print out more information about what theprogram does.\u2022--version, -VDisplay version information and exit.\u2022--view-protocolEvery SELECT statement is wrapped inside a view.",
        "name": "mariadb-test - program to run test cases (mysqltest is now asymlink to mariadb-test)mysqltest_embedded - program to run embedded test cases",
        "section": 1
    },
    {
        "command": "mysqltest_embedded",
        "description": "The mysqltest program runs a test case against a MariaDB serverand optionally compares the output with a result file. Thisprogram reads input written in a special test language.Typically, you invoke mysqltest via mysql-test-run.pl rather thaninvoking it directly.mysqltest_embedded is similar but is built with support for thelibmysqld embedded server.Features of mysqltest:\u2022Can send SQL statements to MariaDB servers for execution\u2022Can execute external shell commands\u2022Can test whether the result from an SQL statement or shellcommand is as expected\u2022Can connect to one or more standalone mysqld servers andswitch between connections\u2022Can connect to an embedded server (libmysqld), if MariaDB iscompiled with support for libmysqld. (In this case, theexecutable is named mysqltest_embedded rather thanmysqltest.)By default, mysqltest reads the test case on the standard input.To run mysqltest this way, you normally invoke it like this:shell> mysqltest [options] [db_name] < test_fileYou can also name the test case file with a --test-file=file_nameoption.The exit value from mysqltest is 0 for success, 1 for failure,and 62 if it skips the test case (for example, if after checkingsome preconditions it decides not to run the test).mysqltest supports the following options:\u2022--help, -?Display a help message and exit.\u2022--basedir=dir_name, -b dir_nameThe base directory for tests.\u2022--character-sets-dir=pathThe directory where character sets are installed.\u2022--compress, -CCompress all information sent between the client and theserver if both support compression.\u2022--connect-timeout=numThis can be used to set the MYSQL_OPT_CONNECT_TIMEOUTparameter of mysql_options to change the number of secondsbefore an unsuccessful connection attempt times out.\u2022--continue-on-errorContinue test even if we got an error. This is mostly usefulwhen testing a storage engine to see what from a test file itcan execute, or to find all syntax errors in a newly createdbig test file.\u2022--cursor-protocolUse cursors for prepared statements.\u2022--database=db_name, -D db_nameThe default database to use.\u2022--debug[=debug_options], -#[debug_options]Write a debugging log if MariaDB is built with debuggingsupport. The default debug_options value is\u00b4d:t:S:i:O,/tmp/mysqltest.trace\u00b4.\u2022--debug-checkPrint some debugging information when the program exits.\u2022--debug-infoPrint debugging information and memory and CPU usagestatistics when the program exits.\u2022--host=host_name, -h host_nameConnect to the MariaDB server on the given host.\u2022--logdir=dir_nameThe directory to use for log files.\u2022--mark-progressWrite the line number and elapsed time to test_file.progress.\u2022--max-connect-retries=numThe maximum number of connection attempts when connecting toserver.\u2022--max-connections=numThe maximum number of simultaneous server connections perclient (that is, per test). If not set, the maximum is 128.Minimum allowed limit is 8, maximum is 5120.\u2022--no-defaultsDo not read default options from any option files. If used,this must be the first option.\u2022--non-blocking-apiUse the non-blocking client API for communication.\u2022--overlay-dir=dir_nameOverlay directory.\u2022--password[=password], -p[password]The password to use when connecting to the server. If you usethe short option form (-p), you cannot have a space betweenthe option and the password. If you omit the password valuefollowing the --password or -p option on the command line,you are prompted for one.\u2022--plugin-dir=dir_nameDirectory for client-side plugins.\u2022--port=port_num, -P port_numThe TCP/IP port number to use for the connection or 0 fordefault to, in order of preference, my.cnf, $MYSQL_TCP_PORT,/etc/services, built-in default (3306).\u2022--prologue=nameInclude the contents of the given file before processing thecontents of the test file. The included file should have thesame format as other mysqltest test files. This option hasthe same effect as putting a --source file_name command asthe first line of the test file.\u2022--protocol={TCP|SOCKET|PIPE|MEMORY}The connection protocol to use for connecting to the server.It is useful when the other connection parameters normallywould cause a protocol to be used other than the one youwant.\u2022--ps-protocolUse the prepared-statement protocol for communication.\u2022--quietSuppress all normal output. This is a synonym for --silent.\u2022--record, -rRecord the output that results from running the test fileinto the file named by the --result-file option, if thatoption is given. It is an error to use this option withoutalso using --result-file.\u2022--result-file=file_name, -R file_nameThis option specifies the file for test case expectedresults.--result-file, together with --record, determineshow mysqltest treats the test actual and expected results fora test case:\u2022If the test produces no results, mysqltest exits with anerror message to that effect, unless --result-file isgiven and the named file is an empty file.\u2022Otherwise, if --result-file is not given, mysqltest sendstest results to the standard output.\u2022With --result-file but not --record, mysqltest reads theexpected results from the given file and compares themwith the actual results. If the results do not match,mysqltest writes a .reject file in the same directory asthe result file, outputs a diff of the two files, andexits with an error.\u2022With both --result-file and --record, mysqltest updatesthe given file by writing the actual test results to it.\u2022--result-format-version=#Version of the result file format to use.\u2022--server-arg=value, -A valuePass the argument as an argument to the embedded server. Forexample, --server-arg=--tmpdir=/tmp or --server-arg=--core.Up to 64 arguments can be given.\u2022--server-file=file_name, -F file_nameRead arguments for the embedded server from the given file.The file should contain one argument per line.\u2022--silent, -sSuppress all normal output.\u2022--sleep=num, -T numCause all sleep commands in the test case file to sleep numseconds. This option does not affect real_sleep commands.An option value of 0 can be used, which effectively disablessleep commands in the test case.\u2022--socket=path, -S pathThe socket file to use when connecting to localhost (which isthe default host).\u2022--sp-protocolExecute DML statements within a stored procedure. For everyDML statement, mysqltest creates and invokes a storedprocedure that executes the statement rather than executingthe statement directly.\u2022--sslEnable SSL for connection (automatically enabled with otherflags). Disable with --skip-ssl.\u2022--ssl-ca=nameCA file in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-capath=nameCA directory (check OpenSSL docs, implies --ssl).\u2022--ssl-cert=nameX509 cert in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-cipher=nameSSL cipher to use (check OpenSSL docs, implies --ssl).\u2022--ssl-key=nameX509 key in PEM format (check OpenSSL docs, implies --ssl).\u2022--ssl-crl=nameCertificate revocation list (check OpenSSL docs, implies--ssl).\u2022--ssl-crlpath=nameCertificate revocation list path (check OpenSSL docs, implies--ssl).\u2022--ssl-verify-server-certVerify server's \"Common Name\" in its cert against hostnameused when connecting. This option is disabled by default.\u2022--suite-dir=dir_nameSuite directory.\u2022--tail-lines=nnSpecify how many lines of the result to include in the outputif the test fails because an SQL statement fails. The defaultis 0, meaning no lines of result printed.\u2022--test-file=file_name, -x file_nameRead test input from this file. The default is to read fromthe standard input.\u2022--timer-file=file_name, -m file_nameIf given, the number of microseconds spent running the testwill be written to this file. This is used bymysql-test-run.pl for its reporting.\u2022--tmpdir=dir_name, -t dir_nameThe temporary directory where socket files are created.\u2022--user=user_name, -u user_nameThe MariaDB user name to use when connecting to the server.\u2022--verbose, -vVerbose mode. Print out more information about what theprogram does.\u2022--version, -VDisplay version information and exit.\u2022--view-protocolEvery SELECT statement is wrapped inside a view.",
        "name": "mariadb-test - program to run test cases (mysqltest is now asymlink to mariadb-test)mysqltest_embedded - program to run embedded test cases",
        "section": 1
    },
    {
        "command": "mytop",
        "description": null,
        "name": "mytop - display MariaDB server performance info like 'top'",
        "section": 1
    },
    {
        "command": "namei",
        "description": "namei interprets its arguments as pathnames to any type of Unixfile (symlinks, files, directories, and so forth). namei thenfollows each pathname until an endpoint is found (a file, adirectory, a device node, etc). If it finds a symbolic link, itshows the link, and starts following it, indenting the output toshow the context.This program is useful for finding \"too many levels of symboliclinks\" problems.For each line of output, namei uses the following characters toidentify the file type found:f: = the pathname currently being resolvedd = directoryl = symbolic link (both the link and its contents are output)s = socketb = block devicec = character devicep = FIFO (named pipe)- = regular file? = an error of some kindnamei prints an informative message when the maximum number ofsymbolic links this system can have has been exceeded.",
        "name": "namei - follow a pathname until a terminal point is found",
        "section": 1
    },
    {
        "command": "ncat",
        "description": "Ncat is a feature-packed networking utility which reads andwrites data across networks from the command line. Ncat waswritten for the Nmap Project and is the culmination of thecurrently splintered family of Netcat incarnations. It isdesigned to be a reliable back-end tool to instantly providenetwork connectivity to other applications and users. Ncat willnot only work with IPv4 and IPv6 but provides the user with avirtually limitless number of potential uses.Among Ncat's vast number of features there is the ability tochain Ncats together; redirection of TCP, UDP, and SCTP ports toother sites; SSL support; and proxy connections via SOCKS4,SOCKS5 or HTTP proxies (with optional proxy authentication aswell). Some general principles apply to most applications andthus give you the capability of instantly adding networkingsupport to software that would normally never support it.",
        "name": "ncat - Concatenate and redirect sockets",
        "section": 1
    },
    {
        "command": "ncurses6-config",
        "description": "This is a shell script which simplifies configuring applicationsagainst a particular set of ncurses libraries.",
        "name": "ncurses6-config - helper script for ncurses libraries",
        "section": 1
    },
    {
        "command": "ndiff",
        "description": "Ndiff is a tool to aid in the comparison of Nmap scans. It takestwo Nmap XML output files and prints the differences betweenthem. The differences observed are:\u2022Host states (e.g. up to down)\u2022Port states (e.g. open to closed)\u2022Service versions (from -sV)\u2022OS matches (from -O)\u2022Script outputNdiff, like the standard diff utility, compares two scans at atime.",
        "name": "ndiff - Utility to compare the results of Nmap scans",
        "section": 1
    },
    {
        "command": "needs-restarting",
        "description": "needs-restarting is a program that reports a list of process idsthat started running before they or some component that they usewere updated.",
        "name": "needs-restarting - report running processes that have beenupdated",
        "section": 1
    },
    {
        "command": "neqn",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "networkctl",
        "description": "networkctl may be used to query or modify the state of thenetwork links as seen by systemd-networkd. Please refer tosystemd-networkd.service(8) for an introduction to the basicconcepts, functionality, and configuration syntax.",
        "name": "networkctl - Query or modify the status of network links",
        "section": 1
    },
    {
        "command": "newgidmap",
        "description": "The newgidmap sets /proc/[pid]/gid_map based on its command linearguments and the gids allowed. Subgid delegation can either bemanaged via /etc/subgid or through the configured NSS subidmodule. These options are mutually exclusive.Note that the root group is not exempted from the requirement fora valid /etc/subgid entry.After the pid argument, newgidmap expects sets of 3 integers:gidBeginning of the range of GIDs inside the user namespace.lowergidBeginning of the range of GIDs outside the user namespace.countLength of the ranges (both inside and outside the usernamespace).newgidmap verifies that the caller is the owner of the processindicated by pid and that for each of the above sets, each of theGIDs in the range [lowergid, lowergid+count) is allowed to thecaller according to /etc/subgid before setting/proc/[pid]/gid_map.Note that newgidmap may be used only once for a given process.Instead of an integer process id, the first argument may bespecified as fd:N, where the integer N is the file descriptornumber for the calling process's opened file for /proc/[pid[. Inthis case, newgidmap will use openat(2) to open the gid_map fileunder that directory, avoiding a TOCTTOU in case the processexits and the pid is immediately reused.",
        "name": "newgidmap - set the gid mapping of a user namespace",
        "section": 1
    },
    {
        "command": "newgrp",
        "description": "The newgrp command is used to change the current group ID duringa login session. If the optional - flag is given, the user'senvironment will be reinitialized as though the user had loggedin, otherwise the current environment, including current workingdirectory, remains unchanged.newgrp changes the current real group ID to the named group, orto the default group listed in /etc/passwd if no group name isgiven.newgrp also tries to add the group to the user groupset.If not root, the user will be prompted for a password if she doesnot have a password (in /etc/shadow if this user has an entry inthe shadowed password file, or in /etc/passwd otherwise) and thegroup does, or if the user is not listed as a member and thegroup has a password. The user will be denied access if the grouppassword is empty and the user is not listed as a member.If there is an entry for this group in /etc/gshadow, then thelist of members and the password of this group will be taken fromthis file, otherwise, the entry in /etc/group is considered.",
        "name": "newgrp - log in to a new group",
        "section": 1
    },
    {
        "command": "newhelp",
        "description": "newhelp generates the Performance Co-Pilot help text files usedby Performance Metric Domain Agents (PMDAs).Normally newhelp operates on the default Performance Metrics NameSpace (PMNS), however if the -n option is specified analternative namespace is loaded from the file pmnsfile.When there is only one input file, the base name of the newdatabase is derived from the name of the input file, otherwisethe -o flag must be given to explicitly name the database.If noinput files are supplied, newhelp reads from the standard inputstream, in which case the -o flag must be given.If the output file name is determined to be foo, newhelp willcreate foo.dir and foo.pag.The -V flag causes verbose messages to be printed while newhelpis parsing its input.The first line of each entry in a help source file consists of an``@'' character beginning the line followed by a space and thenthe performance metric name and a one line description of themetric.Following lines (up to the next line beginning with``@'' or end of file) may contain a verbose help description.E.g.## This is an example of newhelp's input syntax#@ kernel.all.cpu.idle CPU idle timeA cumulative count of the number of millisecondsof CPU idle time, summed over all processors.Three-part numeric metric identifiers (PMIDs) may be used inplace of metric names, e.g. 60.0.23 rather thankernel.all.cpu.idle in the example above.Other than for dynamicmetrics (where the existence of a metric is known to a PMDA, butnot visible in the PMNS and hence has no name that could be knownto newhelp) use of this syntactic variant is not encouraged.Lines beginning with ``#'' are ignored, as are blank lines in thefile before the first ``@''.The verbose help text is optional.As a special case, a ``metric'' name of the form NNN.MM (fornumeric NNN and MM) is interpreted as an instance domainidentification, and the text describes the instance domain.",
        "name": "newhelp - generate a performance metrics help database",
        "section": 1
    },
    {
        "command": "newrole",
        "description": "Run a new shell in a new context.The new context is derivedfrom the old context in which newrole is originally executed.Ifthe -r or --role option is specified, then the new context willhave the role specified by ROLE.If the -t or --type option isspecified, then the new context will have the type (domain)specified by TYPE.If a role is specified, but no type isspecified, the default type is derived from the specified role.If the -l or --level option is specified, then the new contextwill have the sensitivity level specified by LEVEL.If LEVEL isa range, the new context will have the sensitivity level andclearance specified by that range.If the -p or --preserve-environment option is specified, the shell with the new SELinuxcontext will preserve environment variables, otherwise a newminimal environment is created.Additional arguments ARGS may be provided after a -- option, inwhich case they are supplied to the new shell.In particular, anargument of -- -c will cause the next argument to be treated as acommand by most command interpreters.If a command argument is specified to newrole and the commandname is found in /etc/selinux/newrole_pam.conf, then the pamservice name listed in that file for the command will be usedrather than the normal newrole pam configuration.This allowsfor per-command pam configuration when invoked via newrole, e.g.to skip the interactive re-authentication phase.The new shell will be the shell specified in the user's entry inthe /etc/passwd file.The -V or --version shows the current version of newrole",
        "name": "newrole - run a shell with a new SELinux role",
        "section": 1
    },
    {
        "command": "newuidmap",
        "description": "The newuidmap sets /proc/[pid]/uid_map based on its command linearguments and the uids allowed. Subuid delegation can either bemanaged via /etc/subuid or through the configured NSS subidmodule. These options are mutually exclusive.Note that the root user is not exempted from the requirement fora valid /etc/subuid entry.After the pid argument, newuidmap expects sets of 3 integers:uidBeginning of the range of UIDs inside the user namespace.loweruidBeginning of the range of UIDs outside the user namespace.countLength of the ranges (both inside and outside the usernamespace).newuidmap verifies that the caller is the owner of the processindicated by pid and that for each of the above sets, each of theUIDs in the range [loweruid, loweruid+count) is allowed to thecaller according to /etc/subuid before setting/proc/[pid]/uid_map.Note that newuidmap may be used only once for a given process.Instead of an integer process id, the first argument may bespecified as fd:N, where the integer N is the file descriptornumber for the calling process's opened file for /proc/[pid[. Inthis case, newuidmap will use openat(2) to open the uid_map fileunder that directory, avoiding a TOCTTOU in case the processexits and the pid is immediately reused.",
        "name": "newuidmap - set the uid mapping of a user namespace",
        "section": 1
    },
    {
        "command": "nfs4_editfacl",
        "description": "nfs4_setfacl manipulates the NFSv4 Access Control List (ACL) ofone or more files (or directories), provided they are on amounted NFSv4 filesystem which supports ACLs.nfs4_editfacl is equivalent to nfs4_setfacl -e.Refer to the nfs4_acl(5) manpage for information about NFSv4 ACLterminology and syntax.COMMANDS-a acl_specadd the ACEs from acl_spec to file's ACL.ACEs areinserted starting at the default position 1 of file's ACL.-A acl_fileadd the ACEs from the acl_spec in acl_file to file's ACL.ACEs are inserted starting at the default position 1 offile's ACL.-x acl_specdelete ACEs matched from acl_spec from file's ACL.Notethat the ordering of the ACEs in acl_spec does not matter.-X acl_filedelete ACEs matched from the acl_spec in acl_file fromfile's ACL.Note that the ordering of the ACEs in theacl_spec does not matter.-i indexACEs are inserted or deleted starting at the indexthposition (DEFAULT: 1) of file's ACL.It can be used onlywith the add or delete action.-s acl_specset file's ACL to acl_spec.-S acl_fileset file's ACL to the acl_spec in acl_file.-e, --editedit file's ACL in the editor defined in the EDITORenvironment variable (DEFAULT: vi(1)) and set theresulting ACL upon a clean exit, assuming changes made inthe editor were saved.Note that if multiple files arespecified, the editor will be serially invoked once perfile.-m from_ace to_acemodify file's ACL in-place by replacing from_ace withto_ace.-?, -h, --helpdisplay help text and exit.--versiondisplay this program's version and exit.NOTE: if '-' is given as the acl_file with the -A/-X/-S flags,the acl_spec will be read from stdin.OPTIONS-R, --recursiverecursively apply to a directory's files andsubdirectories.Similar to setfacl(1), the defaultbehavior is to follow symlinks given on the command lineand to skip symlinks encountered while recursing throughdirectories.-L, --logicalin conjunction with -R/--recursive, a logical walk followsall symbolic links.-P, --physicalin conjunction with -R/--recursive, a physical walk skipsall symbolic links.--daclacts on the dacl only. This functionality is onlyavailable if the server supports NFSv4 minor version 1 ornewer.--saclacts on the sacl only. This functionality is onlyavailable if the server supports NFSv4 minor version 1 ornewer.--testdisplay results of COMMAND, but do not save changes.",
        "name": "nfs4_setfacl, nfs4_editfacl - manipulate NFSv4 file/directoryaccess control lists",
        "section": 1
    },
    {
        "command": "nfs4_getfacl",
        "description": "nfs4_getfacl will display the NFSv4 Access Control List (ACL) forthe files given as arguments, provided they are on mounted NFSv4filesystems which support ACLs.If the -H/--more-help flag is specified, nfs4_getfacl will printsome information about NFSv4 ACLs and the fields used in ACEs.If the -R/--recursive flag is specified, nfs4_getfacl will listthe NFSv4 ACLs of all files and directories recursively.If the -c/--omit-header flag is specified, nfs4_getfacl will notdisplay the comment header (Do not print filename).If the --dacl flag is specified, nfs4_getfacl will retrieve thedacl. This functionality is only available if the server supportsNFSv4 minor version 1 or newer.If the --sacl flag is specified, nfs4_getfacl will retrieve thesacl. This functionality is only available if the server supportsNFSv4 minor version 1 or newer.The output format for an NFSv4 file ACL, e.g., is:# file: /somedirA::OWNER@:rwatTnNcCyA::alice@nfsdomain.org:rxtncyA::bob@nfsdomain.org:rwadtTnNcCyA:g:GROUP@:rtncyD:g:GROUP@:waxTCA::EVERYONE@:rtncyD::EVERYONE@:waxTCIn the example output above, the user `alice@nfsdomain.org' hasthe equivalent of \"read\" and \"execute\" permissions,`bob@nfsdomain.org' has \"read\" and \"write\", and both `GROUP@' and`EVERYONE@' have \"read\".The ACL listings of multiple files are separated by blank lines.Refer to the nfs4_acl(5) manpage for detailed information aboutNFSv4 ACL terminology and syntax.",
        "name": "nfs4_getfacl - get NFSv4 file/directory access control lists",
        "section": 1
    },
    {
        "command": "nfs4_setfacl",
        "description": "nfs4_setfacl manipulates the NFSv4 Access Control List (ACL) ofone or more files (or directories), provided they are on amounted NFSv4 filesystem which supports ACLs.nfs4_editfacl is equivalent to nfs4_setfacl -e.Refer to the nfs4_acl(5) manpage for information about NFSv4 ACLterminology and syntax.COMMANDS-a acl_specadd the ACEs from acl_spec to file's ACL.ACEs areinserted starting at the default position 1 of file's ACL.-A acl_fileadd the ACEs from the acl_spec in acl_file to file's ACL.ACEs are inserted starting at the default position 1 offile's ACL.-x acl_specdelete ACEs matched from acl_spec from file's ACL.Notethat the ordering of the ACEs in acl_spec does not matter.-X acl_filedelete ACEs matched from the acl_spec in acl_file fromfile's ACL.Note that the ordering of the ACEs in theacl_spec does not matter.-i indexACEs are inserted or deleted starting at the indexthposition (DEFAULT: 1) of file's ACL.It can be used onlywith the add or delete action.-s acl_specset file's ACL to acl_spec.-S acl_fileset file's ACL to the acl_spec in acl_file.-e, --editedit file's ACL in the editor defined in the EDITORenvironment variable (DEFAULT: vi(1)) and set theresulting ACL upon a clean exit, assuming changes made inthe editor were saved.Note that if multiple files arespecified, the editor will be serially invoked once perfile.-m from_ace to_acemodify file's ACL in-place by replacing from_ace withto_ace.-?, -h, --helpdisplay help text and exit.--versiondisplay this program's version and exit.NOTE: if '-' is given as the acl_file with the -A/-X/-S flags,the acl_spec will be read from stdin.OPTIONS-R, --recursiverecursively apply to a directory's files andsubdirectories.Similar to setfacl(1), the defaultbehavior is to follow symlinks given on the command lineand to skip symlinks encountered while recursing throughdirectories.-L, --logicalin conjunction with -R/--recursive, a logical walk followsall symbolic links.-P, --physicalin conjunction with -R/--recursive, a physical walk skipsall symbolic links.--daclacts on the dacl only. This functionality is onlyavailable if the server supports NFSv4 minor version 1 ornewer.--saclacts on the sacl only. This functionality is onlyavailable if the server supports NFSv4 minor version 1 ornewer.--testdisplay results of COMMAND, but do not save changes.",
        "name": "nfs4_setfacl, nfs4_editfacl - manipulate NFSv4 file/directoryaccess control lists",
        "section": 1
    },
    {
        "command": "ngettext",
        "description": "The ngettext program translates a natural language message intothe user's language, by looking up the translation in a messagecatalog, and chooses the appropriate plural form, which dependson the number COUNT and the language of the message catalog wherethe translation was found.Display native language translation of a textual message whosegrammatical form depends on a number.-d, --domain=TEXTDOMAINretrieve translated message from TEXTDOMAIN-c, --context=CONTEXTspecify context for MSGID-eenable expansion of some escape sequences-E(ignored for compatibility)[TEXTDOMAIN]retrieve translated message from TEXTDOMAINMSGID MSGID-PLURALtranslate MSGID (singular) / MSGID-PLURAL (plural)COUNTchoose singular/plural form based on this valueInformative output:-h, --helpdisplay this help and exit-V, --versiondisplay version information and exitIf the TEXTDOMAIN parameter is not given, the domain isdetermined from the environment variable TEXTDOMAIN.If themessage catalog is not found in the regular directory, anotherlocation can be specified with the environment variableTEXTDOMAINDIR.Standard search directory:/usr/local/share/locale",
        "name": "ngettext - translate message and choose plural form",
        "section": 1
    },
    {
        "command": "nice",
        "description": "Run COMMAND with an adjusted niceness, which affects processscheduling.With no COMMAND, print the current niceness.Niceness values range from -20 (most favorable to the process) to19 (least favorable to the process).Mandatory arguments to long options are mandatory for shortoptions too.-n, --adjustment=Nadd integer N to the niceness (default 10)--help display this help and exit--versionoutput version information and exitNOTE: your shell may have its own version of nice, which usuallysupersedes the version described here.Please refer to yourshell's documentation for details about the options it supports.Exit status:125if the nice command itself fails126if COMMAND is found but cannot be invoked127if COMMAND cannot be found-the exit status of COMMAND otherwise",
        "name": "nice - run a program with modified scheduling priority",
        "section": 1
    },
    {
        "command": "nisdomainname",
        "description": "Hostname is the program that is used to either set or display thecurrent host, domain or node name of the system.These names areused by many of the networking programs to identify the machine.The domain name is also used by NIS/YP.GET NAMEWhen called without any arguments, the program displays thecurrent names:hostname will print the name of the system as returned by thegethostname(2) function.domainname, nisdomainname, ypdomainname will print the name ofthe system as returned by the getdomainname(2) function. This isalso known as the YP/NIS domain name of the system.nodename will print the DECnet node name of the system asreturned by the getnodename(2) function.dnsdomainname will print the domain part of the FQDN (FullyQualified Domain Name). The complete FQDN of the system isreturned with hostname --fqdn.SET NAMEWhen called with one argument or with the --file option, thecommands set the host name, the NIS/YP domain name or the nodename.Note, that only the super-user can change the names.It is not possible to set the FQDN or the DNS domain name withthe dnsdomainname command (see THE FQDN below).The host name is usually set once at system startup by readingthe contents of a file which contains the host name, e.g./etc/hostname).THE FQDNYou can't change the FQDN (as returned by hostname --fqdn) or theDNS domain name (as returned by dnsdomainname) with this command.The FQDN of the system is the name that the resolver(3) returnsfor the host name.Technically: The FQDN is the canonical name returned bygethostbyname2(2) when resolving the result of the gethostname(2)name. The DNS domain name is the part after the first dot.Therefore it depends on the configuration (usually in/etc/host.conf) how you can change it. If hosts is the firstlookup method, you can change the FQDN in /etc/hosts.",
        "name": "hostname - show or set the system's host namednsdomainname - show the system's DNS domain namedomainname - show or set the system's NIS/YP domain namenisdomainname - show or set system's NIS/YP domain namenodename - show or set the system's DECnet node nameypdomainname - show or set the system's NIS/YP domain name",
        "section": 1
    },
    {
        "command": "nitrocli",
        "description": "nitrocli provides access to Nitrokey devices.It supports theNitrokey Pro, the Nitrokey Storage, and the Librem Key.It canbe used to access the encrypted volume, the one-time passwordgenerator, and the password safe.Device selectionPer default, nitrocli connects to any attached Nitrokey device.You can use the --model, --serial-number and --usb-path optionsto select the device to connect to.nitrocli fails if more thanone attached Nitrokey device matches this filter or if multipleNitrokey devices are attached and none of the filter options isset.Use the list command to list all attached devices withtheir USB path, model, and serial number (if available).",
        "name": "nitrocli - access Nitrokey devices",
        "section": 1
    },
    {
        "command": "nl",
        "description": "Write each FILE to standard output, with line numbers added.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-b, --body-numbering=STYLEuse STYLE for numbering body lines-d, --section-delimiter=CCuse CC for logical page delimiters-f, --footer-numbering=STYLEuse STYLE for numbering footer lines-h, --header-numbering=STYLEuse STYLE for numbering header lines-i, --line-increment=NUMBERline number increment at each line-l, --join-blank-lines=NUMBERgroup of NUMBER empty lines counted as one-n, --number-format=FORMATinsert line numbers according to FORMAT-p, --no-renumberdo not reset line numbers for each section-s, --number-separator=STRINGadd STRING after (possible) line number-v, --starting-line-number=NUMBERfirst line number for each section-w, --number-width=NUMBERuse NUMBER columns for line numbers--help display this help and exit--versionoutput version information and exitDefault options are: -bt -d'\\:' -fn -hn -i1 -l1 -n'rn' -s<TAB>-v1 -w6CC are two delimiter characters used to construct logical pagedelimiters; a missing second character implies ':'.As a GNUextension one can specify more than two characters, and alsospecifying the empty string (-d '') disables section matching.STYLE is one of:anumber all linestnumber only nonempty linesnnumber no linespBREnumber only lines that contain a match for the basicregular expression, BREFORMAT is one of:lnleft justified, no leading zerosrnright justified, no leading zerosrzright justified, leading zeros",
        "name": "nl - number lines of files",
        "section": 1
    },
    {
        "command": "nm",
        "description": "GNU nm lists the symbols from object files objfile....If noobject files are listed as arguments, nm assumes the file a.out.For each symbol, nm shows:\u2022The symbol value, in the radix selected by options (seebelow), or hexadecimal by default.\u2022The symbol type.At least the following types are used;others are, as well, depending on the object file format.Iflowercase, the symbol is usually local; if uppercase, thesymbol is global (external).There are however a fewlowercase symbols that are shown for special global symbols(\"u\", \"v\" and \"w\").\"A\" The symbol's value is absolute, and will not be changedby further linking.\"B\"\"b\" The symbol is in the BSS data section.This sectiontypically contains zero-initialized or uninitializeddata, although the exact behavior is system dependent.\"C\"\"c\" The symbol is common.Common symbols are uninitializeddata.When linking, multiple common symbols may appearwith the same name.If the symbol is defined anywhere,the common symbols are treated as undefined references.The lower case c character is used when the symbol is ina special section for small commons.\"D\"\"d\" The symbol is in the initialized data section.\"G\"\"g\" The symbol is in an initialized data section for smallobjects.Some object file formats permit more efficientaccess to small data objects, such as a global intvariable as opposed to a large global array.\"i\" For PE format files this indicates that the symbol is ina section specific to the implementation of DLLs.For ELF format files this indicates that the symbol is anindirect function.This is a GNU extension to thestandard set of ELF symbol types.It indicates a symbolwhich if referenced by a relocation does not evaluate toits address, but instead must be invoked at runtime.Theruntime execution will then return the value to be usedin the relocation.Note - the actual symbols display for GNU indirectsymbols is controlled by the --ifunc-chars command lineoption.If this option has been provided then the firstcharacter in the string will be used for global indirectfunction symbols.If the string contains a secondcharacter then that will be used for local indirectfunction symbols.\"I\" The symbol is an indirect reference to another symbol.\"N\" The symbol is a debugging symbol.\"n\" The symbol is in the read-only data section.\"p\" The symbol is in a stack unwind section.\"R\"\"r\" The symbol is in a read only data section.\"S\"\"s\" The symbol is in an uninitialized or zero-initializeddata section for small objects.\"T\"\"t\" The symbol is in the text (code) section.\"U\" The symbol is undefined.\"u\" The symbol is a unique global symbol.This is a GNUextension to the standard set of ELF symbol bindings.For such a symbol the dynamic linker will make sure thatin the entire process there is just one symbol with thisname and type in use.\"V\"\"v\" The symbol is a weak object.When a weak defined symbolis linked with a normal defined symbol, the normaldefined symbol is used with no error.When a weakundefined symbol is linked and the symbol is not defined,the value of the weak symbol becomes zero with no error.On some systems, uppercase indicates that a default valuehas been specified.\"W\"\"w\" The symbol is a weak symbol that has not beenspecifically tagged as a weak object symbol.When a weakdefined symbol is linked with a normal defined symbol,the normal defined symbol is used with no error.When aweak undefined symbol is linked and the symbol is notdefined, the value of the symbol is determined in asystem-specific manner without error.On some systems,uppercase indicates that a default value has beenspecified.\"-\" The symbol is a stabs symbol in an a.out object file.Inthis case, the next values printed are the stabs otherfield, the stabs desc field, and the stab type.Stabssymbols are used to hold debugging information.\"?\" The symbol type is unknown, or object file formatspecific.\u2022The symbol name.If a symbol has version informationassociated with it, then the version information is displayedas well.If the versioned symbol is undefined or hidden fromlinker, the version string is displayed as a suffix to thesymbol name, preceded by an @ character.For examplefoo@VER_1.If the version is the default version to be usedwhen resolving unversioned references to the symbol, then itis displayed as a suffix preceded by two @ characters.Forexample foo@@VER_2.",
        "name": "nm - list symbols from object files",
        "section": 1
    },
    {
        "command": "nmap",
        "description": "Nmap (\u201cNetwork Mapper\u201d) is an open source tool for networkexploration and security auditing. It was designed to rapidlyscan large networks, although it works fine against single hosts.Nmap uses raw IP packets in novel ways to determine what hostsare available on the network, what services (application name andversion) those hosts are offering, what operating systems (and OSversions) they are running, what type of packet filters/firewallsare in use, and dozens of other characteristics. While Nmap iscommonly used for security audits, many systems and networkadministrators find it useful for routine tasks such as networkinventory, managing service upgrade schedules, and monitoringhost or service uptime.The output from Nmap is a list of scanned targets, withsupplemental information on each depending on the options used.Key among that information is the \u201cinteresting ports table\u201d.That table lists the port number and protocol, service name, andstate. The state is either open, filtered, closed, or unfiltered.Open means that an application on the target machine is listeningfor connections/packets on that port.Filtered means that afirewall, filter, or other network obstacle is blocking the portso that Nmap cannot tell whether it is open or closed.Closedports have no application listening on them, though they couldopen up at any time. Ports are classified as unfiltered when theyare responsive to Nmap's probes, but Nmap cannot determinewhether they are open or closed. Nmap reports the statecombinations open|filtered and closed|filtered when it cannotdetermine which of the two states describe a port. The port tablemay also include software version details when version detectionhas been requested. When an IP protocol scan is requested (-sO),Nmap provides information on supported IP protocols rather thanlistening ports.In addition to the interesting ports table, Nmap can providefurther information on targets, including reverse DNS names,operating system guesses, device types, and MAC addresses.A typical Nmap scan is shown in Example 1. The only Nmaparguments used in this example are -A, to enable OS and versiondetection, script scanning, and traceroute; -T4 for fasterexecution; and then the hostname.Example 1. A representative Nmap scan# nmap -A -T4 scanme.nmap.orgNmap scan report for scanme.nmap.org (74.207.244.221)Host is up (0.029s latency).rDNS record for 74.207.244.221: li86-221.members.linode.comNot shown: 995 closed portsPORTSTATESERVICEVERSION22/tcpopensshOpenSSH 5.3p1 Debian 3ubuntu7 (protocol 2.0)| ssh-hostkey: 1024 8d:60:f1:7c:ca:b7:3d:0a:d6:67:54:9d:69:d9:b9:dd (DSA)|_2048 79:f8:09:ac:d4:e2:32:42:10:49:d3:bd:20:82:85:ec (RSA)80/tcpopenhttpApache httpd 2.2.14 ((Ubuntu))|_http-title: Go ahead and ScanMe!646/tcpfiltered ldp1720/tcp filtered H.323/Q.9319929/tcp opennping-echoNping echoDevice type: general purposeRunning: Linux 2.6.XOS CPE: cpe:/o:linux:linux_kernel:2.6.39OS details: Linux 2.6.39Network Distance: 11 hopsService Info: OS: Linux; CPE: cpe:/o:linux:kernelTRACEROUTE (using port 53/tcp)HOP RTTADDRESS[Cut first 10 hops for brevity]1117.65 ms li86-221.members.linode.com (74.207.244.221)Nmap done: 1 IP address (1 host up) scanned in 14.40 secondsThe newest version of Nmap can be obtained from https://nmap.org .The newest version of this man page is available athttps://nmap.org/book/man.html .It is also included as a chapterof Nmap Network Scanning: The Official Nmap Project Guide toNetwork Discovery and Security Scanning (seehttps://nmap.org/book/ ).",
        "name": "nmap - Network exploration tool and security / port scanner",
        "section": 1
    },
    {
        "command": "nodename",
        "description": "Hostname is the program that is used to either set or display thecurrent host, domain or node name of the system.These names areused by many of the networking programs to identify the machine.The domain name is also used by NIS/YP.GET NAMEWhen called without any arguments, the program displays thecurrent names:hostname will print the name of the system as returned by thegethostname(2) function.domainname, nisdomainname, ypdomainname will print the name ofthe system as returned by the getdomainname(2) function. This isalso known as the YP/NIS domain name of the system.nodename will print the DECnet node name of the system asreturned by the getnodename(2) function.dnsdomainname will print the domain part of the FQDN (FullyQualified Domain Name). The complete FQDN of the system isreturned with hostname --fqdn.SET NAMEWhen called with one argument or with the --file option, thecommands set the host name, the NIS/YP domain name or the nodename.Note, that only the super-user can change the names.It is not possible to set the FQDN or the DNS domain name withthe dnsdomainname command (see THE FQDN below).The host name is usually set once at system startup by readingthe contents of a file which contains the host name, e.g./etc/hostname).THE FQDNYou can't change the FQDN (as returned by hostname --fqdn) or theDNS domain name (as returned by dnsdomainname) with this command.The FQDN of the system is the name that the resolver(3) returnsfor the host name.Technically: The FQDN is the canonical name returned bygethostbyname2(2) when resolving the result of the gethostname(2)name. The DNS domain name is the part after the first dot.Therefore it depends on the configuration (usually in/etc/host.conf) how you can change it. If hosts is the firstlookup method, you can change the FQDN in /etc/hosts.",
        "name": "hostname - show or set the system's host namednsdomainname - show the system's DNS domain namedomainname - show or set the system's NIS/YP domain namenisdomainname - show or set system's NIS/YP domain namenodename - show or set the system's DECnet node nameypdomainname - show or set the system's NIS/YP domain name",
        "section": 1
    },
    {
        "command": "nohup",
        "description": "Run COMMAND, ignoring hangup signals.--help display this help and exit--versionoutput version information and exitIf standard input is a terminal, redirect it from an unreadablefile.If standard output is a terminal, append output to'nohup.out' if possible, '$HOME/nohup.out' otherwise.Ifstandard error is a terminal, redirect it to standard output.Tosave output to FILE, use 'nohup COMMAND > FILE'.NOTE: your shell may have its own version of nohup, which usuallysupersedes the version described here.Please refer to yourshell's documentation for details about the options it supports.Exit status:125if the nohup command itself fails126if COMMAND is found but cannot be invoked127if COMMAND cannot be found-the exit status of COMMAND otherwise",
        "name": "nohup - run a command immune to hangups, with output to a non-tty",
        "section": 1
    },
    {
        "command": "nping",
        "description": "Nping is an open-source tool for network packet generation,response analysis and response time measurement. Nping allowsusers to generate network packets of a wide range of protocols,letting them tune virtually any field of the protocol headers.While Nping can be used as a simple ping utility to detect activehosts, it can also be used as a raw packet generator for networkstack stress tests, ARP poisoning, Denial of Service attacks,route tracing, and other purposes.Additionally, Nping offers a special mode of operation called the\"Echo Mode\", that lets users see how the generated probes changein transit, revealing the differences between the transmittedpackets and the packets received at the other end. See section\"Echo Mode\" for details.The output from Nping is a list of the packets that are beingsent and received. The level of detail depends on the optionsused.A typical Nping execution is shown in Example 1. The only Npingarguments used in this example are -c, to specify the number oftimes to target each host, --tcp to specify TCP Probe Mode, -p80,433 to specify the target ports; and then the two targethostnames.Example 1. A representative Nping execution# nping -c 1 --tcp -p 80,433 scanme.nmap.org google.comStarting Nping ( https://nmap.org/nping )SENT (0.0120s) TCP 96.16.226.135:50091 > 64.13.134.52:80 S ttl=64 id=52072 iplen=40seq=1077657388 win=1480RCVD (0.1810s) TCP 64.13.134.52:80 > 96.16.226.135:50091 SA ttl=53 id=0 iplen=44seq=4158134847 win=5840 <mss 1460>SENT (1.0140s) TCP 96.16.226.135:50091 > 74.125.45.100:80 S ttl=64 id=13932 iplen=40seq=1077657388 win=1480RCVD (1.1370s) TCP 74.125.45.100:80 > 96.16.226.135:50091 SA ttl=52 id=52913 iplen=44seq=2650443864 win=5720 <mss 1430>SENT (2.0140s) TCP 96.16.226.135:50091 > 64.13.134.52:433 S ttl=64 id=8373 iplen=40seq=1077657388 win=1480SENT (3.0140s) TCP 96.16.226.135:50091 > 74.125.45.100:433 S ttl=64 id=23624 iplen=40seq=1077657388 win=1480Statistics for host scanme.nmap.org (64.13.134.52):|Probes Sent: 2 | Rcvd: 1 | Lost: 1(50.00%)|_ Max rtt: 169.720ms | Min rtt: 169.720ms | Avg rtt: 169.720msStatistics for host google.com (74.125.45.100):|Probes Sent: 2 | Rcvd: 1 | Lost: 1(50.00%)|_ Max rtt: 122.686ms | Min rtt: 122.686ms | Avg rtt: 122.686msRaw packets sent: 4 (160B) | Rcvd: 2 (92B) | Lost: 2 (50.00%)Tx time: 3.00296s | Tx bytes/s: 53.28 | Tx pkts/s: 1.33Rx time: 3.00296s | Rx bytes/s: 30.64 | Rx pkts/s: 0.67Nping done: 2 IP addresses pinged in 4.01 secondsThe newest version of Nping can be obtained with Nmap athttps://nmap.org . The newest version of this man page isavailable at https://nmap.org/book/nping-man.html .-->.SH \"OPTIONS SUMMARY\"This options summary is printed when Nping is run with noarguments. It helps people remember the most common options, butis no substitute for the in-depth documentation in the rest ofthis manual. Some obscure options aren't even included here.Nping 0.7.92SVN ( https://nmap.org/nping )Usage: nping [Probe mode] [Options] {target specification}TARGET SPECIFICATION:Targets may be specified as hostnames, IP addresses, networks, etc.Ex: scanme.nmap.org, microsoft.com/24, 192.168.0.1; 10.0.*.1-24PROBE MODES:--tcp-connect: Unprivileged TCP connect probe mode.--tcp: TCP probe mode.--udp: UDP probe mode.--icmp: ICMP probe mode.--arp: ARP/RARP probe mode.--tr, --traceroute: Traceroute mode (can only be used withTCP/UDP/ICMP modes).TCP CONNECT MODE:-p, --dest-port <port spec>: Set destination port(s).-g, --source-port <portnumber>: Try to use a custom source port.TCP PROBE MODE:-g, --source-port <portnumber>: Set source port.-p, --dest-port <port spec>: Set destination port(s).--seq <seqnumber>: Set sequence number.--flags <flag list>: Set TCP flags (ACK,PSH,RST,SYN,FIN...)--ack <acknumber>: Set ACK number.--win <size>: Set window size.--badsum: Use a random invalid checksum.UDP PROBE MODE:-g, --source-port <portnumber>: Set source port.-p, --dest-port <port spec>: Set destination port(s).--badsum: Use a random invalid checksum.ICMP PROBE MODE:--icmp-type <type>: ICMP type.--icmp-code <code>: ICMP code.--icmp-id <id>: Set identifier.--icmp-seq <n>: Set sequence number.--icmp-redirect-addr <addr>: Set redirect address.--icmp-param-pointer <pnt>: Set parameter problem pointer.--icmp-advert-lifetime <time>: Set router advertisement lifetime.--icmp-advert-entry <IP,pref>: Add router advertisement entry.--icmp-orig-time<timestamp>: Set originate timestamp.--icmp-recv-time<timestamp>: Set receive timestamp.--icmp-trans-time <timestamp>: Set transmit timestamp.ARP/RARP PROBE MODE:--arp-type <type>: Type: ARP, ARP-reply, RARP, RARP-reply.--arp-sender-mac <mac>: Set sender MAC address.--arp-sender-ip<addr>: Set sender IP address.--arp-target-mac <mac>: Set target MAC address.--arp-target-ip<addr>: Set target IP address.IPv4 OPTIONS:-S, --source-ip: Set source IP address.--dest-ip <addr>: Set destination IP address (used as analternative to {target specification} ).--tos <tos>: Set type of service field (8bits).--id<id>: Set identification field (16 bits).--df: Set Don't Fragment flag.--mf: Set More Fragments flag.--evil: Set Reserved / Evil flag.--ttl <hops>: Set time to live [0-255].--badsum-ip: Use a random invalid checksum.--ip-options <S|R [route]|L [route]|T|U ...> : Set IP options--ip-options <hex string>: Set IP options--mtu <size>: Set MTU. Packets get fragmented if MTU issmall enough.IPv6 OPTIONS:-6, --IPv6: Use IP version 6.--dest-ip: Set destination IP address (used as analternative to {target specification}).--hop-limit: Set hop limit (same as IPv4 TTL).--traffic-class <class> :: Set traffic class.--flow <label>: Set flow label.ETHERNET OPTIONS:--dest-mac <mac>: Set destination mac address. (DisablesARP resolution)--source-mac <mac>: Set source MAC address.--ether-type <type>: Set EtherType value.PAYLOAD OPTIONS:--data <hex string>: Include a custom payload.--data-string <text>: Include a custom ASCII text.--data-length <len>: Include len random bytes as payload.ECHO CLIENT/SERVER:--echo-client <passphrase>: Run Nping in client mode.--echo-server <passphrase>: Run Nping in server mode.--echo-port <port>: Use custom <port> to listen or connect.--no-crypto: Disable encryption and authentication.--once: Stop the server after one connection.--safe-payloads: Erase application data in echoed packets.TIMING AND PERFORMANCE:Options which take <time> are in seconds, or append 'ms' (milliseconds),'s' (seconds), 'm' (minutes), or 'h' (hours) to the value (e.g. 30m, 0.25h).--delay <time>: Adjust delay between probes.--rate<rate>: Send num packets per second.MISC:-h, --help: Display help information.-V, --version: Display current version number.-c, --count <n>: Stop after <n> rounds.-e, --interface <name>: Use supplied network interface.-H, --hide-sent: Do not display sent packets.-N, --no-capture: Do not try to capture replies.--privileged: Assume user is fully privileged.--unprivileged: Assume user lacks raw socket privileges.--send-eth: Send packets at the raw Ethernet layer.--send-ip: Send packets using raw IP sockets.--bpf-filter <filter spec>: Specify custom BPF filter.OUTPUT:-v: Increment verbosity level by one.-v[level]: Set verbosity level. E.g: -v4-d: Increment debugging level by one.-d[level]: Set debugging level. E.g: -d3-q: Decrease verbosity level by one.-q[N]: Decrease verbosity level N times--quiet: Set verbosity and debug level to minimum.--debug: Set verbosity and debug to the max level.EXAMPLES:nping scanme.nmap.orgnping --tcp -p 80 --flags rst --ttl 2 192.168.1.1nping --icmp --icmp-type time --delay 500ms 192.168.254.254nping --echo-server \"public\" -e wlan0 -vvvnping --echo-client \"public\" echo.nmap.org --tcp -p1-1024 --flags ackSEE THE MAN PAGE FOR MANY MORE OPTIONS, DESCRIPTIONS, AND EXAMPLES",
        "name": "nping - Network packet generation tool / ping utility",
        "section": 1
    },
    {
        "command": "nproc",
        "description": "Print the number of processing units available to the currentprocess, which may be less than the number of online processors--allprint the number of installed processors--ignore=Nif possible, exclude N processing units--help display this help and exit--versionoutput version information and exit",
        "name": "nproc - print the number of processing units available",
        "section": 1
    },
    {
        "command": "nroff",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "nsenter",
        "description": "The nsenter command executes program in the namespace(s) that arespecified in the command-line options (described below). Ifprogram is not given, then \"${SHELL}\" is run (default: /bin/sh).Enterable namespaces are:mount namespaceMounting and unmounting filesystems will not affect the restof the system, except for filesystems which are explicitlymarked as shared (with mount --make-shared; see/proc/self/mountinfo for the shared flag). For furtherdetails, see mount_namespaces(7) and the discussion of theCLONE_NEWNS flag in clone(2).UTS namespaceSetting hostname or domainname will not affect the rest ofthe system. For further details, see uts_namespaces(7).IPC namespaceThe process will have an independent namespace for POSIXmessage queues as well as System V message queues, semaphoresets and shared memory segments. For further details, seeipc_namespaces(7).network namespaceThe process will have independent IPv4 and IPv6 stacks, IProuting tables, firewall rules, the /proc/net and/sys/class/net directory trees, sockets, etc. For furtherdetails, see network_namespaces(7).PID namespaceChildren will have a set of PID to process mappings separatefrom the nsenter process. nsenter will fork by default ifchanging the PID namespace, so that the new program and itschildren share the same PID namespace and are visible to eachother. If --no-fork is used, the new program will be exec\u2019edwithout forking. For further details, see pid_namespaces(7).user namespaceThe process will have a distinct set of UIDs, GIDs andcapabilities. For further details, see user_namespaces(7).cgroup namespaceThe process will have a virtualized view of/proc/self/cgroup, and new cgroup mounts will be rooted atthe namespace cgroup root. For further details, seecgroup_namespaces(7).time namespaceThe process can have a distinct view of CLOCK_MONOTONICand/or CLOCK_BOOTTIME which can be changed using/proc/self/timens_offsets. For further details, seetime_namespaces(7).",
        "name": "nsenter - run program in different namespaces",
        "section": 1
    },
    {
        "command": "numfmt",
        "description": "Reformat NUMBER(s), or the numbers from standard input if noneare specified.Mandatory arguments to long options are mandatory for shortoptions too.--debugprint warnings about invalid input-d, --delimiter=Xuse X instead of whitespace for field delimiter--field=FIELDSreplace the numbers in these input fields (default=1); seeFIELDS below--format=FORMATuse printf style floating-point FORMAT; see FORMAT belowfor details--from=UNITauto-scale input numbers to UNITs; default is 'none'; seeUNIT below--from-unit=Nspecify the input unit size (instead of the default 1)--groupinguse locale-defined grouping of digits, e.g. 1,000,000(which means it has no effect in the C/POSIX locale)--header[=N]print (without converting) the first N header lines; Ndefaults to 1 if not specified--invalid=MODEfailure mode for invalid numbers: MODE can be: abort(default), fail, warn, ignore--padding=Npad the output to N characters; positive N willright-align; negative N will left-align; padding isignored if the output is wider than N; the default is toautomatically pad if a whitespace is found--round=METHODuse METHOD for rounding when scaling; METHOD can be: up,down, from-zero (default), towards-zero, nearest--suffix=SUFFIXadd SUFFIX to output numbers, and accept optional SUFFIXin input numbers--to=UNITauto-scale output numbers to UNITs; see UNIT below--to-unit=Nthe output unit size (instead of the default 1)-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exitUNIT options:noneno auto-scaling is done; suffixes will trigger an errorautoaccept optional single/two letter suffix:1K = 1000, 1Ki = 1024, 1M = 1000000, 1Mi = 1048576,siaccept optional single letter suffix:1K = 1000, 1M = 1000000, ...iecaccept optional single letter suffix:1K = 1024, 1M = 1048576, ...iec-iaccept optional two-letter suffix:1Ki = 1024, 1Mi = 1048576, ...FIELDS supports cut(1) style field ranges:NN'th field, counted from 1N-from N'th field, to end of lineN-Mfrom N'th to M'th field (inclusive)-Mfrom first to M'th field (inclusive)-all fieldsMultiple fields/ranges can be separated with commasFORMAT must be suitable for printing one floating-point argument'%f'.Optional quote (%'f) will enable --grouping (if supportedby current locale).Optional width value (%10f) will pad output.Optional zero (%010f) width will zero pad the number. Optionalnegative values (%-10f) will left align.Optional precision(%.1f) will override the input determined precision.Exit status is 0 if all input numbers were successfullyconverted.By default, numfmt will stop at the first conversionerror with exit status 2.With --invalid='fail' a warning isprinted for each conversion error and the exit status is 2.With--invalid='warn' each conversion error is diagnosed, but the exitstatus is 0.With --invalid='ignore' conversion errors are notdiagnosed and the exit status is 0.",
        "name": "numfmt - Convert numbers from/to human-readable strings",
        "section": 1
    },
    {
        "command": "objcopy",
        "description": "The GNU objcopy utility copies the contents of an object file toanother.objcopy uses the GNU BFD Library to read and write theobject files.It can write the destination object file in aformat different from that of the source object file.The exactbehavior of objcopy is controlled by command-line options.Notethat objcopy should be able to copy a fully linked file betweenany two formats. However, copying a relocatable object filebetween any two formats may not work as expected.objcopy creates temporary files to do its translations anddeletes them afterward.objcopy uses BFD to do all itstranslation work; it has access to all the formats described inBFD and thus is able to recognize most formats without being toldexplicitly.objcopy can be used to generate S-records by using an outputtarget of srec (e.g., use -O srec).objcopy can be used to generate a raw binary file by using anoutput target of binary (e.g., use -O binary).When objcopygenerates a raw binary file, it will essentially produce a memorydump of the contents of the input object file.All symbols andrelocation information will be discarded.The memory dump willstart at the load address of the lowest section copied into theoutput file.When generating an S-record or a raw binary file, it may behelpful to use -S to remove sections containing debugginginformation.In some cases -R will be useful to remove sectionswhich contain information that is not needed by the binary file.Note---objcopy is not able to change the endianness of its inputfiles.If the input format has an endianness (some formats donot), objcopy can only copy the inputs into file formats thathave the same endianness or which have no endianness (e.g.,srec).(However, see the --reverse-bytes option.)",
        "name": "objcopy - copy and translate object files",
        "section": 1
    },
    {
        "command": "objdump",
        "description": "objdump displays information about one or more object files.Theoptions control what particular information to display.Thisinformation is mostly useful to programmers who are working onthe compilation tools, as opposed to programmers who just wanttheir program to compile and work.objfile... are the object files to be examined.When you specifyarchives, objdump shows information on each of the member objectfiles.",
        "name": "objdump - display information from object files",
        "section": 1
    },
    {
        "command": "ocount",
        "description": "ocount is an OProfile tool that can be used to count nativehardware events occurring in either a given application, a set ofprocesses or threads, a subset of active system processors, orthe entire system. The data collected during a counting sessionis displayed to stdout by default or, optionally, to a file.When counting multiple events, the kernel may not be able tocount all events simultaneously and, thus, may need to multiplexthe counting of the events.If this happens, the \"Percent timeenabled\" column in the ocount output will be less than 100, butcounts are scaled up to a 100% estimated value.",
        "name": "ocount - Event counting tool for Linux",
        "section": 1
    },
    {
        "command": "ocsptool",
        "description": "On verificationResponses are typically signed/issued by designated certificatesor certificate authorities and thus this tool requires onverification the certificate of the issuer or the fullcertificate chain in order to determine the appropriate signingauthority. The specified certificate of the issuer is assumedtrusted.",
        "name": "ocsptool - GnuTLS OCSP tool",
        "section": 1
    },
    {
        "command": "od",
        "description": "Write an unambiguous representation, octal bytes by default, ofFILE to standard output.With more than one FILE argument,concatenate them in the listed order to form the input.With no FILE, or when FILE is -, read standard input.If first and second call formats both apply, the second format isassumed if the last operand begins with + or (if there are 2operands) a digit.An OFFSET operand means -j OFFSET.LABEL isthe pseudo-address at first byte printed, incremented when dumpis progressing.For OFFSET and LABEL, a 0x or 0X prefixindicates hexadecimal; suffixes may be . for octal and b formultiply by 512.Mandatory arguments to long options are mandatory for shortoptions too.-A, --address-radix=RADIXoutput format for file offsets; RADIX is one of [doxn],for Decimal, Octal, Hex or None--endian={big|little}swap input bytes according the specified order-j, --skip-bytes=BYTESskip BYTES input bytes first-N, --read-bytes=BYTESlimit dump to BYTES input bytes-S BYTES, --strings[=BYTES]output strings of at least BYTES graphic chars; 3 isimplied when BYTES is not specified-t, --format=TYPEselect output format or formats-v, --output-duplicatesdo not use * to mark line suppression-w[BYTES], --width[=BYTES]output BYTES bytes per output line; 32 is implied whenBYTES is not specified--traditionalaccept arguments in third form above--help display this help and exit--versionoutput version information and exitTraditional format specifications may be intermixed; they accumulate:-asame as -t a,select named characters, ignoringhigh-order bit-bsame as -t o1, select octal bytes-csame as -t c,select printable characters or backslashescapes-dsame as -t u2, select unsigned decimal 2-byte units-fsame as -t fF, select floats-isame as -t dI, select decimal ints-lsame as -t dL, select decimal longs-osame as -t o2, select octal 2-byte units-ssame as -t d2, select decimal 2-byte units-xsame as -t x2, select hexadecimal 2-byte unitsTYPE is made up of one or more of these specifications:anamed character, ignoring high-order bitcprintable character or backslash escaped[SIZE]signed decimal, SIZE bytes per integerf[SIZE]floating point, SIZE bytes per floato[SIZE]octal, SIZE bytes per integeru[SIZE]unsigned decimal, SIZE bytes per integerx[SIZE]hexadecimal, SIZE bytes per integerSIZE is a number.For TYPE in [doux], SIZE may also be C forsizeof(char), S for sizeof(short), I for sizeof(int) or L forsizeof(long).If TYPE is f, SIZE may also be F forsizeof(float), D for sizeof(double) or L for sizeof(long double).Adding a z suffix to any type displays printable characters atthe end of each output line.BYTES is hex with 0x or 0X prefix, and may have a multiplier suffix:b512KB1000K1024MB1000*1000M1024*1024and so on for G, T, P, E, Z, Y, R, Q.Binary prefixes can beused, too: KiB=K, MiB=M, and so on.",
        "name": "od - dump files in octal and other formats",
        "section": 1
    },
    {
        "command": "oomctl",
        "description": "oomctl may be used to get information about the various contextsread in by the systemd(1) userspace out-of-memory (OOM) killer,systemd-oomd(8).",
        "name": "oomctl - Analyze the state stored in systemd-oomd",
        "section": 1
    },
    {
        "command": "op-check-perfevents",
        "description": "The small helper program op-check-perfevents determines whetherthe kernel supports the perf interface and returns a zero exitstatus if the perf pmu support is available.",
        "name": "op-check-perfevents - checks for kernel perf pmu support",
        "section": 1
    },
    {
        "command": "opannotate",
        "description": "opannotate outputs annotated source and/or assembly from profiledata of an OProfile session. See oprofile(1) for how to writeprofile specifications.",
        "name": "opannotate - produce source or assembly annotated with profiledata",
        "section": 1
    },
    {
        "command": "oparchive",
        "description": "The oparchive utility is commonly used for collecting profiledata on a \"target\" system for future offline analysis on adifferent (\"host\") machine.oparchive creates a directorypopulated with executables, libraries, debuginfo files, andoprofile sample files. This directory can be tar'ed up and movedto another machine to be analyzed without further use of thetarget machine. Using opreport and other post-profiling toolsagainst archived data requires the use of the archive:<archived-dir> specification. See oprofile(1) for how to write profilespecifications.A complete description of offline analysis canbe found in the chapter titled Analyzing profile data on anothersystem (oparchive) of the OProfile user manual. (See the usermanual URL in the \"SEE ALSO\" section below.)",
        "name": "oparchive - produce archive of oprofile data for offline analysis",
        "section": 1
    },
    {
        "command": "openvt",
        "description": "openvt will find the first available VT, and run on it the givencommand with the given command options , standard input, outputand error are directed to that terminal. The current search path($PATH) is used to find the requested command. If no command isspecified then the environment variable $SHELL is used.OPTIONS-c, --console=VTNUMBERUse the given VT number and not the first available. Noteyou must have write access to the supplied VT for this towork.-f, --forceForce opening a VT without checking whether it is alreadyin use.-e, --execDirectly execute the given command, without forking.Thisoption is meant for use in /etc/inittab.-s, --switchSwitch to the new VT when starting the command. The VT ofthe new command will be made the new current VT.-u, --userFigure out the owner of the current VT, and run login asthat user.Suitable to be called by init. Shouldn't beused with -c or -l.-l, --loginMake the command a login shell. A - is prepended to thename of the command to be executed.-v, --verboseBe a bit more verbose.-w, --waitwait for command to complete. If -w and -s are usedtogether then openvt will switch back to the controllingterminal when the command completes.-V, --versionprint program version and exit.-h, --helpshow this text and exit.--end of options to openvt.",
        "name": "openvt - start a program on a new virtual terminal (VT).",
        "section": 1
    },
    {
        "command": "operf",
        "description": "Operf is the profiler tool provided with OProfile. Operf uses theLinux Performance Events Subsystem and, thus, does not requirethe obsolete oprofile kernel driver.By default, operf uses <current_dir>/oprofile_data as thesession-dir and stores profiling data there.You can change thisby way of the --session-dir option. The usual post-profilinganalysis tools such as opreport(1) and opannotate(1) can be usedto generate profile reports. Unless a session-dir is specified,the post-processing analysis tools will search for samples in<current_dir>/oprofile_data first. If that directory does notexist, the post-processing tools use the standard session-dir of/var/lib/oprofile.Statistics, such as total samples received and lost samples, arewritten to the operf.log file that can be found in the<session_dir>/samples directory.",
        "name": "operf - Performance profiler tool for Linux",
        "section": 1
    },
    {
        "command": "opgprof",
        "description": "opgprof outputs gprof-format profile data for a given binaryimage, from an OProfile session. See oprofile(1) for how to writeprofile specifications.",
        "name": "opgprof - produce gprof-format profile data",
        "section": 1
    },
    {
        "command": "ophelp",
        "description": "By default, ophelp lists the available performance counteroptions. If you give it a symbolic event name, it will return thehardware value (e.g. \"ophelp DATA_MEM_REFS\").",
        "name": "ophelp - list OProfile events",
        "section": 1
    },
    {
        "command": "opimport",
        "description": "opimport converts sample database files from a foreign binaryformat (abi) to the native format.",
        "name": "opimport - converts sample database files",
        "section": 1
    },
    {
        "command": "opjitconv",
        "description": "Convert a jit dump file to an ELF file",
        "name": "opjitconv - convert jit dump file to an ELF",
        "section": 1
    },
    {
        "command": "opreport",
        "description": "opreport outputs binary image summaries, or per-symbol data, fromOProfile profiling sessions. See oprofile(1) for how to writeprofile specifications.",
        "name": "opreport - produce symbol or binary image summaries",
        "section": 1
    },
    {
        "command": "oprofile",
        "description": "OProfile is a profiling system for systems running Linux 2.6.31and greater. OProfile makes use of the hardware performancecounters provided on Intel, AMD, and other processors.OProfilecan profile a selected program or process or the whole system.OProfile can also be used to collect cumulative event counts atthe application, process, or system level.For a gentle guide to using OProfile, please read the HTMLdocumentation listed in SEE ALSO.",
        "name": "oprofile - a statistical profiler for Linux systems, capable ofprofiling all running code at low overhead; also included is aset of post-profiling analysis tools, as well as a simple eventcounting tool",
        "section": 1
    },
    {
        "command": "ovn-detrace",
        "description": "The ovn-detrace program reads ovs-appctl ofproto/trace output onstdin, looking for flow cookies, and expand each cookie withcorresponding OVN logical flows. It expands logical flow furtherwith the north-bound information e.g. the ACL that generated thelogical flow, when relevant.",
        "name": "ovn-detrace - convert ``ovs-appctl ofproto/trace'' output tocombine OVN logical flow information.",
        "section": 1
    },
    {
        "command": "ovn-sim",
        "description": "ovn-sim is a wrapper script that adds ovn related commands on topof ovs-sim.ovs-sim provides a convenient environment for running one or moreOpen vSwitch instances and related software in a sandboxedsimulation environment.To use ovn-sim, first build Open vSwitch, then invoke it directlyfrom the build directory, e.g.:git clone https://github.com/openvswitch/ovs.gitcd ovs./boot.sh && ./configure && makecd ..git clone https://github.com/ovn-org/ovn.gitcd ovn./boot.sh && ./configure --with-ovs-source=${PWD}/../ovsmakeutilities/ovn-simSee documentation on ovs-sim for info on simulator, including theparameters you can use.OVN CommandsThese commands interact with OVN, the Open Virtual Network.ovn_start [options]Creates and initializes the central OVN databases (bothovn-sb(5) and ovn-nb(5)) and starts an instance ofovsdb-server for each one.Also starts an instance ofovn-northd.The following options are available:--nbdb-model modelUses the given database model for the northbounddatabase.The model may be standalone (thedefault), backup, or clustered.--nbdb-servers nFor a clustered northbound database, the numberof servers in the cluster.The default is 3.--sbdb-model modelUses the given database model for the southbounddatabase.The model may be standalone (thedefault), backup, or clustered.--sbdb-servers nFor a clustered southbound database, the numberof servers in the cluster.The default is 3.ovn_attach network bridge ip [masklen]First, this command attaches bridge to interconnectionnetwork network, just like net_attach network bridge.Second, it configures (simulated) IP address ip (withnetwork mask length masklen, which defaults to 24) onbridge. Finally, it configures the Open vSwitch databaseto work with OVN and starts ovn-controller.",
        "name": "ovn-sim - Open Virtual Network simulator environment",
        "section": 1
    },
    {
        "command": "ovs-pcap",
        "description": "The ovs-pcap program reads the pcap file named on the commandline and prints each packet's contents as a sequence of hexdigits on a line of its own.This format is suitable for usewith the ofproto/trace command supported by ovs-vswitchd(8).",
        "name": "ovs-pcap - print packets from a pcap file as hex",
        "section": 1
    },
    {
        "command": "ovs-sim",
        "description": "ovs-sim provides a convenient environment for running one or moreOpen vSwitch instances and related software in a sandboxedsimulation environment.To use ovs-sim, first build Open vSwitch, then invoke it directlyfrom the build directory, e.g.:git clone https://github.com/openvswitch/ovs.gitcd ovs./configuremakeutilities/ovs-simWhen invoked in the most ordinary way as shown above, ovs-simdoesthe following:1. Creates a directory sandbox as a subdirectory of the currentdirectory (first destroying such a directory if it alreadyexists) and makes it the current directory.2. Installs all of the Open vSwitch manpages into a mansubdirectory of sandbox and adjusts the MANPATH environmentvariable so that man and other manpage viewers can find them.3. Creates a simulated Open vSwitch named main and sets it up asthe default target for OVS commands, as if the followingovs-sim commands had been run:sim_add mainas mainSee Commands, below, for an explanation.4. Runsanyscriptsspecified on the command line (seeOptions, below). The scripts can use arbitrary Bashsyntax,plusthe additional commands described under Commands, below.5. If no scripts were specified, or if -i or --interactive wasspecified, invokes an interactive Bash subshell. The user canuse arbitrary Bash commands, plus the additional commandsdescribed under Commands, below.ovs-sim and the sandbox environment that it creates does notrequire superuser or other special privileges.Generally, itshould not be run with such privileges.",
        "name": "ovs-sim - Open vSwitch simulator environment",
        "section": 1
    },
    {
        "command": "ovs-tcpundump",
        "description": null,
        "name": "ovs-tcpundump - convert \"tcpdump -xx\" output to hex strings",
        "section": 1
    },
    {
        "command": "ovsdb-client",
        "description": "The ovsdb-client program is a command-line client for interactingwith a running ovsdb-server process.Each command connects tothe specified OVSDB server, which may be an OVSDB active orpassive connection method, as described in ovsdb(7).The defaultserver is unix:/usr/local/var/run/openvswitch/db.sock and thedefault database is Open_vSwitch.ovsdb-client supports the method1,method2,...,methodN syntaxdescribed in ovsdb(7) for connecting to a cluster.When thissyntax is used, ovsdb-client tries the cluster members in randomorder until it finds the cluster leader.Specify the--no-leader-only option to instead accept any server that isconnected to the cluster.For an introduction to OVSDB and its implementation in OpenvSwitch, see ovsdb(7).The following sections describe the commands that ovsdb-clientsupports.Server-Level CommandsMost ovsdb-client commands work with an individual database, butthese commands apply to an entire database server.list-dbs [server]Connects to server, retrieves the list of known databases,and prints them one per line.These database names arethe ones that other commands may use for database.Database Schema CommandsThese commands obtain the schema from a database and print it orpart of it.get-schema [server] [database]Connects to server, retrieves the schema for database, andprints it in JSON format.list-tables [server] [database]Connects to server, retrieves the schema for database, andprints a table listing the name of each table within thedatabase.list-columns [server] [database] tableConnects to server, retrieves the schema for database, andprints a table listing the name and type of each column.If table is specified, only columns in that table arelisted; otherwise, the tables include columns in alltables.Database Version Management CommandsAn OVSDB schema has a schema version number, and an OVSDBdatabase embeds a particular version of an OVSDB schema.Theseversion numbers take the form x.y.z, e.g. 1.2.3.The OVSDBimplementation does not enforce a particular version numberingscheme, but schemas managed within the Open vSwitch project usethe following approach.Whenever the database schema is changedin a non-backward compatible way (e.g. deleting a column or atable), x is incremented (and y and z are reset to 0).When thedatabase schema is changed in a backward compatible way (e.g.adding a new column), y is incremented (and z is reset to 0).When the database schema is changed cosmetically (e.g.reindenting its syntax), z is incremented.Some OVSDB databases and schemas, especially very old ones, donot have a version number.Schema version numbers and Open vSwitch version numbers areindependent.These commands work with different versions of OVSDB schemas anddatabases.convert [server] schemaReads an OVSDB schema in JSON format, as specified in theOVSDB specification, from schema, then connects to serverand requests the server to convert the database whose nameis specified in schema to the schema also specified inschema.The conversion is atomic, consistent, isolated, anddurable.Following the schema change, the server notifiesclients that use the set_db_change_aware RPC introduced inOpen vSwitch 2.9 and cancels their outstandingtransactions and monitors.The server disconnects otherclients, enabling them to notice the change when theyreconnect.This command can do simple ``upgrades'' and ``downgrades''on a database's schema.The data in the database must bevalid when interpreted under schema, with only oneexception: data for tables and columns that do not existin schema are ignored.Columns that exist in schema butnot in the database are set to their default values.Allof schema's constraints apply in full.Some uses of this command can cause unrecoverable dataloss.For example, converting a database from a schemathat has a given column or table to one that does not willdelete all data in that column or table.Back up criticaldatabases before converting them.This command works with clustered and standalonedatabases.Standalone databases may also be converted(offline) with ovsdb-tool's convert command.needs-conversion [server] schemaReads the schema from schema, then connects to server andrequests the schema from the database whose name isspecified in schema.If the two schemas are the same,prints no on stdout; if they differ, prints yes.get-schema-version [server] [database]Connects to server, retrieves the schema for database, andprints its version number on stdout.If database wascreated before schema versioning was introduced, then itwill not have a version number and this command will printa blank line.get-schema-cksum [server] [database]Connects to server, retrieves the schema for database, andprints its checksum on stdout.If database does notinclude a checksum, prints a blank line.Data Management CommandsThese commands read or modify the data in a database.transact [server] transactionConnects to server, sends it the specified transaction,which must be a JSON array appropriate for use as theparams to a JSON-RPC transact request, and prints thereceived reply on stdout.query [server] transactionThis commands acts like a read-only version of transact.It connects to server, sends it the specified transaction,which must be a JSON array appropriate for use as theparams to a JSON-RPC transact request, and prints thereceived reply on stdout.To ensure that the transactiondoes not modify the database, this command appends anabort operation to the set of operations included intransaction before sending it to the database, and thenremoves the abort result from the reply (if it ispresent).dump [server] [database] [table [column...]]Connects to server, retrieves all of the data in database,and prints it on stdout as a series of tables. If table isspecified, only that table is retrieved.If at least onecolumn is specified, only those columns are retrieved.backup [server] [database] > snapshotConnects to server, retrieves a snapshot of the schema anddata in database, and prints it on stdout in the formatused for OVSDB standalone and active-backup databases.This is an appropriate way to back up any remote database.The database snapshot that it outputs is suitable to beserved up directly by ovsdb-server or used as the input toovsdb-client restore.Another way to back up a standalone or active-backupdatabase is to copy its database file, e.g. with cp.Thisis safe even if the database is in use.The output does not include ephemeral columns, which bydesign do not survive across restarts of ovsdb-server.[--force] restore [server] [database] < snapshotReads snapshot, which must be a OVSDB standalone oractive-backup database (possibly but not necessarilycreated by ovsdb-client backup).Then, connects toserver, verifies that database and snapshot have the sameschema, then deletes all of the data in database andreplaces it by snapshot.The replacement happensatomically, in a single transaction.UUIDs for rows in the restored database will differ fromthose in snapshot, because the OVSDB protocol does notallow clients to specify row UUIDs.Another way torestore a standalone or active-backup database, which doesalso restore row UUIDs, is to stop the server or servers,replace the database file by the snapshot, then restartthe database.Either way, ephemeral columns are notrestored, since by design they do not survive acrossrestarts of ovsdb-server.Normally restore exits with a failure if snapshot and theserver's database have different schemas.In such a case,it is a good idea to convert the database to the newschema before restoring, e.g. with ovsdb-client convert.Use --force to proceed regardless of schema differenceseven though the restore might fail with an error orsucceed with surprising results.monitor [server] [database] table [column[,column]...]...monitor-cond [server] [database] conditions table[column[,column]...]...monitor-cond-since [server] [database] [last-id] conditions table[column[,column]...]...Connects to server and monitors the contents of rows thatmatch conditions in table in database. By default, theinitial contents of table are printed, followed by eachchange as it occurs.If conditions empty, all rows willbe monitored. If at least one column is specified, onlythose columns are monitored.The following column nameshave special meanings:!initialDo not print the initial contents of the specifiedcolumns.!insertDo not print newly inserted rows.!deleteDo not print deleted rows.!modifyDo not print modifications to existing rows.Multiple [column[,column]...] groups may be specified asseparate arguments, e.g. to apply different reportingparameters to each group.Whether multiple groups or onlya single group is specified, any given column may only bementioned once on the command line.conditions is a JSON array of <condition> as defined inRFC 7047 5.1 with the following change: A condition can beeither a 3-element JSON array as described in the RFC or aboolean value.If --detach is used with monitor, monitor-cond ormonitor-cond-since, then ovsdb-client detaches after ithas successfully received and printed the initial contentsof table.The monitor command uses RFC 7047 \"monitor\" method to opena monitor session with the server. The monitor-cond andmonitor-cond-since commandls uses RFC 7047 extension\"monitor_cond\" and \"monitor_cond_since\" methods. Seeovsdb-server(1) for details.monitor [server] [database] ALLConnects to server and monitors the contents of all tablesin database.Prints initial values and all kinds ofchanges to all columns in the database.The --detachoption causes ovsdb-client to detach after it successfullyreceives and prints the initial database contents.The monitor command uses RFC 7047 \"monitor\" method to opena monitor session with the server.wait [server] database stateWaits for database on server to enter a desired state,which may be one of:addedWaits until a database with the given name has beenadded to server.connectedWaits until a database with the given name has beenadded to server.Then, if database is clustered,additionally waits until it has joined andconnected to its cluster.removedWaits until database has been removed from thedatabase server.This can also be used to wait fora database to complete leaving its cluster, becauseovsdb-server removes a database at that point.database is mandatory for this command because it is oftenused to check for databases that have not yet been addedto the server, so that the ovsdb-client semantics ofacting on a default database do not work.This command acts on a particular database server, not ona cluster, so server must name a single server, not acomma-delimited list of servers.Testing commandsThese commands are mostly of interest for testing the correctnessof the OVSDB server.lock [server] locksteal [server] lockunlock [server] lockConnects to server and issues corresponding RFC 7047 lockoperations on lock. Prints json reply or subsequent updatemessages.The --detach option causes ovsdb-client todetach after it successfully receives and prints theinitial reply.When running with the --detach option, lock, steal, unlockand exit commands can be issued by using ovs-appctl. exitcommand causes the ovsdb-client to close its ovsdb-serverconnection before exit.The lock, steal and unlockcommands can be used to issue additional lock operationsover the same ovsdb-server connection. All above commandstake a single lock argument, which does not have to be thesame as the lock that ovsdb-client started with.",
        "name": "ovsdb-client - command-line interface to ovsdb-server(1)",
        "section": 1
    },
    {
        "command": "ovsdb-idlc",
        "description": "The ovsdb-idlc program is a command-line tool for translatingOpen vSwitch database interface definition language (IDL) schemasinto other formats.It is used while building Open vSwitch, notat installation or configuration time.Thus, it is not normallyinstalled as part of Open vSwitch.The idl files used as input for most ovsdb-idlc commands have thesame format as the OVSDB schemas, specified in the OVSDBspecification, with a few additions:\"idlPrefix\" member of <database-schema>This member, which is required, specifies a string that isprefixed to top-level names in C bindings.It shouldprobably end in an underscore.\"idlHeader\" member of <database-schema>This member, which is required, specifies the name of theIDL header.It will be output on an #include line in thesource file generated by the C bindings.It shouldinclude the bracketing \"\" or <>.\"cDecls\" member of <database-schema>\"hDecls\" member of <database-schema>These optional members may specify arbitrary code toinclude in the generated .c or .h file, respectively, ineach case just after the #include directives in thosefiles.\"extensions\" member of <table-schema>\"extensions\" member of <column-schema>This member is optional.If specified, it is an objectwhose contents describes extensions to the OVSDB schemalanguage, for the purpose of specifying interpretation bythe IDL.\"synthetic\" member of <column-schema> \"extensions\" objectIf this optional member is set to true, then it indicatesthat the column is not expected to be found in the actualdatabase.Instead, code supplied by the IDL's clientfills in the desired structure members based on the valueof one or more other database columns.This can be usedto cache the result of a calculation, for example.\"parse\" member of <column-schema> \"extensions\" objectThis member should be present if and only if the column issynthetic.It should be a string that contains C code toset the value of the column's member in an object namedrow, e.g. \"row->column = 1;\" if the column's name iscolumn and has integer type.The code may rely on thecolumns named in dependencies to be initialized.Thefunction can get called for rows that do not satisfy theconstraints in the schema, e.g. that a pointer to anotheris nonnull, so it must not rely on those constraints.\"unparse\" member of <column-schema> \"extensions\" objectThis member is honored only if the column is synthetic.It should be a string that contains C code to free thedata in the column's member in an object named row, e.g.\"free(row->column);\" if the column's name is column andpoints to data that was allocated by the parse functionand needs to be freed.\"dependencies\" member of <column-schema> \"extensions\" objectThis member should be a list of the names of columns whosevalues are used by the code in parse and unparse.The IDLensures that dependencies are parsed before the columnsthat depends on them, and vice versa for unparsing.Commandsannotate schema annotationsReads schema, which should be a file in JSON format(ordinarily an OVSDB schema file), then reads and executesthe Python syntax fragment in annotations.The Pythonsyntax fragment is passed the JSON object as a localvariable named s.It may modify this data in any way.After the Python code returns, the object as modified isre-serialized as JSON on standard output.c-idl-header idlReads idl and prints on standard output a C header filethat defines a structure for each table defined by theschema.If a column name in idl is a C or C++ keyword, itwill be appended with an underscore.c-idl-source idlReads idl and prints on standard output a C source filethat implements C bindings for the database defined by theschema.If a column name in idl is a C or C++ keyword, itwill be appended with an underscore.Options",
        "name": "ovsdb-idlc - Open vSwitch IDL (Interface Definition Language)compiler",
        "section": 1
    },
    {
        "command": "ovsdb-server",
        "description": "The ovsdb-server program provides RPC interfaces to one or moreOpen vSwitch databases (OVSDBs).It supports JSON-RPC clientconnections over active or passive TCP/IP or Unix domain sockets.For an introduction to OVSDB and its implementation in OpenvSwitch, see ovsdb(7).Each OVSDB file may be specified on the command line as database.Relay databases may be specified on the command line asrelay:schema_name:remote.For a detailed description of relaydatabase argument, see ovsdb(7).If none of database files orrelay databases is specified, the default is/usr/local/etc/openvswitch/conf.db.The database files mustalready have been created and initialized using, for example,ovsdb-tool's create, create-cluster, or join-cluster command.This OVSDB implementation supports standalone, active-backup,relay and clustered database service models, as well as databasereplication.See the Service Models section of ovsdb(7) for moreinformation.For clustered databases, when the --detach option is used,ovsdb-server detaches without waiting for the server tosuccessfully join a cluster (if the database file is freshlycreated with ovsdb-tool join-cluster) or connect to a clusterthat it has already joined.Use ovsdb-client wait (seeovsdb-client(1)) to wait until the server has successfully joinedand connected to a cluster.The same is true for relaydatabases.Same commands could be used to wait for a relaydatabase to connect to the relay source (remote).In addition to user-specified databases, ovsdb-server version 2.9and later also always hosts a built-in database named _Server.Please see ovsdb-server(5) for documentation on this database'sschema.",
        "name": "ovsdb-server - Open vSwitch database server",
        "section": 1
    },
    {
        "command": "ovsdb-tool",
        "description": "The ovsdb-tool program is a command-line tool for managing OpenvSwitch database (OVSDB) files.It does not interact directlywith running Open vSwitch database servers (instead, useovsdb-client).For an introduction to OVSDB and itsimplementation in Open vSwitch, see ovsdb(7).Each command that takes an optional db or schema argument has adefault file location if it is not specified..The default db is/usr/local/etc/openvswitch/conf.db.The default schema is/usr/local/share/openvswitch/vswitch.ovsschema.This OVSDB implementation supports standalone and active-backupdatabase service models with one on-disk format and a clustereddatabase service model with a different format.ovsdb-toolsupports both formats, but some commands are appropriate for onlyone format, as documented for individual commands below.For aspecification of these formats, see ovsdb(5).For moreinformation on OVSDB service models, see the Service Modelssection in ovsdb(7).Database Creation CommandsThese commands create a new OVSDB database file.They will notoverwrite an existing database file.To replace an existingdatabase with a new one, first delete the old one.create [db [schema]]Use this command to create the database for controllingovs-vswitchd or another standalone or active-backupdatabase.It creates database file db with the givenschema, which must be the name of a file that contains anOVSDB schema in JSON format, as specified in the OVSDBspecification.The new database is initially empty.(Youcan use cp to copy a database including both its schemaand data.)[--election-timer=ms] create-cluster db contents localUse this command to initialize the first server in a high-availability cluster of 3 (or more) database servers, e.g.for a database in an environment that cannot tolerate asingle point of failure.It creates clustered databasefile db and configures the server to listen on local,which must take the form protocol:ip:port, where protocolis tcp or ssl, ip is the server's IP (either an IPv4address or an IPv6 address enclosed in square brackets),and port is a TCP port number.Only one address isspecified, for the first server in the cluster, ordinarilythe one for the server running create-cluster.Theaddress is used for communication within the cluster, notfor communicating with OVSDB clients, and must not use thesame port used for the OVSDB protocol.The new database is initialized with contents, which mustname a file that contains either an OVSDB schema in JSONformat or a standalone OVSDB database.If it is a schemafile, the new database will initially be empty, with thegiven schema.If it is a database file, the new databasewill have the same schema and contents.Leader election will be initiated by a follower if thereis no heartbeat received from the cluster leader withinthe specified election timer.The default leader electiontimer is 1000 milliseconds. To use a different value whencreating the database, specify --election-timer=ms, wherems is a value in milliseconds between 100 and 600000inclusive.[--cid=uuid] join-cluster db name local remote...Use this command to initialize each server after the firstone in an OVSDB high-availability cluster.It createsclustered database file db for a database named name, andconfigures the server to listen on local and to initiallyconnect to remote, which must be a server that alreadybelongs to the cluster.local and remote use the sameprotocol:ip:port syntax as create-cluster.The name must be the name of the schema or database passedto create-cluster.For example, the name of the OVNSouthbound database schema is OVN_Southbound.Useovsdb-tool's schema-name or db-name command to find outthe name of a schema or database, respectively.This command does not do any network access, which meansthat it cannot actually join the new server to thecluster.Instead, the db file that it creates preparesthe server to join the cluster the first time thatovsdb-server serves it.As part of joining the cluster,the new server retrieves the database schema and obtainsthe list of all cluster members.Only after that does itbecome a full member of the cluster.Optionally, more than one remote may be specified; forexample, in a cluster that already contains multipleservers, one could specify all the existing servers.Thisis beneficial if some of the existing servers are downwhile the new server joins, but it is not otherwiseneeded.By default, the db created by join-cluster will join anyclustered database named name that is available at aremote.In theory, if machines go up and down and IPaddresses change in the right way, it could join the wrongdatabase cluster.To avoid this possibility, specify--cid=uuid, where uuid is the cluster ID of the cluster tojoin, as printed by ovsdb-tool get-cid.Database Migration CommandsThis commands will convert cluster database to standalonedatabase.cluster-to-standalone db clusterdbUse this command to convert to standalone database fromclustered database when the cluster is down and cannot berevived. It creates new standalone db file from the givencluster db file.Version Management CommandsAn OVSDB schema has a schema version number, and an OVSDBdatabase embeds a particular version of an OVSDB schema.Theseversion numbers take the form x.y.z, e.g. 1.2.3.The OVSDBimplementation does not enforce a particular version numberingscheme, but schemas managed within the Open vSwitch project usethe following approach.Whenever the database schema is changedin a non-backward compatible way (e.g. deleting a column or atable), x is incremented (and y and z are reset to 0).When thedatabase schema is changed in a backward compatible way (e.g.adding a new column), y is incremented (and z is reset to 0).When the database schema is changed cosmetically (e.g.reindenting its syntax), z is incremented.Some OVSDB databases and schemas, especially very old ones, donot have a version number.Schema version numbers and Open vSwitch version numbers areindependent.These commands work with different versions of OVSDB schemas anddatabases.convert [db [schema [target]]]Reads db, translating it into to the schema specified inschema, and writes out the new interpretation.If targetis specified, the translated version is written as a newfile named target, which must not already exist.Iftarget is omitted, then the translated version of thedatabase replaces db in-place.In-place conversion cannottake place if the database is currently being served byovsdb-server (instead, either stop ovsdb-server first oruse ovsdb-client's convert command).This command can do simple ``upgrades'' and ``downgrades''on a database's schema.The data in db must be valid wheninterpreted under schema, with only one exception: data indb for tables and columns that do not exist in schema areignored.Columns that exist in schema but not in db areset to their default values.All of schema's constraintsapply in full.Some uses of this command can cause unrecoverable dataloss.For example, converting a database from a schemathat has a given column or table to one that does not willdelete all data in that column or table.Back up criticaldatabases before converting them.This command is for standalone and active-backup databasesonly.For clustered databases, use ovsdb-client's convertcommand to convert them online.needs-conversion [db [schema]]Reads the schema embedded in db and the JSON schema fromschema and compares them.If the schemas are the same,prints no on stdout; if they differ, prints yes.This command is for standalone and active-backup databasesonly.For clustered databases, use ovsdb-client's needs-conversion command instead.db-version [db]schema-version [schema]Prints the version number in the schema embedded withinthe database db or in the JSON schema schema on stdout.If schema or db was created before schema versioning wasintroduced, then it will not have a version number andthis command will print a blank line.The db-version command is for standalone and active-backupdatabases only.For clustered databases, useovsdb-client's schema-version command instead.db-cksum [db]schema-cksum [schema]Prints the checksum in the schema embedded within thedatabase db or of the JSON schema schema on stdout.Ifschema or db was created before schema checksums wereintroduced, then it will not have a checksum and thiscommand will print a blank line.The db-cksum command is for standalone and active-backupdatabases only.For clustered databases, useovsdb-client's schema-cksum command instead.compare-versions a op bCompares a and b according to op.Both a and b must beOVSDB schema version numbers in the form x.y.z, asdescribed in ovsdb(7), and op must be one of < <= == >= >!=.If the comparison is true, exits with status 0; if itis false, exits with status 2.(Exit status 1 indicatesan error, e.g. a or b is the wrong syntax for an OVSDBversion or op is not a valid comparison operator.)Other Commandscompact [db [target]]Reads db and writes a compacted version.If target isspecified, the compacted version is written as a new filenamed target, which must not already exist.If target isomitted, then the compacted version of the databasereplaces db in-place.This command is not needed innormal operation because ovsdb-server from time to timeautomatically compacts a database that grows much largerthan its minimum size.This command does not work if db is currently being servedby ovsdb-server, or if it is otherwise locked for writingby another process.This command also does not work withclustered databases.Instead, in either case, send theovsdb-server/compact command to ovsdb-server, viaovs-appctl).[--rbac-role=role] query [db] transactionOpens db, executes transaction on it, and prints theresults.The transaction must be a JSON array in theformat of the params array for the JSON-RPC transactmethod, as described in the OVSDB specification.This command opens db for read-only access, so it maysafely run concurrently with other database activity,including ovsdb-server and other database writers.Thetransaction may specify database modifications, but thesewill have no effect on db.By default, the transaction is executed using the``superuser'' RBAC role.Use --rbac-role to specify adifferent role.This command does not work with clustered databases.Instead, use ovsdb-client's query command to send thequery to ovsdb-server.[--rbac-role=role] transact [db] transactionOpens db, executes transaction on it, prints the results,and commits any changes to db.The transaction must be aJSON array in the format of the params array for the JSON-RPC transact method, as described in the OVSDBspecification.This command does not work if db is currently being servedby ovsdb-server, or if it is otherwise locked for writingby another process.This command also does not work withclustered databases.Instead, in either case, useovsdb-client's transact command to send the query toovsdb-server.By default, the transaction is executed using the``superuser'' RBAC role.Use --rbac-role to specify adifferent role.[-m | --more]... show-log [db]Prints a summary of the records in db's log, including thetime and date at which each database change occurred andany associated comment.This may be useful for debugging.To increase the verbosity of output, add -m (or --more)one or more times to the command line.With one -m,show-log prints a summary of the records added, deleted,or modified by each transaction.With two -ms, show-logalso prints the values of the columns modified by eachchange to a record.This command works with standalone and active-backupdatabases and with clustered databases, but the outputformats are different.check-cluster db...Reads all of the records in the supplied databases, whichmust be collected from different servers (and ideally allthe servers) in a single cluster.Checks each databasefor self-consistency and the set together for cross-consistency.If ovsdb-tool detects unusual but notnecessarily incorrect content, it prints a warning orwarnings on stdout.If ovsdb-tool find consistencyerrors, it prints an error on stderr and exits with status1.Errors typically indicate bugs in ovsdb-server; pleaseconsider reporting them to the Open vSwitch developers.db-name [db]schema-name [schema]Prints the name of the schema embedded within the databasedb or in the JSON schema schema on stdout.db-cid dbPrints the cluster ID, which is a UUID that identifies thecluster, for db.If db is a database newly created byovsdb-tool cluster-join that has not yet successfullyjoined its cluster, and --cid was not specified on thecluster-join command line, then this command will outputan error, and exit with status 2, because the cluster IDis not yet known.This command works only with clustereddatabases.The all-zeros UUID is not a valid cluster ID.db-sid dbPrints the server ID, which is a UUID that identifies theserver, for db.This command works only with clustereddatabases.It works even if db is a database newlycreated by ovsdb-tool cluster-join that has not yetsuccessfully joined its cluster.db-local-address dbPrints the local address used for database clustering fordb, in the same protocol:ip:port form used oncreate-cluster and join-cluster.db-is-clustered dbdb-is-standalone dbTests whether db is a database file in clustered orstandalone format, respectively.If so, exits with status0; if not, exits with status 2.(Exit status 1 indicatesan error, e.g. db is not an OVSDB database or does notexist.)",
        "name": "ovsdb-tool - Open vSwitch database management utility",
        "section": 1
    },
    {
        "command": "p11tool",
        "description": "Program that allows operations on PKCS #11 smart cards andsecurity modules.To use PKCS #11 tokens with GnuTLS the p11-kit configurationfiles need to be setup.That is create a .module file in/etc/pkcs11/modules with the contents 'module:/path/to/pkcs11.so'.Alternatively the configuration file/etc/gnutls/pkcs11.conf has to exist and contain a number oflines of the form 'load=/usr/lib/opensc-pkcs11.so'.You can provide the PIN to be used for the PKCS #11 operationswith the environment variables GNUTLS_PIN and GNUTLS_SO_PIN.",
        "name": "p11tool - GnuTLS PKCS #11 tool",
        "section": 1
    },
    {
        "command": "package-cleanup",
        "description": "package-cleanup is a program for cleaning up the locally-installed RPMs.",
        "name": "package-cleanup - clean up locally installed, duplicate, ororphaned packages",
        "section": 1
    },
    {
        "command": "passwd",
        "description": "The passwd command changes passwords for user accounts. A normaluser may only change the password for their own account, whilethe superuser may change the password for any account.passwdalso changes the account or associated password validity period.Password ChangesThe user is first prompted for their old password, if one ispresent. This password is then encrypted and compared against thestored password. The user has only one chance to enter thecorrect password. The superuser is permitted to bypass this stepso that forgotten passwords may be changed.After the password has been entered, password aging informationis checked to see if the user is permitted to change the passwordat this time. If not, passwd refuses to change the password andexits.The user is then prompted twice for a replacement password. Thesecond entry is compared against the first and both are requiredto match in order for the password to be changed.Then, the password is tested for complexity.passwd will rejectany password which is not suitably complex. Care must be takennot to include the system default erase or kill characters.Hints for user passwordsThe security of a password depends upon the strength of theencryption algorithm and the size of the key space. The legacyUNIX System encryption method is based on the NBS DES algorithm.More recent methods are now recommended (see ENCRYPT_METHOD). Thesize of the key space depends upon the randomness of the passwordwhich is selected.Compromises in password security normally result from carelesspassword selection or handling. For this reason, you should notselect a password which appears in a dictionary or which must bewritten down. The password should also not be a proper name, yourlicense number, birth date, or street address. Any of these maybe used as guesses to violate system security.As a general guideline, passwords should be long and random. It'sfine to use simple character sets, such as passwords consistingonly of lowercase letters, if that helps memorizing longerpasswords. For a password consisting only of lowercase Englishletters randomly chosen, and a length of 32, there are 26^32(approximately 2^150) different possible combinations. Being anexponential equation, it's apparent that the exponent (thelength) is more important than the base (the size of thecharacter set).You can find advice on how to choose a strong password onhttp://en.wikipedia.org/wiki/Password_strength",
        "name": "passwd - change user password",
        "section": 1
    },
    {
        "command": "paste",
        "description": "Write lines consisting of the sequentially corresponding linesfrom each FILE, separated by TABs, to standard output.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-d, --delimiters=LISTreuse characters from LIST instead of TABs-s, --serialpaste one file at a time instead of in parallel-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exit",
        "name": "paste - merge lines of files",
        "section": 1
    },
    {
        "command": "patch",
        "description": "patch takes a patch file patchfile containing a differencelisting produced by the diff program and applies thosedifferences to one or more original files, producing patchedversions.Normally the patched versions are put in place of theoriginals.Backups can be made; see the -b or --backup option.The names of the files to be patched are usually taken from thepatch file, but if there's just one file to be patched it can bespecified on the command line as originalfile.Upon startup, patch attempts to determine the type of the difflisting, unless overruled by a -c (--context), -e (--ed), -n(--normal), or -u (--unified) option.Context diffs (old-style,new-style, and unified) and normal diffs are applied by the patchprogram itself, while ed diffs are simply fed to the ed(1) editorvia a pipe.patch tries to skip any leading garbage, apply the diff, and thenskip any trailing garbage.Thus you could feed an article ormessage containing a diff listing to patch, and it should work.If the entire diff is indented by a consistent amount, if linesend in CRLF, or if a diff is encapsulated one or more times byprepending \"- \" to lines starting with \"-\" as specified byInternet RFC 934, this is taken into account.After removingindenting or encapsulation, lines beginning with # are ignored,as they are considered to be comments.With context diffs, and to a lesser extent with normal diffs,patch can detect when the line numbers mentioned in the patch areincorrect, and attempts to find the correct place to apply eachhunk of the patch.As a first guess, it takes the line numbermentioned for the hunk, plus or minus any offset used in applyingthe previous hunk.If that is not the correct place, patch scansboth forwards and backwards for a set of lines matching thecontext given in the hunk.First patch looks for a place whereall lines of the context match.If no such place is found, andit's a context diff, and the maximum fuzz factor is set to 1 ormore, then another scan takes place ignoring the first and lastline of context.If that fails, and the maximum fuzz factor isset to 2 or more, the first two and last two lines of context areignored, and another scan is made.(The default maximum fuzzfactor is 2.)Hunks with less prefix context than suffix context (afterapplying fuzz) must apply at the start of the file if their firstline number is 1.Hunks with more prefix context than suffixcontext (after applying fuzz) must apply at the end of the file.If patch cannot find a place to install that hunk of the patch,it puts the hunk out to a reject file, which normally is the nameof the output file plus a .rej suffix, or # if .rej wouldgenerate a file name that is too long (if even appending thesingle character # makes the file name too long, then # replacesthe file name's last character).The rejected hunk comes out in unified or context diff format.If the input was a normal diff, many of the contexts are simplynull.The line numbers on the hunks in the reject file may bedifferent than in the patch file: they reflect the approximatelocation patch thinks the failed hunks belong in the new filerather than the old one.As each hunk is completed, you are told if the hunk failed, andif so which line (in the new file) patch thought the hunk shouldgo on.If the hunk is installed at a different line from theline number specified in the diff, you are told the offset.Asingle large offset may indicate that a hunk was installed in thewrong place.You are also told if a fuzz factor was used to makethe match, in which case you should also be slightly suspicious.If the --verbose option is given, you are also told about hunksthat match exactly.If no original file origfile is specified on the command line,patch tries to figure out from the leading garbage what the nameof the file to edit is, using the following rules.First, patch takes an ordered list of candidate file names asfollows:\u2022 If the header is that of a context diff, patch takes the oldand new file names in the header.A name is ignored if itdoes not have enough slashes to satisfy the -pnum or--strip=num option.The name /dev/null is also ignored.\u2022 If there is an Index: line in the leading garbage and ifeither the old and new names are both absent or if patch isconforming to POSIX, patch takes the name in the Index: line.\u2022 For the purpose of the following rules, the candidate filenames are considered to be in the order (old, new, index),regardless of the order that they appear in the header.Then patch selects a file name from the candidate list asfollows:\u2022 If some of the named files exist, patch selects the first nameif conforming to POSIX, and the best name otherwise.\u2022 If patch is not ignoring RCS, ClearCase, Perforce, and SCCS(see the -g num or --get=num option), and no named files existbut an RCS, ClearCase, Perforce, or SCCS master is found,patch selects the first named file with an RCS, ClearCase,Perforce, or SCCS master.\u2022 If no named files exist, no RCS, ClearCase, Perforce, or SCCSmaster was found, some names are given, patch is notconforming to POSIX, and the patch appears to create a file,patch selects the best name requiring the creation of thefewest directories.\u2022 If no file name results from the above heuristics, you areasked for the name of the file to patch, and patch selectsthat name.To determine the best of a nonempty list of file names, patchfirst takes all the names with the fewest path name components;of those, it then takes all the names with the shortest basename;of those, it then takes all the shortest names; finally, it takesthe first remaining name.Additionally, if the leading garbage contains a Prereq: line,patch takes the first word from the prerequisites line (normallya version number) and checks the original file to see if thatword can be found.If not, patch asks for confirmation beforeproceeding.The upshot of all this is that you should be able to say, whilein a news interface, something like the following:| patch -d /usr/src/local/blurfland patch a file in the blurfl directory directly from thearticle containing the patch.If the patch file contains more than one patch, patch tries toapply each of them as if they came from separate patch files.This means, among other things, that it is assumed that the nameof the file to patch must be determined for each diff listing,and that the garbage before each diff listing containsinteresting things such as file names and revision level, asmentioned previously.",
        "name": "patch - apply a diff file to an original",
        "section": 1
    },
    {
        "command": "pathchk",
        "description": "Diagnose invalid or unportable file names.-pcheck for most POSIX systems-Pcheck for empty names and leading \"-\"--portabilitycheck for all POSIX systems (equivalent to -p -P)--help display this help and exit--versionoutput version information and exit",
        "name": "pathchk - check whether file names are valid or portable",
        "section": 1
    },
    {
        "command": "pcap-config",
        "description": "When run with the --cflags option, pcap-config writes to thestandard output the -I compiler flags required to includelibpcap's header files.When run with the --libs option, pcap-config writes to the standard output the -L and -l linker flagsrequired to link with libpcap, including -l flags for librariesrequired by libpcap.When run with the --additional-libs option,pcap-config writes to the standard output the -L and -l flags forlibraries required by libpcap, but not the -lpcap flag to linkwith libpcap itself.By default, it writes flags appropriate for compiling with adynamically-linked version of libpcap; the --static flag causesit to write flags appropriate for compiling with a statically-linked version of libpcap.",
        "name": "pcap-config - write libpcap compiler and linker flags to standardoutput",
        "section": 1
    },
    {
        "command": "pcp",
        "description": "The pcp command is used in one of two modes.By default, itsummarizes the Performance Co-Pilot (PCP) installation on thelocal host.This mode can also be used to summarize theinstallation from a remote host, or a historical installationfrom a set of PCP archives.This mode indirectly invokes thepcp-summary command (in the absence of any other requestedcommand).Alternatively, a command can be passed to pcp to run, againpossibly in the context of a remote host or set of historicalarchives.",
        "name": "pcp, pcp-summary - run a command or summarize an installation",
        "section": 1
    },
    {
        "command": "pcp-atop",
        "description": "The program pcp-atop is an interactive monitor to view variousaspects of load on a system.It shows the occupation of the mostcritical hardware resources (from a performance point of view) onsystem level, i.e. cpu, memory, disk and network.It also shows which processes are responsible for the indicatedload with respect to cpu and memory load on process level.Diskload is shown per process if \"storage accounting\" is active inthe kernel.Every interval (default: 10 seconds) information is shown aboutthe resource occupation on system level (cpu, memory, disks andnetwork layers), followed by a list of processes which have beenactive during the last interval (note that all processes thatwere unchanged during the last interval are not shown, unless thekey 'a' has been pressed or unless sorting on memory occupationis done).If the list of active processes does not entirely fiton the screen, only the top of the list is shown (sorted in orderof activity).The intervals are repeated till the number of samples (specifiedas command argument) is reached, or till the key 'q' is pressedin interactive mode.When invoked via the pcp(1) command, the PCPIntro(1) options-A/--align, -a/--archive, -h/--host, -O/--origin, -S/--start,-s/--samples, -T/--finish, -t/--interval, -v/--version,-z/--hostzone and -z/--timezone become indirectly available.Additionally, the --hotproc option can be used to request theper-process PCP metrics be used instead of the default procmetrics from pmdaproc(1).When pcp-atop is started, it checks whether the standard outputchannel is connected to a screen, or to a file/pipe.In thefirst case it produces screen control codes (via the ncurseslibrary) and behaves interactively; in the second case itproduces flat ASCII-output.In interactive mode, the output of pcp-atop scales dynamically tothe current dimensions of the screen/window.If the window is resized horizontally, columns will be added orremoved automatically. For this purpose, every column has aparticular weight.The columns with the highest weights that fitwithin the current width will be shown.If the window is resized vertically, lines of the process/threadlist will be added or removed automatically.Furthermore in interactive mode the output of pcp-atop can becontrolled by pressing particular keys.However it is alsopossible to specify such key as flag on the command line.Inthat case pcp-atop switches to the indicated mode on beforehand;this mode can be modified again interactively.Specifying suchkey as flag is especially useful when running pcp-atop withoutput to a pipe or file (non-interactively).These flags arethe same as the keys that can be pressed in interactive mode (seesection INTERACTIVE COMMANDS).Additional flags are available to support storage of pcp-atopdata in PCP archive format (see section PCP DATA STORAGE).",
        "name": "pcp-atop - Advanced System and Process Monitor",
        "section": 1
    },
    {
        "command": "pcp-atopsar",
        "description": "The pcp-atopsar program can be used to report statistics at thesystem level.In the first synopsis line (no sampling interval specified), pcp-atopsar extracts data from a raw logfile that has been recordedpreviously by pmlogger(1) (or via the -w option of the pcp-atopprogram).You can specify the name of the logfile with the -r option of thepcp-atopsar program.When a pmlogger daily logfile is used,named $PCP_LOG_DIR/pmlogger/[host]/YYYYMMDD (where YYYYMMDDreflects the date), the required date of the form YYYYMMDD can bespecified with the -r option instead of the filename, or thesymbolic name 'y' can be used for yesterday's daily logfile (thiscan be repeated so 'yyyy' indicates the logfile of four daysago).If the -r option is not specified at all, today's dailylogfile is used by default.By default, the hostname of the localhost will be used whenresolving pmlogger archives, however an alternative host can bespecified using the -h option.The starting and ending times of the report can be defined usingthe options -b and -e followed by a time argument of the form[yy-mm-dd] hh:mm.In the second synopsis line, pcp-atopsar reads actual activitycounters from the kernel with the specified interval (in seconds)and the specified number of samples (optionally).When pcp-atopsar is activated in this way it immediately sends the outputfor every requested report to standard output.If only one typeof report is requested, the header is printed once and afterevery interval seconds the statistical counters are shown forthat period.If several reports are requested, a header isprinted per sample followed by the statistical counters for thatperiod.When invoked via the pcp(1) command, the PCPIntro(1) options-h/--host, -a/--archive, -O/--origin, -s/--samples,-t/--interval, -Z/--timezone and several other pcp options becomeindirectly available, see PCPIntro(1) for their descriptions.Some generic flags can be specified to influence the behaviour ofthe pcp-atopsar program:-SBy default the timestamp at the beginning of a line issuppressed if more lines are shown for one interval. Withthis flag a timestamp is given for every output-line (easierfor post-processing).-aBy default certain resources as disks and network interfacesare only shown when they were active during the interval.With this flag all resources of a given type are shown, evenif they were inactive during the interval.-xBy default pcp-atopsar only uses colors if output isdirected to a terminal (window).These colors mightindicate that a critical occupation percentage has beenreached (red) or has been almost reached (cyan) for aparticular resource.See the man-page of atop for adetailed description of this feature (section COLORS).With the flag -x the use of colors is suppressedunconditionally.-CBy default pcp-atopsar only uses colors if output isdirected to a terminal (window).These colors mightindicate that a critical occupation percentage has beenreached (red) or has been almost reached (cyan) for aparticular resource.See the man-page of atop for adetailed description of this feature (section COLORS).With the flag -C colors will always be used, even if outputis not directed to a terminal.-MUse markers at the end of a line to indicate that a criticaloccupation percentage has been reached ('*') or has beenalmost reached ('+') for particular resources. The marker'*' is similar to the color red and the marker '+' to thecolor cyan. See the man-page of atop for a detaileddescription of these colors (section COLORS).-HRepeat the header line within a report for every N detaillines. The value of N is determined dynamically in case ofoutput to a tty/window (depending on the number of lines);for output to a file or pipe this value is 23.-RSummarize cnt samples into one sample. When the logfilecontains e.g. samples of 10 minutes, the use of the flag '-R6' shows a report with one sample for every hour.Other flags are used to define which reports are required:-AShow all possible reports.-cReport about CPU utilization (in total and per cpu).-gReport about GPU utilization (per GPU).-pReport about processor-related matters, like load-averagesand hardware interrupts.-PReport about processes.-mCurrent memory- and swap-occupation.-sReport about paging- and swapping-activity, andovercommitment.-BReport about Pressure Stall Information (PSI).-lReport about utilization of logical volumes.-fReport about utilization of multiple devices.-dReport about utilization of disks.-nReport about NFS mounted filesystems on NFS client.-jReport about NFS client activity.-JReport about NFS server activity.-iReport about the network interfaces.-IReport about errors for network-interfaces.-wReport about IP version 4 network traffic.-WReport about errors for IP version 4 traffic.-yGeneral report about ICMP version 4 layer activity.-YPer-type report about ICMP version 4 layer activity.-uReport about UDP version 4 network traffic.-zReport about IP version 6 network traffic.-ZReport about errors for IP version 6 traffic.-kGeneral report about ICMP version 6 layer activity.-KPer-type report about ICMP version 6 layer activity.-UReport about UDP version 6 network traffic.-tReport about TCP network traffic.-TReport about errors for TCP-traffic.-hReport about Infiniband utilization.-OReport about top-3 processes consuming most processorcapacity.This report is only available when using a logfile (not when specifying an interval).-GReport about top-3 processes consuming most resident memory.This report is only available when using a log file (notwhen specifying an interval).-DReport about top-3 processes issuing most disk transfers.This report is only available when using a log file (notwhen specifying an interval).-NReport about top-3 processes issuing most IPv4/IPv6 sockettransfers.This report is only available when using a logfile (not when specifying an interval).",
        "name": "pcp-atopsar - Advanced System Activity Report (pcp-atop related)",
        "section": 1
    },
    {
        "command": "pcp-collectl",
        "description": null,
        "name": "PCPCompat, pcp-collectl, pmmgr, pmwebd - backward-compatibilityin the Performance Co-Pilot (PCP)",
        "section": 1
    },
    {
        "command": "pcp-dmcache",
        "description": "pcp-dmcache reports on the activity of any configured DeviceMapper Cache targets.The reported information includes deviceIOPs, cache and metadata device utilization, as well as hit andmiss rates and ratios for both reads and writes for each cachedevice.pcp-lvmcache (Logical Volume Manager cache) is an exact synonymfor pcp-dmcache (Device Mapper cache).By default, pcp-dmcache reports on all available cache targetdevices (one line each, per sample), but this can be restrictedto specific devices on the command line.",
        "name": "pcp-dmcache, pcp-lvmcache - report on logical storage devicecaches",
        "section": 1
    },
    {
        "command": "pcp-dstat",
        "description": "pcp-dstat is a general performance analysis tool allowing you toview multiple system resources instantly, for example you cancompare disk usage in combination with interrupts from a diskcontroller, or compare the network bandwidth numbers directlywith the disk throughput (in the same interval).It also cleverly gives you the most detailed information incolumns and clearly indicates in what magnitude and unit theoutput is being displayed.Less confusion, fewer mistakes, moreefficient.The delay is the delay in seconds between each update, and thecount is the number of updates to display before exiting.Thedefault delay is 1 second and count is unspecified (run untilinterrupted or end of archive is reached).This latest generation of Dstat, pcp-dstat, allows for analysisof historical performance data (in the PCP archive format createdby pmlogger(1)), as well as distributed systems analysis of liveperformance data from remote hosts running the pmcd(1) process.The original Dstat notion of ``plugins'' is replaced by use ofnamed metrics in a Performance Metric Name Space (PMNS(5))supplied by Performance Metric Domain Agents (PMDAs).Metricsand other formatting information is now specified as pluginconfiguration files in pcp-dstat(5) format.This new style ofplugin is either built-in (time-related reporting only), orsourced from the system-wide location ($PCP_SYSCONF_DIR/dstat)and/or sourced from an individual users set of personal plugins($HOME/.pcp/dstat).The list of all available plugins can be seen using the --listdstat command line option.",
        "name": "pcp-dstat - versatile tool for generating system resourcestatistics",
        "section": 1
    },
    {
        "command": "pcp-free",
        "description": "pcp-free gives a summary display of the total amount of free andused physical memory and swap in the system, as well as thecaches used by the kernel.When invoked via the pcp(1) command, the -h/--host, -a/--archive,-O/--origin, -s/--samples, -t/--interval, -Z/--timezone andseveral other pcp options become indirectly available, seePCPIntro(1) for their descriptions.The displayed columns are:totalTotal installed memory (MemTotal and SwapTotal in/proc/meminfo)usedUsed memory (calculated as total - free - buffers - cache)freeUnused memory (MemFree and SwapFree in /proc/meminfo)shared Memory used (mostly) by tmpfs (Shmem in /proc/meminfo)buffersMemory used by kernel buffers (Buffers in /proc/meminfo)cacheMemory used by the page cache and slabs (Cached andSReclaimable in /proc/meminfo)buff/cacheSum of buffers and cacheavailableEstimation of how much memory is available for startingnew applications, without swapping.Unlike the dataprovided by the cache or free fields, this field takesinto account page cache and also that not all reclaimablememory slabs will be reclaimed due to items being in use(MemAvailable in /proc/meminfo).",
        "name": "pcp-free - report on free and used memory in the system",
        "section": 1
    },
    {
        "command": "pcp-htop",
        "description": "htop is a cross-platform ncurses-based process viewer.It is similar to top, but allows you to scroll vertically andhorizontally, and interact using a pointing device (mouse).Youcan observe all processes running on the system, along with theircommand line arguments, as well as view them in a tree format,select multiple processes and act on them all at once.Tasks related to processes (killing, renicing) can be donewithout entering their PIDs.pcp-htop is a version of htop built using the Performance Co-Pilot (PCP) Metrics API (see PCPIntro(1), PMAPI(3)), allowing toextend htop to display values from arbitrary metrics.See thesection below titled CONFIG FILES for further details.",
        "name": "htop, pcp-htop - interactive process viewer",
        "section": 1
    },
    {
        "command": "pcp-iostat",
        "description": "pcp-iostat reports I/O statistics for SCSI (by default) or otherdevices (if the -x option is specified).",
        "name": "pmiostat, pcp-iostat - report block I/O statistics",
        "section": 1
    },
    {
        "command": "pcp-ipcs",
        "description": "pcp-ipcs provides information on the inter-process communicationfacilities for which the calling process has read access.",
        "name": "pcp-ipcs - provide information on IPC facilities",
        "section": 1
    },
    {
        "command": "pcp-kube-pods",
        "description": "pcp-kube-pods uses kubectl(1) to provide a list of IP addressesfor PODs running in a local Kubenetes cluster, that may berunning PCP services like pmcd(1) and pmproxy(1).It is used by the pmfind(1) command and the pmDiscoverServices(3)API as a ``shell'' command.The script invokes the kubectl get pod command line (seekubectl-get(1)) to discover IP addresses for pods.The.status.podIP output field is extracted from the Kubernetes podobject(s).Additional options can be specified via configurationfile, such as -l service=database to restrict the results usingKubernetes pod labels.The default configuration file is$PCP_SYSCONF_DIR/discover/pcp-kube-pods.conf.If no local kubectl command is found, nothing is reported and anexit code indicating success is returned.",
        "name": "pcp-kube-pods - list Kubernetes pods to scan for running PCPservices",
        "section": 1
    },
    {
        "command": "pcp-lvmcache",
        "description": "pcp-dmcache reports on the activity of any configured DeviceMapper Cache targets.The reported information includes deviceIOPs, cache and metadata device utilization, as well as hit andmiss rates and ratios for both reads and writes for each cachedevice.pcp-lvmcache (Logical Volume Manager cache) is an exact synonymfor pcp-dmcache (Device Mapper cache).By default, pcp-dmcache reports on all available cache targetdevices (one line each, per sample), but this can be restrictedto specific devices on the command line.",
        "name": "pcp-dmcache, pcp-lvmcache - report on logical storage devicecaches",
        "section": 1
    },
    {
        "command": "pcp-mpstat",
        "description": "pcp-mpstat command writes to standard output activities for eachavailable processor, processor 0 being the first one.If noactivity/option has been selected, then the default report is theCPU utilization (-u) report.The interval parameter specifies the amount of time in secondsbetween each report.The default is one second.The value ofcount parameter determines the number of samples to be displayed.The default is continous.",
        "name": "pcp-mpstat - Report CPU and interrupt related statistics.",
        "section": 1
    },
    {
        "command": "pcp-numastat",
        "description": "pcp-numastat displays NUMA allocation statistics from the kernelmemory allocator.Each process has NUMA policies that specify onwhich node pages are allocated.The performance counters in thekernel track on which nodes memory is allocated and these valuesare sampled and reported by pcp-numastat.Counters are maintained individually for each NUMA node.Detailsof the semantics of each reported metric can be retrieved usingthe following command:# pminfo \u2010dt mem.numa.alloc",
        "name": "pcp-numastat - report on NUMA memory allocation",
        "section": 1
    },
    {
        "command": "pcp-pidstat",
        "description": "The pcp-pidstat command is used for monitoring individual tasksrunning on the system.Using various options it helps a user tosee useful information related to the processes.Thisinformation includes CPU percentage, memory and stack usage,scheduling and priority.By default pcp-pidstat reports livedata for the local host.",
        "name": "pcp-pidstat - Report statistics for Linux tasks.",
        "section": 1
    },
    {
        "command": "pcp-ps",
        "description": "The pcp-ps command is used for monitoring individual processrunning on the system.Using various options it helps a user tosee useful information related to the processes.Thisinformation includes CPU percentage, memory and stack usage,scheduling and priority.By default pcp-ps reports live data forthe local host.",
        "name": "pcp-ps - Report statistics for Linux Process.",
        "section": 1
    },
    {
        "command": "pcp-python",
        "description": "pcp-python has been replaced by pmpython(1) which is preferred,however pcp-python is still installed to provided backwardscompatibility.pcp-python provides a way to run python scripts using acustomisable python binary, rather than embedding any particularversion of python into each script.This can be useful as it allows version-independent python codeto be run anywhere.All python modules shipped with PCP supportversions 2.6 and later (in the python2 series), and 3.3 and later(in the python3 release series).Due to python monitoring and collecting scripts being relativelysimple in PCP (not requiring new modules, language features,etc), it has been possible to ensure they work for all of theabove python versions.Thus, it is common for PCP python scriptsto use a \"shebang\" line that invokes pcp-python as follows:#!/usr/bin/pcp pythonThis allows the custom setting to be injected instead of a hard-coded python version, while still allowing the user to overridethe python version as follows:$ PCP_PYTHON_PROG=python3 /usr/bin/pcp python --versionPython 3.3.2$ PCP_PYTHON_PROG=python2 /usr/bin/pcp python --versionPython 2.7.5This is convenient for shipping identical scripts on multipleplatforms, and for testing different python versions with the onescript (e.g. in the case where multiple versions of python areinstalled, PCP_PYTHON_PROG can be set in the local environment tooverride the global setting).By default, the value of PCP_PYTHON_PROG from /etc/pcp.conf willbe used.The default value of this configuration parameter isset depending on some heuristics about the target build platform.These heuristics favour the use of python3 in all recent releasesof PCP, for those platforms that support it.",
        "name": "pcp-python - run a python script using a preferred python variant",
        "section": 1
    },
    {
        "command": "pcp-shping",
        "description": "pcp-shping samples and reports on the shell-ping service metricsexported by the pmdashping(1) agent.The default report from pcp-shping shows two columns for eachservice tag, the first showing service status (zero indicatingsuccess) and the second service response time, for the lastpmdashping command refresh cycle.When invoked via the pcp(1) command, the -h/--host, -a/--archive,-O/--origin, -s/--samples, -t/--interval, -Z/--timezone andseveral other pcp options become indirectly available, seePCPIntro(1) for their descriptions.",
        "name": "pcp-shping - report on shell service availability and response",
        "section": 1
    },
    {
        "command": "pcp-ss",
        "description": "pcp-ss reports socket statistics collected by the pmdasockets(1)PMDA agent.The command is intended to be reasonably compatiblewith many of the ss(8) command line options and reportingformats, but also offer the advantages of local or remotemonitoring (in live mode) and also historical replay from apreviously recorded PCP archive.Note that since ss(1) has manycommand line options, many of which are the same as standard PCPcommand line options as described in PCPIntro(1), the pcp-ss toolshould always be invoked by users using the pcp front-end.Thisallows standard PCP commandline options such as -h, -a, -S, -T,-O, -z, etc to be passed without conflict with ss(1) options.See the EXAMPLES sections below for typical usage and commandlines.Live mode uses the pcp -h host option and requires thepmdasockets(1) PMDA to be installed and enabled on the targethost (local or remote), see pmdasockets(1) for details on how toenable the sockets PMDA on a particular host.The default sourceis live metrics collected on localhost, if neither of the -h or-a options are given.Historical/archive replay uses the pcp -a archive option, wherearchive is the basename of a previously recorded PCP archive.The archive replay feature is particularly useful because socketstatistics can be reported for a designated time using the pcp--origin option (which defaults to the start time of thearchive).",
        "name": "pcp-ss - report socket statistics",
        "section": 1
    },
    {
        "command": "pcp-summary",
        "description": "The pcp command is used in one of two modes.By default, itsummarizes the Performance Co-Pilot (PCP) installation on thelocal host.This mode can also be used to summarize theinstallation from a remote host, or a historical installationfrom a set of PCP archives.This mode indirectly invokes thepcp-summary command (in the absence of any other requestedcommand).Alternatively, a command can be passed to pcp to run, againpossibly in the context of a remote host or set of historicalarchives.",
        "name": "pcp, pcp-summary - run a command or summarize an installation",
        "section": 1
    },
    {
        "command": "pcp-tapestat",
        "description": "pcp-tapestat reports I/O statistics for tape devices.",
        "name": "pcp-tapestat - report tape I/O statistics",
        "section": 1
    },
    {
        "command": "pcp-uptime",
        "description": "pcp-uptime gives a one line display of the following information.The current time, how long the system has been running, how manyusers are currently logged on, and the system load averages forthe past 1, 5, and 15 minutes.When invoked via the pcp(1) command, the -h/--host, -a/--archive,-O/--origin, -Z/--timezone and several other pcp options becomeindirectly available.",
        "name": "pcp-uptime - tell how long the system has been running",
        "section": 1
    },
    {
        "command": "pcp-verify",
        "description": "pcp-verify inspects various aspects of a PCP collectorinstallation and reports on whether it is configured correctlyfor certain modes of operation.By default, pcp-verify checks that PMCD is running and no agentsare in a failed state.These checks can be extended and refinedusing the command line options.",
        "name": "pcp-verify - verify aspects of a PCP installation",
        "section": 1
    },
    {
        "command": "pcp-vmstat",
        "description": "pmstat provides a one line summary of system performance everyinterval unit of time (the default is 5 seconds).pmstat isintended to monitor system performance at the highest level,after which other tools may be used to examine subsystems inwhich potential performance problems may be observed in greaterdetail.pcp-vmstat is a simple wrapper for use with the pcp(1) command,providing a more familiar command line format for some users.Italso enables the extended reporting option by default, see the -xoption below.Multiple hosts may be monitored by supplying more than one hostwith multiple -h flags (for live monitoring) or by providing aname of the hostlist file, where each line contain one host name,with -H, or multiple -a flags (for retrospective monitoring fromsets of archives).By default, pmstat fetches metrics by connecting to thePerformance Metrics Collector Daemon (PMCD) on the local host.If the -L option is specified, then pmcd(1) is bypassed, andmetrics are fetched from PMDAs on the local host using thestandalone PM_CONTEXT_LOCAL variant of pmNewContext(3).When the-h option is specified, pmstat connects to the pmcd(1) on hostand fetches metrics from there.As mentioned above, multiplehosts may be monitored by supplying multiple -h flags.Alternatively, if the -a option is used, the metrics areretrieved from the Performance Co-Pilot archive log filesidentified by archive, which is a comma-separated list of names,each of which may be the base name of an archive or the name of adirectory containing one or more archives.Multiple sets ofarchives may be replayed by supplying multiple -a flags.Whenthe -a flag is used, the -P flag may also be used to pause theoutput after each interval.Standalone mode can only connect to the local host, using a setof archives implies a host name, and nominating a host precludesusing an archive, so the options -L, -a and -h are mutuallyexclusive.pmstat may relinquish its own timing control, and operate underthe control of a pmtime(1) process that uses a GUI dialog toprovide timing control.In this case, either the -g optionshould be used to start pmstat as the sole client of a newpmtime(1) instance, or -p should be used to attach pmstat to anexisting pmtime(1) instance via the IPC channel identified by theport argument.The -S, -T, -O and -A options may be used to define a time windowto restrict the samples retrieved, set an initial origin withinthe time window, or specify a ``natural'' alignment of the sampletimes; refer to PCPIntro(1) for a complete description of theseoptions.",
        "name": "pcp-vmstat, pmstat - high-level system performance overview",
        "section": 1
    },
    {
        "command": "pcp2XXX",
        "description": "pcp2XXX is a customizable performance metrics exporter tool fromPCP to XXX.Any available performance metric, live or archived,system and/or application, can be selected for exporting usingeither command line arguments or a configuration file.pcp2XXX is a close relative of pmrep(1).Refer to pmrep(1) forthe metricspec description accepted on pcp2XXX command line.Seepmrep.conf(5) for description of the pcp2XXX.conf configurationfile syntax.This page describes pcp2XXX specific options andconfiguration file differences with pmrep.conf(5).pmrep(1) alsolists some usage examples of which most are applicable withpcp2XXX as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2XXX - pcp-to-XXX metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2csv",
        "description": "pmrep is a customizable performance metrics reporting tool.Anyavailable performance metric, live or archived, system and/orapplication, can be selected for reporting using one of theoutput alternatives listed below together with applicableformatting options.pmrep collects selected metric values through the facilities ofthe Performance Co-Pilot (PCP), see PCPIntro(1).The metrics tobe reported are specified on the command line, in configurationfiles, or both.Metrics can be automatically converted andscaled using the PCP facilities, either by default or by per-metric scaling specifications.In addition to the existingmetrics, derived metrics can be defined using the arithmeticexpressions described in pmRegisterDerived(3).A wide range of metricsets (see below) is included by default,providing reports on per-process details, NUMA performance,mimicking other tools like sar(1) and more, see the pmrepconfiguration files in $PCP_SYSCONF_DIR/pmrep (typically/etc/pcp/pmrep) for details.Tab completion for options,metrics, and metricsets is available for bash and zsh.Unless directed to another host by the -h option, pmrep willcontact the Performance Metrics Collector Daemon (PMCD, seepmcd(1)) on the local host.The -a option causes pmrep to use the specified set of archivelogs rather than connecting to a PMCD.The -a and -h options aremutually exclusive.The -L option causes pmrep to use a local context to collectmetrics from DSO PMDAs (Performance Metrics Domain Agents,``plugins'') on the local host without PMCD.Only some metricsare available in this mode.The -a, -h, and -L options aremutually exclusive.The metrics of interest are named in the metricspec argument(s).If a metricspec specifies a non-leaf node in the PerformanceMetrics Name Space (PMNS), then pmrep will recursively descendthe PMNS and report on all leaf nodes (i.e., metrics) for thatmetricspec.Use pminfo(1) to list all the metrics (PMNS leadnodes) and their descriptions.A metricspec has three different forms.First, on the commandline it can start with a colon (``:'') to indicate a metricset tobe read from pmrep configuration files (see -c andpmrep.conf(5)), which may then consist of any number of metrics.Second, a metricspec starting with non-colon specifies a PMNSnode as described above, optionally followed by metric outputformatting definitions.This so-called compact form of ametricspec is defined as follows:metric[,label[,instances[,unit/scale[,type[,width[,precision[,limit]]]]]]]A valid PMNS node (metric) is mandatory.It may be followed by atext label used with stdout output.The optional instancesdefinition restricts csv and stdout reporting to the specifiedinstances of the metric so non-matching instances will befiltered out (see -i).An optional unit/scale is applicable fordimension-compatible, non-string metrics.See below forsupported unit/scale specifications.By default, cumulativecounter metrics are converted to rates, an optional type can beset to raw to disable this rate conversion.For stdout output anumeric width can be used to set the width of the output columnfor this metric.Too wide strings in the output will betruncated to fit the column.A metric-specific precision can beprovided for numeric non-integer output values.Lastly, ametric-specific limit can be set for filtering out numeric valuesper the limit.As a special case for metrics that are counters with time units(nanoseconds to hours), the unit/scale can be used to change thedefault reporting (for example, milliseconds / second) tonormalize to the range zero to one by setting this to sec (seealso -y and -Y).The following metricspec requests the metric kernel.all.sysforkto be reported under the text label forks, converting to themetric default rate count/s in an 8 wide column.Although thedefinitions in this compact form are optional, they must alwaysbe provided in the order specified above, thus the commas.kernel.all.sysfork,forks,,,,8The third form of a metricspec, verbose form, is described andvalid only in pmrep.conf(5).Derived metrics are specified like regular PMNS leaf nodemetrics.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pmrep - performance metrics reporter",
        "section": 1
    },
    {
        "command": "pcp2elasticsearch",
        "description": "pcp2elasticsearch is a customizable performance metrics exportertool from PCP to Elasticsearch.Any available performancemetric, live or archived, system and/or application, can beselected for exporting using either command line arguments or aconfiguration file.pcp2elasticsearch is a close relative of pmrep(1).Refer topmrep(1) for the metricspec description accepted onpcp2elasticsearch command line.See pmrep.conf(5) fordescription of the pcp2elasticsearch.conf configuration filesyntax.This page describes pcp2elasticsearch specific optionsand configuration file differences with pmrep.conf(5).pmrep(1)also lists some usage examples of which most are applicable withpcp2elasticsearch as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2elasticsearch - pcp-to-elasticsearch metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2graphite",
        "description": "pcp2graphite is a customizable performance metrics exporter toolfrom PCP to Graphite.Any available performance metric, live orarchived, system and/or application, can be selected forexporting using either command line arguments or a configurationfile.pcp2graphite is a close relative of pmrep(1).Refer to pmrep(1)for the metricspec description accepted on pcp2graphite commandline.See pmrep.conf(5) for description of the pcp2graphite.confconfiguration file syntax.This page describes pcp2graphitespecific options and configuration file differences withpmrep.conf(5).pmrep(1) also lists some usage examples of whichmost are applicable with pcp2graphite as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2graphite - pcp-to-graphite metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2influxdb",
        "description": "pcp2influxdb is a customizable performance metrics exporter toolfrom PCP to InfluxDB.Any available performance metric, live orarchived, system and/or application, can be selected forexporting using either command line arguments or a configurationfile.pcp2influxdb is a close relative of pmrep(1).Refer to pmrep(1)for the metricspec description accepted on pcp2influxdb commandline.See pmrep.conf(5) for description of the pcp2influxdb.confconfiguration file syntax.This page describes pcp2influxdbspecific options and configuration file differences withpmrep.conf(5).pmrep(1) also lists some usage examples of whichmost are applicable with pcp2influxdb as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2influxdb - pcp-to-influxdb metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2json",
        "description": "pcp2json is a customizable performance metrics exporter tool fromPCP to JSON.Any available performance metric, live or archived,system and/or application, can be selected for exporting usingeither command line arguments or a configuration file.pcp2json is a close relative of pmrep(1).Refer to pmrep(1) forthe metricspec description accepted on pcp2json command line.See pmrep.conf(5) for description of the pcp2json.confconfiguration file syntax.This page describes pcp2json specificoptions and configuration file differences with pmrep.conf(5).pmrep(1) also lists some usage examples of which most areapplicable with pcp2json as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2json - pcp-to-json metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2spark",
        "description": "pcp2spark is a customizable performance metrics exporter toolfrom PCP to Apache Spark.Any available performance metric, liveor archived, system and/or application, can be selected forexporting using either command line arguments or a configurationfile.pcp2spark acts as a bridge which provides a network socket streamon a given address/port which an Apache Spark worker task canconnect to and pull the configured PCP metrics from pcp2sparkexporting them using the streaming extensions of the Apache SparkAPI.pcp2spark is a close relative of pmrep(1).Refer to pmrep(1) forthe metricspec description accepted on pcp2spark command line.See pmrep.conf(5) for description of the pcp2spark.confconfiguration file syntax.This page describes pcp2sparkspecific options and configuration file differences withpmrep.conf(5).pmrep(1) also lists some usage examples of whichmost are applicable with pcp2spark as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2spark - pcp-to-spark metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2template",
        "description": "pcp2XXX is a customizable performance metrics exporter tool fromPCP to XXX.Any available performance metric, live or archived,system and/or application, can be selected for exporting usingeither command line arguments or a configuration file.pcp2XXX is a close relative of pmrep(1).Refer to pmrep(1) forthe metricspec description accepted on pcp2XXX command line.Seepmrep.conf(5) for description of the pcp2XXX.conf configurationfile syntax.This page describes pcp2XXX specific options andconfiguration file differences with pmrep.conf(5).pmrep(1) alsolists some usage examples of which most are applicable withpcp2XXX as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2XXX - pcp-to-XXX metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2xlsx",
        "description": "pcp2xlsx is a customizable performance metrics exporter tool fromPCP to XLSX.Any available performance metric, live or archived,system and/or application, can be selected for exporting usingeither command line arguments or a configuration file.pcp2xlsx is a close relative of pmrep(1).Refer to pmrep(1) forthe metricspec description accepted on pcp2xlsx command line.See pmrep.conf(5) for description of the pcp2xlsx.confconfiguration file syntax.This page describes pcp2xlsx specificoptions and configuration file differences with pmrep.conf(5).pmrep(1) also lists some usage examples of which most areapplicable with pcp2xlsx as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2xlsx - pcp-to-xlsx metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2xml",
        "description": "pcp2xml is a customizable performance metrics exporter tool fromPCP to XML.Any available performance metric, live or archived,system and/or application, can be selected for exporting usingeither command line arguments or a configuration file.pcp2xml is a close relative of pmrep(1).Refer to pmrep(1) forthe metricspec description accepted on pcp2xml command line.Seepmrep.conf(5) for description of the pcp2xml.conf configurationfile syntax.This page describes pcp2xml specific options andconfiguration file differences with pmrep.conf(5).pmrep(1) alsolists some usage examples of which most are applicable withpcp2xml as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2xml - pcp-to-xml metrics exporter",
        "section": 1
    },
    {
        "command": "pcp2zabbix",
        "description": "pcp2zabbix is a customizable performance metrics exporter toolfrom PCP to Zabbix.Any available performance metric, live orarchived, system and/or application, can be selected forexporting using either command line arguments or a configurationfile.pcp2zabbix is a close relative of pmrep(1).Refer to pmrep(1)for the metricspec description accepted on pcp2zabbix commandline.See pmrep.conf(5) for description of the pcp2zabbix.confconfiguration file syntax.This page describes pcp2zabbixspecific options and configuration file differences withpmrep.conf(5).pmrep(1) also lists some usage examples of whichmost are applicable with pcp2zabbix as well.Only the command line options listed on this page are supported,other options available for pmrep(1) are not supported.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pcp2zabbix - pcp-to-zabbix metrics exporter",
        "section": 1
    },
    {
        "command": "pcpcompat",
        "description": null,
        "name": "PCPCompat, pcp-collectl, pmmgr, pmwebd - backward-compatibilityin the Performance Co-Pilot (PCP)",
        "section": 1
    },
    {
        "command": "pcpintro",
        "description": null,
        "name": "PCPIntro - introduction to the Performance Co-Pilot (PCP)",
        "section": 1
    },
    {
        "command": "pcre2-config",
        "description": "pcre2-config returns the configuration of the installed PCRE2libraries and the options required to compile a program to usethem. Some of the options apply only to the 8-bit, or 16-bit, or32-bit libraries, respectively, and are not available forlibraries that have not been built. If an unavailable option isencountered, the \"usage\" information is output.",
        "name": "pcre2-config - program to return PCRE2 configuration",
        "section": 1
    },
    {
        "command": "pcre2grep",
        "description": "pcre2grep searches files for character patterns, in the same wayas other grep commands do, but it uses the PCRE2 regularexpression library to support patterns that are compatible withthe regular expressions of Perl 5. See pcre2syntax(3) for aquick-reference summary of pattern syntax, or pcre2pattern(3) fora full description of the syntax and semantics of the regularexpressions that PCRE2 supports.Patterns, whether supplied on the command line or in a separatefile, are given without delimiters. For example:pcre2grep Thursday /etc/motdIf you attempt to use delimiters (for example, by surrounding apattern with slashes, as is common in Perl scripts), they areinterpreted as part of the pattern. Quotes can of course be usedto delimit patterns on the command line because they areinterpreted by the shell, and indeed quotes are required if apattern contains white space or shell metacharacters.The first argument that follows any option settings is treated asthe single pattern to be matched when neither -e nor -f ispresent.Conversely, when one or both of these options are usedto specify patterns, all arguments are treated as path names. Atleast one of -e, -f, or an argument pattern must be provided.If no files are specified, pcre2grep reads the standard input.The standard input can also be referenced by a name consisting ofa single hyphen.For example:pcre2grep some-pattern file1 - file3By default, input files are searched line by line. Each line thatmatches a pattern is copied to the standard output, and if thereis more than one file, the file name is output at the start ofeach line, followed by a colon.However, there are options thatcan change how pcre2grep behaves. For example, the -M optionmakes it possible to search for strings that span lineboundaries. What defines a line boundary is controlled by the -N(--newline) option. The -h and -H options control whether or notfile names are shown, and the -Z option changes the file nameterminator to a zero byte.The amount of memory used for buffering files that are beingscanned is controlled by parameters that can be set by the--buffer-size and --max-buffer-size options. The first of thesesets the size of buffer that is obtained at the start ofprocessing. If an input file contains very long lines, a largerbuffer may be needed; this is handled by automatically extendingthe buffer, up to the limit specified by --max-buffer-size. Thedefault values for these parameters can be set when pcre2grep isbuilt; if nothing is specified, the defaults are set to 20KiB and1MiB respectively. An error occurs if a line is too long and thebuffer can no longer be expanded.The block of memory that is actually used is three times the\"buffer size\", to allow for buffering \"before\" and \"after\" lines.If the buffer size is too small, fewer than requested \"before\"and \"after\" lines may be output.When matching with a multiline pattern, the size of the buffermust be at least half of the maximum match expected or thepattern might fail to match.Patterns can be no longer than 8KiB or BUFSIZ bytes, whichever isthe greater.BUFSIZ is defined in <stdio.h>. When there is morethan one pattern (specified by the use of -e and/or -f), eachpattern is applied to each line in the order in which they aredefined, except that all the -e patterns are tried before the -fpatterns.By default, as soon as one pattern matches a line, no furtherpatterns are considered. However, if --colour (or --color) isused to colour the matching substrings, or if --only-matching,--file-offsets, --line-offsets, or --output is used to outputonly the part of the line that matched (either shown literally,or as an offset), the behaviour is different. In this situation,all the patterns are applied to the line. If there is more thanone match, the one that begins nearest to the start of thesubject is processed; if there is more than one match at thatposition, the one with the longest matching substring isprocessed; if the matching substrings are equal, the first matchfound is processed.Scanning with all the patterns resumes immediately following thematch, so that later matches on the same line can be found. Note,however, that an overlapping match that starts in the middle ofanother match will not be processed.The above behaviour was changed at release 10.41 to be morecompatible with GNU grep. In earlier releases, pcre2grep did notrecognize matches from later patterns that were earlier in thesubject.Patterns that can match an empty string are accepted, but emptystring matches are never recognized. An example is the pattern\"(super)?(man)?\", in which all components are optional. Thispattern finds all occurrences of both \"super\" and \"man\"; theoutput differs from matching with \"super|man\" when only thematching substrings are being shown.If the LC_ALL or LC_CTYPE environment variable is set, pcre2grepuses the value to set a locale when calling the PCRE2 library.The --locale option can be used to override this.",
        "name": "pcre2grep - a grep with Perl-compatible regular expressions.",
        "section": 1
    },
    {
        "command": "pcre2test",
        "description": "If pcre2test is given two filename arguments, it reads from thefirst and writes to the second. If the first name is \"-\", inputis taken from the standard input. If pcre2test is given only oneargument, it reads from that file and writes to stdout.Otherwise, it reads from stdin and writes to stdout.When pcre2test is built, a configuration option can specify thatit should be linked with the libreadline or libedit library. Whenthis is done, if the input is from a terminal, it is read usingthe readline() function. This provides line-editing and historyfacilities. The output from the -help option states whether ornot readline() will be used.The program handles any number of tests, each of which consistsof a set of input lines. Each set starts with a regularexpression pattern, followed by any number of subject lines to bematched against that pattern. In between sets of test data,command lines that begin with # may appear. This file format,with some restrictions, can also be processed by the perltest.shscript that is distributed with PCRE2 as a means of checking thatthe behaviour of PCRE2 and Perl is the same. For a specificationof perltest.sh, see the comments near its beginning. See also the#perltest command below.When the input is a terminal, pcre2test prompts for each line ofinput, using \"re>\" to prompt for regular expression patterns, and\"data>\" to prompt for subject lines. Command lines starting with# can be entered only in response to the \"re>\" prompt.Each subject line is matched separately and independently. If youwant to do multi-line matches, you have to use the \\n escapesequence (or \\r or \\r\\n, etc., depending on the newline setting)in a single line of input to encode the newline sequences. Thereis no limit on the length of subject lines; the input buffer isautomatically extended if it is too small. There are replicationfeatures that makes it possible to generate long repetitivepattern or subject lines without having to supply themexplicitly.An empty line or the end of the file signals the end of thesubject lines for a test, at which point a new pattern or commandline is expected if there is still input to be read.",
        "name": "pcre2test - a program for testing Perl-compatible regularexpressions.",
        "section": 1
    },
    {
        "command": "pdfmom",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "pdfroff",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "peekfd",
        "description": "peekfd attaches to a running process and intercepts all reads andwrites to file descriptors.You can specify the desired filedescriptor numbers or dump all of them.",
        "name": "peekfd - peek at file descriptors of running processes",
        "section": 1
    },
    {
        "command": "perf",
        "description": "Performance counters for Linux are a new kernel-based subsystemthat provide a framework for all things performance analysis. Itcovers hardware level (CPU/PMU, Performance Monitoring Unit)features and software features (software counters, tracepoints)as well.",
        "name": "perf - Performance analysis tools for Linux",
        "section": 1
    },
    {
        "command": "perf-annotate",
        "description": "This command reads the input file and displays an annotatedversion of the code. If the object file has debug symbols thenthe source code will be displayed alongside assembly code.If there is no debug info in the object, then annotated assemblyis displayed.",
        "name": "perf-annotate - Read perf.data (created by perf record) anddisplay annotated code",
        "section": 1
    },
    {
        "command": "perf-archive",
        "description": "This command runs perf-buildid-list --with-hits, and collects thefiles with the buildids found so that analysis of perf.datacontents can be possible on another machine.",
        "name": "perf-archive - Create archive with object files with build-idsfound in perf.data file",
        "section": 1
    },
    {
        "command": "perf-arm-spe",
        "description": "The SPE (Statistical Profiling Extension) feature providesaccurate attribution of latencies and events down to individualinstructions. Rather than being interrupt-driven, it picks aninstruction to sample and then captures data for it duringexecution. Data includes execution time in cycles. For loads andstores it also includes data address, cache miss events, and dataorigin.The sampling has 5 stages:1. Choose an operation2. Collect data about the operation3. Optionally discard the record based on a filter4. Write the record to memory5. Interrupt when the buffer is fullChoose an operationThis is chosen from a sample population, for SPE this is anIMPLEMENTATION DEFINED choice of all architectural instructionsor all micro-ops. Sampling happens at a programmable interval.The architecture provides a mechanism for the SPE driver to inferthe minimum interval at which it should sample. This minimuminterval is used by the driver if no interval is specified. Apseudo-random perturbation is also added to the sampling intervalby default.Collect data about the operationProgram counter, PMU events, timings and data addresses relatedto the operation are recorded. Sampling ensures there is only onesampled operation is in flight.Optionally discard the record based on a filterBased on programmable criteria, choose whether to keep the recordor discard it. If the record is discarded then the flow stopshere for this sample.Write the record to memoryThe record is appended to a memory bufferInterrupt when the buffer is fullWhen the buffer fills, an interrupt is sent and the driversignals Perf to collect the records. Perf saves the raw data inthe perf.data file.",
        "name": "perf-arm-spe - Support for Arm Statistical Profiling Extensionwithin Perf tools",
        "section": 1
    },
    {
        "command": "perf-bench",
        "description": "This perf bench command is a general framework for benchmarksuites.",
        "name": "perf-bench - General framework for benchmark suites",
        "section": 1
    },
    {
        "command": "perf-buildid-cache",
        "description": "This command manages the build-id cache. It can add, remove,update and purge files to/from the cache. In the future it shouldas well set upper limits for the space used by the cache, etc.This also scans the target binary for SDT (Statically DefinedTracing) and record it along with the buildid-cache, which willbe used by perf-probe. For more details, see perf-probe(1).",
        "name": "perf-buildid-cache - Manage build-id cache.",
        "section": 1
    },
    {
        "command": "perf-buildid-list",
        "description": "This command displays the buildids found in a perf.data file, sothat other tools can be used to fetch packages with matchingsymbol tables for use by perf report.It can also be used to show the build id of the running kernel orin an ELF file using -i/--input.",
        "name": "perf-buildid-list - List the buildids in a perf.data file",
        "section": 1
    },
    {
        "command": "perf-c2c",
        "description": "C2C stands for Cache To Cache.The perf c2c tool provides means for Shared Data C2C/HITManalysis. It allows you to track down the cacheline contentions.On Intel, the tool is based on load latency and precise storefacility events provided by Intel CPUs. On PowerPC, the tool usesrandom instruction sampling with thresholding feature. On AMD,the tool uses IBS op pmu (due to hardware limitations, perf c2cis not supported on Zen3 cpus). On Arm64 it uses SPE to sampleload and store operations, therefore hardware and kernel supportis required. See perf-arm-spe(1) for a setup guide. Due to thestatistical nature of Arm SPE sampling, not every memoryoperation will be sampled.These events provide: - memory address of the access - type ofthe access (load and store details) - latency (in cycles) of theload accessThe c2c tool provide means to record this data and report backaccess details for cachelines with highest contention - highestnumber of HITM accesses.The basic workflow with this tool follows the standardrecord/report phase. User uses the record command to recordevents data and report command to display it.",
        "name": "perf-c2c - Shared Data C2C/HITM Analyzer.",
        "section": 1
    },
    {
        "command": "perf-config",
        "description": "You can manage variables in a configuration file with thiscommand.",
        "name": "perf-config - Get and set variables in a configuration file.",
        "section": 1
    },
    {
        "command": "perf-daemon",
        "description": "This command allows to run simple daemon process that starts andmonitors configured record sessions.You can imagine perf daemon of background process with severalperf record child tasks, like:# ps axjf...1916507 ... perf daemon start916507916508 ...\\_ perf record --control=fifo:control,ack -m 10M -e cycles --overwrite --switch-output -a916507916509 ...\\_ perf record --control=fifo:control,ack -m 20M -e sched:* --overwrite --switch-output -aNot every perf record session is suitable for running underdaemon. User need perf session that either produces data onquery, like the flight recorder sessions in above example orsession that is configured to produce data periodically, likewith --switch-output configuration for time and size.Each session is started with control setup (with perf record--control options).Sessions are configured through config file, see CONFIG FILEsection with EXAMPLES.",
        "name": "perf-daemon - Run record sessions on background",
        "section": 1
    },
    {
        "command": "perf-data",
        "description": "Data file related processing.",
        "name": "perf-data - Data file related processing",
        "section": 1
    },
    {
        "command": "perf-diff",
        "description": "This command displays the performance difference amongst two ormore perf.data files captured via perf record.If no parameters are passed it will assume perf.data.old andperf.data.The differential profile is displayed only for events matchingboth specified perf.data files.If no parameters are passed the samples will be sorted by dso andsymbol. As the perf.data files could come from differentbinaries, the symbols addresses could vary. So perf diff is basedon the comparison of the files and symbols name.",
        "name": "perf-diff - Read perf.data files and display the differentialprofile",
        "section": 1
    },
    {
        "command": "perf-dlfilter",
        "description": "This option is used to process data through a custom filterprovided by a dynamically loaded shared object file. Argumentscan be passed using --dlarg and retrieved usingperf_dlfilter_fns.args().If file.so does not contain \"/\", then it will be found either inthe current directory, or perf tools exec path which is~/libexec/perf-core/dlfilters for a local build and install(refer perf --exec-path), or the dynamic linker paths.",
        "name": "perf-dlfilter - Filter sample events using a dynamically loadedshared object file",
        "section": 1
    },
    {
        "command": "perf-evlist",
        "description": "This command displays the names of events sampled in a perf.datafile.",
        "name": "perf-evlist - List the event names in a perf.data file",
        "section": 1
    },
    {
        "command": "perf-ftrace",
        "description": "The perf ftrace command provides a collection of subcommandswhich use kernel\u2019s ftrace infrastructure.'perf ftrace trace' is a simple wrapper of the ftrace.It only supportssingle thread tracing currently and just reads trace_pipe in text and thenwrite it to stdout.'perf ftrace latency' calculates execution latency of a given function(optionally with BPF) and display it as a histogram.The following options apply to perf ftrace.",
        "name": "perf-ftrace - simple wrapper for kernel's ftrace functionality",
        "section": 1
    },
    {
        "command": "perf-help",
        "description": "With no options and no COMMAND given, the synopsis of the perfcommand and a list of the most commonly used perf commands areprinted on the standard output.If the option --all or -a is given, then all available commandsare printed on the standard output.If a perf command is named, a manual page for that command isbrought up. The man program is used by default for this purpose,but this can be overridden by other options or configurationvariables.Note that perf --help ... is identical to perf help ... becausethe former is internally converted into the latter.",
        "name": "perf-help - display help information about perf",
        "section": 1
    },
    {
        "command": "perf-inject",
        "description": "perf-inject reads a perf-record event stream and repipes it tostdout. At any point the processing code can inject other eventsinto the event stream - in this case build-ids (-b option) areread and injected as needed into the event stream.Build-ids are just the first user of perf-inject - potentiallyanything that needs userspace processing to augment the eventsstream with additional information could make use of thisfacility.",
        "name": "perf-inject - Filter to augment the events stream with additionalinformation",
        "section": 1
    },
    {
        "command": "perf-intel-pt",
        "description": "Intel Processor Trace (Intel PT) is an extension of IntelArchitecture that collects information about software executionsuch as control flow, execution modes and timings and formats itinto highly compressed binary packets. Technical details aredocumented in the Intel 64 and IA-32 Architectures SoftwareDeveloper Manuals, Chapter 36 Intel Processor Trace.Intel PT is first supported in Intel Core M and 5th generationIntel Core processors that are based on the Intelmicro-architecture code name Broadwell.Trace data is collected by perf record and stored within theperf.data file. See below for options to perf record.Trace data must be decoded which involves walking the object codeand matching the trace data packets. For example a TNT packetonly tells whether a conditional branch was taken or not taken,so to make use of that packet the decoder must know preciselywhich instruction was being executed.Decoding is done on-the-fly. The decoder outputs samples in thesame format as samples output by perf hardware events, forexample as though the \"instructions\" or \"branches\" events hadbeen recorded. Presently 3 tools support this: perf script, perfreport and perf inject. See below for more information on usingthose tools.The main distinguishing feature of Intel PT is that the decodercan determine the exact flow of software execution. Intel PT canbe used to understand why and how did software get to a certainpoint, or behave a certain way. The software does not have to berecompiled, so Intel PT works with debug or release builds,however the executed images are needed - which makes use inJIT-compiled environments, or with self-modified code, achallenge. Also symbols need to be provided to make sense ofaddresses.A limitation of Intel PT is that it produces huge amounts oftrace data (hundreds of megabytes per second per core) whichtakes a long time to decode, for example two or three orders ofmagnitude longer than it took to collect. Another limitation isthe performance impact of tracing, something that will varydepending on the use-case and architecture.",
        "name": "perf-intel-pt - Support for Intel Processor Trace within perftools",
        "section": 1
    },
    {
        "command": "perf-iostat",
        "description": "Mode is intended to provide four I/O performance metrics per eachPCIe root port:\u2022Inbound Read - I/O devices below root port read from the hostmemory, in MB\u2022Inbound Write - I/O devices below root port write to the hostmemory, in MB\u2022Outbound Read - CPU reads from I/O devices below root port,in MB\u2022Outbound Write - CPU writes to I/O devices below root port,in MB",
        "name": "perf-iostat - Show I/O performance metrics",
        "section": 1
    },
    {
        "command": "perf-kallsyms",
        "description": "This command searches the running kernel kallsyms file for thegiven symbol(s) and prints information about it, including theDSO, the kallsyms begin/end addresses and the addresses in theELF kallsyms symbol table (for symbols in modules).",
        "name": "perf-kallsyms - Searches running kernel for symbols",
        "section": 1
    },
    {
        "command": "perf-kmem",
        "description": "There are two variants of perf kmem:'perf kmem [<options>] record [<perf-record-options>] <command>' torecord the kmem events of an arbitrary workload. Additional 'perfrecord' options may be specified after record, such as '-o' tochange the output file name.'perf kmem [<options>] stat' to report kernel memory statistics.",
        "name": "perf-kmem - Tool to trace/measure kernel memory properties",
        "section": 1
    },
    {
        "command": "perf-kvm",
        "description": "There are a couple of variants of perf kvm:'perf kvm [options] top <command>' to generates and displaysa performance counter profile of guest os in realtimeof an arbitrary workload.'perf kvm record <command>' to record the performance counter profileof an arbitrary workload and save it into a perf data file. We set thedefault behavior of perf kvm as --guest, so if neither --host nor --guestis input, the perf data file name is perf.data.guest. If --host is input,the perf data file name is perf.data.kvm. If you want to record data intoperf.data.host, please input --host --no-guest. The behaviors are shown asfollowing:Default('')->perf.data.guest--host->perf.data.kvm--guest->perf.data.guest--host --guest->perf.data.kvm--host --no-guest->perf.data.host'perf kvm report' to display the performance counter profile informationrecorded via perf kvm record.'perf kvm diff' to displays the performance difference amongst two perf.datafiles captured via perf record.'perf kvm buildid-list' todisplay the buildids found in a perf data file,so that other tools can be used to fetch packages with matching symbol tablesfor use by perf report. As buildid is read from /sys/kernel/notes in os, thenif you want to list the buildid for guest, please make sure your perf data filewas captured with --guestmount in perf kvm record.'perf kvm stat <command>' to run a command and gather performance counterstatistics.Especially, perf 'kvm stat record/report' generates a statistical analysisof KVM events. Currently, vmexit, mmio (x86 only) and ioport (x86 only)events are supported. 'perf kvm stat record <command>' records kvm eventsand the events between start and end <command>.And this command produces a file which contains tracing results of kvmevents.'perf kvm stat report' reports statistical data which includes eventshandled sample, percent_sample, time, percent_time, max_t, min_t, mean_t.'perf kvm stat live' reports statistical data in a live mode (similar torecord + report but with statistical data updated live at a given displayrate).",
        "name": "perf-kvm - Tool to trace/measure kvm guest os",
        "section": 1
    },
    {
        "command": "perf-kwork",
        "description": "There are several variants of perf kwork:'perf kwork record <command>' to record the kernel workof an arbitrary workload.'perf kwork report' to report the per kwork runtime.'perf kwork latency' to report the per kwork latencies.'perf kwork timehist' provides an analysis of kernel work events.Example usage:perf kwork record -- sleep 1perf kwork reportperf kwork report -bperf kwork latencyperf kwork latency -bperf kwork timehistBy default it shows the individual work events such as irq, workqeueu,including the run time and delay (time between raise and actually entry):Runtime startRuntime endCpuKwork nameRuntimeDelaytime(TYPE)NAME:NUM(msec)(msec)-------------------------------------------------------------------------------------1811186.9760621811186.976327[0000](s)RCU:90.2660.1141811186.9784521811186.978547[0000](s)SCHED:70.0950.1711811186.9803271811186.980490[0000](s)SCHED:70.1620.0831811186.9812211811186.981271[0000](s)SCHED:70.0500.0771811186.9842671811186.984318[0000](s)SCHED:70.0510.0751811186.9872521811186.987315[0000](s)SCHED:70.0630.0811811186.9877851811186.987843[0006](s)RCU:90.0580.6451811186.9883191811186.988383[0000](s)SCHED:70.0640.1431811186.9894041811186.989607[0002](s)TIMER:10.2030.1111811186.9896601811186.989732[0002](s)SCHED:70.0720.3101811186.9912951811186.991407[0002]eth0:100.1121811186.9916391811186.991734[0002](s)NET_RX:30.0950.2771811186.9898601811186.991826[0002](w)vmstat_shepherd1.9660.345...Times are in msec.usec.",
        "name": "perf-kwork - Tool to trace/measure kernel work properties(latencies)",
        "section": 1
    },
    {
        "command": "perf-list",
        "description": "This command displays the symbolic event types which can beselected in the various perf commands with the -e option.",
        "name": "perf-list - List all symbolic event types",
        "section": 1
    },
    {
        "command": "perf-lock",
        "description": "You can analyze various lock behaviours and statistics with thisperf lock command.'perf lock record <command>' records lock eventsbetween start and end <command>. And this commandproduces the file \"perf.data\" which contains tracingresults of lock events.'perf lock report' reports statistical data.'perf lock script' shows raw lock events.'perf lock info' shows metadata like threads or addressesof lock instances.'perf lock contention' shows contention statistics.",
        "name": "perf-lock - Analyze lock events",
        "section": 1
    },
    {
        "command": "perf-mem",
        "description": "\"perf mem record\" runs a command and gathers memory operationdata from it, into perf.data. Perf record options are acceptedand are passed through.\"perf mem report\" displays the result. It invokes perf reportwith the right set of options to display a memory access profile.By default, loads and stores are sampled. Use the -t option tolimit to loads or stores.Note that on Intel systems the memory latency reported is theuse-latency, not the pure load (or store latency). Use latencyincludes any pipeline queueing delays in addition to the memorysubsystem latency.On Arm64 this uses SPE to sample load and store operations,therefore hardware and kernel support is required. Seeperf-arm-spe(1) for a setup guide. Due to the statistical natureof SPE sampling, not every memory operation will be sampled.",
        "name": "perf-mem - Profile memory accesses",
        "section": 1
    },
    {
        "command": "perf-probe",
        "description": "This command defines dynamic tracepoint events, by symbol andregisters without debuginfo, or by C expressions (C line numbers,C function names, and C local variables) with debuginfo.",
        "name": "perf-probe - Define new dynamic tracepoints",
        "section": 1
    },
    {
        "command": "perf-record",
        "description": "This command runs a command and gathers a performance counterprofile from it, into perf.data - without displaying anything.This file can then be inspected later on, using perf report.",
        "name": "perf-record - Run a command and record its profile into perf.data",
        "section": 1
    },
    {
        "command": "perf-report",
        "description": "This command displays the performance counter profile informationrecorded via perf record.",
        "name": "perf-report - Read perf.data (created by perf record) and displaythe profile",
        "section": 1
    },
    {
        "command": "perf-sched",
        "description": "There are several variants of perf sched:'perf sched record <command>' to record the scheduling eventsof an arbitrary workload.'perf sched latency' to report the per task scheduling latenciesand other scheduling properties of the workload.'perf sched script' to see a detailed trace of the workload thatwas recorded (aliased to 'perf script' for now).'perf sched replay' to simulate the workload that was recordedvia perf sched record. (this is done by starting up mockup threadsthat mimic the workload based on the events in the trace. Thesethreads can then replay the timings (CPU runtime and sleep patterns)of the workload as it occurred when it was recorded - and can repeatit a number of times, measuring its performance.)'perf sched map' to print a textual context-switching outline ofworkload captured via perf sched record.Columns stand forindividual CPUs, and the two-letter shortcuts stand for tasks thatare running on a CPU. A '*' denotes the CPU that had the event, anda dot signals an idle CPU.'perf sched timehist' provides an analysis of scheduling events.Example usage:perf sched record -- sleep 1perf sched timehistBy default it shows the individual schedule events, including the waittime (time between sched-out and next sched-in events for the task), thetask scheduling delay (time between wakeup and actually running) and runtime for the task:timecputask namewait timesch delayrun time[tid/pid](msec)(msec)(msec)-------------- -----------------------------------------------------79371.874569 [0011]gcc[31949]0.0140.0001.14879371.874591 [0010]gcc[31951]0.0000.0000.02479371.874603 [0010]migration/10[59]3.3500.0040.01179371.874604 [0011]<idle>1.1480.0000.03579371.874723 [0005]<idle>0.0160.0001.38379371.874746 [0005]gcc[31949]0.1530.0780.022...Times are in msec.usec.",
        "name": "perf-sched - Tool to trace/measure scheduler properties(latencies)",
        "section": 1
    },
    {
        "command": "perf-script",
        "description": "This command reads the input file and displays the tracerecorded.There are several variants of perf script:'perf script' to see a detailed trace of the workload that wasrecorded.You can also run a set of pre-canned scripts that aggregate andsummarize the raw trace data in various ways (the list of scripts isavailable via 'perf script -l').The following variants allow you torecord and run those scripts:'perf script record <script> <command>' to record the events requiredfor 'perf script report'.<script> is the name displayed in theoutput of 'perf script --list' i.e. the actual script name minus anylanguage extension.If <command> is not specified, the events arerecorded using the -a (system-wide) 'perf record' option.'perf script report <script> [args]' to run and display the resultsof <script>.<script> is the name displayed in the output of 'perfscript --list' i.e. the actual script name minus any languageextension.The perf.data output from a previous run of 'perf scriptrecord <script>' is used and should be present for this command tosucceed.[args] refers to the (mainly optional) args expected bythe script.'perf script <script> <required-script-args> <command>' to bothrecord the events required for <script> and to run the <script>using 'live-mode' i.e. without writing anything to disk.<script>is the name displayed in the output of 'perf script --list' i.e. theactual script name minus any language extension.If <command> isnot specified, the events are recorded using the -a (system-wide)'perf record' option.If <script> has any required args, theyshould be specified before <command>.This mode doesn't allow foroptional script args to be specified; if optional script args aredesired, they can be specified using separate 'perf script record'and 'perf script report' commands, with the stdout of the record steppiped to the stdin of the report script, using the '-o -' and '-i -'options of the corresponding commands.'perf script <top-script>' to both record the events required for<top-script> and to run the <top-script> using 'live-mode'i.e. without writing anything to disk.<top-script> is the namedisplayed in the output of 'perf script --list' i.e. the actualscript name minus any language extension; a <top-script> is definedas any script name ending with the string 'top'.[<record-options>] can be passed to the record steps of 'perf scriptrecord' and 'live-mode' variants; this isn't possible however for<top-script> 'live-mode' or 'perf script report' variants.See the 'SEE ALSO' section for links to language-specificinformation on how to write and run your own trace scripts.",
        "name": "perf-script - Read perf.data (created by perf record) and displaytrace output",
        "section": 1
    },
    {
        "command": "perf-script-perl",
        "description": "This perf script option is used to process perf script data usingperf\u2019s built-in Perl interpreter. It reads and processes theinput file and displays the results of the trace analysisimplemented in the given Perl script, if any.",
        "name": "perf-script-perl - Process trace data with a Perl script",
        "section": 1
    },
    {
        "command": "perf-script-python",
        "description": "This perf script option is used to process perf script data usingperf\u2019s built-in Python interpreter. It reads and processes theinput file and displays the results of the trace analysisimplemented in the given Python script, if any.",
        "name": "perf-script-python - Process trace data with a Python script",
        "section": 1
    },
    {
        "command": "perf-stat",
        "description": "This command runs a command and gathers performance counterstatistics from it.",
        "name": "perf-stat - Run a command and gather performance counterstatistics",
        "section": 1
    },
    {
        "command": "perf-test",
        "description": "This command does assorted sanity tests, initially through linkedroutines but also will look for a directory with more tests inthe form of scripts.To get a list of available tests use perf test list, specifying atest name fragment will show all tests that have it.To run just specific tests, inform test name fragments or thenumbers obtained from perf test list.",
        "name": "perf-test - Runs sanity tests.",
        "section": 1
    },
    {
        "command": "perf-timechart",
        "description": "There are two variants of perf timechart:'perf timechart record <command>' to record the system level eventsof an arbitrary workload. By default timechart records only schedulerand CPU events (task switches, running times, CPU power states, etc),but it's possible to record IO (disk, network) activity using -I argument.'perf timechart' to turn a trace into a Scalable Vector Graphics file,that can be viewed with popular SVG viewers such as 'Inkscape'. Dependingon the events in the perf.data file, timechart will contain scheduler/cpuevents or IO events.In IO mode, every bar has two charts: upper and lower.Upper bar shows incoming events (disk reads, ingress network packets).Lower bar shows outgoing events (disk writes, egress network packets).There are also poll bars which show how much time application spentin poll/epoll/select syscalls.",
        "name": "perf-timechart - Tool to visualize total system behavior during aworkload",
        "section": 1
    },
    {
        "command": "perf-top",
        "description": "This command generates and displays a performance counter profilein real time.",
        "name": "perf-top - System profiling tool.",
        "section": 1
    },
    {
        "command": "perf-trace",
        "description": "This command will show the events associated with the target,initially syscalls, but other system events like pagefaults, tasklifetime events, scheduling events, etc.This is a live mode tool in addition to working with perf.datafiles like the other perf tools. Files can be generated using theperf record command but the session needs to include theraw_syscalls events (-e raw_syscalls:*). Alternatively, perftrace record can be used as a shortcut to automatically includethe raw_syscalls events when writing events to a file.The following options apply to perf trace; options to perf tracerecord are found in the perf record man page.",
        "name": "perf-trace - strace inspired tool",
        "section": 1
    },
    {
        "command": "perf-version",
        "description": "With no options given, the perf version prints the perf versionon the standard output.If the option --build-options is given, then the status ofcompiled-in libraries are printed on the standard output.",
        "name": "perf-version - display the version of perf binary",
        "section": 1
    },
    {
        "command": "perfalloc",
        "description": "perfalloc is a command that notifies the pmdaperfevent(1) todisable hardware counter event collection.This allowunprivileged processes to use the hardware counters.If the reservation request fails, then perfalloc exitsimmediately with exit code EXIT_FAILURE.If successful, theperfalloc will run until a kill signal is received.Thereservation request persists while perfalloc is running.Note that pmdaperfevent is affected by the value of thekernel.perf_event_paranoid setting, which can be adjusted bysysctl(8).If a commandline is given, this is executed as a subprocess ofthe agent.When the command dies, so does the agent.A brief description of the command line options follows:-Drun in the foreground (the default)-drun in the background-f FILEuse FILE as the lock file (default$PCP_PMDAS_DIR/perfevent/perflock)-hdisplay a help message and exit-voutput version number and exit",
        "name": "perfalloc - notify pmdaperfevent(1) to disable hardware counterallocation.",
        "section": 1
    },
    {
        "command": "perror",
        "description": "For most system errors, MariaDB displays, in addition to aninternal text message, the system error code in one of thefollowing styles:message ... (errno: #)message ... (Errcode: #)You can find out what the error code means by examining thedocumentation for your system or by using the perror utility.perror prints a description for a system error code or for astorage engine (table handler) error code.Invoke perror like this:shell> perror [options] errorcode ...Example:shell> perror 13 64OS error code13:Permission deniedOS error code64:Machine is not on the networkNote that the meaning of system error messages may be dependenton your operating system. A given error code may mean differentthings on different operating systems.perror supports the following options.\u2022--help, --info, -I, -?Display a help message and exit.\u2022--silent, -sSilent mode. Print only the error message.\u2022--verbose, -vVerbose mode. Print error code and message. This is thedefault behavior.\u2022--version, -VDisplay version information and exit.",
        "name": "perror - explain error codes",
        "section": 1
    },
    {
        "command": "pfbtops",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "pgrep",
        "description": "pgrep looks through the currently running processes and lists theprocess IDs which match the selection criteria to stdout.Allthe criteria have to match.For example,$ pgrep -u root sshdwill only list the processes called sshd AND owned by root.Onthe other hand,$ pgrep -u root,daemonwill list the processes owned by root OR daemon.pkill will send the specified signal (by default SIGTERM) to eachprocess instead of listing them on stdout.pidwait will wait for each process instead of listing them onstdout.",
        "name": "pgrep, pkill, pidwait - look up, signal, or wait for processesbased on name and other attributes",
        "section": 1
    },
    {
        "command": "pic",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "pic2graph",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "pidof",
        "description": "Pidof finds the process id's (pids) of the named programs. Itprints those id's on the standard output.",
        "name": "pidof - find the process ID of a running program",
        "section": 1
    },
    {
        "command": "pidstat",
        "description": "The pidstat command is used for monitoring individual taskscurrently being managed by the Linux kernel.It writes tostandard output activities for every task selected with option -por for every task managed by the Linux kernel if option -p ALLhas been used. Not selecting any tasks is equivalent tospecifying -p ALL but only active tasks (tasks with non-zerostatistics values) will appear in the report.The pidstat command can also be used for monitoring the childprocesses of selected tasks.Read about option -T below.The interval parameter specifies the amount of time in secondsbetween each report.A value of 0 (or no parameters at all)indicates that tasks statistics are to be reported for the timesince system startup (boot). The count parameter can be specifiedin conjunction with the interval parameter if this one is not setto zero. The value of count determines the number of reportsgenerated at interval seconds apart. If the interval parameter isspecified without the count parameter, the pidstat commandgenerates reports continuously.You can select information about specific task activities usingflags.Not specifying any flags selects only CPU activity.",
        "name": "pidstat - Report statistics for Linux tasks.",
        "section": 1
    },
    {
        "command": "pidwait",
        "description": "pgrep looks through the currently running processes and lists theprocess IDs which match the selection criteria to stdout.Allthe criteria have to match.For example,$ pgrep -u root sshdwill only list the processes called sshd AND owned by root.Onthe other hand,$ pgrep -u root,daemonwill list the processes owned by root OR daemon.pkill will send the specified signal (by default SIGTERM) to eachprocess instead of listing them on stdout.pidwait will wait for each process instead of listing them onstdout.",
        "name": "pgrep, pkill, pidwait - look up, signal, or wait for processesbased on name and other attributes",
        "section": 1
    },
    {
        "command": "pinky",
        "description": "-lproduce long format output for the specified USERs-bomit the user's home directory and shell in long format-homit the user's project file in long format-pomit the user's plan file in long format-sdo short format output, this is the default-fomit the line of column headings in short format-womit the user's full name in short format-iomit the user's full name and remote host in short format-qomit the user's full name, remote host and idle time inshort format--help display this help and exit--versionoutput version information and exitA lightweight 'finger' program;print user information.Theutmp file will be /var/run/utmp.",
        "name": "pinky - lightweight finger",
        "section": 1
    },
    {
        "command": "pipesz",
        "description": "Pipes and FIFOs maintain an internal buffer used to transfer databetween the read end and the write end. In some cases, thedefault size of this internal buffer may not be appropriate. Thisprogram provides facilities to set and examine the size of thesebuffers.The --set operation sets pipe buffer sizes. If it is specified,it must be specified with an explicit size. Otherwise, it isimplied and the size is read from /proc/sys/fs/pipe-max-size. Thekernel may adjust size as described in fcntl(2). To determine theactual buffer sizes set, use the --verbose option. If neither--file nor --fd are specified, --set acts on standard output.The --set operation permits an optional command to execute aftersetting the pipe buffer sizes. This command is executed with theadjusted pipes.The --get operation outputs data in a tabular format. The firstcolumn is the name of the pipe as passed to pipesz. Filedescriptors are named as \"fd N\". The second column is the size,in bytes, of the pipe\u2019s internal buffer. The third column is thenumber of unread bytes currently in the pipe. The columns areseparated by tabs ('\\t', ASCII 09h). If --verbose is specified, adescriptive header is also emitted. If neither --file nor --fdare specified, --get acts on standard input.Unless the --check option is specified, pipesz does not exit ifit encounters an error while manipulating a file or filedescriptor. This allows pipesz to be used generically withoutfear of disrupting the execution of pipelines should the type ofcertain files be later changed. For minimal disruption, the--quiet option prevents warnings from being emitted in thesecases.The kernel imposes limits on the amount of pipe buffer spaceunprivileged processes can use, though see BUGS below. The kernelwill also refuse to shrink a pipe buffer if this would cause aloss of buffered data. See pipe(7) for additional details.pipesz supports specifying multiple short options consecutively,in the usual getopt(3) fashion. The first non-option argument isinterpreted as command. If command might begin with '-', use '--'to separate it from arguments to pipesz. In shell scripts, it isgood practice to use '--' when parameter expansion is involved.pipesz itself does not read from standard input and does notwrite to standard output unless --get, --help, or --version arespecified.",
        "name": "pipesz - set or examine pipe and FIFO buffer sizes",
        "section": 1
    },
    {
        "command": "pkgdata",
        "description": "pkgdata takes a set of data files and packages them for use byICU or applications that use ICU. The typical reason to packagefiles using pkgdata is to make their distribution easier andtheir loading by ICU faster and less consuming of limited systemresources such as file descriptors.Packaged data also allowapplications to be distributed with fewer resource files, or evenwith none at all if they link against the packaged data directly.pkgdata supports a few different methods of packaging data thatserve different purposes.The default packaging mode is common, or archive.In this mode,the different data files are bundled together as an architecture-dependent file that can later be memory mapped for use by ICU.Data packaged using this mode will be looked up under the ICUdata directory. Such packaging is easy to use for applicationsresource bundles, for example, as long as the application caninstall the packaged file in the ICU data directory.Another packaging mode is the dll, or library, mode, where thedata files are compiled into a shared library. ICU used to beable to dynamically load these shared libraries, but as of ICU2.0, such support has been removed. This mode is still useful fortwo main purposes: to build ICU itself, as the ICU data ispackaged as a shared library by default; and to build resourcebundles that are linked to the application that uses them. Suchresource bundles can then be placed anywhere where the system'sdynamic linker will be looking for shared libraries, instead ofbeing forced to live inside the ICU data directory.The static packaging mode is similar to the shared library oneexcept that it produces a static library.Finally, pkgdata supports a files mode which simply copies thedata files instead of packaging them as a single file or library.This mode is mainly intended to provide support for building ICUbefore it is packaged as separate small packages for distributionwith operating systems such as Debian GNU/Linux for example.Please refer to the packaging documentation in the ICU sourcedistribution for further information on the use of this mode.pkgdata builds, packages, installs, or cleans the appropriatedata based on the options given without the need to call GNU makeanymore.",
        "name": "pkgdata - package data for use by ICU",
        "section": 1
    },
    {
        "command": "pkill",
        "description": "pgrep looks through the currently running processes and lists theprocess IDs which match the selection criteria to stdout.Allthe criteria have to match.For example,$ pgrep -u root sshdwill only list the processes called sshd AND owned by root.Onthe other hand,$ pgrep -u root,daemonwill list the processes owned by root OR daemon.pkill will send the specified signal (by default SIGTERM) to eachprocess instead of listing them on stdout.pidwait will wait for each process instead of listing them onstdout.",
        "name": "pgrep, pkill, pidwait - look up, signal, or wait for processesbased on name and other attributes",
        "section": 1
    },
    {
        "command": "pldd",
        "description": "The pldd command displays a list of the dynamic shared objects(DSOs) that are linked into the process with the specifiedprocess ID (PID).The list includes the libraries that have beendynamically loaded using dlopen(3).",
        "name": "pldd - display dynamic shared objects linked into a process",
        "section": 1
    },
    {
        "command": "plog",
        "description": "This manual page describes the pon, plog and poff scripts, whichallow users to control PPP connections.ponpon, invoked without arguments, runs the /etc/ppp/ppp_on_bootfile, if it exists and is executable. Otherwise, a PPP connectionwill be started using configuration from /etc/ppp/peers/provider.This is the default behaviour unless an isp-name argument isgiven.For instance, to use ISP configuration \"myisp\" run:pon myisppon will then use the options file /etc/ppp/peers/myisp.You canpass additional options after the ISP name, too.pon can be usedto run multiple, simultaneous PPP connections.poffpoff closes a PPP connection. If more than one PPP connectionexists, the one named in the argument to poff will be killed,e.g.poff myprovider2will terminate the connection to myprovider2, and leave the PPPconnections to e.g. \"myprovider1\" or \"myprovider3\" up andrunning.poff takes the following command line options:-rcauses the connection to be redialed after it isdropped.-dtoggles the state of pppd's debug option.-ccauses pppd(8) to renegotiate compression.-astops all running ppp connections. If the argumentisp-name is given it will be ignored.-hdisplays help information.-vprints the version and exits.If no argument is given, poff will stop or signal pppd ifand only if there is exactly one running. If more than oneconnection is active, it will exit with an error code of1.plogplog shows you the last few lines of /var/log/ppp.log. If thatfile doesn't exist, it shows you the last few lines of your/var/log/syslog file, but excluding the lines not generated bypppd.This script makes use of the tail(1) command, so argumentsthat can be passed to tail(1) can also be passed to plog.Note: the plog script can only be used by root or another systemadministrator in group \"adm\", due to security reasons. Also, tohave all pppd-generated information in one logfile, that plog canshow, you need the following line in your /etc/syslog.conf file:local2.*-/var/log/ppp.log",
        "name": "pon, poff, plog - starts up, shuts down or lists the log of PPPconnections",
        "section": 1
    },
    {
        "command": "pmafm",
        "description": "A collection of one or more Performance Co-Pilot (PCP) archivelogs may be combined with a control file to produce a PCP archivefolio.Archive folios are created using either mkaf(1) or theinteractive ``record mode'' services of PCP clients likepmchart(1).pmafm provides a number of services that may be used to processfolios.In particular, it provides support for execution of PCPtools using one or more of the component archive logs within anarchive folio.The target folio is identified by the folio control filefolioname.The syntax for a folio control file is described inmkaf(1).If present, the command and arguments following folioname areinterpreted and executed as a single command, otherwise commandsare read from standard input.The following commands are supported.archivesSubsequent commands apply to all archives in the folio.archives N[,...]Archives within a folio are numbered 1, 2, etc.Subsequent commands are restricted to apply only to thedesignated archives.archives name[,...]Archives within a folio have unique names.Subsequentcommands are restricted to apply only to the designatedarchives.checkValidate the presence and format of each file in the folioand the component archives.helpA brief reminder of the command syntax.?is a synonymfor help.hostsSubsequent commands apply to all archives in the folio.hosts hostname[,...]Subsequent commands are restricted to apply only to thosearchives that match the designated hostnames.list [verbose]Display the contents of the folio.By default the controlheader and the ordinal number, hostname and archive basename for each archive in the folio.The verbose optioncauses pmafm to dump the label record from each archiveusing pmdumplog -l.The first named archive in the folio is assumed to beassociated with the default host for any tool that triesto replay multiple archives from the folio.quitExit pmafm.removeEcho on standard output the sh(1) commands required toremove all of the physical files associated with thisarchive folio.repeat tool [arg ...]Execute the known PCP tool once per selected archive.Forexample, the commandrepeat pmval -t60 kernel.all.loadwould run pmval(1) once per archive, with an appropriate-a argument.replaySome archive folios are created by tools (e.g. pmchart(1))that provide sufficient information to allow all of theinformation in all of the archives of a folio to bereplayed.[run] tool [arg ...]Execute the known PCP tool on the selected archives.SomePCP tools are able to process multiple concurrentarchives, and in this case the tool is run once with thelist of all selected archives passed via a -a argument.Otherwise, this command is synonymous with repeat.selectionsDisplay those archives that would be selected forprocessing with a repeat, replay or run command.The restrictions via any hosts and archives commands areconjuncted.These restrictions serve to limit the specificarchives processed in the subsequent repeat, replay, run andselections commands.By default, all archives are selected.Keywords in commands may be abbreviated provided no ambiguity isintroduced, e.g.help, hel and he are synonymous, but h isambiguous.",
        "name": "pmafm - Performance Co-Pilot archive folio manager",
        "section": 1
    },
    {
        "command": "pmap",
        "description": "The pmap command reports the memory map of a process orprocesses.",
        "name": "pmap - report memory map of a process",
        "section": 1
    },
    {
        "command": "pmcd",
        "description": "pmcd is the collector used by the Performance Co-Pilot (seePCPIntro(1)) to gather performance metrics on a system.As arule, there must be an instance of pmcd running on a system forany performance metrics to be available to the PCP.pmcd accepts connections from client applications running eitheron the same machine or remotely and provides them with metricsand other related information from the machine that pmcd isexecuting on.pmcd delegates most of this request servicing to acollection of Performance Metrics Domain Agents (or just agents),where each agent is responsible for a particular group ofmetrics, known as the domain of the agent.For instance, thepostgresql agent is responsible for reporting informationrelating to the PostgreSQL database, such as the transaction andquery counts, indexing and replication statistics, and so on.The agents may be processes started by pmcd, independentprocesses or Dynamic Shared Objects (DSOs, see dlopen(3))attached to pmcd's address space.The configuration sectionbelow describes how connections to agents are specified.Note that if a PDU exchange with an agent times out, the agenthas violated the requirement that it delivers metrics with littleor no delay.This is deemed a protocol failure and the agent isdisconnected from pmcd.Any subsequent requests for informationfrom the agent will fail with a status indicating that there isno agent to provide it.It is possible to specify access control to pmcd based on users,groups and hosts.This allows one to prevent users, groups ofusers, and certain hosts from accessing the metrics provided bypmcd and is described in more detail in the access controlsection below.",
        "name": "pmcd - performance metrics collector daemon",
        "section": 1
    },
    {
        "command": "pmcd_wait",
        "description": "pmcd_wait waits for the Performance Metrics Collector Daemon(PMCD) to be running and accepting client connections.Unless directed to another host by the -h option, pmcd_wait willtry to contact pmcd(1) on the local host.pmcd_wait will timeout and abandon the attempt to connect to pmcdafter 60 seconds.This default timeout interval may be changedusing the -t option, where the interval argument follows thesyntax described in PCPIntro(1) and in the simplest form may bean unsigned integer (the implied units in this case are seconds).On successful connection to pmcd an exit status of zero isreturned.If an error or timeout occurs, then a non-zero exit status isreturned as described below.",
        "name": "pmcd_wait - wait for PMCD to accept client connections",
        "section": 1
    },
    {
        "command": "pmchart",
        "description": "pmchart is a graphical utility that plots performance metricsvalues available through the facilities of the Performance Co-Pilot (PCP).Multiple charts can be displayed simultaneously,either aligned on the unified time axis (X-axis), and through theuse of multiple interface Tabs.Metric values can be sourced from one or more live hosts(simultaneously).Alternatively, one or more sets of PCParchives can be used as a source of historical data.SeePCPIntro(1) for an in-depth discussion of the capabilities of thePCP framework, many of which are used by pmchart.Many aspects of the behaviour of pmchart can be customisedthrough the interface.In particular, the use of \"views\" (referto the section describing VIEWS later in this document) allowspredefined sets of metrics and charting parameters like colors,scaling, titles, legends, and so on to be stored for later use,or use with different hosts and sets of archives.In addition,the Preferences dialog allows customisations to the rest of thepmchart user interface to be saved and restored between differentinvocations of the tool.This allows the default backgroundcolor, highlight color, contents and location of the toolbar, andmany other aspects to be configured.pmchart makes extensive use of the pmtime(1) utility for timecontrol, refer to the pmtime manual page for further details ofits operation.",
        "name": "pmchart - strip chart tool for Performance Co-Pilot",
        "section": 1
    },
    {
        "command": "pmclient",
        "description": "pmclient and pmclient_fg are simple clients that use thePerformance Metrics Application Programming Interface (PMAPI) toreport some high-level system performance metrics.The real value of these tools is as sample clients using thePMAPI(3), interfaces and to this end the source code is includedwith the Performance Co-Pilot (PCP) package (see PCPIntro(1)),and is typically installed in /usr/share/pcp/demos/pmclient.The pmclient_fg program differs to pmclient in that it uses thefetchgroup API extension to the PMAPI, see pmFetchGroup(3).Normally pmclient operates on the distributed Performance MetricsName Space (PMNS), however if the -n option is specified analternative local PMNS is loaded from the file pmnsfile.Unless directed to another host by the -h option, or to anarchive by the -a option, pmclient will contact the PerformanceMetrics Collector Daemon (PMCD) on the local host to obtain therequired information.The argument to -a is a comma-separatedlist of names, each of which may be the base name of an archiveor the name of a directory containing one or more archives.The-a and -h options are mutually exclusive.By default, pmclient reports the time of day according to thelocal timezone on the system where pmclient is run.The -Zoption changes the timezone to timezone in the format of theenvironment variable TZ as described in environ(7).The -zoption changes the timezone to the local timezone at the hostthat is the source of the performance metrics, as identified viaeither the -h or -a options.The output from pmclient is directed to standard output, andlists+Aggregate CPU utilization, in the range 0 to 1.+If the system has more than 1 CPU, the ordinal number of thebusiest CPU, in the range 0 to ...+If the system has more than 1 CPU, the CPU utilization for thebusiest CPU.+Real free memory in Mbytes.+Aggregate physical disk I/O operations per second (IOPS).+Load average over the last 1 minute and over the last 15minutes.",
        "name": "pmclient, pmclient_fg - a simple performance metrics client",
        "section": 1
    },
    {
        "command": "pmclient_fg",
        "description": "pmclient and pmclient_fg are simple clients that use thePerformance Metrics Application Programming Interface (PMAPI) toreport some high-level system performance metrics.The real value of these tools is as sample clients using thePMAPI(3), interfaces and to this end the source code is includedwith the Performance Co-Pilot (PCP) package (see PCPIntro(1)),and is typically installed in /usr/share/pcp/demos/pmclient.The pmclient_fg program differs to pmclient in that it uses thefetchgroup API extension to the PMAPI, see pmFetchGroup(3).Normally pmclient operates on the distributed Performance MetricsName Space (PMNS), however if the -n option is specified analternative local PMNS is loaded from the file pmnsfile.Unless directed to another host by the -h option, or to anarchive by the -a option, pmclient will contact the PerformanceMetrics Collector Daemon (PMCD) on the local host to obtain therequired information.The argument to -a is a comma-separatedlist of names, each of which may be the base name of an archiveor the name of a directory containing one or more archives.The-a and -h options are mutually exclusive.By default, pmclient reports the time of day according to thelocal timezone on the system where pmclient is run.The -Zoption changes the timezone to timezone in the format of theenvironment variable TZ as described in environ(7).The -zoption changes the timezone to the local timezone at the hostthat is the source of the performance metrics, as identified viaeither the -h or -a options.The output from pmclient is directed to standard output, andlists+Aggregate CPU utilization, in the range 0 to 1.+If the system has more than 1 CPU, the ordinal number of thebusiest CPU, in the range 0 to ...+If the system has more than 1 CPU, the CPU utilization for thebusiest CPU.+Real free memory in Mbytes.+Aggregate physical disk I/O operations per second (IOPS).+Load average over the last 1 minute and over the last 15minutes.",
        "name": "pmclient, pmclient_fg - a simple performance metrics client",
        "section": 1
    },
    {
        "command": "pmconfig",
        "description": "pmconfig displays the values for some or all configurationparameters of the local Performance Co-Pilot toolkitinstallation.",
        "name": "pmconfig - Performance Co-Pilot configuration parameters",
        "section": 1
    },
    {
        "command": "pmconfirm",
        "description": "pmquery provides a command-line-option compatible implementationof the xconfirm and xmessage tools, using a look-and-feel that isconsistent with pmchart.Several extensions to the functionalityof the original tools have been made, in order to improve theirspecific utility for pmchart, but wherever possible the originalsemantics remain.pmconfirm displays a line of text for each -t option specified(or a file when the -file option is used), and a button for each-b option specified.When one of the buttons is pressed, thelabel of that button is written to pmquery's standard output.This provides a means of communication/feedback from within shellscripts and a means to display useful information to a user froman application.pmmessage displays a window containing a message from the commandline, a file, or standard input.It additionally allows buttonsto be associated with an exit status, and only optionally willwrite the label of the button to standard output.pmquery extends the above tools to additionally support limiteduser input, as free form text.In this -input mode, any textentered will be output when the default button is pressed.Adefault text can be entered using the same mechanisms as theother tools.Command line options are available to specify font style, framestyle, modality and one of several different icons to bepresented for tailored visual feedback to the user.",
        "name": "pmconfirm, pmmessage, pmquery - general purpose dialog box",
        "section": 1
    },
    {
        "command": "pmcpp",
        "description": "pmcpp provides a very simple pre-processor originally designedfor manipulating Performance Metric Name Space (PMNS) files forthe Performance Co-Pilot (PCP), but later generalized to provideconditional blocks, include file processing, in-line shellcommand execution and macro substitution for arbitrary files.Itis most commonly used internally to process the PMNS file(s)after pmLoadNameSpace(3) or pmLoadASCIINameSpace(3) is called andto pre-process the configuration files for pmlogger(1).Input lines are read from infile (or standard input if infile isnot specified), processed and written to outfile (standard outputif outfile is not specified).All C-style comments of the form /* ... */ are stripped from theinput stream.There are no predefined macros for pmcpp although macros may bedefined on the command line using the -D option, where name andvalue must follow the same rules as described below for the#define directive.pmcpp accepts the following directives in the input stream (likecpp(1)):\u2022#include \"filename\"or#include <filename>In either case the directory search path for filename triesfilename first, then the directory for the command line infile(if any), followed by any directories named in -I command linearguments, and finally the $PCP_VAR_DIR/pmns directory (thelatter is for backwards compatibility with earlier versions ofpmcpp and the implied used from pmLoadASCIINameSpace(3)).#include directives may be nested, up to a maximum depth of 5.\u2022#shell \"command\"or#shell 'command'The shell command will be executed and the standard output isinserted into the stream of data to be processed by pmcpp.Functionally this is similar to a #include directive, exceptinput lines are read from a command rather than a file.The#shell directive is most useful for including or excluding#define or #undef directives based on run-time logic in thecommand.\u2022#define name valueor#define name \"value\"or#define name 'value'Defines a value for the macro name which must be a valid C-style name, so leading alphabetic or underscore followed byzero or more alphanumerics or underscores.value is optional(and defaults to an empty string).There is no characterescape mechanism, but either single quotes or double quotesmay be used to define a value with special characters orembedded horizontal white space (no newlines).\u2022#undef nameRemoves the macro definition, if any, for name.\u2022#ifdef name...#endifor#ifndef name...#endifThe enclosing lines will be stripped or included, depending ifthe macro name is defined or not.\u2022#elseWithin a #ifdef or #ifndef block, #else may be used to delimitlines to be included if the preceding ``if'' condition isfalse.Macro substitution is achieved by breaking the input stream intowords separated by white space or characters that are not validin a macro name, i.e. not alphanumeric and not underscore.Eachword is checked and if it matches a macro name, the word isreplaced by the macro value, otherwise the word is unchanged.There is generally one output line for each input line, althoughthe line may be empty if the text has been stripped due to thehandling of comments or conditional directives.When there is achange in the input stream, an additional output line isgenerated of the form:# lineno \"filename\"to indicate the following line of output corresponds to linenumber lineno of the input file filename.",
        "name": "pmcpp - simple preprocessor for the Performance Co-Pilot",
        "section": 1
    },
    {
        "command": "pmdaactivemq",
        "description": "pmdaactivemq is a Performance Metrics Domain Agent (PMDA) whichexports performance metrics from ActiveMQ.",
        "name": "pmdaactivemq - ActiveMQ performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdaaix",
        "description": "Each supported platform has a kernel Performance Metrics DomainAgent (PMDA) which extracts performance metrics from the kernelof that platfrom.A variety of platform-specific metrics areavailable, with an equally varied set of access mechanisms -typically this involves special system calls, or reading fromfiles in kernel virtual filesystems such as the Linux sysfs andprocfs filesystems.The platform kernel PMDA is one of the most critical componentsof the PCP installation, and must be as efficient and reliable aspossible.In all installations the default kernel PMDA will beinstalled as a shared library and thus executes directly withinthe pmcd(1) process.This slightly reduces overheads associatedwith querying the metadata and values associated with thesemetrics (no message passing is required).Unlike many other PMDAs, the kernel PMDA exports a number ofmetric namespace subtrees, such as kernel, network, swap, mem,ipc, filesys, nfs, disk and hinv (hardware inventory).Despite usually running as shared libraries, most installationsalso include a stand-alone executable for the kernel PMDA.Thisis to aid profiling and debugging activities, with dbpmda(1) forexample.In this case (but not for shared libraries), thefollowing command line options are available:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file named[platform].log is written in the current directory ofpmcd(1) when pmda[platform] is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default iseither the privileged \"root\" account on some platforms(Linux, for example) or the unprivileged \"pcp\" account(wherever possible).",
        "name": "pmdaaix, pmdadarwin, pmdafreebsd, pmdalinux, pmdanetbsd,pmdasolaris, pmdawindows - operating system kernel performancemetrics domain agents",
        "section": 1
    },
    {
        "command": "pmdaapache",
        "description": "pmdaapache is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics describing the state of an Apacheweb server.The apache PMDA exports metrics that measure the request rate,cumulative request sizes, uptime and various connection statesfor active clients.This information is obtained by performing a HTTP request to theserver status URL, which must be enabled in the httpd.confconfiguration file.ExtendedStatus on<Location /server-status>SetHandler server-statusOrder deny,allowDeny from allAllow from localhost</Location>A brief description of the pmdaapache command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedapache.log is written in the current directory of pmcd(1)when pmdaapache is started, i.e.$PCP_LOG_DIR/pmcd .Ifthe log file cannot be created or is not writable, output iswritten to the standard error instead.-SQuery the Apache status information from the named serverrather than the local host.-PQuery the Apache status information from the given portrather than the default (80).-LSpecify an alternative location for finding the server-status page.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdaapache - Apache2 web server performance metrics domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdabash",
        "description": "pmdabash is an experimental Performance Metrics Domain Agent(PMDA) which exports \"xtrace\" events from a traced bash(1)process.This includes the command execution information thatwould usually be sent to standard error with the set -x option tothe shell.Event metrics are exported showing each command executed, thefunction name and line number in the script, and a timestamp.Additionally, the process identifier for the shell and its parentprocess are exported.This requires bash version 4 or later.A brief description of the pmdabash command line options follows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedbash.log is written in the current directory of pmcd(1) whenpmdabash is started, i.e.$PCP_LOG_DIR/pmcd.If the logfile cannot be created or is not writable, output is writtento the standard error instead.-sAmount of time (in seconds) between subsequent evaluationsof the shell trace file descriptor(s).The default is 2seconds.-mMaximum amount of memory to be allowed for each event queue(one per traced process).The default is 2 megabytes.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdabash - Bourne-Again SHell trace performance metrics domainagent",
        "section": 1
    },
    {
        "command": "pmdabcc",
        "description": "pmdabcc is a Performance Co-Pilot (PCP) Performance MetricsDomain Agent (PMDA) which extracts live performance data fromextended BPF (Berkeley Packet Filter) in-kernel programs by usingBCC (BPF Compiler Collection) Python frontend.pmdabcc loads and acts as a bridge for any number of configured,separate PCP BCC PMDA Python modules running BPF programs.Existing BCC Python tools and programs should be possible to beutilized with PCP BCC PMDA modules with reasonable effort.See the BPF and BCC documentation for detailed description ofboth.",
        "name": "pmdabcc - BCC PMDA",
        "section": 1
    },
    {
        "command": "pmdabind2",
        "description": "This PMDA extracts performance data from BIND (Berkeley InternetName Domain).It enables collection of most of the statisticsmetrics from the Bind server version 9 or later, which includes:\u2022 overall memory statistics\u2022 overall per-query statistics (general queries, EDNS/truncatedresponses, Update/Notify/AXFR/IXFR messages)\u2022 overall error statistics (Rejected, SERVFAIL, Update/XFRfailures ...)\u2022 overall statistics per transport protocol, EDNS and per versionof IP protocol\u2022 resolver statistics (successes, errors, round-trip times inseveral ranges)\u2022 detailed per-socket statistics with respect to the transportprotcol and IP version including errors\u2022 detailed per-file-descriptor statistics including errorsThe PMDA performs per-second collection of the whole data set(148 metrics on the test environment) with modest requirements(2% CPU usage on Intel i7-4700MQ @2.4 GHz, cca 30 MB RAM).If more than 1 requests/sec is performed, the memoized values areused so that the statistics interface of the Bind server does notget overloaded.",
        "name": "pmdabind2 - BIND performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdabonding",
        "description": "pmdabonding is a Performance Metrics Domain Agent (PMDA) whichexports metric values from bonded network interfaces in the Linuxkernel.",
        "name": "pmdabonding - Linux bonded interface performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdabpf",
        "description": "pmdabpf is a Performance Co-Pilot (PCP) Performance MetricsDomain Agent (PMDA) which extracts live performance data fromeBPF programs utilizing BPF CO-RE (libbpf and BTF).pmdabpf loads and acts as a bridge for any number of configured,separate bpf PMDA modules. Existing libbpf tools should bepossible to be utilized with the bpf PMDA modules with reasonableeffort.See the eBPF, libbpf and BPF CO-RE documentation for detaileddescriptions.",
        "name": "pmdabpf - eBPF PMDA",
        "section": 1
    },
    {
        "command": "pmdabpftrace",
        "description": "pmdabpftrace is a Performance Co-Pilot (PCP) Performance MetricsDomain Agent (PMDA) which exports metrics from bpftrace(8)scripts.",
        "name": "pmdabpftrace - bpftrace PMDA",
        "section": 1
    },
    {
        "command": "pmdacifs",
        "description": "pmdacifs is a Performance Metrics Domain Agent (PMDA) whichexports metric values about mounted CIFS shares from the/proc/fs/cifs directory. This PMDA requires at least the CIFSkernel module to be loaded to return some metric values and atleast one mounted CIFS share in order to provide data for allmetrics.",
        "name": "pmdacifs - Common Internet Filesystem (CIFS) PMDA",
        "section": 1
    },
    {
        "command": "pmdacisco",
        "description": "pmdacisco is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics from one or more Cisco routers.A brief description of the pmdacisco command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedcisco.log is written in the current directory of pmcd(1)when pmdacisco is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-PBy default, it is assumed that no user-level password isrequired to access the Cisco's telnet port.If user-levelpasswords have been enabled on the Ciscos, then thosepasswords must be specified to pmdacisco.If specified withthe -P option, password will be used as the default user-level password for all Ciscos.See also the INTERFACEIDENTIFICATION section below.-rpmdacisco will refresh the current values for allperformance metrics by contacting each Cisco router onceevery refresh seconds.The default refresh is 120 seconds.-sThe Cisco command prompt ends with the string prompt.Thedefault value is ``>''.The only way pmdacisco cansynchronize the sending of commands and the parsing ofoutput is by recognizing prompt as a unique string thatcomes at the end of all output, i.e. as the command promptwhen waiting for the next command.-UBy default, it is assumed that no username login is requiredto access the Cisco's telnet port.If username login hasbeen enabled on the Ciscos, then the corresponding usernamesmust be specified to pmdacisco.If specified with the -Uoption, username will be used as the default username loginfor all Ciscos.See also the INTERFACE IDENTIFICATIONsection below.-MUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.-xConnect to the Cisco via TCP port number port rather thanthe default 23 for a telnet connection.For each interface, once the telnet connection is established,pmdacisco is willing to wait up to 5 seconds for the Cisco toprovide a new snapshot of the requested information.If thisdoes not happen, the telnet connection is broken and no valuesare returned.This prevents pmdacisco tying up the Cisco'stelnet ports waiting indefinitely when the response from therouter is not what is expected, e.g. if the format of the ``showint'' output changes, or the command is in error because aninterface is no longer configured on the router.",
        "name": "pmdacisco - Cisco router performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdadarwin",
        "description": "Each supported platform has a kernel Performance Metrics DomainAgent (PMDA) which extracts performance metrics from the kernelof that platfrom.A variety of platform-specific metrics areavailable, with an equally varied set of access mechanisms -typically this involves special system calls, or reading fromfiles in kernel virtual filesystems such as the Linux sysfs andprocfs filesystems.The platform kernel PMDA is one of the most critical componentsof the PCP installation, and must be as efficient and reliable aspossible.In all installations the default kernel PMDA will beinstalled as a shared library and thus executes directly withinthe pmcd(1) process.This slightly reduces overheads associatedwith querying the metadata and values associated with thesemetrics (no message passing is required).Unlike many other PMDAs, the kernel PMDA exports a number ofmetric namespace subtrees, such as kernel, network, swap, mem,ipc, filesys, nfs, disk and hinv (hardware inventory).Despite usually running as shared libraries, most installationsalso include a stand-alone executable for the kernel PMDA.Thisis to aid profiling and debugging activities, with dbpmda(1) forexample.In this case (but not for shared libraries), thefollowing command line options are available:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file named[platform].log is written in the current directory ofpmcd(1) when pmda[platform] is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default iseither the privileged \"root\" account on some platforms(Linux, for example) or the unprivileged \"pcp\" account(wherever possible).",
        "name": "pmdaaix, pmdadarwin, pmdafreebsd, pmdalinux, pmdanetbsd,pmdasolaris, pmdawindows - operating system kernel performancemetrics domain agents",
        "section": 1
    },
    {
        "command": "pmdadbping",
        "description": "pmdadbping is a database response time measurement PMDA.pmdadbping runs dbprobe(1), and exports the performancemeasurements it makes available as PCP metrics.dbprobe(1) should be configured to use the type of DBIappropriate for the local database, which includes: RDBMSflavour, user/password, delay between \"ping\" requests, and theSQL statement to use.",
        "name": "pmdadbping - database response time and availability PMDA",
        "section": 1
    },
    {
        "command": "pmdadenki",
        "description": "pmdadenki is a Performance Metrics Domain Agent (PMDA) whichextracts electricity related performance metrics.Currently, metrics from RAPL (on Intel cpus) and battery chargevalues are available, if supported by the hardware.-l Location of the log file.By default, a log file nameddenki.log is written in the current directory of pmcd(1) whenpmdadenki is started, i.e.$PCP_LOG_DIR/pmcd.If the log filecannot be created or is not writable, output is written to thestandard error instead.",
        "name": "pmdadenki - metrics related to the systems electrical consumption",
        "section": 1
    },
    {
        "command": "pmdadm",
        "description": "pmdadm is a Performance Metrics Domain Agent (PMDA) which exportsmetric values for Device Mapper on the local system.This PMDA collects its data through the dmsetup(8) utility andthe dmstats API and requires that the program is installed inorder to function.In addition, at least one statistics regionmust be created using the dmstats(8) utility in order to getbasic counter values.See below for examples.Note that device-mapper statistics collection is not enabled bydefault and is not persistent across reboots (unless a systemadministrator has configured something to run in rc.local orequivalent).This is because there are overheads associated withstatistics collection.In addition, the pmdadm PMDA does notautomatically enable any statistics collection because it may notbe the only consumer of the data.",
        "name": "pmdadm - Device Mapper PMDA",
        "section": 1
    },
    {
        "command": "pmdadocker",
        "description": "pmdadocker is a docker Performance Metrics Domain Agent (PMDA)which exposes performance metrics as reported from the DockerRemote API.A brief description of the pmdadocker command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file nameddocker.log is written in the current directory of pmcd(1)when pmdadocker is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.Remote API metric requests are activated automatically and arefetched on a timer.This timer is represented as a by pmdadockervia the docker.control.timing.By default, pmdadocker will beset to fetch on a 1 second interval.pmdadocker will iterate over three different docker remote APIcalls:/containers/$ID/jsonContainer metrics regarding the current state of thecontainer. Such as PID, name or if the container isrunning./versionBasic version metrics about the current docker deamon inuse./containers/$ID/stats?stream=0More in depth memory and cpu metrics of the container.",
        "name": "pmdadocker - docker performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdads389",
        "description": "pmdads389 is a Performance Metrics Domain Agent (PMDA) whichextracts live performance data from a running 389 DirectoryServer instance.See the Red Hat Directory Server Administration Guide fordescription for each metric.",
        "name": "pmdads389 - 389 Directory Server PMDA",
        "section": 1
    },
    {
        "command": "pmdads389log",
        "description": "pmdads389log is a Performance Metrics Domain Agent (PMDA) whichextracts statistics from 389 Directory Server access log usingthe logconv.pl(1) utility.",
        "name": "pmdads389log - 389 Directory Server Log PMDA",
        "section": 1
    },
    {
        "command": "pmdaelasticsearch",
        "description": "pmdaelasticsearch is a Performance Metrics Domain Agent (PMDA)which exports performance metrics from elasticsearch.",
        "name": "pmdaelasticsearch - elasticsearch performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdafreebsd",
        "description": "Each supported platform has a kernel Performance Metrics DomainAgent (PMDA) which extracts performance metrics from the kernelof that platfrom.A variety of platform-specific metrics areavailable, with an equally varied set of access mechanisms -typically this involves special system calls, or reading fromfiles in kernel virtual filesystems such as the Linux sysfs andprocfs filesystems.The platform kernel PMDA is one of the most critical componentsof the PCP installation, and must be as efficient and reliable aspossible.In all installations the default kernel PMDA will beinstalled as a shared library and thus executes directly withinthe pmcd(1) process.This slightly reduces overheads associatedwith querying the metadata and values associated with thesemetrics (no message passing is required).Unlike many other PMDAs, the kernel PMDA exports a number ofmetric namespace subtrees, such as kernel, network, swap, mem,ipc, filesys, nfs, disk and hinv (hardware inventory).Despite usually running as shared libraries, most installationsalso include a stand-alone executable for the kernel PMDA.Thisis to aid profiling and debugging activities, with dbpmda(1) forexample.In this case (but not for shared libraries), thefollowing command line options are available:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file named[platform].log is written in the current directory ofpmcd(1) when pmda[platform] is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default iseither the privileged \"root\" account on some platforms(Linux, for example) or the unprivileged \"pcp\" account(wherever possible).",
        "name": "pmdaaix, pmdadarwin, pmdafreebsd, pmdalinux, pmdanetbsd,pmdasolaris, pmdawindows - operating system kernel performancemetrics domain agents",
        "section": 1
    },
    {
        "command": "pmdagfs2",
        "description": "pmdagfs2 is a Performance Metrics Domain Agent (PMDA) whichexports metric values about mounted GFS2 filesystems from thedebugfs filesystem.This PMDA requires debugfs along with atleast one mounted GFS2 filesystem to be mounted in order to beable to provide metric data.This PMDA can be used with GFS2 filesystems which are bothmounted as local filesystems and filesystems which aremountedas shared storage within a clustered environment. However thereare some metrics which specifically require GFS2 to be setup in aclustered environment to be able to provide metric data. This isdue to them expecting locking messages to be passed via thedistributed lock manager (DLM) between nodes of a cluster inorder to generate their output.These cluster-environment-only metrics can be distinguished bythe inclusion of their corresponding control metrics so that theycan be optionally enabled or disabled on systems where they arenot desired to be monitored or not supported.pmstore(3) can be used to assign values to these control metricsin order to enable (1) or disable (0) them.This mechanism isalso useful on distributions that do not currently have fullsupport for the GFS2 trace-points or provide older versions ofthe GFS2 driver.",
        "name": "pmdagfs2 - Global Filesystem v2 (GFS2) PMDA",
        "section": 1
    },
    {
        "command": "pmdagluster",
        "description": "pmdagluster is a Performance Metrics Domain Agent (PMDA) whichexports metric values about mounted gluster filesystems using thegluster(8) command.This PMDA exports metrics about volumes andbricks both local and remote to the node where pmdagluster isrunning.The gluster filesystem supports fine-grained control overenabling statistics on individual volumes, so that the values areoptionally enabled or disabled on systems where they are notdesired to be monitored.The pmstore(1) command can be used to enable and disableprofiling of volumes.Using the individual instances of thegluster.volume.profile metric, one can set their values (andassociated profiling) either on (1) or off (0).Additionally,pminfo(1) can report on the current status of profiling of eachvolume.# pminfo \u2010f gluster.volume.profilegluster.volume.profileinst [0 or \"gv0\"] value 0inst [1 or \"gv1\"] value 1# pmstore \u2010i \"gv0\" gluster.volume.profile 1gluster.volume.profile inst [0 or \"gv0\"] old value=0 new value=1Further details on the gluster filesystem can be found athttp://www.gluster.org .",
        "name": "pmdagluster - Gluster Filesystem PMDA",
        "section": 1
    },
    {
        "command": "pmdagpfs",
        "description": "pmdagpfs is a Performance Metrics Domain Agent (PMDA) whichexports metric values from the /usr/lpp/mmfs/bin/mmpmon programto provide information on mounted gpfs filesystems.",
        "name": "pmdagpfs - gpfs filesystem statistics performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdahacluster",
        "description": "pmdahacluster is capable of collecting metric information toenable the monitoring of Pacemaker based HA Clusters throughPerformance Co-Pilot.The PMDA collects it's metric data from the following componentsthat make up a Pacemkaer based HA Cluster: Pacemaker, Corosync,SBD, DRBD.For more detailed information regarding the metrics availableplease see the included pmns and helpfile with the PMDA.",
        "name": "pmdahacluster - High Availability Cluster PMDA",
        "section": 1
    },
    {
        "command": "pmdahaproxy",
        "description": "pmdahaproxy is a Performance Metrics Domain Agent (PMDA) whichextracts live performance data from HAProxy statistics socket orURL.By default the HAProxy stats socket is used to retrieve themetric but if the optional URL option is set (see below), thenthe HAProxy URL is used instead.See the HAProxy documentation for detailed description of eachmetric.",
        "name": "pmdahaproxy - HAProxy PMDA",
        "section": 1
    },
    {
        "command": "pmdaib",
        "description": "pmdaib is a Performance Metrics Domain Agent (PMDA) which exportsinformation and performance metrics about local Infiniband HCAsand local or remote Infiniband GUIDs.A brief description of the pmdaib command line options follows:-cLocation of the config file.By default, the config file isnamed $PCP_PMDAS_DIR/infiniband/config.See CONFIG FILE formore information.-DA debug values, as specified by pmdbg(1)-dSpecify an alternate performance metrics domain number.Almost never necessary.-lLocation of the log file.By default, a log file namedib.log is written to $PCP_LOG_DIR/pmcd.If the log filecannot be created or is not writable, output is written tothe standard error instead.-wWrite out the default config file to$PCP_PMDAS_DIRS/infiniband and exit immediately.Thewritten config file will contain the local HCA ports.Itwill not overwrite an existing file.This argument shouldonly be used to create the template config file and shouldnever appear in pmcd.conf.See CONFIG FILE for moreinformation on the file format and on monitoring remoteGUIDs.",
        "name": "pmdaib - Infiniband performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdajbd2",
        "description": "pmdajbd2 is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics from the Journal Block Devicesubsystem (version 2) in the Linux kernel.These metrics areexported by the kernel in procfs files, one file per blockdevice.The JBD2 subsystem is used by several filesystemsincluding ext3, ext4 and ocfs2.The jbd2 PMDA exports metrics that measure detailed journaltransaction information, such as time spent waiting and locked,request rates, blocks used and so on.A brief description of the pmdajbd2 command line options follows(these are only relevant when running the PMDA as a daemon, andnot as a shared library):-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, when running as adaemon a log file named jbd2.log is written in the currentdirectory of when pmdajbd2 is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.When running in shared library mode, anddiagnostic information will be written into the pmcd logfile, namely $PCP_LOG_DIR/pmcd/pmcd.log.-jAllows an alternate path to the jbd2 statistics files to bespecified.The default path is /proc/fs/jbd2.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdajbd2 - journal block device (JBD) performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdajson",
        "description": "pmdajson is a Performance Metrics Domain Agent (PMDA) whichexports metrics from arbitrary sources generating JavaScriptObject Notation (JSON) syntax.At least one pair of JSON inputs are required for pmdajson toprovide metrics for PCP clients; one describing metric metadataand one containing metric values data.Metadata is read oncefrom a file at PMDA startup while the data is read every time arequest for metric values is made by a PCP client.The data isread either from a JSON file or an external command generatingJSON output.More than one pair of JSON inputs can be used tosupport arbitrary number of metric sources in differentconfigured directories.The overall JSON format description is at http://www.json.org/ .",
        "name": "pmdajson - JSON PMDA",
        "section": 1
    },
    {
        "command": "pmdakernel",
        "description": "Each supported platform has a kernel Performance Metrics DomainAgent (PMDA) which extracts performance metrics from the kernelof that platfrom.A variety of platform-specific metrics areavailable, with an equally varied set of access mechanisms -typically this involves special system calls, or reading fromfiles in kernel virtual filesystems such as the Linux sysfs andprocfs filesystems.The platform kernel PMDA is one of the most critical componentsof the PCP installation, and must be as efficient and reliable aspossible.In all installations the default kernel PMDA will beinstalled as a shared library and thus executes directly withinthe pmcd(1) process.This slightly reduces overheads associatedwith querying the metadata and values associated with thesemetrics (no message passing is required).Unlike many other PMDAs, the kernel PMDA exports a number ofmetric namespace subtrees, such as kernel, network, swap, mem,ipc, filesys, nfs, disk and hinv (hardware inventory).Despite usually running as shared libraries, most installationsalso include a stand-alone executable for the kernel PMDA.Thisis to aid profiling and debugging activities, with dbpmda(1) forexample.In this case (but not for shared libraries), thefollowing command line options are available:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file named[platform].log is written in the current directory ofpmcd(1) when pmda[platform] is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default iseither the privileged \"root\" account on some platforms(Linux, for example) or the unprivileged \"pcp\" account(wherever possible).",
        "name": "pmdaaix, pmdadarwin, pmdafreebsd, pmdalinux, pmdanetbsd,pmdasolaris, pmdawindows - operating system kernel performancemetrics domain agents",
        "section": 1
    },
    {
        "command": "pmdakvm",
        "description": "pmdakvm is a Performance Metrics Domain Agent (PMDA) whichexports metric values from the Linux KVM (Kernel Virtual Machine)virtualization subsystem.Per-processor KVM trace metrics from the kernel events enumeratedbelow /sys/kernel/debug/tracing/events/kvm can be configuredstatically using the pmdakvm configuration file,/etc/pcp/kvm/kvm.conf.",
        "name": "pmdakvm - Linux virtualization performance metrics domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdalibvirt",
        "description": "pmdalibvirt is a Performance Metrics Domain Agent (PMDA) whichextracts live performance data from libvirt hypervisor anddomains (VMs).See the libvirt documentation for detailed description of eachmetric.",
        "name": "pmdalibvirt - libvirt PMDA",
        "section": 1
    },
    {
        "command": "pmdalinux",
        "description": "Each supported platform has a kernel Performance Metrics DomainAgent (PMDA) which extracts performance metrics from the kernelof that platfrom.A variety of platform-specific metrics areavailable, with an equally varied set of access mechanisms -typically this involves special system calls, or reading fromfiles in kernel virtual filesystems such as the Linux sysfs andprocfs filesystems.The platform kernel PMDA is one of the most critical componentsof the PCP installation, and must be as efficient and reliable aspossible.In all installations the default kernel PMDA will beinstalled as a shared library and thus executes directly withinthe pmcd(1) process.This slightly reduces overheads associatedwith querying the metadata and values associated with thesemetrics (no message passing is required).Unlike many other PMDAs, the kernel PMDA exports a number ofmetric namespace subtrees, such as kernel, network, swap, mem,ipc, filesys, nfs, disk and hinv (hardware inventory).Despite usually running as shared libraries, most installationsalso include a stand-alone executable for the kernel PMDA.Thisis to aid profiling and debugging activities, with dbpmda(1) forexample.In this case (but not for shared libraries), thefollowing command line options are available:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file named[platform].log is written in the current directory ofpmcd(1) when pmda[platform] is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default iseither the privileged \"root\" account on some platforms(Linux, for example) or the unprivileged \"pcp\" account(wherever possible).",
        "name": "pmdaaix, pmdadarwin, pmdafreebsd, pmdalinux, pmdanetbsd,pmdasolaris, pmdawindows - operating system kernel performancemetrics domain agents",
        "section": 1
    },
    {
        "command": "pmdalio",
        "description": "pmdalio is a Performance Metrics Domain Agent (PMDA) whichexports metric values about the Linux I/O target subsystem, whichprovides for protocols like iSCSI, FCP, FCoE.These allowstorage available on one host to be exported and consumed byother hosts using industry standard protocols.This PMDA exports summary metrics which are performance valueaggregations and configuration per LIO target instance.Additionally, it provides per LUN performance metrics includingIOPS, and READ and WRITE throughput.The LIO configuration is maintained within the kernel's configfsvirtual filesystem.The python-rtslib module provides aninterface to configfs, allowing tools like pmdalio to interactwith the settings and metadata held in configfs.",
        "name": "pmdalio - Linux LIO subsystem PMDA",
        "section": 1
    },
    {
        "command": "pmdalmsensors",
        "description": "pmdalmsensors is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics describing the state of hardwareusing the lm-sensors software on compatible motherboards.The lmsensors PMDA exports metrics that measure fan speeds andcore temperatures.",
        "name": "pmdalmsensors - Linux hardware monitoring performance metricsdomain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdalogger",
        "description": "pmdalogger is a configurable log file monitoring PerformanceMetrics Domain Agent (PMDA).It can be seen as analogous to the-f option to tail(1) and converts each new log line into aperformance event.It was the first PMDA to make extensive useof event metrics, which can be consumed by client tools likepmevent(1).The logger PMDA exports both event-style metrics reflectingtimestamped event records for text logged to a file (or set offiles or output from a process), as well as the more orthodoxsample-style metrics such as event counts and throughput sizevalues.The PMDA is configured via a configfile which contains one linefor each source of events (file or process).This file is setupby the Install script described in the later section on``INSTALLATION'' of the PMDA.A brief description of the pmdalogger command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedlogger.log is written in the current directory of pmcd(1)when pmdalogger is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-mLimit the physical memory used by the PMDA to buffer eventrecords to maxsize bytes.As log events arrive at the PMDA,they must be buffered until individual client tools requestthe next batch since their previous batch of events.Thedefault maximum is 2 megabytes.-sSets the polling interval for detecting newly arrived loglines.Mirrors the same option from the tail(1) command.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdalogger - log file performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdalustre",
        "description": "pmdalustre is a Performance Metrics Domain Agent (PMDA) whichreads and exports metric values from the statistics interfaces ofa Lustre filesystem.pmdalustre searches for statistics interfaces from severallocations, which vary depending on the version of Lustreinstalled locally.Recent Lustre versions (v2.12 and later) export statistics fromthe kernel debugfs pseudo filesystem, in the directories/sys/kernel/debug/lustre/llite and /sys/kernel/debug/lnet.Ifthese interfaces are not found during startup, pmdalustre willautomatically check if the statistics interfaces are availablefrom the procfs pseudo filesystem below the /proc/fs/lustre/lliteand /proc/sys/lnet directories.These are the default locationsof the statistics for Lustre versions less than v2.12.If neither of the above filesystem interfaces are detected, or ifthe user wants to override the default locations, pmdalustre alsosupports an optional configuration file named$PCP_PMDAS_DIR/lustre/lustre.conf.Note that $PCP_PMDAS_DIR isset to /var/lib/pcp/pmdas on most Linux basedsystems.Theconfiguration file supports perl(1) variable assignment syntax.An example configuration file suitable for Lustre v2.12 and lateris:$LLITE_PATH=\"/sys/kernel/debug/lustre/llite/\";$LNET_PATH=\"/sys/kernel/debug/lustre/lnet/\";See comments in the shipped lustre.conf file for further details.By default, this file is installed with everything commented (andso it has no effect unless edited) because the built-inheuristics used by pmdalustre should suffice.Finally, overriding all of the above, the LUSTRE_LLITE_PATH andLUSTRE_LNET_PATH environment variables may be set (and exported)to specify the directory locations of the statistics interfacesto be used.This mechanism using environment variabes isintended to be used for development and testing purposes only.The pmdalustre process runs as the root user because debugfsdirectories are not normally readable by unprivileged users.",
        "name": "pmdalustre - lustre filesystem statistics performance metricsdomain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdalustrecomm",
        "description": "pmdalustrecomm is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics from the Linux procfs filesystemabout the state of various aspects of the Lustre filesystem.The lustrecomm PMDA exports metrics that focus on distributedcommunication in the filesystem, including metrics related totimeouts, network drops, send/recv information and route lengths.However, it also covers the memory use of some of the Lustrefilesystem components.A brief description of the pmdalustrecomm command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedlustrecomm.log is written in the current directory ofpmcd(1) when pmdalustrecomm is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdalustrecomm - Lustre filesystem comms performance metricsdomain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdamailq",
        "description": "pmdamailq is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics describing the state of the e-mailqueues managed by sendmail(1) and other mail transfer agents.The mailq PMDA exports metrics that measure the total number ofentries in the mail queue, and the subtotals for entries thathave been queued for various time periods.A brief description of the pmdamailq command line optionsfollows:-bThe binlist argument specifies a list of delay thresholdsused to ``bin'' the entries in the queue into a a histogrambased on how long the entry has been in the mail queue.Thedefault thresholds are: 1 hour, 4 hours, 8 hours, 1 day, 3days and 7 days.The entries in binlist are comma separatedtime intervals, using the syntax described in PCPIntro(1)for an update or reporting interval, e.g. the default listcould be specified using the value1hr,4hrs,8hrs,1day,3days,7days.Values in binlist are assumed to be in ascending order, andmail items in the queue less than the first threshold arebinned into a special bin labeled ``recent''.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedmailq.log is written in the current directory of pmcd(1)when pmdamailq is started, i.e.$PCP_LOG_DIR/pmcd .If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-rUse an extended regular expression to match file names inthe mail queue directory, rather than assuming all \"df\"prefixed files in the directory are mail files (the \"df\"prefix is the sendmail convention, but this convention isnot followed by other mail daemons).The regex patternspecified should conform to the POSIX format described inregex(3), and it describes file names that should beconsidered mail.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.The optional queuedir argument defines the directory in whichpmdamailq expects to find the mail queue.The default is/var/spool/mqueue.",
        "name": "pmdamailq - mail queue performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdamemcache",
        "description": "This PMDA extracts performance data from memcached, a distributedmemory caching daemon commonly used to improve web servingperformance.A farm of memcached processes over multiple serverscan be utilised by a single web application, increasing the totalavailable object cache size, and decreasing the database loadassociated with smaller cache sizes.This system is described indetail at http://www.danga.com/memcached .",
        "name": "pmdamemcache - memcached performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdamic",
        "description": "pmdamic is a Performance Metrics Domain Agent (PMDA) whichexports metric values about Intel MIC cards using thelibmicmgmt(7) python bindings.This provides information on core, memory and power utilization",
        "name": "pmdamic - MIC card PMDA",
        "section": 1
    },
    {
        "command": "pmdammv",
        "description": "pmdammv is a Performance Metrics Domain Agent (PMDA) whichexports application level performance metrics using memory mappedfiles.It offers an extremely low overhead instrumentationfacility that is well-suited to long running, mission criticalapplications where it is desirable to have performance metricsand availability information permanently enabled.The mmv PMDA exports instrumentation that has been added to anapplication using the MMV APIs (refer to mmv_stats_init(3) andmmv(5) for further details).These APIs can be called fromseveral languages, including C, C++, Perl, Python, Java (via theseparate ``Parfait'' class library) and GoLang (via the separate``Speed'' library).A brief description of the pmdammv command line options follows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedmmv.log is written in the current directory of pmcd(1) whenpmdammv is started, i.e.$PCP_LOG_DIR/pmcd.If the logfile cannot be created or is not writable, output is writtento the standard error instead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdammv - memory mapped values performance metrics domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdamongodb",
        "description": "pmdamongodb is a Performance Co-Pilot (PCP) Performance MetricsDomain Agent (PMDA) which extracts live performance data from arunning MongoDB database server.",
        "name": "pmdamongodb - MongoDB database metrics",
        "section": 1
    },
    {
        "command": "pmdamounts",
        "description": "pmdamounts is a simple Performance Metrics Domain Agent (PMDA)which monitors availability of a given set of filesystem mounts.The mounts PMDA exports metrics that reflect whether theconfigured filesystems are mounted (\"up\") or not.The list ofmount points to monitor is specified via the$PCP_PMDAS_DIR/mounts/mounts.conf file which simply contains oneline for each mount point.A brief description of the pmdamounts command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedmounts.log is written in the current directory of pmcd(1)when pmdamounts is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdamounts - filesystem mounts performance metrics domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdamssql",
        "description": "pmdamssql is a Performance Co-Pilot (PCP) Performance MetricsDomain Agent (PMDA) which extracts live performance data from arunning Microsoft SQL Server database server.",
        "name": "pmdamssql - Microsoft SQL Server database PMDA",
        "section": 1
    },
    {
        "command": "pmdamysql",
        "description": "pmdamysql is a Performance Co-Pilot PMDA which extracts liveperformance data from a running MySQL or MariaDB database.",
        "name": "pmdamysql - MySQL and MariaDB database PMDA",
        "section": 1
    },
    {
        "command": "pmdanetbsd",
        "description": "Each supported platform has a kernel Performance Metrics DomainAgent (PMDA) which extracts performance metrics from the kernelof that platfrom.A variety of platform-specific metrics areavailable, with an equally varied set of access mechanisms -typically this involves special system calls, or reading fromfiles in kernel virtual filesystems such as the Linux sysfs andprocfs filesystems.The platform kernel PMDA is one of the most critical componentsof the PCP installation, and must be as efficient and reliable aspossible.In all installations the default kernel PMDA will beinstalled as a shared library and thus executes directly withinthe pmcd(1) process.This slightly reduces overheads associatedwith querying the metadata and values associated with thesemetrics (no message passing is required).Unlike many other PMDAs, the kernel PMDA exports a number ofmetric namespace subtrees, such as kernel, network, swap, mem,ipc, filesys, nfs, disk and hinv (hardware inventory).Despite usually running as shared libraries, most installationsalso include a stand-alone executable for the kernel PMDA.Thisis to aid profiling and debugging activities, with dbpmda(1) forexample.In this case (but not for shared libraries), thefollowing command line options are available:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file named[platform].log is written in the current directory ofpmcd(1) when pmda[platform] is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default iseither the privileged \"root\" account on some platforms(Linux, for example) or the unprivileged \"pcp\" account(wherever possible).",
        "name": "pmdaaix, pmdadarwin, pmdafreebsd, pmdalinux, pmdanetbsd,pmdasolaris, pmdawindows - operating system kernel performancemetrics domain agents",
        "section": 1
    },
    {
        "command": "pmdanetcheck",
        "description": "pmdanetcheck is a Performance Co-Pilot (PCP) Performance MetricsDomain Agent (PMDA) which does basic network checks on the localhost by using simple Python modules and, in some cases, externalutilities such as ping(1).pmdanetcheck loads and acts as a bridge for any number ofconfigured, separate PCP netcheck PMDA Python modules runningPython code or external programs.Existing Python modules andprograms should be possible to be utilized with PCP netcheck PMDAmodules with minimal effort.Note that on SELinux enabled systems for pmdanetcheck to be ableto use the ping(1) command the pcp group must be able to createICMP Echo sockets; please make sure the group id for pcp isincluded in the range at /proc/sys/net/ipv4/ping_group_range andrefer to icmp(7) for more details on this.",
        "name": "pmdanetcheck - netcheck PMDA",
        "section": 1
    },
    {
        "command": "pmdanetfilter",
        "description": "pmdanetfilter is a Performance Metrics Domain Agent (PMDA) whichexports metric values from the IP connection tracking module inthe Linux kernel.",
        "name": "pmdanetfilter - Linux netfilter IP connection trackingperformance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdanfsclient",
        "description": "pmdanfsclient is a Performance Metrics Domain Agent (PMDA) whichexports metric values from the /proc/self/mountstats interface toprovide information on NFS mounts.",
        "name": "pmdanfsclient - NFS client statistics performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdanginx",
        "description": "pmdanginx is a Performance Metrics Domain Agent (PMDA) whichexports performance metrics from nginx(8) - an HTTP and reverseproxy server, a mail proxy server, and a generic TCP proxyserver.",
        "name": "pmdanginx - nginx performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdanutcracker",
        "description": "This PMDA extracts performance data from NutCracker (orTwemProxy), a fast and lightweight proxy for memcached and Redisprotocol.",
        "name": "pmdanutcracker - NutCracker performance metrics domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdanvidia",
        "description": "pmdanvidia is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics describing the metrics available onNVIDIA GPU cards via the NVML library.The nvidia PMDA exports metrics that measure gpu activity, memoryutilization, fan speed, etc on NVIDIA Tesla and Quadro cards.Metrics are unlikely to be available for consumer class cards.A brief description of the pmdanvidia command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namednvidia.log is written in the current directory of pmcd(1)when pmdanvidia is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-tEnables and sets a sampling interval for automaticrefreshing of metric values.The functionality is disabledby default, however this option allows a time interval to bespecified on which all values are sampled - this has theeffect of constantly updating the accumulating metrics, withthe goal of assisting client tools such as pcp-atop(1) andpmlogger(1) to observe sub-sample time changes in GPU andprocess state.Typically these tools have longer samplingintervals, and can thus 'miss' activity happening duringtheir sampling interval.",
        "name": "pmdanvidia - nvidia gpu metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdaopenmetrics",
        "description": "pmdaopenmetrics is a Performance Metrics Domain Agent (PMDA)which dynamically creates PCP metrics from configured OpenMetricsendpoints, which provide HTTP based access to applicationmetrics.The PMDA essentially implements a bridge betweenPrometheus and PCP, allowing PCP to easily ingest performancedata from more than 650 registered end-points and many otherapplication specific end-points.The default config directory is$PCP_PMDAS_DIR/openmetrics/config.d/, see ``CONFIGURATIONSOURCES'' below.The default URL fetch timeout is 2 seconds.The default user, if not specified with the -u option, is thecurrent user.If the -n option is given, the list ofconfiguration files will not be sorted prior to processing.Thislist is sorted by default but that can be expensive if there area large number of configuration files (URLs and/or scripts).If the -D option is given, additional diagnostic messages will bewritten to the PMDA log file, which is$PCP_LOG_DIR/pmcd/openmetrics.log by default (see also -lbelow).In addition, the metric openmetrics.control.debug controls thesame debug flag and can be set with the following command:pmstore openmetrics.control.debug valuewhere value is either 1 (to enable verbose log messages) or 0 (todisable verbose log messages).This is particularly useful forexamining the http headers passed to each fetch request, filtersettings and other processing details that are logged when thedebugging flag is enabled.The -d option may be used to override the default performancemetrics domain number, which defaults to 144.It is stronglyrecommended not to change this.The domain number should bedifferent for every PMDA on the one host, and the same domainnumber should be used for pmdaopenmetrics PMDA on all hosts.Seealso the -r option, which allows the root of the dynamicnamespace to be changed from the default openmetrics.The -l option may be used to specify logfile as the destinationfor PMDA messages instead of the default,$PCP_LOG_DIR/pmcd/openmetrics.log.As a special case, logfilemay be \"-\" to send messages to the stderr stream instead, e.g.-l-.This would normally be the stderr stream for the parentprocess, pmcd(1), which may itself have redirected stderr.Thisredirection is normally most useful in a containerizedenvironment, or when using dbpmda(1).The -r option allows the root of the dynamic namespace to bechanged to root from the default, openmetrics.In conjunctionwith other command line options, this allows pmdaopenmetrics tobe deployed as a different PMDA with distinct metrics namespaceand metrics domain on the same host system.Note that all PMDAsrequire a unique domain number so the -d option must also bespecified.Use of the -r option may also change the defaults forsome other command line options, e.g. the default log file nameand the default configuration directory.",
        "name": "pmdaopenmetrics - OpenMetrics PMDA",
        "section": 1
    },
    {
        "command": "pmdaopenvswitch",
        "description": "pmdaopenvswitch is a Performance Metrics Domain Agent (PMDA)which exports metric values for each openvswitch virtual switchconfigured on the local system.",
        "name": "pmdaopenvswitch - OpenvSwitch PMDA",
        "section": 1
    },
    {
        "command": "pmdaoracle",
        "description": "pmdaoracle is a Performance Co-Pilot PMDA which extracts liveperformance data from a running Oracle database.",
        "name": "pmdaoracle - Oracle database PMDA",
        "section": 1
    },
    {
        "command": "pmdaoverhead",
        "description": "pmdaoverhead is a configurable Performance Metrics Domain Agent(PMDA) for exporting resource consumption for groups of relatedprocesses.The pmdaoverhead command line options are:-Cparse the configuration file(s) and exit after reporting anyerrors.-cconfiguration file(s), config may be either a file or adirectory; in the latter case all the files within config areassumed to be configuration files for pmdaoverhead and theywill all be processed.Each configuration file defines one or more ``groups'' ofprocesses of interest, using the syntax described in the``CONFIGURATION'' section below.By default all configuration files below the$PCP_SYSCONF_DIR/overhead/conf.d/ directory are used.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedoverhead.log is written in the current directory of pmcd(1)when pmdaoverhead is started, i.e.$PCP_LOG_DIR/pmcd.Ifthe log file cannot be created or is not writable, output iswritten to the standard error instead.-RThe PMDA uses a separate thread to periodically scan allprocesses to determine which processes are deemed``interesting'' in each group, and extracting resourceconsumption for those processes.The interval (in seconds)determines how often this scanning and resource calculationis done, the default is 60 (seconds).",
        "name": "pmdaoverhead - performance metrics domain agent (PMDA) exportingresource consumption metrics for groups of processes",
        "section": 1
    },
    {
        "command": "pmdaperfevent",
        "description": "pmdaperfevent is a Performance Metrics Domain Agent (PMDA) thatconfigures and reads the hardware performance counters using theLinux kernel perf_event API.The perfevent PMDA exports metrics for hardware performancecounters that are configurable from the Linux kernel perf_eventAPI.The PMDA uses the libpfm4 library to access the hardwareperformance counters so any counters that are supported inlibpfm4 should be available.Also included is the ability toread the Intel RAPL counters via direct MSR access.The PMDAsupports automatically loading different counters for eachhardware architecture.A single configuration file is used tospecify the desired counters for each hardware performancemonitoring unit (PMU).The configuration file allows differentcounters to be programmed on different CPUs and supports round-robin assignment of uncore counters needed for some AMD chips.The PMDA also counts for events exposed in the kernel via/sys/bus/event_source/devices directory. The PMU device name andthe event name must be mentioned in the configuration file.Otherwise, the metric won't be available to monitor through thisPMDA.The PMDA configures the counters to count events in both user andkernel mode.This means that the hardware counters areunavailable to use by normal unprivileged user applications whenthey are in use by the PMDA.The PMDA provides a mechanism totemporarily disable the system-wide counters in order to allownormal users to be able to use the counters if they wish.Seeperfalloc(1) for details.Note that pmdaperfevent is affected by the value of thekernel.perf_event_paranoid setting, which can be adjusted bysysctl(8).A brief description of the pmdaperfevent command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedperfevent.log is written in the current directory of pmcd(1)when pmdaperfevent is started, i.e.$PCP_LOG_DIR/pmcd.Ifthe log file cannot be created or is not writable, output iswritten to the standard error instead.-UUser account under which to run the agent.The default isthe privileged \"root\" account.-ilisten on given port number for connection from pmcd(1)-pcommunicate with pmcd(1) via stdin/stdout-uexpect pmcd(1) to connect on given unix domain socket-6expect pmcd(1) to connect on given ipv6 port (number orname)",
        "name": "pmdaperfevent - hardware performance counter performance metricsdomain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdapipe",
        "description": "pmdapipe is a configurable command output monitoring PerformanceMetrics Domain Agent (PMDA).It can be seen as analogous to arestricted shell, where options can be passed to preset commands,and each line of their output is converted into a performanceevent.These events can be consumed by client tools likepmval(1).The pipe PMDA exports both event-style metrics reflectingtimestamped event records for text-oriented command output, aswell as the more orthodox sample-style metrics such as eventcounts and throughput size values.The PMDA is configured via a configfile which contains one linefor each process from which output can be captured, as describedin the ``CONFIGURATION'' section below.A brief description of the pmdapipe command line options follows:-cspecifies an alternate configuration file for the PMDA.Bydefault, a file named $PCP_PMDAS_DIR/pipe/pipe.conf and anyfiles below the $PCP_SYSCONF_DIR/pipe.conf.d/ directory areused.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedpipe.log is written in the current directory of pmcd(1) whenpmdapipe is started, i.e.$PCP_LOG_DIR/pmcd.If the logfile cannot be created or is not writable, output is writtento the standard error instead.-mLimit the physical memory used by the PMDA to buffer eventrecords to maxsize bytes.As log events arrive at the PMDA,they must be buffered until individual client tools requestthe next batch since their previous batch of events.Thedefault maximum is 2 megabytes.",
        "name": "pmdapipe - command output capture performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdapodman",
        "description": "podman(1) is a utility for managing pods, containers andcontainer images, following the container pod concept popularizedby Kubernetes.pmdapodman is a Performance Metrics Domain Agent(PMDA) which extracts performance metrics describing the state ofcontainers and pods managed by podman.The podman PMDA exports metrics that measure information aboutindividual containers and groups of containers, called pods.A brief description of the pmdapodman command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedpodman.log is written in the current directory of pmcd(1)when pmdapodman is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.",
        "name": "pmdapodman - podman container performance metrics domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdapostfix",
        "description": "pmdapostfix is a Performance Metrics Domain Agent (PMDA) whichexports mail queue sizes as reported by qshape(1), as well asaggregate statistics collected from mail.log.",
        "name": "pmdapostfix - Postfix performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdapostgresql",
        "description": "pmdapostgresql is a Performance Co-Pilot (PCP) PerformanceMetrics Domain Agent (PMDA) which extracts live performance datafrom a running PostgreSQL database server.Many of the statistics available from a PostgreSQL server may notbe enabled by default.Refer to the online documentationhttps://www.postgresql.org/docs/current/static/monitoring-stats.html which describes each of the available parameters related tostatistics collection and how to enable them.",
        "name": "pmdapostgresql - PostgreSQL database PMDA",
        "section": 1
    },
    {
        "command": "pmdaproc",
        "description": "pmdaproc is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics describing the state of theindividual processes running on a Linux system.The proc PMDA exports metrics that measure the memory, processorand other resource use of each process, as well as summaryinformation collated across all of the running processes.ThePMDA uses credentials passed from the PMAPI(3) monitoring toolidentifying the user requesting the information, to ensure thatonly values the user is allowed to access are returned by thePMDA.This involves the PMDA temporarily changing its effectiveuser and group identifiers for the duration of requests forinstances and values.In other words, system calls to extractinformation are performed as the user originating the request andnot as a privileged user.The mechanisms available for transferof user credentials are described further in the PCPIntro(1)page.A brief description of the pmdaproc command line options follows:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-LChanges the per-process instance domain used by mostpmdaproc metrics to include threads as well.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedproc.log is written in the current directory of pmcd(1) whenpmdaproc is started, i.e.$PCP_LOG_DIR/pmcd.If the logfile cannot be created or is not writable, output is writtento the standard error instead.-rRestrict the set of processes exported in the per-processinstance domain to only those processes that are containedby the specified cgroup resource container.This optionprovides an optional finer granularity to the monitoring,and can also be used to reduce the resources consumed bypmdaproc during requests for instances and values.-UUser account under which to run the agent.The default isthe privileged \"root\" account, with seteuid (2) and setegid(2) switching for accessing most information.",
        "name": "pmdaproc - process performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdarabbitmq",
        "description": "pmdarabbitmq is a Performance Metrics Domain Agent (PMDA) whichexports metric values for each RabbitMQ queue configured on thelocal system.",
        "name": "pmdarabbitmq - Rabbit Message Queue subsystem PMDA",
        "section": 1
    },
    {
        "command": "pmdaredis",
        "description": "This PMDA extracts performance data from sending the INFO commandto a Redis (redis.io) server, which includes:\u2022 General information about the Redis server\u2022 Client connections\u2022 Memory consumption\u2022 Persistence statistics\u2022 Replication statistics\u2022 CPU consumption statistics\u2022 Redis command statistics\u2022 Redis Cluster statistics\u2022 Database related statisticsThe hostname (localhost), port (6379 by default) and otherconfiguration information must be specified in the$PCP_PMDAS_DIR/redis/redis.conf file.# cd $PCP_PMDAS_DIR/redis# [ edit redis.conf ]host=localhost.localdomain:6379To uninstall, the following must be done as root:# cd $PCP_PMDAS_DIR/redis# ./RemoveOnce this is setup, you can access the names and values for theredis performance metrics by doing the following as root:# cd $PCP_PMDAS_DIR/redis# ./InstallTo uninstall, the following must be done as root:# cd $PCP_PMDAS_DIR/redis# ./Removepmdaredis is launched by pmcd(1) and should never be executeddirectly. The Install and Remove scripts notify pmcd(1) when theagent is installed or removed.",
        "name": "pmdaredis - Redis performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdaroomtemp",
        "description": "pmdaroomtemp is a Performance Metrics Domain Agent (PMDA) whichexports the temperature from one or more sensors built using theDS2480 and DS1280 chipsets and MicroLAN technology from DallasSemiconductor Corporation.The roomtemp PMDA exports metrics that reflect the temperaturesfrom one or more of these devices, in both degrees Celcius andFahrenheit.Each metric has one instance for each temperaturesensor device.The external instance identifiers are the serialnumbers (in hex) of the DS1280 chips discovered when the MicroLANwas probed.A brief description of the pmdaroomtemp command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedroomtemp.log is written in the current directory of pmcd(1)when pmdaroomtemp is started, i.e.$PCP_LOG_DIR/pmcd.Ifthe log file cannot be created or is not writable, output iswritten to the standard error instead.",
        "name": "pmdaroomtemp - room temperature performance metrics domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdaroot",
        "description": "pmdaroot is a special Performance Metrics Domain Agent (PMDA)which cooperates closely with pmcd(1) and other PMDAs to providelimited privileged services to these unprivileged processes.In this role it is used to discover operating system containersrunning on the local host.It also enables access to performancedata from within those containers by other PMDAs running on thebare-metal host, using the pmdaRootConnect(3) and the associatednamespace interfaces.Like all other PMDAs, it also exports performance metrics fromthe domain it controls.Currently, this is limited toinformation about the containers on the local system; currentlyDocker and LXC containers can be detected.If a non-default Docker parent-cgroup name is being used, thisvalue can be indicated to pmdaroot through addition of a$PCP_SYSTEMD_CGROUP variable in /etc/pcp.conf.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedroot.log is written in the current directory of pmcd(1) whenpmdaroot is started, i.e.$PCP_LOG_DIR/pmcd.If the logfile cannot be created or is not writable, output is writtento the standard error instead.-sLocation of the unix(7) domain socket for communication withclients seeking privileged operations.By default, a socketfile named $PCP_TMP_DIR/pmcd/root.socket is used.",
        "name": "pmdaroot - a privileged PMCD helper performance metrics domainagent",
        "section": 1
    },
    {
        "command": "pmdarsyslog",
        "description": "pmdarsyslog is a Performance Metrics Domain Agent (PMDA) whichexports metric values from the rsyslogd(8) server.Further details about rsyslog can be found athttp://www.rsyslog.com/ .",
        "name": "pmdarsyslog - rsyslog (reliable and extended syslog) PMDA",
        "section": 1
    },
    {
        "command": "pmdasample",
        "description": "pmdasample is a sample Performance Metrics Domain Agent (PMDA)which exports a variety of synthetic performance metrics.This PMDA was developed as part of the quality assurance testingfor the PCP product, but has other uses, most notably in thedevelopment of new PCP clients.The metrics exported by the sample PMDA cover the full range ofdata types, data semantics, value cardinality, instance domainstability and error conditions found in real PMDAs.A brief description of the pmdasample command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-iExpect PMCD to connect to pmdasample on the specified TCP/IPport.port may be a port number or port name.-lLocation of the log file.By default, a log file namedsample.log is written in the current directory of pmcd(1)when pmdasample is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-pExpect PMCD to create a pipe and the connection topmdasample is via standard input and standard output.Thisis the default connection mode.-uExpect PMCD to connect to pmdasample on the Unix domainsocket named socket.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.At most one of the options -i, -p and -u may be specified.",
        "name": "pmdasample - sample performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdasendmail",
        "description": "pmdasendmail is a sendmail Performance Metrics Domain Agent(PMDA) which exports mail traffic statistics as collected bysendmail(1).Before the sendmail PMDA can export any metrics, sendmail(1) musthave statistics collection enabled.This involves checking thename of the statistics file, as given by the OS or O StatusFilecontrol lines in /etc/sendmail.cf, and then creating this file ifit does not already exist.Removing the file will terminatestatistics collection by sendmail(1) and hence the sendmail PMDA.A brief description of the pmdasendmail command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedsendmail.log is written in the current directory of pmcd(1)when pmdasendmail is started, i.e.$PCP_LOG_DIR/pmcd.Ifthe log file cannot be created or is not writable, output iswritten to the standard error instead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.There are no communication options, as the Install script ensuresthe sendmail PMDA will be connected to PMCD by a pipe.",
        "name": "pmdasendmail - sendmail performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdashping",
        "description": "pmdashping is a Performance Metrics Domain Agent (PMDA) whichexports quality of service and response time measurements forarbitrary commands as might be run from a shell such as sh(1).These measurements are intended to be used to quantify servicequality and service availability for those services that areeither mission critical or act as early indicators of adversesystem performance.The sample configuration monitors simple shell commands (exit anddate(1)), a short computationally intensive task using sum(1), ashort C compilation, DNS lookup via nslookup(1), YP lookup viaypcat(1), bind/portmapper service using rpcbind(1), SMTP byconnecting to telnet port 25 and sending an ``expn root''request, and NNTP by connecting to telnet port 119 and running a``listgroup'' command.It is expected that other commands would follow the examples inthe sample configuration file, and most deployments of thepmdashping PMDA are expected to use a customized configurationfile.A brief description of the pmdashping command line optionsfollows:-CParse configfile, reporting any errors and exiting with non-zero status if the file contains syntactical errors.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedshping.log is written in the current directory of pmcd(1)when pmdashping is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-IAmount of time (in seconds) between subsequent executions ofthe list of commands provided via the configuration fileconfigfile.The default is 2 minutes.-tAmount of time (in seconds) to wait before timing outawaiting a response for a command from configfile.Thedefault is 20 seconds.-UUser account under which to run the agent and all commands.The default is the unprivileged \"pcp\" account in currentversions of PCP, but in older versions the superuser account(\"root\") was used by default.The required configfile specifies ``tag'' and ``command'' pairs,each on a separate line.All of the commands are run one afteranother, with the whole group rescheduled to be run once perinterval.For each command that is run, pmdashping recordsinformation related to the success (or timeout), exit status,elapsed time and CPU time (system and user), and this informationis exported by the PMDA.The tags are used to identify theindividual commands amongst the values exported by the PMDA, andform the external instance domain identifiers for the pmdashpingmetrics which relate to each command.",
        "name": "pmdashping - \"shell-ping\" performance metrics domain agent",
        "section": 1
    },
    {
        "command": "pmdasimple",
        "description": "pmdasimple is a simple Performance Metrics Domain Agent (PMDA)which exports a small number of synthetic performance metrics.The simple PMDA is shipped as source code and is designed to bean aid for PMDA developers.In terms of code size and features,it is more complex than the trivial PMDA, about the same as thetxmon PMDA and less complex than the sample PMDA.The source forthe simple PMDA is a good template from which production,customized PMDAs can be developed.A brief description of the pmdasimple command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-iExpect PMCD to connect to pmdasimple on the specified TCP/IPport.port may be a port number or port name.-lLocation of the log file.By default, a log file namedsimple.log is written in the current directory of pmcd(1)when pmdasimple is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-pExpect PMCD to create a pipe and the connection topmdasimple is via standard input and standard output.Thisis the default connection mode.-uExpect PMCD to connect to pmdasimple on the Unix domainsocket named socket.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.At most one of the options -i, -p and -u may be specified.",
        "name": "pmdasimple - simple performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdaslurm",
        "description": "pmdaslurm is a Performance Metrics Domain Agent (PMDA) whichexports metric values from the slurm perlapi interface to provideinformation on slurm jobs, nodes and users.",
        "name": "pmdaslurm - SLURM statistics performance metrics domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdasmart",
        "description": "pmdasmart is a Performance Metrics Domain Agent (PMDA) whichexports metric values for Device Mapper on the local system.This PMDA collects its data through the smartctl(8) utility andrequires that the program is installed in order to function.Further details on smartctl and smartmontools can be found athttps://smartmontools.org .",
        "name": "pmdasmart - S.M.A.R.T Data PMDA",
        "section": 1
    },
    {
        "command": "pmdasockets",
        "description": "pmdasockets is a Performance Metrics Domain Agent (PMDA) whichexports metric values for current sockets on the local system.This PMDA currently collects its data through the ss(8) utilityand requires that the program is installed in order to function.This dependency may change in the future.",
        "name": "pmdasockets - sockets statistics PMDA",
        "section": 1
    },
    {
        "command": "pmdasolaris",
        "description": "Each supported platform has a kernel Performance Metrics DomainAgent (PMDA) which extracts performance metrics from the kernelof that platfrom.A variety of platform-specific metrics areavailable, with an equally varied set of access mechanisms -typically this involves special system calls, or reading fromfiles in kernel virtual filesystems such as the Linux sysfs andprocfs filesystems.The platform kernel PMDA is one of the most critical componentsof the PCP installation, and must be as efficient and reliable aspossible.In all installations the default kernel PMDA will beinstalled as a shared library and thus executes directly withinthe pmcd(1) process.This slightly reduces overheads associatedwith querying the metadata and values associated with thesemetrics (no message passing is required).Unlike many other PMDAs, the kernel PMDA exports a number ofmetric namespace subtrees, such as kernel, network, swap, mem,ipc, filesys, nfs, disk and hinv (hardware inventory).Despite usually running as shared libraries, most installationsalso include a stand-alone executable for the kernel PMDA.Thisis to aid profiling and debugging activities, with dbpmda(1) forexample.In this case (but not for shared libraries), thefollowing command line options are available:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file named[platform].log is written in the current directory ofpmcd(1) when pmda[platform] is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default iseither the privileged \"root\" account on some platforms(Linux, for example) or the unprivileged \"pcp\" account(wherever possible).",
        "name": "pmdaaix, pmdadarwin, pmdafreebsd, pmdalinux, pmdanetbsd,pmdasolaris, pmdawindows - operating system kernel performancemetrics domain agents",
        "section": 1
    },
    {
        "command": "pmdastatsd",
        "description": "StatsD is simple, text-based UDP protocol for receivingmonitoring data of applications in architecture client-server.pmdastatsd is an Performance Metrics Domain Agent that collectsStatsD data, aggregates them and makes them available to anyPerformance Co-Pilot client, which is ideal for easily trackingstats in your application.The statsd PMDA supports Counter, Gauge and Duration (withinstances for minimum, maximum, median, average, 90th percentile,95th percentile, 99th percentile, count and standard deviation)metric types, offers multiple parsing options: Ragel orhandwritten/custom parser, offers multiple aggregating optionsfor duration metric type: basic histogram or HDR histogram,supports custom form of labels, logging, exports multiple metricsabout itself and may be configured either with an ini file orcommand line options.",
        "name": "pmdastatsd - StatsD protocol performance metric domain agent(PMDA)",
        "section": 1
    },
    {
        "command": "pmdasummary",
        "description": "pmdasummary is a Performance Metrics Domain Agent (PMDA) whichderives performance metrics values from values made available byother PMDAs.pmdasummary consists of two processes:pmie processThe inference engine for performance values pmie(1) isused to periodically sample values for the base metricsand compute the derived values.This process is launchedwith the given pmie-command-line arguments by the mainprocess, described below.main processThe main process reads and buffers the values computed bypmie(1) and makes them available to the performancemetrics collector daemon pmcd(1).A brief description of the pmdasummary command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-hThis option specifies an alternative help text file helpfilefor describing the metrics that pmdasummary represents.-lLocation of the log file.By default, a log file namedsummary.log is written in the current directory of pmcd(1)when pmdasummary is started, i.e.$PCP_LOG_DIR/pmcd.Ifthe log file cannot be created or is not writable, output iswritten to the standard error instead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdasummary - summary performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdasystemd",
        "description": "pmdasystemd is a systemd log file monitoring Performance MetricsDomain Agent (PMDA).It can be seen as analagous to the -foption to journalctl(1) and converts each new log line into aperformance event, suitable for consumption by PMAPI(3) clienttools like pmevent(1).The systemd PMDA exports both event-style metrics reflectingtimestamped event records for messages logged to the system logs,as well as the more orthodox sample-style metrics such as messagecounts and throughput size values.A brief description of the pmdasystemd command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-fDisables per-uid/gid record filtering.By default the userand group credentials will be used to filter log recordsreturned to the client tool, preventing information exposureto arbitrary users.This option disables that, so use onlywith extreme caution.-lLocation of the log file.By default, a log file namedsystemd.log is written in the current directory of pmcd(1)when pmdasystemd is started, i.e.$PCP_LOG_DIR/pmcd.Ifthe log file cannot be created or is not writable, output iswritten to the standard error instead.-mLimit the physical memory used by the PMDA to buffer eventrecords to maxsize bytes.As log events arrive at the PMDA,they must be buffered until individual client tools requestthe next batch since their previous batch of events.Thedefault maximum is 2 megabytes.-sSets the polling interval for detecting newly arrived loglines.Mirrors the same option from the tail(1) command.-UUser account under which to run the agent.The default isthe \"adm\" user account.",
        "name": "pmdasystemd - systemd performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdate",
        "description": "pmdate displays the current date and/or time, with an optionaloffset.An offset is specified with a leading sign (``+'' or ``-''),followed by an integer value, followed by one of the following``scale'' specifiers;SsecondsMminutesHhoursddaysmmonthsyyearsThe output format follows the same rules as for date(1) andstrftime(3).For example, the following will display the date a week ago asDDMMYYYY;pmdate -7d %d%m%Y",
        "name": "pmdate - display an offset date",
        "section": 1
    },
    {
        "command": "pmdatrace",
        "description": "pmdatrace is a Performance Metrics Domain Agent (PMDA) whichexports transaction performance metrics from applicationprocesses which use the pcp_trace library described inpmdatrace(3).A brief description of the pmdatrace command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedtrace.log is written in the current directory of pmcd(1)when pmdatrace is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-AHost-based access control for pmdatrace.access must beeither an allow or deny specification, using eitherallow:hostspec:maxconns or disallow:hostspec, where `allow'and `disallow' are keywords, `hostspec' is a hostspecification conforming to the format used by both pmcd(1)and pmlogger(1), and `maxconns' is the maximum number ofconnections allowed from a given `hostspec'.Using amaximum connections of zero specifies an unlimited number ofconnections for the accompanying `hostspec'.-ICommunicate with pcp_trace clients via the given Internetport.This can alternatively be specified by setting$PCP_TRACE_PORT in the environment to some valid port number(use of the -I option overrides this).The default portnumber is 4323.-Tperiod defines the aggregation period used to compute therecent averages and extrema.Specified as a time intervalusing the syntax described in PCPIntro(1) for the common -tPCP argument, e.g. 30 seconds or 1 min.The default is 60seconds.-MUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.-NInternally, the aggregation period is divided into bucketdivisions, and the rolling average is recomputed everyperiod/bucket seconds.For example, the defaults correspondto -T 60 and -N 12, which means the average is recomputedevery five seconds for a period covering the prior 60seconds.-UThis option allows the dimension and scale associated withthe observation value metric to be configured.units is acomma-separated string of six integer values, which are thespace dimension, time dimension, count dimension, spacescale, time scale, and count scale, respectively.Thedefault dimension and scale is ``none'', which is equivalentto presenting ``0,0,0,0,0,0'' as the argument to -U.Theunits associated with a metric are most easily viewed usingthe -d (metric description) option to pminfo(1).TheInstall script described below steps through this optionquite explicitly, so it is recommended that the Installscript be used for building up the units specification.Essentially, the exported metrics provide statistics on the timefor completion of each transaction, and an average count oftransactions completed and watch points passed over a given timeperiod.",
        "name": "pmdatrace - application-level transaction performance metricsdomain agent",
        "section": 1
    },
    {
        "command": "pmdatrivial",
        "description": "pmdatrivial is the simplest possible Performance Metrics DomainAgent (PMDA) which exports a single performance metric, the timein seconds since the 1st of January, 1970.The trivial PMDA is shipped as source code and is designed to bean aid for PMDA developers.A brief description of the pmdatrivial command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedtrivial.log is written in the current directory of pmcd(1)when pmdatrivial is started, i.e.$PCP_LOG_DIR/pmcd.Ifthe log file cannot be created or is not writable, output iswritten to the standard error instead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdatrivial - trivial performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdatxmon",
        "description": "pmdatxmon is an example Performance Metrics Domain Agent (PMDA)which exports a small number of performance metrics from asimulated transaction monitor.The txmon PMDA is shipped as both binary and source code and isdesigned to be an aid for PMDA developers; the txmon PMDAdemonstrates how performance data can be exported from anapplication (in this case txrecord) to the PCP infrastructure viaa shared memory segment.As a matter of convenience, pmdatxmoncreates (and destroys on exit) the shared memory segment.The tx_type arguments are arbitrary unique tags used to identifydifferent transaction types.The txrecord application simulates the processing of one or moretransactions identified by tx_type and with an observed servicetime of servtime .With the -l option, txrecord displays the current summary of thetransaction activity from the shared memory segment.genload is a shell and awk(1) script that acts as a front-end totxrecord to generate a constant load of simulated transactionactivity.A brief description of the pmdatxmon command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedtxmon.log is written in the current directory of pmcd(1)when pmdatxmon is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdatxmon, txrecord, genload - txmon performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdaunbound",
        "description": "pmdaunbound is a Performance Metrics Domain Agent (PMDA) whichexports metric values about the Unbound DNS resolver using theunbound-control(8) stats_noreset command.This gives lots of insight into query types, response time, cachehits/misses, etc. Please see the unbound-control(8) man page forexplanation about each statistics counter.Further details on the Unbound DNS resolver can be found athttps://unbound.net/ .",
        "name": "pmdaunbound - Unbound resolver PMDA",
        "section": 1
    },
    {
        "command": "pmdaweblog",
        "description": "pmdaweblog is a Performance Metrics Domain Agent (PMDA(3)) thatscans Web server logs to extract metrics characterizing Webserver activity.These performance metrics are then madeavailable through the infrastructure of the Performance Co-Pilot(PCP).The configfile specifies which Web servers are to be monitored,their associated access logs and error logs, and a regular-expression based scheme for extracting detailed information abouteach Web access.This file is maintained as part of the PMDAinstallation and/or de-installation by the scripts Install andRemove in the directory $PCP_PMDAS_DIR/weblog.For more details,refer to the section below covering installation.Once started, pmdaweblog monitors a set of log files and inresponse to a request for information, will process any newinformation that has been appended to the log files, similar to atail(1).There is also periodic \"catch up\" to process newinformation from all log files, and a scheme to detect therotation of log files.Like all other PMDAs, pmdaweblog is launched by pmcd(1) usingcommand line options specified in $PCP_PMCDCONF_PATH - theInstall script will prompt for appropriate values for the commandline options, and update $PCP_PMCDCONF_PATH.A brief description of the pmdaweblog command line optionsfollows:-CCheck the configuration and exit.-d domainSpecify the domain number.It is absolutely crucial thatthe performance metrics domain number specified here isunique and consistent.That is, domain should bedifferent for every PMDA on the one host, and the samedomain number should be used for the pmdaweblog PMDA onall hosts.For most installations, the default domain as encapsulatedin the file $PCP_PMDAS_DIR/weblog/domain.h will suffice.For alternate values, check $PCP_PMCDCONF_PATH for thedomain values already in use on this host, and the file$PCP_VAR_DIR/pmns/stdpmid contains a repository of ``wellknown'' domain assignments that probably should beavoided.-h helpfileGet the help text from the supplied helpfile rather thanfrom the default location.-i portCommunicate with pmcd(1) on the specified Internet port(which may be a number or a name).-l logfileLocation of the log file.By default, a log file namedweblog.log is written in the current directory of pmcd(1)when pmdaweblog is started, i.e.$PCP_LOG_DIR/pmcd.Ifthe log file cannot be created or is not writable, outputis written to the standard error instead.-n idlesecIf a Web server log file has not been modified for idlesecseconds, then the file will be closed and re-opened.Thisis the only way pmdaweblog can detect any asynchronousrotation of the logs by Web server administrative scripts.The default period is 20 seconds.This value may bechanged dynamically using pmstore(1) to modify the valueof the performance metric web.config.check.-pCommunicate with pmcd(1) via a pipe.-S num Specify the maximum number of Web servers per sproc.Itmay be desirable (from a latency and load balancingperspective) or necessary (due to file descriptor limits)to delegate responsibility for scanning the Web server logfiles to several sprocs.pmdaweblog will ensure that eachsproc handles the log files for at most num Web servers.The default value is 80 Web servers per sproc.-t delayTo avoid the need to scan a lot of information from theWeb server logs in response to a single request forperformance metrics, all log files will be checked atleast once every delay seconds.The default is 15seconds.This value may by changed dynamically usingpmstore(1) to modify the value of the performance metricweb.config.catchup.-u socketCommunicate with pmcd(1) via the given Unix domain socket.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdaweblog - performance metrics domain agent (PMDA) for Webserver logs",
        "section": 1
    },
    {
        "command": "pmdawindows",
        "description": "Each supported platform has a kernel Performance Metrics DomainAgent (PMDA) which extracts performance metrics from the kernelof that platfrom.A variety of platform-specific metrics areavailable, with an equally varied set of access mechanisms -typically this involves special system calls, or reading fromfiles in kernel virtual filesystems such as the Linux sysfs andprocfs filesystems.The platform kernel PMDA is one of the most critical componentsof the PCP installation, and must be as efficient and reliable aspossible.In all installations the default kernel PMDA will beinstalled as a shared library and thus executes directly withinthe pmcd(1) process.This slightly reduces overheads associatedwith querying the metadata and values associated with thesemetrics (no message passing is required).Unlike many other PMDAs, the kernel PMDA exports a number ofmetric namespace subtrees, such as kernel, network, swap, mem,ipc, filesys, nfs, disk and hinv (hardware inventory).Despite usually running as shared libraries, most installationsalso include a stand-alone executable for the kernel PMDA.Thisis to aid profiling and debugging activities, with dbpmda(1) forexample.In this case (but not for shared libraries), thefollowing command line options are available:-ADisables use of the credentials provided by PMAPI clienttools, and simply runs everything under the \"root\" account.Only enable this option if you understand the risksinvolved, and are sure that all remote accesses will be frombenevolent users.If enabled, unauthenticated remote PMAPIclients will be able to access potentially sensitiveperformance metric values which an unauthenticated PMAPIclient usually would not be able to.Refer to CVE-2012-3419for additional details.-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file named[platform].log is written in the current directory ofpmcd(1) when pmda[platform] is started, i.e.$PCP_LOG_DIR/pmcd.If the log file cannot be created or isnot writable, output is written to the standard errorinstead.-UUser account under which to run the agent.The default iseither the privileged \"root\" account on some platforms(Linux, for example) or the unprivileged \"pcp\" account(wherever possible).",
        "name": "pmdaaix, pmdadarwin, pmdafreebsd, pmdalinux, pmdanetbsd,pmdasolaris, pmdawindows - operating system kernel performancemetrics domain agents",
        "section": 1
    },
    {
        "command": "pmdaxfs",
        "description": "pmdaxfs is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics describing the state of the XFSfilesystem from the Linux kernel.The xfs PMDA exports metrics that measure information aboutmetadata buffer usage, the journal, btree operations, inodeoperations, extended attributes, directories, quotas, read andwrite operation counts and of course throughput.The PMDA provides a facility to reset the values of all countersto zero using pmstore(1) with the xfs.control.reset metric.A brief description of the pmdaxfs command line options follows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedxfs.log is written in the current directory of pmcd(1) whenpmdaxfs is started, i.e.$PCP_LOG_DIR/pmcd.If the logfile cannot be created or is not writable, output is writtento the standard error instead.",
        "name": "pmdaxfs - XFS filesystem performance metrics domain agent (PMDA)",
        "section": 1
    },
    {
        "command": "pmdazfs",
        "description": "pmdazfs is a Performance Metrics Domain Agent (PMDA) whichextracts performance metrics describing the state of the ZFSfilesystem from the stats files located in /proc.The zfs PMDA exports metrics that measure information aboutcaching (ARC, L2ARC, ZIL, VDEV), buffering, RAIDZ, dnodeoperations and pools IO.A brief description of the pmdazfs command line options follows:-dThe performance metrics domain number must be set to aunique value.-lLocation of the log file.By default, a log file namedzfs.log is written in the current directory of pmcd(1) whenpmdazfs is started, i.e.$PCP_LOG_DIR/pmcd.If the logfile cannot be created or is not writable, output is writtento the standard error instead.",
        "name": "pmdazfs - ZFS filesystem performance metrics domain agent (PMDA)for Linux",
        "section": 1
    },
    {
        "command": "pmdazimbra",
        "description": "pmdazimbra is a Performance Metrics Domain Agent (PMDA) whichexports metric values from several subsystems of the ZimbraSuite.Further details on Zimbra can be found at http://www.zimbra.com/ .",
        "name": "pmdazimbra - Zimbra Collaboration Suite (ZCS) PMDA",
        "section": 1
    },
    {
        "command": "pmdazswap",
        "description": "pmdazswap is a Performance Metrics Domain Agent (PMDA) whichexports metric values about compressed swap operation, as trackedby the zswap Linux kernel module.Zswap is a lightweight compressed cache for swap pages.It takespages that are in the process of being swapped out and attemptsto compress them into a dynamically allocated RAM-based memorypool.Zswap trades CPU cycles for potentially reduced swap I/O.This tradeoff can also result in a performance improvement ifreads from the compressed cache are faster than reads from a swapdevice.This PMDA exports metrics about pool size, number of pagesstored, and various counters for the reasons pages are rejected.",
        "name": "pmdazswap - zswap (compressed swap) PMDA",
        "section": 1
    },
    {
        "command": "pmdbg",
        "description": "The components of the Performance Co-Pilot (PCP) use a globalmechanism to control diagnostic and debug output.Historicallythis was a vector of bit-fields but this was later replaced by anarray of debug options.All of the bit-field debug controls havean equivalent in the new scheme, but some new debug optionscannot be represented in the old bit-field scheme.pmdbg with a -l option prints out all the debug options.Ifthere is no -g and no -o option then the output lists the name ofeach option and some descriptive text.With the -l and -g options the descriptive text is replaced withan expression that can be used to set or print the correspondingdebug flag in gdb(1).With the -l and -o options the output is for only the old bit-fields with the mnemonic and decimal values of each the bit-fieldalong with some descriptive text.Obviously the -o and -g options are mutually exclusive.pmdbg with a -D option parses the list of names(s) using__pmParseDebug(3) and reports the corresponding decimal value.This use is not required in the new scheme, but for the old bit-fields scheme it was useful when debugging and wanting to set theinternal value of the control vector (pmDebug) via a debugger,e.g.gdb(1).For the new scheme, the same effect can beachieved using the name of the option(s) and callingpmSetDebug(3) from within the debugger.The alternative usage also relates to the old bit-field schemeand the code arguments are values for the debug vector, and thebit-fields that are enabled by each of these values is listed.Each code may be an integer, a hexadecimal value or a hexadecimalvalue prefixed by either ``0x'' or ``0X''.Most applications using the facilities of the PCP support a -Dname[,name ...]command-line syntax to enable debug controlusing the name(s) of the desired debug options.Alternatively the initial value of the debug control flags may beset to either a value N (old scheme) or a comma-separated list ifoption name(s) (new scheme) using the environment variable$PCP_DEBUG.If both mechanisms are used the effect is additive,so the resultant flags are those set via $PCP_DEBUG combined withthose set via any -D command line options.",
        "name": "pmdbg - report Performance Co-Pilot debug options",
        "section": 1
    },
    {
        "command": "pmdiff",
        "description": "pmdiff compares the average values for every metric in either oneor two sets of archives, in a given time window, for changes thatare likely to be of interest when searching for performanceregressions.The archive specifiers archive1 and archive2 may be comma-separated lists of names, each of which may be the base name ofan archive or the name of a directory containing one or morearchives. Each archive in the resulting set of archives must havebeen previously created using pmlogger(1).The pmlogsummary(1)utility is used to obtain the average values used for comparison.There are two sorts of invocation of the tool: with either one ortwo sets of archives.In the first case, the only sensible command line requires use ofall four time window arguments.These are specified using thesame time window format described in PCPIntro(1), and are-S/--start and -T/--finish for the start and end times of thefirst time window of interest in the archive set, and -B/--beforeand -E/--end for the start and end times of the second timewindow of interest.In the second case, with two sets of archives, the -B/--beforeand -E/--end options might be unnecessary.This might be thecase, for example, when comparing the same time window of twoconsecutive days (usually two separate sets of archives), or atime window on the same day of different weeks.In either case, pmdiff produces a sorted summary of those metricsin the specified window whose values have deviated the most froma minimal threshold.The level of deviation is calculated bydividing the average value of each metric in both logs, and thencalculating whether the ratio falls outside of a range considerednormal.This ratio can be adjusted using the -q/--thresholdoption, and by default it is 2 (i.e. report all metrics withaverage values that have more than doubled in the two timewindows or more than halved in the two time windows).If the baseline value is zero and the comparison value is non-zero, the ratio is reported as ``|+|'' (infinitely large).Ifthe comparison value is zero and the baseline value is non-zero,the ratio is reported as ``|-|'' (infinitely small).Reported metrics are sorted in ascending ratio order.Should any metrics be present in one window but missing from theother, a diagnostic will be displayed listing each missing metricand the archive set from which it was missing.Metrics with counter semantics are converted to rates beforebeing evaluated.",
        "name": "pmdiff - compares archives and report significant differences",
        "section": 1
    },
    {
        "command": "pmdumplog",
        "description": "pmlogdump dumps assorted control, metadata, index and stateinformation from the files of a Performance Co-Pilot (PCP)archive log.The archive log has the base name archive and musthave been previously created using pmlogger(1).Historically, pmlogdump was known as pmdumplog but the lattername is not consistent with the other PCP commands that operateon PCP archives, so pmlogdump is preferred, however pmdumplog ismaintained for backwards compatibility.Normally pmlogdump operates on the distributed PerformanceMetrics Name Space (PMNS), however if the -n option is specifiedan alternative local PMNS is loaded from the file pmnsfile.If any metricname arguments appear, the report will be restrictedto information relevant to the named performance metrics.Ifmetricname is a non-leaf node in the namespace (see PMNS(5)),then pmlogdump will recursively descend the archive's namespaceand report on all leaf nodes.Command line options control the specific information to bereported.",
        "name": "pmlogdump, pmdumplog - dump internal details of a performancemetrics archive log",
        "section": 1
    },
    {
        "command": "pmdumptext",
        "description": "pmdumptext outputs the values of performance metrics collectedlive or from a set of Performance Co-Pilot (PCP) archives.Bydefault, the metric values are displayed in tab separatedcolumns, prefixed by a timestamp.Unless directed to another host by the -h option, or to one ormore sets of archives by the -a option, or an explict host: orarchive/ prefix in the metric (see below for more information),pmdumptext will contact the Performance Metrics Collector Daemon(PMCD) on the local host to obtain the required information.pmdumptext may be run in interactive mode with the -i optionwhich displays the values in equal width columns.Without thisoption, no attempt is made to line up any values allowing theoutput to be easily parsed by other applications.The format of the output can be further controlled by changingthe precision of the values with -P, the width of the columnswith -w, and the format of the values with the -G and -F optionsfor the shortest of scientific or fixed digits, and a fixed widthformat, respectively.By default pmdumptext will scale metric values to ``canonical''units of bytes, seconds and counts.The one exception is withthe -r option where the values are not scaled.The -u optionreports the units of each metric.The metrics to be dumped can be listed on the command line, in aconfig file, or piped to pmdumptext on stdin.A metric consistsof an optional source (host or archive), the metric name, and anoptional instance list immediately after the name.A colon isused to separate a host name from the metric, and a forward slash(``/'') to separate an archive name from the metric.Instancesare enclosed in square brackets and a comma is used between eachinstance if more than one is stated.For example, some legalmetrics are:kernel.all.cpu.idlemyhost:kernel.all.cpu.idle[cpu0,cpu3]/path/to/myarchive/kernel.all.cpu.idle[cpu1]When a metric does not contain a host: or archive/ prefix, e.g.kernel.all.cpu.idle above, then the source of the metric isdetermined by the following rules:(a) PMCD on host from the -h option if any, else(b) the archive from the first -a option if any, else(c) the host from the first metric prior to this one with a host:prefix if any, else(d) the archive from the first metric prior to this one with anarchive/ prefix if any, else(e) PMCD on the local host, which is equivalent to local::metric.The format of a metric is further described in PCPIntro(1) in thePERFORMANCE METRIC SPECIFICATIONS section.A normalization valuemay optionally follow a metric name in a config file or on stdin.The metric value will be scaled by this value.For example, ifthe file system ``/dev/root'' has a capacity of 1965437 bytes,then the percentage of the file system that is used could bedumped with this config:filesys.used[/dev/root] 19654.37A normalization value may not be used with metrics specified ascommand line arguments.A metric name is not required to be a leaf node in thePerformance Metrics Name Space (PMNS), except when one or moreinstances are specified.For example, to dump all file systemmetrics, only filesys is required to dump filesys.capacity,filesys.used, filesys.free etc.",
        "name": "pmdumptext - dump performance metrics to an ASCII table",
        "section": 1
    },
    {
        "command": "pmerr",
        "description": "pmerr accepts standard Performance Co-Pilot (PCP) error codes viathe code argument(s) and generates the corresponding error text.Each code may be an integer, a hexadecimal value or a hexadecimalvalue prefixed by either ``0x'' or ``0X''.Error codes must be less than zero, so if code is a positivenumber, a warning message is produced, and the negated value isused.",
        "name": "pmerr - translate Performance Co-Pilot error codes into errormessages",
        "section": 1
    },
    {
        "command": "pmevent",
        "description": "pmval prints current or archived values for the nominatedperformance metric.The metric of interest is named in themetricname argument, subject to instance qualification with the-i flag as described below.Unless directed to another host by the -h option, or to a set ofarchives by the -a or -U options, pmval will contact thePerformance Metrics Collector Daemon (PMCD) on the local host toobtain the required information.The metricname argument may also be given in the metricspecification syntax, as described in PCPIntro(1), where thesource, metric and instance may all be included in themetricname, e.g. thathost:kernel.all.load[\"1 minute\"].When thisformat is used, none of the -h or -a or -U options may bespecified.When using the metric specification syntax, the ``hostname'' @ istreated specially and causes pmval to use a local context tocollect metrics from PMDAs on the local host without PMCD.Onlysome metrics are available in this mode.When processing a set of archives, pmval may relinquish its owntiming control, and operate under the control of a a pmtime(1)process that uses a GUI dialog to provide timing control.Inthis case, either the -g option should be used to start pmval asthe sole client of a new pmtime(1) instance, or -p should be usedto attach pmval to an existing pmtime(1) instance via the IPCchannel identified by the port argument.The -S, -T, -O and -A options may be used to define a time windowto restrict the samples retrieved, set an initial origin withinthe time window, or specify a ``natural'' alignment of the sampletimes; refer to PCPIntro(1) for a complete description of theseoptions.The output from pmval is directed to standard output.Thefollowing symbols may occasionally appear, in place of a metricvalue, in pmval output:A question mark symbol (?) indicatesthat a value is no longer available for that metric instance.Anexclamation mark (!)indicates that a 64-bit counter wrappedduring the sample.pmevent is an alias for pmval.",
        "name": "pmval, pmevent - arbitrary performance metrics value dumper",
        "section": 1
    },
    {
        "command": "pmfind",
        "description": "pmfind searches for instances of the specified PCP service beingadvertised on the network and prints a list of URLs correspondingto the services discovered.It can be used in conjunction withpmfind_check(1) to automate the monitoring of remote PCPcollector systems.",
        "name": "pmfind - find PCP services on the network",
        "section": 1
    },
    {
        "command": "pmfind_check",
        "description": "This shell script is used to integrate the Performance Co-Pilot(see PCPIntro(1)) collector service discovery mechanisms withpmie(1) and pmlogger(1) service administration, such thatmultiple collector hosts can be monitored from a singlecentralized host.It is important to note that the pmfindservice is tightly integrated with the pmie and pmloggerservices, and these must also be enabled if the services are tobe started for each collector system that pmfind discovers.pmfind_check is designed to be enabled via a service managementdaemon such as systemd(1) on Linux.It is run from a systemtimer (either by systemd or by cron on other systems).Once pertimer interval pmfind attempts discovery of PCP collectorsystems.For each unique system found pmfind_check createspmie_check and pmlogger_check control file entries, such thatthese latter process can manage one pmie and pmlogger service foreach discovered collector system.When run through systemd these processes will be launchedimmediately because systemd monitors the service control filesfor changes.With systemd one may also initiate an immediateservice discovery with pmfind and pmfind_check by modifying the$PCP_SYSCONF_DIR/pmfind directory in any way.When run from cron the next (also timer-based) invocation ofpmie_check or pmlogger_check will start and manage the associatedpmie and pmlogger processes.The control file entries created by pmfind_check follow theconvention of individual files for each collector host.Thefiles are named using the (unique) source identifier that pmfindcalculates.This identifier is a hash calculated based on (non-optional) context labels available from every collector host, andis the same source identifier reported and used by pminfo(1) andpmseries(1).",
        "name": "pmfind_check - administration of Performance Co-Pilot discovery",
        "section": 1
    },
    {
        "command": "pmgenmap",
        "description": "Given one or more lists of metric names in infile or on standardinput, pmgenmap generates C declarations and cpp(1) macrossuitable for use across the Performance Metrics ProgrammingInterface (PMAPI) on standard output.The declarations produced by pmgenmap simplify the coding forclient applications using the PMAPI.The input should consist of one or more lists of metric names ofthe formlistname {metricname1 symbolname1metricname2 symbolname2...}which will generate C and cpp(1) declarations of the formchar *listname[] = {#define symbolname1 0\"metricname1\",#define symbolname2 1\"metricname2\",...};The array declarations produced are suitable as parameters topmLookupName(3) and the #defined constants may be used to indexthe vsets in the pmResult structure returned by a pmFetch(3)call.Obviously, listname must conform to the C identifier namingrules, each symbolname must conform to the cpp(1) macro namingrules, and each metricname is expected to be a valid performancemetrics name (see PMNS(5) for more details).The input may include sh-style comment lines, i.e. with a `#' asthe first non-blank character of a line, and these are translatedon output to either single line or multi-line C comments in theK&R style.For example, the input:# leading block of multi-line comments# initialization groupfoo {a.b.cONEd.e.f.g TWO# embedded block of multi-lines# comments and boring pad textxx.yy.zzTHREE}# trailing single line commentProduces the output:/** leading block of multi-line comments* initialization group*/char *foo[] = {#define ONE 0\"a.b.c\",#define TWO 1\"d.e.f.g\",/** embedded block of multi-lines* comments and boring pad text*/#define THREE 2\"xx.yy.zz\",};/* trailing single line comment */",
        "name": "pmgenmap - generate C code to simplify handling of performancemetrics",
        "section": 1
    },
    {
        "command": "pmgetopt",
        "description": "pmgetopt is used to perform command line option parsing for shellscripts used in the Performance Co-Pilot (PCP toolkit).It isalso used to generate usage messages for those scripts.The parameters given to pmgetopt take two forms: initially,options specific to pmgetopt itself are passed in, and terminatedusing the -- mechanism.Thereafter, all of the parameters passedinto the shell script should be passed (usually this is simplythe \"$@\" variable).",
        "name": "pmgetopt - Performance Co-Pilot shell script option parser",
        "section": 1
    },
    {
        "command": "pmhostname",
        "description": "pmhostname reports the name of the host hostname as returned bygethostbyname(3).If hostname is not specified, then the local host name isretrieved using gethostname(2) and this is than passed togethostbyname(3).pmhostname provides a service for shell scripts that mimics thelogic formerly used by Performance Co-Pilot applications whentrying to determine the official name of a host.PCPapplications no longer use DNS-based heuristics, and thereforethis command is deprecated.If gethostbyname(3) fails, the input host name (either hostnameor the result from calling gethostname(2)) is reported.",
        "name": "pmhostname - report hostname",
        "section": 1
    },
    {
        "command": "pmie",
        "description": "pmie accepts a collection of arithmetic, logical, and ruleexpressions to be evaluated at specified frequencies.The basedata for the expressions consists of performance metrics valuesdelivered in real-time from any host running the PerformanceMetrics Collection Daemon (PMCD), or using historical data fromPerformance Co-Pilot (PCP) archive logs.As well as computing arithmetic and logical values, pmie canexecute actions (popup alarms, write system log messages, andlaunch programs) in response to specified conditions.Suchactions are extremely useful in detecting, monitoring andcorrecting performance related problems.The expressions to be evaluated are read from configuration filesspecified by one or more filename arguments.In the absence ofany filename, expressions are read from standard input.Output from pmie is directed to standard output and standarderror as follows:stdoutExpression values printed in the verbose -v mode and theoutput of print actions.stderrError and warning messages for any syntactic or semanticproblems during expression parsing, and any semantic orperformance metrics availability problems during expressionevaluation.",
        "name": "pmie - inference engine for performance metrics",
        "section": 1
    },
    {
        "command": "pmie2col",
        "description": "pmie2col is a simple tool that converts output from pmie(1) intoregular column format.Each column is 7 characters wide (bydefault, may be changed with the -w option) with a single spacebetween columns.That single space can be substituted with analternate delimiter using the -d option (this is useful forimporting the data into a spreadsheet, for example).The precision of the tabulated values from pmie can be specifiedwith the -p option (default is 2 decimal places).This optioncan and will override any width setting in order to present therequested precision.The pmie(1) configuration must follow these rules:(1)Each pmie(1) expression is of the form ``NAME = expr;''.NAME will be used as the column heading, and must containno white space, although special characters can be escapedby enclosing NAME in single quotes.(2)The ``expr'' must be a valid pmie(1) expression thatproduces a singular value.In addition, pmie(1) must be run with the -v command line option.It is also possible to use the -e command line to pmie(1) andoutput lines will be prefixed by a timestamp.",
        "name": "pmie2col - convert pmie output to multi-column format",
        "section": 1
    },
    {
        "command": "pmie_check",
        "description": "This series of shell scripts and associated control files may beused to create a customized regime of administration andmanagement for the Performance Co-Pilot (see PCPIntro(1))inference engine, pmie(1).pmie_check may be run at any time of the day and verifies that adesired set of pmie processes is running.If not, it (re-)startsany missing inference engine processes.pmie_daily is intended to be run once per day, preferably in theearly morning, as soon after midnight as practicable.Its taskis to rotate the log files for the running pmie processes - thesefiles may grow without bound if the ``print'' action is used, orany other pmie action writes to its stdout/stderr streams.Aftersome period, old pmie log files are discarded.",
        "name": "pmie_check, pmie_daily - administration of the Performance Co-Pilot inference engine",
        "section": 1
    },
    {
        "command": "pmie_daily",
        "description": "This series of shell scripts and associated control files may beused to create a customized regime of administration andmanagement for the Performance Co-Pilot (see PCPIntro(1))inference engine, pmie(1).pmie_check may be run at any time of the day and verifies that adesired set of pmie processes is running.If not, it (re-)startsany missing inference engine processes.pmie_daily is intended to be run once per day, preferably in theearly morning, as soon after midnight as practicable.Its taskis to rotate the log files for the running pmie processes - thesefiles may grow without bound if the ``print'' action is used, orany other pmie action writes to its stdout/stderr streams.Aftersome period, old pmie log files are discarded.",
        "name": "pmie_check, pmie_daily - administration of the Performance Co-Pilot inference engine",
        "section": 1
    },
    {
        "command": "pmie_dump_stats",
        "description": "Each pmie(1) process mantains a file of state and statistics inbinary files in the $PCP_TMP_DIR/pmie directory, named with theprocess' PID.These files are used by the pmcd PMDA to instantiate thepmcd.pmie performance metrics.pmie_dump_stats is a stand alone utility that dumps the contentsof these files in a terse, but script-friendly format.It isdesigned for use in the pmiectl(1) script.",
        "name": "pmie_dump_stats - dump the contents of pmie stats files",
        "section": 1
    },
    {
        "command": "pmieconf",
        "description": "pmieconf is a utility for viewing and configuring variables fromgeneralized pmie(1) rules.The set of generalized rules is readin from rulepath, and the output file produced by pmieconf is avalid input file for pmie.",
        "name": "pmieconf - display and set configurable pmie rule variables",
        "section": 1
    },
    {
        "command": "pmiestatus",
        "description": "pmiestatus displays information used to identify a runningpmie(1) process.It is mostly used by pmie_check(1) andpmie_daily(1) when they hunt for instances of pmie to checkagainst the control file.",
        "name": "pmiestatus - display information from pmie stats file",
        "section": 1
    },
    {
        "command": "pminfo",
        "description": "pminfo displays various types of information about performancemetrics available through the facilities of the Performance Co-Pilot (PCP).The metrics of interest are named in the metricname arguments.If metricname is a non-leaf node in the PMNS, then pminfo willrecursively descend the PMNS and report on all leaf nodes.If nometricname argument is given, the root of the PMNS is used.If the metricname argument is in numeric dotted notation, it isinterpreted as either a 3-dotted pmid (metric identifier -domain, cluster, item numbers) or a 2-dotted indom (instancedomain identifier - domain, serial number).In the pmid case, areverse PMID-to-name lookup is performed, and in the indom case,the instance domain is reported directly.This latter mode canbe used to report the instance domain ``one line'' and long formhelp text summaries.Unless directed to another host by the -h option, by defaultpminfo will contact the Performance Metrics Collector Daemon(PMCD) on the local host.The connection to a PMCD is onlyrequired if pminfo requires distributed PMNS information, and/ormeta-data describing metrics, and/or metric values, and/or helptext.The -a option causes pminfo to use the specified set of archivesrather than connecting to a PMCD.The -L option causes pminfo to use a local context to collectmetrics from PMDAs on the local host without PMCD.Only somemetrics are available in this mode.The -a, -h and -L options are mutually exclusive.",
        "name": "pminfo - display information about performance metrics",
        "section": 1
    },
    {
        "command": "pmiostat",
        "description": "pcp-iostat reports I/O statistics for SCSI (by default) or otherdevices (if the -x option is specified).",
        "name": "pmiostat, pcp-iostat - report block I/O statistics",
        "section": 1
    },
    {
        "command": "pmjson",
        "description": "pmjson is used to manipulate JSON (JavaScript Object Notation)formatted text used in the Performance Co-Pilot (PCP toolkit).It can produce minimal and human readable output formats when itis supplied with valid JSON input.",
        "name": "pmjson - Performance Co-Pilot JSON dumping utility",
        "section": 1
    },
    {
        "command": "pmlc",
        "description": "pmlc may be used to change those metrics and instances which apmlogger(1) writes to a Performance Co-Pilot archive (seePCPIntro(1)), the frequency with which the metrics are collectedand whether the logging is mandatory, advisory, on or off.Italso reports the current logging status of metrics and instances.pmlc may be used to control pmlogger instances on remote hosts aswell as those on the local host.Normally pmlc operates on the distributed Performance MetricsName Space (PMNS), however if the -n option is specified analternative local PMNS is loaded from the file pmnsfile.If the -P option is specified, pmlc will attempt to start with aconnection to the primary pmlogger on the local host.If the -poption is specified, then pmlc will attempt to start with aconnection to the pmlogger on this TCP/IP port.Alternatively,if pid is specified, a connection to the pmlogger instance withthat process id will be attempted on startup.The -h option mayonly be used if -P, -p port or a pid is also specified.In thatcase pmlc will initially connect to the specified (remote)pmlogger instance on host rather than the local host.If theconnection to the specified pmlogger instance cannot beestablished, pmlc will start with no connection.These optionstypically allow the same file of pmlc commands to be directed tomultiple pmlogger instances by varying the command linearguments.Note that -P, -p port, pid and -h are used only whenmaking an initial connection to a pmlogger instance.They arenot used as defaults if subsequent connections are madeinteractively (see the connect command below).By default, pmlc reports the time of day according to the localtimezone on the system where pmlc is run.The -Z option changesthe timezone to timezone in the format of the environmentvariable TZ as described in environ(7).The -z option changesthe timezone to the timezone of the pmlogger instance from whichinformation is being obtained.Only one of -z or -Z may bespecified.If standard input is from a tty, pmlc is interactive, withprompts.The -i flag may be used to force interactive behavior,and is typically used in conjunction with -e to echo all commandinput on standard output.",
        "name": "pmlc - configure active Performance Co-Pilot pmlogger(s)interactively",
        "section": 1
    },
    {
        "command": "pmlock",
        "description": "pmlock attempts to acquire an exclusive lock by creating filewith a mode of 0.The exit status is 0 for success, 1 for failure.To release the lock, unlink file.In the event of a failure, the -v option produces an explanatorymessage on stdout.",
        "name": "pmlock - simple file-based mutex",
        "section": 1
    },
    {
        "command": "pmlogcheck",
        "description": "pmlogcheck prints information about the nature of any invaliddata which it detects in the files of a PCP archive.The archive has the base name archive and must have beenpreviously created using pmlogger(1).",
        "name": "pmlogcheck - checks for invalid data in a PCP archive",
        "section": 1
    },
    {
        "command": "pmlogconf",
        "description": "pmlogconf may be used to create and modify a genericconfiguration file for the PCP archive logger, pmlogger(1).If configfile does not exist, pmlogconf will create a genericconfiguration file with a default set of enabled metrics andlogging intervals.Once created, configfile may be used with the -c option topmlogger(1) to select performance metrics and specify loggingintervals for a PCP archive.If configfile does exist, pmlogconf will prompt for input fromthe user to enable or disable groups of related performancemetrics and to control the logging interval for each enabledgroup.Group selection requires a simple y (yes) or n (no) response tothe prompt Log this group?.Other responses at this point may be used to select additionalcontrol functions as follows:mReport the names of the metrics in the current group.qFinish with group selection (quit) and make no furtherchanges to this group or any subsequent group./patternMake no change to this group but search for a groupcontaining pattern in the description of the group orthe names of the associated metrics.A logging interval is specified by responding to the Logginginterval?prompt with the keywords once or default or a validpmlogger(1) interval specification of the form ``every Ntimeunits'' or simply ``N timeunits '' (the every is optional)where N is an unsigned integer and timeunits is one of thekeywords msec, millisecond, sec, second, min, minute, hour or theplural form of one of the keywords.When run from automated logging setup processes, the -c option isused to indicate that pmlogconf is in auto-create mode and nointeractive dialog takes place.The output configfile has anadditional comment message and timestamp indicating this fact, sothat it can be identified and subsequently updated using -cagain.This option is not appropriate for interactive use of thetool.The -q option suppresses the logging interval dialog andpreserves the current interval from configfile.More verbose output may be enabled with the -v option.",
        "name": "pmlogconf - create/edit a pmlogger configuration file",
        "section": 1
    },
    {
        "command": "pmlogctl",
        "description": "pmlogctl may be used to manage non-primary instances of thePerformance Co-Pilot (PCP) archiver pmlogger(1).This would bemost relevant in a PCP archive logger ``farm'' where manypmlogger(1) instances would be creating archives of performancedata collected from pmcd(1) on many remote hosts.The primary pmlogger(1) instance is closely linked to the localpmcd(1) process and as a consequence shares the same controlinfrastructure, namely systemd(1) or the PCP init(1) ``rcscripts''.This is why the primary pmlogger(1) instance cannotbe managed with pmlogctl.For brevity in the description below, the term ``instance'' meansa pmlogger(1) instance.All instances managed by pmlogctl, pmlogger_check(1) andpmlogger_daily(1) abide by the following rules:1. Each instance is fetching performance data from a singlepmcd(1) (i.e. one host), but each pmcd(1) may be providingperformance data to zero, one or more pmlogger(1) processesrunning on one or more hosts.2. On the local host, each pmlogger(1) instance must be specifiedonce in a pmlogger_check(1) control file and pmlogger(1)creates archives in a unique directory, named in the 4thparameter of the associated control file entry (see theCONFIGURATION section of pmlogger_check(1)).3. Each instance belongs to exactly one class, optionally namedusing a $class=...assignment in the associated control file.The special default class is reserved for all instances thatdo not have an associated $class=...assignment.Forreporting purposes (refer to the summary command below), theprimary pmlogger(1) instance is automatically assigned to thespecial primary class.Each pmlogctl execution manages one or more instances updatingthe associated control files and then running pmlogger_check(1)to effect the desired change.The host arguments are usually valid host names.For allcommands except create and cond-create (described below) the hostarguments may also be egrep(1) regular expressions that match thewhole of a valid host name, so the pattern used is actually^host$.For example foo.*(matches all host names beginningwith ``foo'') or .*foo (matches all host names ending with``foo'') or .*[fF][oO][oO].*(matches all host names containing``foo'' in upper, lower or mixed case).The combination of a class from the optional -c option (ordefault) and the host arguments to each command identifies atarget of set instances to which the command operation should beapplied.The -i option may be used with the create or cond-create commandsto override the instance identity that is specified in the identsection of the class policy file (see the CLASS POLICY FILEsection below).Since the identifier must be unique across allinstances and all classes, it only makes sense to use this optionwhen there is a single host argument.Given the tasks that pmlogctl is undertaking it usually must berun as ``root'', the exceptions being the status command or whenthe -N option is specified.",
        "name": "pmlogctl - manage and control Performance Co-Pilot archiveloggers",
        "section": 1
    },
    {
        "command": "pmlogdump",
        "description": "pmlogdump dumps assorted control, metadata, index and stateinformation from the files of a Performance Co-Pilot (PCP)archive log.The archive log has the base name archive and musthave been previously created using pmlogger(1).Historically, pmlogdump was known as pmdumplog but the lattername is not consistent with the other PCP commands that operateon PCP archives, so pmlogdump is preferred, however pmdumplog ismaintained for backwards compatibility.Normally pmlogdump operates on the distributed PerformanceMetrics Name Space (PMNS), however if the -n option is specifiedan alternative local PMNS is loaded from the file pmnsfile.If any metricname arguments appear, the report will be restrictedto information relevant to the named performance metrics.Ifmetricname is a non-leaf node in the namespace (see PMNS(5)),then pmlogdump will recursively descend the archive's namespaceand report on all leaf nodes.Command line options control the specific information to bereported.",
        "name": "pmlogdump, pmdumplog - dump internal details of a performancemetrics archive log",
        "section": 1
    },
    {
        "command": "pmlogextract",
        "description": "pmlogextract reads one or more Performance Co-Pilot (PCP)archives identified by input and creates a merged and/or reducedPCP archive in output.Each input argument is either a name or acomma-separated list of names, and each name is the name of onefile from an archive or the base name of an archive or the nameof a directory containing one or more archives.The nature ofmerging is controlled by the number of input archives, while thenature of data reduction is controlled by the command linearguments.The input arguments must be archives created bypmlogger(1) with performance data collected from the same host,but usually over different time periods and possibly (althoughnot usually) with different performance metrics being logged.If only one input is specified, then the default behavior simplycopies the input PCP archive (with possible conversion to a newerversion of the archive format, see -V below), into the output PCParchive.When two or more PCP archives are specified as input,the archives are merged (or concatenated) and written to output.In the output archive a <mark> record may be inserted at a timejust past the end of each of the input archive to indicate apossible temporal discontinuity between the end of one inputarchive and the start of the next input archive.See the MARKRECORDS section below for more information.There is no <mark>record after the end of the last (in temporal order) of therecords from the input archive(s).",
        "name": "pmlogextract - reduce, extract, concatenate and merge PerformanceCo-Pilot archives",
        "section": 1
    },
    {
        "command": "pmlogger",
        "description": "pmlogger creates the archive logs of performance metric valuesthat may be ``played back'' by other Performance Co-Pilot (seePCPIntro(1)) tools.These logs form the basis of the VCRparadigm and retrospective performance analysis services commonto the PCP toolkit.The mandatory argument archive is the base name for the physicalfiles that constitute an archive log.The archive argument maycontain strftime(3) meta-characters, which will be substitutedprior to creating the archive log files.When pmlogger is run asa service (see pmlogger_daily(1)), the standard archive base nametemplate is %Y%m%d.%H.%M.The -V option specifies the version for the archive that isgenerated.By default the archive version $PCP_ARCHIVE_VERSION(set to 2 in current PCP releases) is used, and the only valuescurrently supported for version are 2 or 3.Unless directed to another host by the -h option or when directlyusing PMDAs via the -o option, pmlogger will contact thePerformance Metrics Collector Daemon (PMCD) on the local host anduse that as the source of the metric values to be logged.To support the required flexibility and control over what islogged and when, pmlogger maintains an independent two levellogging state for each instance of each performance metric.Atthe first (mandatory) level, logging is allowed to be on (with anassociated interval between samples), or off or maybe.In thelatter case, the second (advisory) level logging is allowed to beon (with an associated interval between samples), or off.The mandatory level allows universal specification that somemetrics must be logged, or must not be logged.The default statefor all instances of all metrics when pmlogger starts ismandatory maybe and advisory off.Use pmlc(1) to interrogate and change the logging state oncepmlogger is running.If a metric's state is mandatory (on or off) and a request ismade to change it to mandatory maybe, the new state is mandatorymaybe and advisory off.If a metric's state is already advisory(on or off) and a request is made to change it to mandatorymaybe, the current state is retained.It is not possible for pmlogger to log specific instances of ametric and all instances of the same metric concurrently.Ifspecific instances are being logged and a request to log allinstances is made, then all instances of the metric will belogged according to the new request, superseding any priorlogging request for the metric.A request to log all instancesof a metric will supersede any previous request to log allinstances.A request to log specific instances of a metric whenall instances are already being logged is refused.To do thisone must turn off logging for all instances of the metric first.In each case, the validity of the request is checked first; forexample a request to change a metric's logging state to advisoryon when it is currently mandatory off is never permitted (it isnecessary to change the state to mandatory maybe first).Optionally, each system running pmcd(1) may also be configured torun a ``primary'' pmlogger instance.This pmlogger instance islaunched by $PCP_RC_DIR/pmlogger, and is affected by the files$PCP_SYSCONF_DIR/pmlogger/control,$PCP_SYSCONF_DIR/pmlogger/control.d/*, (use chkconfig(8),systemctl(1) or similar platform-specific commands to activate ordisable the primary pmlogger instance),$PCP_SYSCONFIG_DIR/pmlogger (environment variable settings forthe primary pmlogger) $PCP_SYSCONF_DIR/pmlogger/pmlogger.options(command line options passed to the primary pmlogger) and$PCP_VAR_DIR/config/pmlogger/config.default (the default initialconfiguration file for the primary pmlogger).The primary pmlogger instance is identified by the -P option.There may be at most one ``primary'' pmlogger instance on eachsystem.The primary pmlogger instance (if any) must be runningon the same host as the pmcd(1) to which it connects (if any), sothe -h and -P options are mutually exclusive.Logging of some metrics is possible even in the absence of alocal pmcd(1), using the \"local context\" mode of operation.Thisis activated using the -o option, and causes pmlogger to make useof local DSO PMDAs instead of communicating with pmcd(1).Whenoperating using a local context, the -K option may be used tocontrol the DSO PMDAs that should be made accessible.The specargument conforms to the syntax described in pmSpecLocalPMDA(3).More than one -K option may be used.When launched as a non-primary instance, pmlogger will exitimmediately if the configuration file causes no metric logging tobe scheduled.The -L option overrides this behavior, and causesa non-primary pmlogger instance to ``linger'', presumably pendingsome future dynamic re-configuration and state change viapmlc(1).pmlogger will also linger without the -L option beingused if all the metrics to be logged are logged as once onlymetrics.When the once only metrics have been logged, a warningmessage will be generated stating that the event queue is emptyand no more events will be scheduled.By default all diagnostics and errors from pmlogger are writtento the file pmlogger.log in the directory where pmlogger islaunched.The -l option may be used to override the defaultbehavior.If the log file cannot be created or is not writable,output is written to standard error instead.If the logfile forthe -l option is \"-\" (i.e.-l-) then log messages are written tothe standard output stream.This can be particularly useful whenrunning pmlogger manually, rather than as a service daemon.The -N option directs pmlogger to notify a service manager,typically systemd(1), when it has started and is about to beginwriting PCP archive logs.This option would only normally beused when pmlogger is run as a daemon service under the controlof a service manager.For more details, see__pmServerNotifyServiceManagerReady(3) and systemd(1).Onplatforms that do not use a service manager that supportsnotifications, the -N option is basically a no-op.If specified, the -s option instructs pmlogger to terminate aftera certain size in records, bytes or time units has beenaccumulated.If endsize is an integer then endsize records willbe written to the log.If endsize is an integer suffixed by b orbytes then endsize bytes of the archive data will be written out(note, however, that archive log record boundaries will not bebroken and so this limit may be slightly surpassed).Otherviable file size units include: K, Kb, KiB, Kbyte, Kilobyte forkilobytes and M, Mb, MiB, Mbyte, Megabyte for megabytes and G,Gb, GiB, Gbyte, Gigabyte for gigabytes.These units may beoptionally suffixed by an s and may be of mixed case.Alternatively endsize may be an integer or a floating pointnumber suffixed using a time unit as described in PCPIntro(1) forthe interval argument (to the standard PCP -t command lineoption).Some examples of different formats:-s 100-s 100bytes-s 100K-s 100Mb-s 10Gbyte-s 10mins-s 1.5hoursThe default is for pmlogger to run forever.The -r option causes the size of the physical record(s) for eachgroup of metrics and the expected contribution of the group tothe size of the PCP archive for one full day of collection to bereported in the log file.This information is reported the firsttime each group is successfully written to the archive.The -U option specifies the user account under which to runpmlogger.The default is the current user account forinteractive use.When run as a daemon, the unprivileged \"pcp\"account is used in current versions of PCP, but in older versionsthe superuser account (\"root\") was used by default.The log file is potentially a multi-volume data set, and the -voption causes pmlogger to start a new volume after a certain sizein records, bytes, or time units has been accumulated for thecurrent volume.The format of this size specification isidentical to that of the -s option (see above).The default isfor pmlogger to create a single volume log.Additional volumeswitches can also be forced asynchronously by either usingpmlc(1) or sending pmlogger a SIGHUP signal (see below).Note,if a scheduled volume switch is in operation due to the -voption, then its counters will be reset after an asynchronousswitch.Independent of any -v option, each volume of an archive islimited to no more than 2^31 bytes, so pmlogger willautomatically create a new volume for the archive before thislimit is reached.Normally pmlogger operates on the distributed Performance MetricsName Space (PMNS), however if the -n option is specified analternative local PMNS is loaded from the file pmnsfile.Under normal circumstances, pmlogger will run forever (except fora -s option or a termination signal).The -T option may be usedto limit the execution time using the format of time asprescribed by PCPIntro(1).The time is interpreted within thetime zone of the PMCD server, unless the -y option is given,within which case the time zone at this logger host is used.Some examples of different formats:-T 10mins-T '@ 11:30'From this it can be seen that -T 10mins and -s 10mins performidentical actions.Alternatively, pmlogger runtime may be limited to the lifetime ofanother process by using the -p or --PID option to nominate thePID of the process of interest.In this case the pmlogger willexit when the other process no longer exists.When pmlogger receives a SIGHUP signal, the current volume of thelog is closed, and a new volume is opened.This mechanism (orthe alternative mechanism via pmlc(1)) may be used to manage thegrowth of the log files - once a log volume is closed, that filemay be archived without ill-effect on the continued operation ofpmlogger.See also the -v option above.When pmlogger receives a SIGUSR2 signal, the current archive logis closed, and a new archive is opened.For this to succeed, theoriginal archive argument must include strftime(3) metacharacters (e.g.%Y%m%d.%H.%M), otherwise pmlogger will exitbecause the archive files will already exist and pmlogger willnot over-write existing archive files.Note that SIGUSR2triggers pmlogger to re-exec itself and re-parse all originalarguments.This means that any relative time limits placed onit's termination time or sampling limit are reset and beginagain.This only affects relative termination times, notabsolute times e.g.-T 5s is affected, but -T 5pm is not.Historically the buffers for the current log may be flushed todisk using the flush command of pmlc(1), or by using the -uoption.The current version of pmlogger and the libpcp routinesthat underpin pmlogger unconditionally use unbuffered writes anda single fwrite(3) for each logical record written, and so``flushing'' does not force any additional data to be written tothe file system.The -u option and the pmlc(1) flush command areretained for backwards compatibility.When launched with the -x option, pmlogger will acceptasynchronous control requests on the file descriptor fd.Thisoption is only expected to be used internally by PCP applicationsthat support ``live record mode''.The -m option allows the string note to be appended to the mapfile for this instance of pmlogger in the $PCP_TMP_DIR/pmloggerdirectory.This is currently used internally to document thefile descriptor (fd) when the -x option is used, or to indicatethat this pmlogger instance was started under the control ofpmlogger_check(1), (-m pmlogger_check) or was re-exec'd (seeexecvp(3)) due to a SIGUSR2 signal being recieved as describedabove (-m reexec).The -H option allows the hostname written into the archive labelto be overridden.This mirrors the -H option of pmcd(1) , butallows it to be specified on the pmlogger process.Without thisoption, the value returned from the logged pmcd(1) is used.The -C option will cause the configuration file to be parsed andpmlogger will then exit without creating an output archive, sowhen -C is specified, the archive command line argument is notrequired.Any errors in the configuration file are reported.The -d or --directory option may be used to specify the directorywhere the archive should be created.directory may include sh(1)metacharacters, like $(...)or `...` or $var and pmlogger willexpand these to produce a final directory path.The resultantpath must be absolute, not relative.The -d option is intendedprimarily for use by pmlogger_check(1) and normal users would nottypically need to use the option, as the directory for archive iseither implied by archive if it contains directory componentselse the current directory by default.",
        "name": "pmlogger - create archive log for performance metrics",
        "section": 1
    },
    {
        "command": "pmlogger_check",
        "description": "pmlogger_check and the related pmlogger_daily(1) tools along withassociated control files (see pmlogger.control(5)) may be used tocreate a customized regime of administration and management forhistorical archives of performance data within the PerformanceCo-Pilot (see PCPIntro(1)) infrastructure.pmlogger_check may be run at any time of the day and is intendedto check that a desired set of pmlogger(1) processes are running.If not, it (re-)starts any missing logger processes.By default,pmlogger_check also calls pmlogger_daily(1) with a -K option toexecute any required archive compression tasks.",
        "name": "pmlogger_check - administration of Performance Co-Pilot archivelog files",
        "section": 1
    },
    {
        "command": "pmlogger_daily",
        "description": "pmlogger_daily and the related pmlogger_check(1) tools along withassociated control files (see pmlogger.control(5)) may be used tocreate a customized regime of administration and management forhistorical archives of performance data within the PerformanceCo-Pilot (see PCPIntro(1)) infrastructure.pmlogger_daily is intended to be run once per day, preferably inthe early morning, as soon after midnight as practicable.Itstask is to aggregate, rotate and perform general housekeeping oneor more sets of PCP archives.To accommodate the evolution of PMDAs and changes in productionlogging environments, pmlogger_daily is integrated withpmlogrewrite(1) to allow optional and automatic rewriting ofarchives before merging.If there are global rewriting rules tobe applied across all archives mentioned in the control file(s),then create the directory $PCP_SYSCONF_DIR/pmlogrewrite and placeany pmlogrewrite(1) rewriting rules in this directory.Forrewriting rules that are specific to only one family of archives,use the directory name from the control file(s) - i.e. the fourthfield - and create a file, or a directory, or a symbolic linknamed pmlogrewrite within this directory and place the requiredrewriting rule(s) in the pmlogrewrite file or in files within thepmlogrewrite subdirectory.pmlogger_daily will choose rewritingrules from the archive directory if they exist, else rewritingrules from $PCP_SYSCONF_DIR/pmlogrewrite if that directoryexists, else no rewriting is attempted.As an alternate mechanism, if the file$PCP_LOG_DIR/pmlogger/.NeedRewrite exists when pmlogger_dailystarts then this is treated the same as specifying -R on thecommand line and $PCP_LOG_DIR/pmlogger/.NeedRewrite will beremoved once all the rewriting has been done.",
        "name": "pmlogger_daily - administration of Performance Co-Pilot archivelog files",
        "section": 1
    },
    {
        "command": "pmlogger_daily_report",
        "description": "pmlogger_daily_report writes daily performance summary reports,much like those produced by sadc(1) and the sa2(8) utility.All of the command line arguments are optional and intended to beself explanatory.By default, the reports are not generated, but if the pcp-zeroconf package has been installed (there will be files in$PCP_VAR_DIR/config/pmlogconf/zeroconf) then the necessarymetrics will have been included in the default pmlogger(1)configuration file and the reports will be generated.The reports are generated (by default) soon after midnight, oncepmlogger_daily(1) has completed the merging of any partalarchives to generate one archive for yesterday's activity.Each performance summary report is named sarXX (where XX isyesterdays day-of-the-month).The outputfile may be changed withthe -f option.The report will be written to the $PCP_LOG_DIR/sadirectory by default, but this may be changed with the -o optionto a different directory.Note that there are suffciently flexible command line options forpmlogger_daily_report to be used to read any archivefile andwrite the report to any output directory.If the -a option is not given, the default input archivefile is$PCP_ARCHIVE_DIR/HOSTNAME/YYYYMMDD, where HOSTNAME defaults tothe local hostname (unless changed with the -h option) andYYYYMMDD is the base name of yesterdays merged archive, asproduced by pmlogger(1) and pmlogger_daily(1).If archivefile isa directory, then pmlogger_daily_report will use all PCP archivesfound in that directory to write the report (this is known asmulti-archive mode, and may be considerably slower thanspecifying a single archive as the input).The reports themselves are created by the pmrep(1) utility usingits default configuration file, see pmrep.conf(5).The pmrep(1)configuration entries used to write the reports is currentlyhardwired into the pmlogger_daily_report script.Finally, the input archives must contain sufficient metrics asneeded by pmrep(1) to write the report.On platforms thatsupport it, the pcp-zeroconf package configures PCP logging asrequired for this - hence pmlogger_daily_report should be usedwith the pmlogger(1) configuration that is set up by pcp-zeroconf.As the name suggests, pcp-zeroconf requires noadditional configuration after installation in order to capturethe required archives needed by pmlogger_daily_report.",
        "name": "pmlogger_daily_report - write Performance Co-Pilot daily summaryreports",
        "section": 1
    },
    {
        "command": "pmlogger_merge",
        "description": "pmlogger_merge is a helper script that is used bypmlogger_daily(1) when merging all of the Performance Co-Pilot(PCP) archives for a single host on a single day into a combinedPCP archive.pmlogger_merge is a wrapper script for pmlogextract(1) thatmerges all of the archive logs matching the input-basenamearguments, and creates a new archive using output-name as thebase name for the physical files that constitute an archive log.The input-basename arguments may contain meta characters in thestyle of sh(1).",
        "name": "pmlogger_merge - helper script to merge Performance Co-Pilotarchives",
        "section": 1
    },
    {
        "command": "pmlogger_rewrite",
        "description": "pmlogger_rewrite is a helper script that is used bypmlogger_daily(1) when rewriting Performance Co-Pilot (PCP)archives.Rewriting is usually required to accommodateevolutionary changes in metadata so old archives can be updatedand then merged with current archives.pmlogger_rewrite is a wrapper script for pmlogrewrite(1) thatwill potentially rewrite all of the archive logs matching thearchive arguments.Each archive argument may be a PCP archivefile name, the basename for the files in a PCP archive, or adirectory (in which case all subordinate PCP archives are foundby recursive descent).pmlogrewrite is run with the -i option so rewriting is done ``inplace''.The -q option is also used, so if no rewriting isrequired then no changes are made to the archive files.",
        "name": "pmlogger_rewrite - helper script to rewrite Performance Co-Pilotarchives",
        "section": 1
    },
    {
        "command": "pmloglabel",
        "description": "pmloglabel verifies, reports on, and can modify all details ofthe labels in each of the files of a Performance Co-Pilot (PCP)archive log.The archive log has the base name archive and musthave been previously created using pmlogger(1).Each of the files in a PCP archive (metadata, temporal index, andone or more data volumes) must contain a valid label at thestart, else the PCP tools will refuse to open the archive at all.Thus, the primary function of pmloglabel is to be able to repairany inconsistent or corrupt label fields, such that the entirearchive is not lost.It will not check the remainder of thearchive, but it will give you a fighting chance to recoverotherwise lost data.Together, pmloglabel and pmlogextract areable to produce a valid PCP archive from many forms ofcorruption.Note that if the temporal index is found to be corrupt, the\"*.index\" file can be safely moved aside and the archive willstill be accessible, however retrievals may take longer withoutthe index.",
        "name": "pmloglabel - check and repair a performance metrics archive label",
        "section": 1
    },
    {
        "command": "pmlogmv",
        "description": "A Performance Co-Pilot (PCP) archive consists of multiple filesas created by pmlogger(1).pmlogmv allows all the files of asingle PCP archive to be moved or renamed as a group in a singleoperation.The oldname argument identifies the target archive, and may beeither the basename that is common to all files in that archiveor one of the archive's files.The new archive's basename isnewname.Because PCP archives are important records of system activity,special care is taken to ensure the integrity of an archive'sfiles.For recoverable problems encountered during the executionof pmlogmv, all the files associated with oldname will bepreserved, and no new files with the newname prefix will becreated.``Recoverable problems'' include signals that can becaught (such as SIGHUP, SIGINT, SIGQUIT and SIGTERM), permissionsissues, new files already existing, file system full events, etc.The implementation of pmlogmv tries to use hard links in the filesystem and so follows the semantic restrictions of ln(2) whichfor most systems means the directories containing both theoldname and the newname PCP archive files need to be within thesame file system.When this is not possible, pmlogmv falls backto using cp(1) to copy oldname to newname.",
        "name": "pmlogmv - move (rename) Performance Co-Pilot archive files",
        "section": 1
    },
    {
        "command": "pmlogpaste",
        "description": "pmlogpaste takes input text from a file or the command line, andwrites it as a metric value in a new PCP archive.This metricvalue is timestamped with the current time, and is stored as astring type metric.The main purpose of this tool is to take captured output andpreserve this in a PCP archive.This allows, for example, theoutput of a benchmark run to be stored along with performancemetrics captured during that run, in a single archive.Archivescan be merged using the pmlogextract(1) utility.pmlogpaste uses the LOGIMPORT(3) library interfaces internally,which support the creation of archives from external sources ofperformance data.",
        "name": "pmlogpaste - paste text into a metric in a PCP archive",
        "section": 1
    },
    {
        "command": "pmlogreduce",
        "description": "pmlogreduce reads one set of Performance Co-Pilot (PCP) archivesidentified by input and creates a temporally reduced PCP archivein output.input is a comma-separated list of names, each ofwhich may be the base name of an archive or the name of adirectory containing one or more archives.The data reductioninvolves statistical and temporal reduction of samples with anoutput sampling interval defined by the -t option in the outputarchive (independent of the sampling intervals in the inputarchives), and is further controlled by other command linearguments.For some metrics, temporal data reduction is not going to behelpful, so for metrics with types PM_TYPE_AGGREGATE orPM_TYPE_EVENT, a warning is issued if these metrics are found ininput and they will be skipped and not appear in the outputarchive.",
        "name": "pmlogreduce - temporal reduction of Performance Co-Pilot archives",
        "section": 1
    },
    {
        "command": "pmlogrewrite",
        "description": "pmlogrewrite reads a set of Performance Co-Pilot (PCP) archivelogs identified by inlog and creates a PCP archive log in outlog.Under normal usage, the -c option will be used to nominate aconfiguration file or files that contains specifications (see theREWRITING RULES SYNTAX section below) that describe how the dataand metadata from inlog should be transformed to produce outlog.The typical uses for pmlogrewrite would be to accommodate theevolution of Performance Metric Domain Agents (PMDAs) where thenames, metadata and semantics of metrics and their associatedinstance domains may change over time, e.g. promoting the type ofa metric from a 32-bit to a 64-bit integer, or renaming a groupof metrics.Refer to the EXAMPLES section for some additionaluse cases.pmlogrewrite is most useful where PMDA changes, or errors in theproduction environment, result in archives that cannot becombined with pmlogextract(1).By pre-processing the archiveswith pmlogrewrite the resulting archives may be able to be mergedwith pmlogextract(1).The input inlog must be a set of PCP archive logs created bypmlogger(1), or possibly one of the tools that read and createPCP archives, e.g.pmlogextract(1) and pmlogreduce(1).inlog isa comma-separated list of names, each of which may be the basename of an archive or the name of a directory containing one ormore archives.If no -c option is specified, then the default behavior simplycreates outlog as a copy of inlog.This is a little morecomplicated than cat(1), as each PCP archive is made up ofseveral physical files.While pmlogrewrite may be used to repair some data consistencyissues in PCP archives, there is also a class of repair tasksthat cannot be handled by pmlogrewrite and pmloglabel(1) may be auseful tool in these cases.",
        "name": "pmlogrewrite - rewrite Performance Co-Pilot archives",
        "section": 1
    },
    {
        "command": "pmlogsize",
        "description": "pmlogsize prints information about the size of the index,metadata and data sections of a Performance Co-Pilot (PCP)archive.The output is intended to guide improvements in archiveencoding format for PCP developers and to help trim pmlogger(1)configuration files to remove metrics that are bloating the PCParchives with low-value data in production environments.The archive arguments can be any mixture of the names of thephysical files of a PCP archive or the basename that is common toall the component physical files in a single archive.In thelatter case archive is replaced by a list of all of the matchingcomponent file names.Note the semantics is a little different to other PCP tools inthat foo.meta means just the file foo.meta, not foo.index,foo.meta, foo.0, etc.",
        "name": "pmlogsize - report sizes for parts of PCP archive(s)",
        "section": 1
    },
    {
        "command": "pmlogsummary",
        "description": "pmlogsummary prints statistical information about metrics ofnumeric type contained within the files of a set of PerformanceCo-Pilot (PCP) archive logs.The default output prints timeaverages for both counter and non-counter metrics.The set ofarchive logs is identified by archive, which is a comma-separatedlist of names, each of which may be the base name of an archiveor the name of a directory containing one or more archives.Thearchive logs are typically created using pmlogger(1).The metrics of interest are named in the metricname arguments.If metricname is a non-leaf node in the Performance Metrics NameSpace (PMNS(5)), then pmlogsummary will recursively descend thePMNS and report on all leaf nodes.If no metricname argument isgiven, the root of the namespace is used.Metrics with counter semantics are converted to rates beforebeing evaluated.",
        "name": "pmlogsummary - calculate averages of metrics stored in a set ofPCP archives",
        "section": 1
    },
    {
        "command": "pmmessage",
        "description": "pmquery provides a command-line-option compatible implementationof the xconfirm and xmessage tools, using a look-and-feel that isconsistent with pmchart.Several extensions to the functionalityof the original tools have been made, in order to improve theirspecific utility for pmchart, but wherever possible the originalsemantics remain.pmconfirm displays a line of text for each -t option specified(or a file when the -file option is used), and a button for each-b option specified.When one of the buttons is pressed, thelabel of that button is written to pmquery's standard output.This provides a means of communication/feedback from within shellscripts and a means to display useful information to a user froman application.pmmessage displays a window containing a message from the commandline, a file, or standard input.It additionally allows buttonsto be associated with an exit status, and only optionally willwrite the label of the button to standard output.pmquery extends the above tools to additionally support limiteduser input, as free form text.In this -input mode, any textentered will be output when the default button is pressed.Adefault text can be entered using the same mechanisms as theother tools.Command line options are available to specify font style, framestyle, modality and one of several different icons to bepresented for tailored visual feedback to the user.",
        "name": "pmconfirm, pmmessage, pmquery - general purpose dialog box",
        "section": 1
    },
    {
        "command": "pmmgr",
        "description": null,
        "name": "PCPCompat, pcp-collectl, pmmgr, pmwebd - backward-compatibilityin the Performance Co-Pilot (PCP)",
        "section": 1
    },
    {
        "command": "pmnsadd",
        "description": "pmnsmerge(1) performs the same function as pmnsadd and is faster,more robust and more flexible.It is therefore recommended thatpmnsmerge(1) be used instead.pmnsadd adds subtree(s) of new names into a Performance MetricsName Space (PMNS), as used by the components of the PerformanceCo-Pilot (PCP).Normally pmnsadd operates on the default Performance Metrics NameSpace (PMNS), however if the -n option is specified analternative namespace is used from the file namespace.The default PMNS is found in the file $PCP_VAR_DIR/pmns/rootunless the environment variable PMNS_DEFAULT is set, in whichcase the value is assumed to be the pathname to the filecontaining the default PMNS.The new names are specified in the file, arguments and conform tothe syntax for PMNS specifications, see PMNS(5).There is onePMNS subtree in each file, and the base PMNS pathname to theinserted subtree is identified by the first group named in eachfile, e.g. if the specifications beginmyagent.foo.stuff {mumble123:45:1fumble123:45:2}then the new names will be added into the PMNS at the non-leafposition identified by myagent.foo.stuff, and following all othernames with the prefix myagent.foo.The new names must be contained within a single subtree of thenamespace.If disjoint subtrees need to be added, these must bepackaged into separate files and pmnsadd used on each, one at atime.All of the files defining the PMNS must be located within thedirectory that contains the root of the PMNS, this wouldtypically be $PCP_VAR_DIR/pmns for the default PMNS, and thiswould typically imply running pmnsadd as root.As a special case, if file contains a line that begins root {then it is assumed to be a complete PMNS that needs to be merged,so none of the subtree extraction and rewriting is performed andfile is handed directly to pmnsmerge(1).Provided some initial integrity checks are satisfied, pmnsaddwill update the PMNS using pmnsmerge(1) - if this fails for anyreason, the original namespace remains unchanged.",
        "name": "pmnsadd - add new names to the Performance Co-Pilot PMNS",
        "section": 1
    },
    {
        "command": "pmnscomp",
        "description": "pmnscomp compiles a Performance Metrics Name Space (PMNS) inASCII format into a more efficient binary representation.pmLoadNameSpace(3) is able to load this binary representationsignificantly faster than the equivalent ASCII representation.If outfile already exists pmnscomp will exit without overwritingit.By convention, the name of the compiled namespace is that of theroot file of the ASCII namespace, with .bin appended.Forexample, the root of the default PMNS is a file named root andthe compiled version of the entire namespace is root.bin.The options are;-dBy default the PMNS to be compiled is expected to contain atmost one name for each unique Performance Metric Identifier(PMID).The -d option relaxes this restriction and allowsthe compilation of a PMNS in which multiple names may beassociated with a single PMID.Duplicate names are usefulwhen a particular metric may be logically associated withmore than one group of related metrics, or when it isdesired to create abbreviated aliases to name a set offrequently used metrics.-fForce overwriting of an existing outfile if it alreadyexists.-nNormally pmnscomp operates on the default PMNS, however ifthe -n option is specified an alternative namespace isloaded from the file namespace.-vBy default, pmnscomp writes a version 2 compiled namespace.If version is 1 then pmnscomp will write a version 1namespace which is similar to version 2, without the extraintegrity afforded by checksums.Note that PCP version 2.0or later can handle both versions 1 and 2 of the binary PMNSformat.The default input PMNS is found in the file$PCP_VAR_DIR/pmns/root unless the environment variablePMNS_DEFAULT is set, in which case the value is assumed to be thepathname to the file containing the default input PMNS.",
        "name": "pmnscomp - compile an ASCII performance metrics namespace intobinary format.",
        "section": 1
    },
    {
        "command": "pmnsdel",
        "description": "pmnsdel removes subtrees of names from a Performance Metrics NameSpace (PMNS), as used by the components of the Performance Co-Pilot (PCP).Normally pmnsdel operates on the default Performance Metrics NameSpace (PMNS), however if the -n option is specified analternative namespace is used from the file namespace.The default PMNS is found in the file $PCP_VAR_DIR/pmns/rootunless the environment variable PMNS_DEFAULT is set, in whichcase the value is assumed to be the pathname to the filecontaining the default PMNS.The metric names to be deleted are all those for which one of themetricpath arguments is a prefix in the PMNS, see PMNS(5).All of the files defining the PMNS must be located within thedirectory that contains the root of the PMNS, and this wouldtypically be $PCP_VAR_DIR/pmns for the default PMNS, and thiswould typically imply running pmnsdel as root.Provided some initial integrity checks are satisfied, pmnsdelwill update the necessary PMNS files.Should an error beencountered the original namespace is restored.Note that anyPMNS files that are no longer referenced by the modifiednamespace will not be removed, even though their contents are notpart of the new namespace.",
        "name": "pmnsdel - delete a subtree of names from the Performance Co-PilotPMNS",
        "section": 1
    },
    {
        "command": "pmnsmerge",
        "description": "pmnsmerge merges multiple instances of a Performance Metrics NameSpace (PMNS), as used by the components of the Performance Co-Pilot (PCP).Each infile argument names a file that includes the root of aPMNS, of the formroot {/* arbitrary stuff */}The order in which the infile files are processed is determinedby the presence or absence of embedded control lines of the form#define _DATESTAMP YYYYMMDDFiles without a control line are processed first and in the orderthey appear on the command line.The other files are thenprocessed in order of ascending _DATESTAMP.The -a option suppresses the argument re-ordering and processesall files in the order they appear on the command line.The merging proceeds by matching names in PMNS, only those newnames in each PMNS are considered, and these are added after anyexisting metrics with the longest possible matching prefix intheir names.For example, merging these two input PMNSroot {root {surprise1:1:3mine1:1:1mine1:1:1foofooyawnyours1:1:2}}foo {foo {fumble1:2:1mumble1:2:3stumble1:2:2stumble1:2:2}}yawn {sleepy1:3:1}Produces the resulting PMNS in out.root {mine1:1:1fooyours1:1:2surprise1:1:3yawn}foo {fumble1:2:1stumble1:2:2mumble1:2:3}yawn {sleepy1:3:1}To avoid accidental over-writing of PMNS files, outfile isexpected to not exist when pmnsmerge starts.The -f optionallows an existing outfile to be unlinked (if possible) andtruncated before writing starts.Normally duplicate names for the same Performance MetricIdentifier (PMID) in a PMNS are allowed.The -d option is thedefault option and is included for backwards compatibility.The-x option reverses the default and pmnsmerge will report an errorand exit with a non-zero status if a duplicate name is found fora PMID in any of the input PMNS files or in the merged outputPMNS.The -v option produces one line of diagnostic output as eachinfile is processed.Once all of the merging has been completed, pmnsmerge willattempt to load the resultant namespace usingpmLoadASCIINameSpace(3) - if this fails for any reason, outfilewill still be created, but pmnsmerge will report the problem andexit with non-zero status.Using pmnsmerge with a single input argument allows that PMNSfile to be checked.In addition to syntactic checking,specifying -x will also enable a check for duplicate names forall PMIDs.",
        "name": "pmnsmerge - merge multiple versions of a Performance Co-PilotPMNS",
        "section": 1
    },
    {
        "command": "pmpause",
        "description": "pmpause sleeps indefinitely, until interrupted by SIGKILL.pmsleep sleeps for the specified interval.The interval argumentfollows the syntax described in PCPIntro(1) for -t, and in thesimplest form may be an unsigned integer or floating pointconstant (the implied units in this case are seconds).",
        "name": "pmpause, pmsleep - portable subsecond-capable sleep",
        "section": 1
    },
    {
        "command": "pmpost",
        "description": "pmpost will append the text message to the end of the PerformanceCo-Pilot (PCP) notice board file ($PCP_LOG_DIR/NOTICES) in anatomic manner that guards against corruption of the notice boardfile by concurrent invocations of pmpost.The PCP notice board is intended to be a persistent store andclearing house for important messages relating to the operationof the PCP and the notification of performance alerts frompmie(1) when other notification options are either unavailable orunsuitable.Before being written, messages are prefixed by the current time,and when the current day is different to the last time the noticeboard file was written, pmpost will prepend the message with thefull date.If the notice board file does not exist, pmpost will create it.pmpost would usually run from long-running PCP daemons executingunder the (typically unprivileged) $PCP_USER and $PCP_GROUPaccounts.The file should be owned and writable by the $PCP_USERuser, and readable by others.",
        "name": "pmpost - append messages to the Performance Co-Pilot notice board",
        "section": 1
    },
    {
        "command": "pmprobe",
        "description": "pmprobe determines the availability of performance metricsexported through the facilities of the Performance Co-Pilot(PCP).The metrics of interest are named in the metricname arguments.If metricname is a non-leaf node in the Performance Metrics NameSpace (PMNS(5)), then pmprobe will recursively descend the PMNSand report on all leaf nodes.If no metricname argument isgiven, the root of the namespace is used.This recursive expansion of the PMNS can be inhibited by the -F(go faster) option, which reduces the number of roundtrips topmcd(1) when the metricname arguments are known to be leaf nodesahead of time.The output format is spartan and intended for use in wrapperscripts creating configuration files for other PCP tools.Bydefault, there is one line of output per metric, with the metricname followed by a count of the number of available values.Error conditions are encoded as a negative value count (as perthe PMAPI(3) protocols, but may be decoded using pmerr(1)) andfollowed by a textual description of the error.Unless directed to another host by the -h option, pmprobe willcontact the Performance Metrics Collector Daemon (PMCD) on thelocal host.The -a option causes pmprobe to use the specified set of archivesrather than connecting to a PMCD.The -L option causes pmprobe to use a local context to collectmetrics from PMDAs on the local host without PMCD.Only somemetrics are available in this mode.The -a, -h and -L options are mutually exclusive.",
        "name": "pmprobe - lightweight probe for performance metrics",
        "section": 1
    },
    {
        "command": "pmproxy",
        "description": "pmproxy acts as a protocol proxy, allowing Performance Co-Pilot(PCP) monitoring clients to connect to one or more pmcd(1) and/orredis-server(1) instances via pmproxy.In its default mode of operation pmproxy provides the REST APIfor PCP services (see PMWEBAPI(3) for details).This includesprovision of an Open Metrics - https://openmetrics.io- textinterface for PCP metrics at /metrics, realtime access to PCPmetrics through the /pmapi interfaces, and access to the fast,scalable PCP time series query capabilities offered inconjunction with a redis-server(1) (see pmseries(1) for details)via the /query REST interfaces.pmproxy can be deployed in a firewall domain, or on a cluster``head'' node where the IP (Internet Protocol) address of thehosts where pmcd and/or redis-server are running may be unknownto the PCP monitoring clients, but where the IP address of thehost running pmproxy is known to these clients.Similarly, theclients may have network connectivity only to the host wherepmproxy is running, while there is network connectivity from thathost to the hosts of interest where pmcd and/or redis-server arerunning.The behaviour of the PCP monitoring clients is controlled byeither the PMPROXY_HOST environment variable or through theextended hostname specification (see PCPIntro(1) for details).If neither of these mechanisms is used, clients will make theirPMAPI(3) connections directly to pmcd.If the proxy hostnamesyntax is used or PMPROXY_HOST is set, then this should be thehostname or IP address of the system where pmproxy is running,and the clients will connect to pmcd or redis-server indirectlythrough the protocol proxy services of pmproxy.",
        "name": "pmproxy - proxy for performance metrics collector and querying",
        "section": 1
    },
    {
        "command": "pmpython",
        "description": "pmpython provides a way to run python scripts using acustomisable python interpreter, rather than embedding the nameof a particular version of python into each script.This can be useful as it allows version-independent python codeto be run anywhere.All python modules shipped with PCP supportversions 2.6 and later (in the python2 series), and 3.3 and later(in the python3 release series).Due to python monitoring and collecting scripts being relativelysimple in PCP (not requiring new modules, language features,etc), it has been possible to ensure they work for all of theabove python versions.However, the name of the python interpreter is not always thesame, thus, it is common for PCP python scripts to use a\u201cshebang\u201d line that launches the python interpreter indirectly asfollows:#!/usr/bin/env pmpythonenv(1) is used to find the correct path for the pmpythonexecutable from the user's $PATH.By default the name of the python interpreter is found from thethe value of $PCP_PYTHON_PROG from the environment (if set) elsefrom /etc/pcp.conf.The latter is the more typical case wherethis value is based on some heuristics about the platform at thetime the PCP packages were build and favour the use of python3 inall recent releases of PCP (for those platforms that support it).This allows an appropriate name to be used for the pythoninterpreter instead of a hard-coded python version name, whilestill allowing the user to override the python interpreter asfollows:$ PCP_PYTHON_PROG=python3 pmpython --versionPython 3.4.2$ PCP_PYTHON_PROG=python2 pmpython --versionPython 2.7.9This is convenient for shipping identical scripts on multipleplatforms, and for testing different python versions with the onescript (e.g. in the case where multiple versions of python areinstalled, PCP_PYTHON_PROG can be set in the local environment tooverride the global setting).pmpython is a replacement for an earlier tool with similarfunction, namely pcp-python(1).",
        "name": "pmpython - run a python script using a preferred python variant",
        "section": 1
    },
    {
        "command": "pmquery",
        "description": "pmquery provides a command-line-option compatible implementationof the xconfirm and xmessage tools, using a look-and-feel that isconsistent with pmchart.Several extensions to the functionalityof the original tools have been made, in order to improve theirspecific utility for pmchart, but wherever possible the originalsemantics remain.pmconfirm displays a line of text for each -t option specified(or a file when the -file option is used), and a button for each-b option specified.When one of the buttons is pressed, thelabel of that button is written to pmquery's standard output.This provides a means of communication/feedback from within shellscripts and a means to display useful information to a user froman application.pmmessage displays a window containing a message from the commandline, a file, or standard input.It additionally allows buttonsto be associated with an exit status, and only optionally willwrite the label of the button to standard output.pmquery extends the above tools to additionally support limiteduser input, as free form text.In this -input mode, any textentered will be output when the default button is pressed.Adefault text can be entered using the same mechanisms as theother tools.Command line options are available to specify font style, framestyle, modality and one of several different icons to bepresented for tailored visual feedback to the user.",
        "name": "pmconfirm, pmmessage, pmquery - general purpose dialog box",
        "section": 1
    },
    {
        "command": "pmrep",
        "description": "pmrep is a customizable performance metrics reporting tool.Anyavailable performance metric, live or archived, system and/orapplication, can be selected for reporting using one of theoutput alternatives listed below together with applicableformatting options.pmrep collects selected metric values through the facilities ofthe Performance Co-Pilot (PCP), see PCPIntro(1).The metrics tobe reported are specified on the command line, in configurationfiles, or both.Metrics can be automatically converted andscaled using the PCP facilities, either by default or by per-metric scaling specifications.In addition to the existingmetrics, derived metrics can be defined using the arithmeticexpressions described in pmRegisterDerived(3).A wide range of metricsets (see below) is included by default,providing reports on per-process details, NUMA performance,mimicking other tools like sar(1) and more, see the pmrepconfiguration files in $PCP_SYSCONF_DIR/pmrep (typically/etc/pcp/pmrep) for details.Tab completion for options,metrics, and metricsets is available for bash and zsh.Unless directed to another host by the -h option, pmrep willcontact the Performance Metrics Collector Daemon (PMCD, seepmcd(1)) on the local host.The -a option causes pmrep to use the specified set of archivelogs rather than connecting to a PMCD.The -a and -h options aremutually exclusive.The -L option causes pmrep to use a local context to collectmetrics from DSO PMDAs (Performance Metrics Domain Agents,``plugins'') on the local host without PMCD.Only some metricsare available in this mode.The -a, -h, and -L options aremutually exclusive.The metrics of interest are named in the metricspec argument(s).If a metricspec specifies a non-leaf node in the PerformanceMetrics Name Space (PMNS), then pmrep will recursively descendthe PMNS and report on all leaf nodes (i.e., metrics) for thatmetricspec.Use pminfo(1) to list all the metrics (PMNS leadnodes) and their descriptions.A metricspec has three different forms.First, on the commandline it can start with a colon (``:'') to indicate a metricset tobe read from pmrep configuration files (see -c andpmrep.conf(5)), which may then consist of any number of metrics.Second, a metricspec starting with non-colon specifies a PMNSnode as described above, optionally followed by metric outputformatting definitions.This so-called compact form of ametricspec is defined as follows:metric[,label[,instances[,unit/scale[,type[,width[,precision[,limit]]]]]]]A valid PMNS node (metric) is mandatory.It may be followed by atext label used with stdout output.The optional instancesdefinition restricts csv and stdout reporting to the specifiedinstances of the metric so non-matching instances will befiltered out (see -i).An optional unit/scale is applicable fordimension-compatible, non-string metrics.See below forsupported unit/scale specifications.By default, cumulativecounter metrics are converted to rates, an optional type can beset to raw to disable this rate conversion.For stdout output anumeric width can be used to set the width of the output columnfor this metric.Too wide strings in the output will betruncated to fit the column.A metric-specific precision can beprovided for numeric non-integer output values.Lastly, ametric-specific limit can be set for filtering out numeric valuesper the limit.As a special case for metrics that are counters with time units(nanoseconds to hours), the unit/scale can be used to change thedefault reporting (for example, milliseconds / second) tonormalize to the range zero to one by setting this to sec (seealso -y and -Y).The following metricspec requests the metric kernel.all.sysforkto be reported under the text label forks, converting to themetric default rate count/s in an 8 wide column.Although thedefinitions in this compact form are optional, they must alwaysbe provided in the order specified above, thus the commas.kernel.all.sysfork,forks,,,,8The third form of a metricspec, verbose form, is described andvalid only in pmrep.conf(5).Derived metrics are specified like regular PMNS leaf nodemetrics.Options via environment values (see pmGetOptions(3)) override thecorresponding built-in default values (if any).Configurationfile options override the corresponding environment variables (ifany).Command line options override the correspondingconfiguration file options (if any).",
        "name": "pmrep - performance metrics reporter",
        "section": 1
    },
    {
        "command": "pmrepconf",
        "description": "pmrepconf may be used to create and modify a genericconfiguration file for pmrep(1) and related utilities in thepmrep.conf(5) format.If configfile does not exist, pmrepconf will create a genericconfiguration file with a set of discovered metrics in a[metrics] section.Once created, configfile may be used with the -c option topmrep(1) and related utilities such as pcp2elasticsearch(2) andpcp2spark(2).If configfile does exist, pmrepconf will prompt for input fromthe user to enable or disable groups of related performancemetrics.Group selection requires a simple y (yes) or n (no) response tothe prompt Log this group?.Other responses at this point may be used to select additionalcontrol functions as follows:mReport the names of the metrics in the current group.qFinish with group selection (quit) and make no furtherchanges to this group or any subsequent group./patternMake no change to this group but search for a groupcontaining pattern in the description of the group orthe names of the associated metrics.When run from automated setup processes, the -c option is used toindicate that pmrepconf is in auto-create mode and no interactivedialog takes place.The output configfile has an additionalcomment message and timestamp indicating this fact, so that itcan be identified and subsequently updated using -c again.Thisoption is not appropriate for interactive use of the tool.More verbose output may be enabled with the -v option.",
        "name": "pmrepconf - create/edit a pmrep configuration file",
        "section": 1
    },
    {
        "command": "pmsearch",
        "description": "pmsearch performs full text search queries to find metrics usingnames and help text from metrics, instance domains and instances.It makes use of capabilties of the Performance Co-Pilot (PCP)pmproxy(1) service, the Redis distributed data store, and theRediSearch module.Note that in order to use these services, it is mandatory thatpmproxy is communicating with a redis-server(1) that has theredisearch.so module loaded.When configured to do so, pmproxywill then automatically index PCP metric names, instance names,metric and instance domain help text into the RediSearch store,from PCP archives that it discovers locally.Refer topmlogger(1) and pmlogger_daily(1) for further details.By default pmsearch communicates with a local redis-server(1),however the -h and -p options can be used to specify an alternateRedis instance.If this instance is a node of a Redis cluster,all other instances in the cluster will be discovered and usedautomatically.",
        "name": "pmsearch - help text and names search for metrics, instances andinstance domains",
        "section": 1
    },
    {
        "command": "pmseries",
        "description": "pmseries displays various types of information about performancemetrics available through the scalable timeseries facilities ofthe Performance Co-Pilot (PCP) and the Redis distributed datastore.By default pmseries communicates with a local redis-server(1),however the -h and -p options can be used to specify an alternateRedis instance.If this instance is a node of a Redis cluster,all other instances in the cluster will be discovered and usedautomatically.pmseries runs in several different modes - either queryingtimeseries identifiers, metadata or values (already stored inRedis), or manually loading timeseries into Redis.The lattermode is generally only used for loading previously collected(inactive) archives, since pmproxy(1) automatically performs thisfunction for \"live\" (actively growing) local pmlogger(1)instances, when running in its default time series mode.See theTIMESERIES LOADING section below and the -L option for furtherdetails.Without command line options specifying otherwise, pmseries willissue a timeseries query to find matching timeseries and values.All timeseries are identified using a unique SHA-1 hash which isalways displayed in a 40-hexdigit human readable form.Thesehashes are formed using the metadata associated with everymetric.Importantly, this includes all metric metadata (labels, names,descriptors).Metric labels in particular are (as far aspossible) unique for every machine - on Linux for example thelabels associated with every metric include the unique/etc/machine-id, the hostname, domainname, and otherautomatically generated machine labels, as well as anyadministrator-defined labels from /etc/pcp/labels.These labelscan be reported with pminfo(1) and the pmcd.labels metric.See pmLookupLabels(3), pmLookupInDom(3), pmLookupName(3) andpmLookupDesc(3) for detailed information about metric labels andother metric metadata used in each timeseries identifier hashcalculation.The timeseries identifiers provide a higher level (and machineindependent) identifier than the traditional PCP performancemetric identifiers (pmID), instance domain identifiers (pmInDom)and metric names.See PCPIntro(1) for more details about thesetraditional identifiers.However, pmseries uses timeseriesidentifiers in much the same way that pminfo(1) uses the lowerlevel indom, metric identifiers and metric names.The default mode of pmseries operation (i.e. with no command lineoptions) depends on the arguments it is presented.If all non-option arguments appear to be timeseries identifiers (in 40 hexdigit form) pmseries will report metadata for these timeseries -refer to the -a option for details.Otherwise, the parameterswill be treated as a timeseries query.",
        "name": "pmseries - display information about performance metrictimeseries",
        "section": 1
    },
    {
        "command": "pmsignal",
        "description": "pmsignal provides a cross-platform event signalling mechanism foruse with tools from the Performance Co-Pilot toolkit.It can beused to send a named signal (only HUP, USR1, TERM, and KILL areaccepted) to one or more processes.The processes are specified directly using PIDs or as programnames (with either the -a or -p options).In the all case, theset of all running processes is searched for a basename(1) matchon name.In the program case, process identifiers are extractedfrom files in the $PCP_RUN_DIR directrory where file names arematched on name.pid.The -n option reports the list of process identifiers that wouldhave been signalled, but no signals are actually sent.If a signal is not specified, then the TERM signal will be sent.The list of supported signals is reported when using the -loption.On Linux and UNIX platforms, pmsignal is a simple wrapper aroundthe kill(1) command.On Windows, the is no direct equivalent tothis mechanism, and so an alternate mechanism has beenimplemented - this is only honoured by PCP tools, however, notall Windows utilities.",
        "name": "pmsignal - send a signal to one or more processes",
        "section": 1
    },
    {
        "command": "pmsleep",
        "description": "pmpause sleeps indefinitely, until interrupted by SIGKILL.pmsleep sleeps for the specified interval.The interval argumentfollows the syntax described in PCPIntro(1) for -t, and in thesimplest form may be an unsigned integer or floating pointconstant (the implied units in this case are seconds).",
        "name": "pmpause, pmsleep - portable subsecond-capable sleep",
        "section": 1
    },
    {
        "command": "pmsnap",
        "description": "pmsnap is a shell script that is normally run periodically fromcrontab(1) to generate graphic images of pmchart(1) performancecharts.These images can be in any of the supported pmchartformats, including png, bmp, and jpeg, and may be incorporatedinto the content offered by the local web server.By defaultpmsnap generates no textual output unless some error or warningcondition is encountered.pmsnap generates images according to its control file,$PCP_PMSNAPCONTROL_PATH (or dir/control if the -C option isspecified), and uses archive logs created by pmlogger(1) or PCParchive folios created by pmafm(1) and pmlogger_check(1).Beforeattempting to configure pmsnap, it is strongly recommended thatpmlogger be configured according to the descriptions inpmlogger_daily(1), pmlogger_check(1) and pmlogger(1).Once pmlogger has been configured, it is necessary to configurepmsnap as follows;1.Edit the control file $PCP_PMSNAPCONTROL_PATH.The syntaxof this file is described in the comment at the head ofthe file and an example is supplied for one and twelvehour \"Summary\" performance charts for the local host.Suitable arguments for pmchart are also described in thecomment.The user should consult pmchart for furtherdetails.Note that when pmsnap is run, it globallysubstitutes the string LOCALHOSTNAME with the name of thelocal host in the control file.2.Test the configuration by running$PCP_BINADM_DIR/pmsnap.Without any arguments pmsnap will process every non-comment line in $PCP_PMSNAPCONTROL_PATH.The outputimages will be placed in the files named in the firstfield of each line in the control file, with the fileformat appended if necessary.If these file names do notstart with / or .then they are assumed relative to dir,as specified with the -o option.The default dir is thecurrent directory.Note that if pmlogger has only beenrecently started (within about the last 15 minutes),snapshot images may not be produced and no error messageswill be issued - the reason is that pmchart can not usevery short archives and hence, neither can pmsnap.Fordebugging purposes the -V flag should be used.3.Add an appropriate entry for pmsnap in the root user'scrontab.An example is supplied in$PCP_VAR_DIR/config/pmlogger/crontab.4.Incorporate the pmsnap images into the local WWW content.Usually, WWW pages use images that are relative to aparticular document root, so it is often convenient to usethe -o command line option to specify a sub-directory ofthe local WWW content, and then create a web page in thisdirectory that shows the snapshot images with text andother content appropriate to the local environment.A sample HTML page, suitable for the Summary snapshot may befound in $PCP_VAR_DIR/config/pmsnap/Summary.html.Although pmsnap attempts to flush stdio(3) output buffers in therelevant pmlogger processes before generating snapshots images,this may fail for assorted reasons and no error message will begiven.pmsnap should not be invoked immediately after pmlogger_daily hasrolled the logs because the new archive logs will be too short toobtain meaningful results.Note however that pmsnap will notreport errors from pmchart about not being able to comply withthe -A option on very short archives.In these cases no errorwill be reported and no output images will be produced.",
        "name": "pmsnap - generate performance summary snapshot images",
        "section": 1
    },
    {
        "command": "pmsocks",
        "description": "pmsocks allows Performance Co-Pilot (PCP) clients running onhosts located on the internal side of a TCP/IP firewall tomonitor remote hosts on the other side of the firewall.Thisassumes the firewall has been configured with a compliant sockddaemon and the necessary access controls are satisfied.pmsocks is a thin shell wrapper around the tsocks(8) libraryusing the tsocks(1) utility, which are not included with PCP.You can obtain tsocks from https://tsocks.sourceforge.net/ .",
        "name": "pmsocks - shell wrapper for performance monitoring acrossfirewalls",
        "section": 1
    },
    {
        "command": "pmstat",
        "description": "pmstat provides a one line summary of system performance everyinterval unit of time (the default is 5 seconds).pmstat isintended to monitor system performance at the highest level,after which other tools may be used to examine subsystems inwhich potential performance problems may be observed in greaterdetail.pcp-vmstat is a simple wrapper for use with the pcp(1) command,providing a more familiar command line format for some users.Italso enables the extended reporting option by default, see the -xoption below.Multiple hosts may be monitored by supplying more than one hostwith multiple -h flags (for live monitoring) or by providing aname of the hostlist file, where each line contain one host name,with -H, or multiple -a flags (for retrospective monitoring fromsets of archives).By default, pmstat fetches metrics by connecting to thePerformance Metrics Collector Daemon (PMCD) on the local host.If the -L option is specified, then pmcd(1) is bypassed, andmetrics are fetched from PMDAs on the local host using thestandalone PM_CONTEXT_LOCAL variant of pmNewContext(3).When the-h option is specified, pmstat connects to the pmcd(1) on hostand fetches metrics from there.As mentioned above, multiplehosts may be monitored by supplying multiple -h flags.Alternatively, if the -a option is used, the metrics areretrieved from the Performance Co-Pilot archive log filesidentified by archive, which is a comma-separated list of names,each of which may be the base name of an archive or the name of adirectory containing one or more archives.Multiple sets ofarchives may be replayed by supplying multiple -a flags.Whenthe -a flag is used, the -P flag may also be used to pause theoutput after each interval.Standalone mode can only connect to the local host, using a setof archives implies a host name, and nominating a host precludesusing an archive, so the options -L, -a and -h are mutuallyexclusive.pmstat may relinquish its own timing control, and operate underthe control of a pmtime(1) process that uses a GUI dialog toprovide timing control.In this case, either the -g optionshould be used to start pmstat as the sole client of a newpmtime(1) instance, or -p should be used to attach pmstat to anexisting pmtime(1) instance via the IPC channel identified by theport argument.The -S, -T, -O and -A options may be used to define a time windowto restrict the samples retrieved, set an initial origin withinthe time window, or specify a ``natural'' alignment of the sampletimes; refer to PCPIntro(1) for a complete description of theseoptions.",
        "name": "pcp-vmstat, pmstat - high-level system performance overview",
        "section": 1
    },
    {
        "command": "pmstore",
        "description": "Under certain circumstances, it is useful to be able to modifythe values of performance metrics, for example to re-initializecounters or to assign new values to metrics that act as controlvariables.pmstore changes the current values for the nominated instances ofa single performance metric, as identified by metricname and thelist of instance identifiers following the -i argument.instances must be a single argument, with elements of the listseparated by commas and/or white space.By default all instancesof metricname will be updated.Normally pmstore operates on the default Performance Metrics NameSpace (PMNS), see PMNS(5), however if the -n option is specifiedan alternative namespace is loaded from the file pmnsfile.Unless directed to another host by the -h option, pmstore willinteract with the Performance Metric Collector Daemon (PMCD) onthe local host.The -L option causes pmstore to use a local context to store tometrics from PMDAs on the local host without PMCD.Only somemetrics are available in this mode.The -h and -L options aremutually exclusive.The -f option forces the given value to be stored, even if thereis no current value set.The interpretation of value is dependent on the syntax used inits specification and the underlying data type of metricname, asfollows.1.If the metric has an integer type, then value should be anoptional leading hyphen, followed either by decimal digits or``0x'' and some hexadecimal digits.``0X'' is alsoacceptable in lieu of ``0x''.See strtol(3) and the relatedroutines.2.If the metric has a floating point type, then value should beeither in the form of an integer described above, or a fixedpoint number, or a number in scientific notation.Seestrtod(3).3.If the metric has a string type, then value is interpreted asa literal string of ASCII characters.4.If the metric has any other type (i.e.PM_TYPE_EVENT orPM_TYPE_AGGREGATE) then no encoding of value from the commandline makes sense, and the values of these metrics cannot bemodified with pmstore.The output reports the old value and the new value for eachupdated instance of the requested metric.When using the -L option to fetch metrics from a local context,the -K option may be used to control the DSO PMDAs that should bemade accessible.The spec argument conforms to the syntaxdescribed in pmSpecLocalPMDA(3).More than one -K option may beused.Normally pmstore will report the old value (as initiallyretrieved using pmFetch(3)) and the new value from the commandline.The -F option forces another pmFetch(3) after thepmStore(3) and the returned value is reported as the new value.This is useful in cases where metricname is a metric thatprovides different semantics for the store operation, e.g. toincrement the current value or reset a counter (independent ofthe value from the command line).",
        "name": "pmstore - modify performance metric values",
        "section": 1
    },
    {
        "command": "pmtime",
        "description": "pmtime is a graphical user interface for performance monitoringapplications using the PCP infrastructure and requiringinteractive time control.pmtime is not normally invoked directly by users.Rather, it ismore typical for it to be started by client applications (e.g.pmchart(1), pmstat(1) or pmval(1)).There are two modes of interacting with a pmtime process - livehost mode, and historical archive mode.In archive mode thewindow presents time controls suitable for manipulating thearchive position, allowing full VCR control to move forwards andbackwards in time at configurable rates and intervals.In livemode the pmtime window contains the simpler time controlssuitable for live monitoring.Note that the pmtime window is only made visible when explicitlyrequested.Multiple client applications can be connected to asingle pmtime server - when the final client application exits,pmtime will also exit.",
        "name": "pmtime - time control server for Performance Co-Pilot",
        "section": 1
    },
    {
        "command": "pmtrace",
        "description": "pmtrace provides a simple command line interface to the tracePerformance Metrics Domain Agent (PMDA) and the associatedpcp_trace library.The default pmtrace behavior is to provide point trace data tothe trace PMDA, using the tag argument as the identifying nameassociated with each trace point.The tag then becomes aninstance identifier within the set of trace.point metrics.",
        "name": "pmtrace - command line performance instrumentation",
        "section": 1
    },
    {
        "command": "pmval",
        "description": "pmval prints current or archived values for the nominatedperformance metric.The metric of interest is named in themetricname argument, subject to instance qualification with the-i flag as described below.Unless directed to another host by the -h option, or to a set ofarchives by the -a or -U options, pmval will contact thePerformance Metrics Collector Daemon (PMCD) on the local host toobtain the required information.The metricname argument may also be given in the metricspecification syntax, as described in PCPIntro(1), where thesource, metric and instance may all be included in themetricname, e.g. thathost:kernel.all.load[\"1 minute\"].When thisformat is used, none of the -h or -a or -U options may bespecified.When using the metric specification syntax, the ``hostname'' @ istreated specially and causes pmval to use a local context tocollect metrics from PMDAs on the local host without PMCD.Onlysome metrics are available in this mode.When processing a set of archives, pmval may relinquish its owntiming control, and operate under the control of a a pmtime(1)process that uses a GUI dialog to provide timing control.Inthis case, either the -g option should be used to start pmval asthe sole client of a new pmtime(1) instance, or -p should be usedto attach pmval to an existing pmtime(1) instance via the IPCchannel identified by the port argument.The -S, -T, -O and -A options may be used to define a time windowto restrict the samples retrieved, set an initial origin withinthe time window, or specify a ``natural'' alignment of the sampletimes; refer to PCPIntro(1) for a complete description of theseoptions.The output from pmval is directed to standard output.Thefollowing symbols may occasionally appear, in place of a metricvalue, in pmval output:A question mark symbol (?) indicatesthat a value is no longer available for that metric instance.Anexclamation mark (!)indicates that a 64-bit counter wrappedduring the sample.pmevent is an alias for pmval.",
        "name": "pmval, pmevent - arbitrary performance metrics value dumper",
        "section": 1
    },
    {
        "command": "pmview",
        "description": "pmview is a generalized 3D performance metrics visualization toolfor the Performance Co-Pilot (PCP(1)).pmview is the base utility behind performance metricsvisualization tools such as dkvis(1), mpvis(1), osvis(1) andnfsvis(1), It is also used by a range of related tools that arespecific to optional Performance Domain Agents (PMDA) and/or PCPadd-on products.pmview may also be used to construct customized3D performance displays.pmview displays performance metrics as colored blocks andcylinders arranged on monochrome base planes. Each object mayrepresent a single performance metric, or a stack of severalperformance metrics.Since the objects are modulated by thevalue of the metric they represent, only numerical metrics may bevisualized.Objects representing a single metric may bemodulated in terms of height, color, or height and color.Objects in a stack may only be height modulated, but the stackcan be normalized to the maximum height.Labels may be added tothe scene to help identify groups of metrics.A configuration file (as specified by the -c option, or read fromstandard input) is used to specify the position, color, maximumvalue and labels of metrics and metric instances in the scene.The maximum value acts as a normalization factor and is used toscale the object height and/or color in proportion to the metricvalues.Metric values which exceed the associated maximum valueare displayed as solid white objects.If a metric isunavailable, the object will have minimum height and will becolored grey.Normally, the tool operates in ``live'' mode where performancemetrics are fetched in real-time.The user can view metrics fromany host running pmcd(1).pmview can also replay archives ofperformance metrics (see pmlogger(1)) and allow the user tointeractively control the current replay time and rate using theVCR paradigm.This is particularly useful for retrospectivecomparisons and for post-mortem analysis of performance problemswhere a remote system is not accessible or a performance analystis not available on-site.All metrics in the Performance Metrics Name Space (PMNS) withnumeric value semantics from any number of hosts or archives maybe visualized.pmview examines the semantics of the metrics andwhere sensible, converts metric values to a rate before scaling.",
        "name": "pmview - performance metrics 3D visualization back-end",
        "section": 1
    },
    {
        "command": "pmwebd",
        "description": null,
        "name": "PCPCompat, pcp-collectl, pmmgr, pmwebd - backward-compatibilityin the Performance Co-Pilot (PCP)",
        "section": 1
    },
    {
        "command": "poff",
        "description": "This manual page describes the pon, plog and poff scripts, whichallow users to control PPP connections.ponpon, invoked without arguments, runs the /etc/ppp/ppp_on_bootfile, if it exists and is executable. Otherwise, a PPP connectionwill be started using configuration from /etc/ppp/peers/provider.This is the default behaviour unless an isp-name argument isgiven.For instance, to use ISP configuration \"myisp\" run:pon myisppon will then use the options file /etc/ppp/peers/myisp.You canpass additional options after the ISP name, too.pon can be usedto run multiple, simultaneous PPP connections.poffpoff closes a PPP connection. If more than one PPP connectionexists, the one named in the argument to poff will be killed,e.g.poff myprovider2will terminate the connection to myprovider2, and leave the PPPconnections to e.g. \"myprovider1\" or \"myprovider3\" up andrunning.poff takes the following command line options:-rcauses the connection to be redialed after it isdropped.-dtoggles the state of pppd's debug option.-ccauses pppd(8) to renegotiate compression.-astops all running ppp connections. If the argumentisp-name is given it will be ignored.-hdisplays help information.-vprints the version and exits.If no argument is given, poff will stop or signal pppd ifand only if there is exactly one running. If more than oneconnection is active, it will exit with an error code of1.plogplog shows you the last few lines of /var/log/ppp.log. If thatfile doesn't exist, it shows you the last few lines of your/var/log/syslog file, but excluding the lines not generated bypppd.This script makes use of the tail(1) command, so argumentsthat can be passed to tail(1) can also be passed to plog.Note: the plog script can only be used by root or another systemadministrator in group \"adm\", due to security reasons. Also, tohave all pppd-generated information in one logfile, that plog canshow, you need the following line in your /etc/syslog.conf file:local2.*-/var/log/ppp.log",
        "name": "pon, poff, plog - starts up, shuts down or lists the log of PPPconnections",
        "section": 1
    },
    {
        "command": "pon",
        "description": "This manual page describes the pon, plog and poff scripts, whichallow users to control PPP connections.ponpon, invoked without arguments, runs the /etc/ppp/ppp_on_bootfile, if it exists and is executable. Otherwise, a PPP connectionwill be started using configuration from /etc/ppp/peers/provider.This is the default behaviour unless an isp-name argument isgiven.For instance, to use ISP configuration \"myisp\" run:pon myisppon will then use the options file /etc/ppp/peers/myisp.You canpass additional options after the ISP name, too.pon can be usedto run multiple, simultaneous PPP connections.poffpoff closes a PPP connection. If more than one PPP connectionexists, the one named in the argument to poff will be killed,e.g.poff myprovider2will terminate the connection to myprovider2, and leave the PPPconnections to e.g. \"myprovider1\" or \"myprovider3\" up andrunning.poff takes the following command line options:-rcauses the connection to be redialed after it isdropped.-dtoggles the state of pppd's debug option.-ccauses pppd(8) to renegotiate compression.-astops all running ppp connections. If the argumentisp-name is given it will be ignored.-hdisplays help information.-vprints the version and exits.If no argument is given, poff will stop or signal pppd ifand only if there is exactly one running. If more than oneconnection is active, it will exit with an error code of1.plogplog shows you the last few lines of /var/log/ppp.log. If thatfile doesn't exist, it shows you the last few lines of your/var/log/syslog file, but excluding the lines not generated bypppd.This script makes use of the tail(1) command, so argumentsthat can be passed to tail(1) can also be passed to plog.Note: the plog script can only be used by root or another systemadministrator in group \"adm\", due to security reasons. Also, tohave all pppd-generated information in one logfile, that plog canshow, you need the following line in your /etc/syslog.conf file:local2.*-/var/log/ppp.log",
        "name": "pon, poff, plog - starts up, shuts down or lists the log of PPPconnections",
        "section": 1
    },
    {
        "command": "portablectl",
        "description": "portablectl may be used to attach, detach or inspect portableservice images. It's primarily a command interfacing withsystemd-portabled.service(8).Portable service images contain an OS file system tree along withsystemd(1) unit file information. A service image may be\"attached\" to the local system. If attached, a set of unit filesare copied from the image to the host, and extended withRootDirectory= or RootImage= assignments (in case of serviceunits) pointing to the image file or directory, ensuring theservices will run within the file system context of the image.Portable service images are an efficient way to bundle multiplerelated services and other units together, and transfer them as awhole between systems. When these images are attached the localsystem the contained units may run in most ways like regularsystem-provided units, either with full privileges or insidestrict sandboxing, depending on the selected configuration. Formore details, see Portable Services Documentation[1].Specifically portable service images may be of the followingkind:\u2022Directory trees containing an OS, including the top-leveldirectories /usr/, /etc/, and so on.\u2022btrfs subvolumes containing OS trees, similar to normaldirectory trees.\u2022Binary \"raw\" disk images containing MBR or GPT partitiontables and Linux file system partitions. (These must beregular files, with the .raw suffix.)",
        "name": "portablectl - Attach, detach or inspect portable service images",
        "section": 1
    },
    {
        "command": "ppdc",
        "description": "ppdc compiles PPDC source files into one or more PPD files.Thisprogram is deprecated and will be removed in a future release ofCUPS.",
        "name": "ppdc - cups ppd compiler (deprecated)",
        "section": 1
    },
    {
        "command": "ppdhtml",
        "description": "ppdhtml reads a driver information file and produces a HTMLsummary page that lists all of the drivers in a file and thesupported options.This program is deprecated and will beremoved in a future release of CUPS.",
        "name": "ppdhtml - cups html summary generator (deprecated)",
        "section": 1
    },
    {
        "command": "ppdi",
        "description": "ppdi imports one or more PPD files into a PPD compiler sourcefile.Multiple languages of the same PPD file are merged into asingle printer definition to facilitate accurate changes for alllocalizations.This program is deprecated and will be removed ina future release of CUPS.",
        "name": "ppdi - import ppd files (deprecated)",
        "section": 1
    },
    {
        "command": "ppdmerge",
        "description": "ppdmerge merges two or more PPD files into a single, multi-language PPD file.This program is deprecated and will beremoved in a future release of CUPS.",
        "name": "ppdmerge - merge ppd files (deprecated)",
        "section": 1
    },
    {
        "command": "ppdpo",
        "description": "ppdpo extracts UI strings from PPDC source files and updateseither a GNU gettext or macOS strings format message catalogsource file for translation.This program is deprecated and willbe removed in a future release of CUPS.",
        "name": "ppdpo - ppd message catalog generator (deprecated)",
        "section": 1
    },
    {
        "command": "pr",
        "description": "Paginate or columnate FILE(s) for printing.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.+FIRST_PAGE[:LAST_PAGE], --pages=FIRST_PAGE[:LAST_PAGE]begin [stop] printing with page FIRST_[LAST_]PAGE-COLUMN, --columns=COLUMNoutput COLUMN columns and print columns down, unless -a isused. Balance number of lines in the columns on each page-a, --acrossprint columns across rather than down, used together with-COLUMN-c, --show-control-charsuse hat notation (^G) and octal backslash notation-d, --double-spacedouble space the output-D, --date-format=FORMATuse FORMAT for the header date-e[CHAR[WIDTH]], --expand-tabs[=CHAR[WIDTH]]expand input CHARs (TABs) to tab WIDTH (8)-F, -f, --form-feeduse form feeds instead of newlines to separate pages (by a3-line page header with -F or a 5-line header and trailerwithout -F)-h, --header=HEADERuse a centered HEADER instead of filename in page header,-h \"\" prints a blank line, don't use -h\"\"-i[CHAR[WIDTH]], --output-tabs[=CHAR[WIDTH]]replace spaces with CHARs (TABs) to tab WIDTH (8)-J, --join-linesmerge full lines, turns off -W line truncation, no columnalignment, --sep-string[=STRING] sets separators-l, --length=PAGE_LENGTHset the page length to PAGE_LENGTH (66) lines (defaultnumber of lines of text 56, and with -F 63).implies -tif PAGE_LENGTH <= 10-m, --mergeprint all files in parallel, one in each column, truncatelines, but join lines of full length with -J-n[SEP[DIGITS]], --number-lines[=SEP[DIGITS]]number lines, use DIGITS (5) digits, then SEP (TAB),default counting starts with 1st line of input file-N, --first-line-number=NUMBERstart counting with NUMBER at 1st line of first pageprinted (see +FIRST_PAGE)-o, --indent=MARGINoffset each line with MARGIN (zero) spaces, do not affect-w or -W, MARGIN will be added to PAGE_WIDTH-r, --no-file-warningsomit warning when a file cannot be opened-s[CHAR], --separator[=CHAR]separate columns by a single character, default for CHARis the <TAB> character without -w and 'no char' with -w.-s[CHAR] turns off line truncation of all 3 column options(-COLUMN|-a -COLUMN|-m) except -w is set-S[STRING], --sep-string[=STRING]separate columns by STRING, without -S: Default separator<TAB> with -J and <space> otherwise (same as -S\" \"), noeffect on column options-t, --omit-headeromit page headers and trailers; implied if PAGE_LENGTH <=10-T, --omit-paginationomit page headers and trailers, eliminate any paginationby form feeds set in input files-v, --show-nonprintinguse octal backslash notation-w, --width=PAGE_WIDTHset page width to PAGE_WIDTH (72) characters for multipletext-column output only, -s[char] turns off (72)-W, --page-width=PAGE_WIDTHset page width to PAGE_WIDTH (72) characters always,truncate lines, except -J option is set, no interferencewith -S or -s--help display this help and exit--versionoutput version information and exit",
        "name": "pr - convert text files for printing",
        "section": 1
    },
    {
        "command": "preconv",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "printenv",
        "description": "Print the values of the specified environment VARIABLE(s).If noVARIABLE is specified, print name and value pairs for them all.-0, --nullend each output line with NUL, not newline--help display this help and exit--versionoutput version information and exitNOTE: your shell may have its own version of printenv, whichusually supersedes the version described here.Please refer toyour shell's documentation for details about the options itsupports.",
        "name": "printenv - print all or part of environment",
        "section": 1
    },
    {
        "command": "printf",
        "description": "Print ARGUMENT(s) according to FORMAT, or execute according toOPTION:--help display this help and exit--versionoutput version information and exitFORMAT controls the output as in C printf.Interpreted sequencesare:\\\"double quote\\\\backslash\\aalert (BEL)\\bbackspace\\cproduce no further output\\eescape\\fform feed\\nnew line\\rcarriage return\\thorizontal tab\\vvertical tab\\NNNbyte with octal value NNN (1 to 3 digits)\\xHHbyte with hexadecimal value HH (1 to 2 digits)\\uHHHH Unicode (ISO/IEC 10646) character with hex value HHHH (4digits)\\UHHHHHHHHUnicode character with hex value HHHHHHHH (8 digits)%%a single %%bARGUMENT as a string with '\\' escapes interpreted, exceptthat octal escapes are of the form \\0 or \\0NNN%qARGUMENT is printed in a format that can be reused asshell input, escaping non-printable characters with theproposed POSIX $'' syntax.and all C format specifications ending with one of diouxXfeEgGcs,with ARGUMENTs converted to proper type first.Variable widthsare handled.NOTE: your shell may have its own version of printf, whichusually supersedes the version described here.Please refer toyour shell's documentation for details about the options itsupports.",
        "name": "printf - format and print data",
        "section": 1
    },
    {
        "command": "prlimit",
        "description": "Given a process ID and one or more resources, prlimit tries toretrieve and/or modify the limits.When command is given, prlimit will run this command with thegiven arguments.The limits parameter is composed of a soft and a hard value,separated by a colon (:), in order to modify the existing values.If no limits are given, prlimit will display the current values.If one of the values is not given, then the existing one will beused. To specify the unlimited or infinity limit (RLIM_INFINITY),the -1 or 'unlimited' string can be passed.Because of the nature of limits, the soft limit must be lower orequal to the high limit (also called the ceiling). To see allavailable resource limits, refer to the RESOURCE OPTIONS section.\u2022soft:_hard_ Specify both limits.\u2022soft: Specify only the soft limit.\u2022:hard Specify only the hard limit.\u2022value Specify both limits to the same value.",
        "name": "prlimit - get and set process resource limits",
        "section": 1
    },
    {
        "command": "prtstat",
        "description": "prtstat prints the statistics of the specified process.Thisinformation comes from the /proc/pid/stat file.",
        "name": "prtstat - print statistics of a process",
        "section": 1
    },
    {
        "command": "ps",
        "description": "ps displays information about a selection of the activeprocesses.If you want a repetitive update of the selection andthe displayed information, use top instead.This version of ps accepts several kinds of options:1UNIX options, which may be grouped and must be preceded by adash.2BSD options, which may be grouped and must not be used with adash.3GNU long options, which are preceded by two dashes.Options of different types may be freely mixed, but conflicts canappear.There are some synonymous options, which arefunctionally identical, due to the many standards and psimplementations that this ps is compatible with.Note that ps -aux is distinct from ps aux.The POSIX and UNIXstandards require that ps -aux print all processes owned by auser named x, as well as printing all processes that would beselected by the -a option.If the user named x does not exist,this ps may interpret the command as ps aux instead and print awarning.This behavior is intended to aid in transitioning oldscripts and habits.It is fragile, subject to change, and thusshould not be relied upon.By default, ps selects all processes with the same effective userID (euid=EUID) as the current user and associated with the sameterminal as the invoker.It displays the process ID (pid=PID),the terminal associated with the process (tname=TTY), thecumulated CPU time in [DD-]hh:mm:ss format (time=TIME), and theexecutable name (ucmd=CMD).Output is unsorted by default.The use of BSD-style options will add process state (stat=STAT)to the default display and show the command args (args=COMMAND)instead of the executable name.You can override this with thePS_FORMAT environment variable. The use of BSD-style options willalso change the process selection to include processes on otherterminals (TTYs) that are owned by you; alternately, this may bedescribed as setting the selection to be the set of all processesfiltered to exclude processes owned by other users or not on aterminal.These effects are not considered when options aredescribed as being \"identical\" below, so -M will be consideredidentical to Z and so on.Except as described below, process selection options areadditive.The default selection is discarded, and then theselected processes are added to the set of processes to bedisplayed.A process will thus be shown if it meets any of thegiven selection criteria.",
        "name": "ps - report a snapshot of the current processes.",
        "section": 1
    },
    {
        "command": "psfaddtable",
        "description": "psfaddtable takes a console font in .psf format given by fontfileand merges it with the Unicode character table given by tablefileto produce a font file with an embedded character table, which iswritten to outfile.An input file name of \"-\" denotes standardinput, and an output file name of \"-\" denotes standard output.If the fontfile already contains an embedded character table, itis ignored.",
        "name": "psfaddtable - add a Unicode character table to a console font",
        "section": 1
    },
    {
        "command": "psfgettable",
        "description": "psfgettable extracts the embedded Unicode character table from a.psf format console font into a human readable ASCII file of theformat used by psfaddtable(1).If the font file name is a singledash (-), the font is read from standard input.",
        "name": "psfgettable - extract the embedded Unicode character table from aconsole font",
        "section": 1
    },
    {
        "command": "psfstriptable",
        "description": "psfstriptable reads a .psf format console font from fontfile,removes the embedded Unicode font table if there is one, andwrites the result to outfile.An input file name of \"-\" denotesstandard input, and an output file name of \"-\" denotes standardoutput.",
        "name": "psfstriptable - remove the embedded Unicode character table froma console font",
        "section": 1
    },
    {
        "command": "psfxtable",
        "description": "psfxtable handles the embedded Unicode character table for .psfformat console fonts. It reads a font and possibly a table andwrites a font and/or a table.psfaddtable(1), psfgettable(1) andpsfstriptable(1) are links to it.Each of the filenames infont, outfont, intable, and outtable maybe replaced by a single dash (-), in which case standard input orstandard output is used.If no -i option is given, the font isread from standard input.If no -it or -o or -ot option isgiven, no input table is read or no output font or output tableis written.By default the output font (if any) will have a Unicode tablewhen either the input font has one, or an explicit table (whichoverrides an input font table) has been provided.The option -ntcauses output of a font without table.When outfont is requestedit will get a psf1 header when infont has a psf1 header andintable does not have sequences and a psf2 header otherwise.",
        "name": "psfxtable - handle Unicode character tables for console fonts",
        "section": 1
    },
    {
        "command": "psktool",
        "description": "Programthat generates random keys for use with TLS-PSK. Thekeys are stored in hexadecimal format in a key file.",
        "name": "psktool - GnuTLS PSK tool",
        "section": 1
    },
    {
        "command": "pslog",
        "description": "The pslog command reports the current working logs of a process.",
        "name": "pslog - report current logs path of a process",
        "section": 1
    },
    {
        "command": "pstree",
        "description": "pstree shows running processes as a tree.The tree is rooted ateither pid or init if pid is omitted.If a user name isspecified, all process trees rooted at processes owned by thatuser are shown.pstree visually merges identical branches by putting them insquare brackets and prefixing them with the repetition count,e.g.init-+-getty|-getty|-getty`-gettybecomesinit---4*[getty]Child threads of a process are found under the parent process andare shown with the process name in curly braces, e.g.icecast2---13*[{icecast2}]If pstree is called as pstree.x11 then it will prompt the user atthe end of the line to press return and will not return untilthat has happened.This is useful for when pstree is run in axterminal.Certain kernel or mount parameters, such as the hidepid optionfor procfs, will hide information for some processes. In thesesituations pstree will attempt to build the tree without thisinformation, showing process names as question marks.",
        "name": "pstree - display a tree of processes",
        "section": 1
    },
    {
        "command": "ptx",
        "description": "Output a permuted index, including context, of the words in theinput files.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-A, --auto-referenceoutput automatically generated references-G, --traditionalbehave more like System V 'ptx'-F, --flag-truncation=STRINGuse STRING for flagging line truncations.The default is'/'-M, --macro-name=STRINGmacro name to use instead of 'xx'-O, --format=roffgenerate output as roff directives-R, --right-side-refsput references at right, not counted in -w-S, --sentence-regexp=REGEXPfor end of lines or end of sentences-T, --format=texgenerate output as TeX directives-W, --word-regexp=REGEXPuse REGEXP to match each keyword-b, --break-file=FILEword break characters in this FILE-f, --ignore-casefold lower case to upper case for sorting-g, --gap-size=NUMBERgap size in columns between output fields-i, --ignore-file=FILEread ignore word list from FILE-o, --only-file=FILEread only word list from this FILE-r, --referencesfirst field of each line is a reference-t, --typeset-mode- not implemented --w, --width=NUMBERoutput width in columns, reference excluded--help display this help and exit--versionoutput version information and exit",
        "name": "ptx - produce a permuted index of file contents",
        "section": 1
    },
    {
        "command": "pv",
        "description": "pv shows the progress of data through a pipeline by givinginformation such as time elapsed, percentage completed (withprogress bar), current throughput rate, total data transferred,and ETA.To use it, insert it in a pipeline between two processes, withthe appropriate options.Its standard input will be passedthrough to its standard output and progress will be shown onstandard error.pv will copy each supplied FILE in turn to standard output (-means standard input), or if no FILEs are specified just standardinput is copied. This is the same behaviour as cat(1).A simple example to watch how quickly a file is transferred usingnc(1):pv file | nc -w 1 somewhere.com 3000A similar example, transferring a file from another process andpassing the expected size to pv:cat file | pv -s 12345 | nc -w 1 somewhere.com 3000A more complicated example using numeric output to feed into thedialog(1) program for a full-screen progress display:(tar cf - . \\| pv -n -s $(du -sb . | awk '{print $1}') \\| gzip -9 > out.tgz) 2>&1 \\| dialog --gauge 'Progress' 7 70Taking an image of a disk, skipping errors:pv -EE /dev/your/disk/device > disk-image.imgWriting an image back to a disk:pv disk-image.img > /dev/your/disk/deviceZeroing a disk:pv < /dev/zero > /dev/your/disk/deviceNote that if the input size cannot be calculated, and the outputis a block device, then the size of the block device will be usedand pv will automatically stop at that size as if -S had beengiven.(Linux only): Watching file descriptor 3 opened by anotherprocess 1234:pv -d 1234:3(Linux only): Watching all file descriptors used by process 1234:pv -d 1234",
        "name": "pv - monitor the progress of data through a pipe",
        "section": 1
    },
    {
        "command": "pwd",
        "description": "Print the full filename of the current working directory.-L, --logicaluse PWD from environment, even if it contains symlinks-P, --physicalavoid all symlinks--help display this help and exit--versionoutput version information and exitIf no option is specified, -P is assumed.NOTE: your shell may have its own version of pwd, which usuallysupersedes the version described here.Please refer to yourshell's documentation for details about the options it supports.",
        "name": "pwd - print name of current/working directory",
        "section": 1
    },
    {
        "command": "pwdx",
        "description": null,
        "name": "pwdx - report current working directory of a process",
        "section": 1
    },
    {
        "command": "quilt",
        "description": "Quilt is a tool to manage large sets of patches by keeping trackof the changes each patch makes.Patches can be applied,unapplied, refreshed, and so forth.The key philosophicalconcept is that your primary working material is patches.With quilt, all work occurs within a single directory tree.Commands can be invoked from anywhere within the source tree.Like CVS, Subversion, or Git, quilt takes commands of the form\u201cquilt command\u201d.A command can be truncated (abbreviated) aslong as the specified part of the command is unambiguous.Ifcommand is ambiguously short, quilt lists all commands matchingthat prefix and exits.All commands print a brief contextualhelp message and exit if given the \u201c-h\u201d option.Quilt manages a stack of patches.Patches are appliedincrementally on top of the base tree plus all preceding patches.They can be pushed onto the stack (\u201cquilt push\u201d), and popped offthe stack (\u201cquilt pop\u201d).Commands are available for querying thecontents of the stack (\u201cquilt applied\u201d, \u201cquilt previous\u201d, \u201cquilttop\u201d) and the patches that are not applied at a particular moment(\u201cquilt next\u201d, \u201cquilt unapplied\u201d).By default, most commandsapply to the topmost patch on the stack.Patch files are located in the patches subdirectory of the sourcetree (see Example of working tree, under FILES, below).TheQUILT_PATCHES environment variable overrides this defaultlocation.When not found in the current directory, thatsubdirectory is searched recursively in the parent directories(this is similar to the way Git searches for its configurationfiles).The patches directory may contain subdirectories.Itmay also be a symbolic link instead of a directory.Quilt creates and maintains a file called series, which definesthe order in which patches are applied.The QUILT_SERIESenvironment variable overrides this default name.You can querythe contents of the series file at any time with \u201cquilt series\u201d.In this file, each patch file name is on a separate line.Patchfiles are identified by path names that are relative to thepatches directory; patches may be in subdirectories below thisdirectory.Lines in the series file that start with a hashcharacter (#) are ignored.Patch options, such as the striplevel or whether the patch is reversed, can be added after eachpatch file name.Options are introduced by a space, separated byspaces, and follow the syntax of the patch(1) options (e.g.,\u201c-p2\u201d).Quilt records patch options automatically when a commandsupporting them is used.Without options, strip level 1 isassumed.You can also add a comment after each patch file nameand options, introduced by a space followed by a hash character.When quilt adds, removes, or renames patches, it automaticallyupdates the series file.Users of quilt can modify series fileswhile some patches are applied, as long as the applied patchesremain in their original order.Unless there are means by whicha series file can be generated automatically, you should provideit along with any set of quilt-managed patches you distribute.Different series files can be used to assemble patches indifferent ways, corresponding (for example) to differentdevelopment branches.Before a patch is applied, copies of all files the patch modifiesare saved to the .pc/patch-name directory, where patch-name isthe name of the patch (for example, fix-buffer-overflow.patch).The patch is added to the list of currently applied patches(.pc/applied-patches).Later, when a patch is regenerated(\u201cquilt refresh\u201d), the backup copies in .pc/patch-name arecompared with the current versions of the files in the sourcetree using GNU diff(1).A similar process occurs when starting a new patch (\u201cquilt new\u201d);the new patch file name is added to the series file.A file tobe changed by the patch is backed up and opened for editing(\u201cquilt edit\u201d).After editing, inspect the impact of yourchanges (\u201cquilt diff\u201d); the changes stay local to your workingtree until you call \u201cquilt refresh\u201d to write them to the patchfile.Documentation related to a patch can be put at the beginning ofits patch file (\u201cquilt header\u201d).Quilt is careful to preserveall text that precedes the actual patch when doing a refresh.(This is limited to patches in unified format; see the GNUDiffutils manual.)The series file is looked up in the .pc directory, in the root ofthe source tree, and in the patches directory.The first seriesfile that is found is used.This may also be a symbolic link, ora file with multiple hard links.Usually, only one series fileis used for a set of patches, making the patches subdirectory aconvenient location.The .pc directory cannot be relocated, but it can be a symboliclink.Its subdirectories must not be renamed or restructured.While patches are applied to the source tree, this directory isessential for many operations, including popping patches off thestack and refreshing them.Files in the .pc directory areautomatically removed when they are no longer needed, so there isno need to clean up manually.Quilt commands referenceadd [-P patch] {file} ...Add one or more files to the topmost or named patch.Filesmust be added to the patch before being modified.Files thatare modified by patches already applied on top of thespecified patch cannot be added.-P patchPatch to add files to.annotate [-P patch] {file}Print an annotated listing of the specified file showingwhich patches modify which lines. Only applied patches areincluded.-P patchStop checking for changes at the specified rather thanthe topmost patch.applied [patch]Print a list of applied patches, or all patches up to andincluding the specified patch in the file series.delete [-r] [--backup] [patch|-n]Remove the specified or topmost patch from the series file.If the patch is applied, quilt will attempt to remove itfirst. (Only the topmost patch can be removed right now.)-nDelete the next patch after topmost, rather than thespecified or topmost patch.-rRemove the deleted patch file from the patches directoryas well.--backupRename the patch file to patch~ rather than deleting it.Ignored if not used with `-r'.diff [-p n|-p ab] [-u|-U num|-c|-C num] [--combine patch|-z] [-R][-P patch] [--snapshot] [--diff=utility] [--no-timestamps] [--no-index] [--sort] [--color[=always|auto|never]] [file ...]Produces a diff of the specified file(s) in the topmost orspecified patch.If no files are specified, all files thatare modified are included.-p nCreate a -p n style patch (-p0 or -p1 are supported).-p abCreate a -p1 style patch, but use a/file and b/file asthe original and new filenames instead of the defaultdir.orig/file and dir/file names.-u, -U num, -c, -C numCreate a unified diff (-u, -U) with num lines of context.Create a context diff (-c, -C) with num lines of context.The number of context lines defaults to 3.--no-timestampsDo not include file timestamps in patch headers.--no-indexDo not output Index: lines.-zWrite to standard output the changes that have been maderelative to the topmost or specified patch.-RCreate a reverse diff.-P patchCreate a diff for the specified patch.(Defaults to thetopmost patch.)--combine patchCreate a combined diff for all patches between this patchand the patch specified with -P. A patch name of `-' isequivalent to specifying the first applied patch.--snapshotDiff against snapshot (see `quilt snapshot -h').--diff=utilityUse the specified utility for generating the diff. Theutility is invoked with the original and new file name asarguments.--color[=always|auto|never]Use syntax coloring (auto activates it only if the outputis a tty).--sortSort files by their name instead of preserving theoriginal order.edit file ...Edit the specified file(s) in $EDITOR after adding it (them)to the topmost patch.files [-v] [-a] [-l] [--combine patch] [patch]Print the list of files that the topmost or specified patchchanges.-aList all files in all applied patches.-lAdd patch name to output.-vVerbose, more user friendly output.--combine patchCreate a listing for all patches between this patch andthe topmost or specified patch. A patch name of `-' isequivalent to specifying the first applied patch.fold [-R] [-q] [-f] [-p strip-level]Integrate the patch read from standard input into the topmostpatch: After making sure that all files modified are part ofthe topmost patch, the patch is applied with the specifiedstrip level (which defaults to 1).-RApply patch in reverse.-qQuiet operation.-fForce apply, even if the patch has rejects. Unless inquiet mode, apply the patch interactively: the patchutility may ask questions.-p strip-levelThe number of pathname components to strip from filenames when applying patchfile.fork [new_name]Fork the topmost patch.Forking a patch means creating averbatim copy of it under a new name, and use that new nameinstead of the original one in the current series.This isuseful when a patch has to be modified, but the originalversion of it should be preserved, e.g.because it is usedin another series, or for the history.A typical sequence ofcommands would be: fork, edit, refresh.If new_name is missing, the name of the forked patch will bethe current patch name, followed by `-2'.If the patch namealready ends in a dash-and-number, the number is furtherincremented (e.g., patch.diff, patch-2.diff, patch-3.diff).graph [--all] [--reduce] [--lines[=num]] [--edge-labels=files][-T ps] [patch]Generate a dot(1) directed graph showing the dependenciesbetween applied patches. A patch depends on another patch ifboth touch the same file or, with the --lines option, iftheir modifications overlap. Unless otherwise specified, thegraph includes all patches that the topmost patch depends on.When a patch name is specified, instead of the topmost patch,create a graph for the specified patch. The graph willinclude all other patches that this patch depends on, as wellas all patches that depend on this patch.--allGenerate a graph including all applied patches and theirdependencies. (Unapplied patches are not included.)--reduceEliminate transitive edges from the graph.--lines[=num]Compute dependencies by looking at the lines the patchesmodify.Unless a different num is specified, two linesof context are included.--edge-labels=filesLabel graph edges with the file names that the adjacentpatches modify.-T psDirectly produce a PostScript output file.grep [-h|options] {pattern}Grep through the source files, recursively, skipping patchesand quilt meta-information. If no filename argument is given,the whole source tree is searched. Please see the grep(1)manual page for options.-hPrint this help. The grep -h option can be passed after adouble-dash (--). Search expressions that start with adash can be passed after a second double-dash (-- --).header [-a|-r|-e] [--backup] [--strip-diffstat] [--strip-trailing-whitespace] [patch]Print or change the header of the topmost or specified patch.-a, -r, -eAppend to (-a) or replace (-r) the exiting patch header,or edit (-e) the header in $EDITOR. If none of theseoptions is given, print the patch header.--strip-diffstatStrip diffstat output from the header.--strip-trailing-whitespaceStrip trailing whitespace at the end of lines of theheader.--backupCreate a backup copy of the old version of a patch aspatch~.import [-p num] [-R] [-P patch] [-f] [-d {o|a|n}] patchfile ...Import external patches.The patches will be insertedfollowing the current top patch, and must be pushed afterimport to apply them.-p numNumber of directory levels to strip when applying(default=1)-RApply patch in reverse.-P patchPatch filename to use inside quilt. This option can onlybe used when importing a single patch.-fOverwrite/update existing patches.-d {o|a|n}When overwriting in existing patch, keep the old (o), all(a), or new (n) patch header. If both patches includeheaders, this option must be specified. This option isonly effective when -f is used.mail {--mbox file|--send} [-m text] [-M file] [--prefix prefix][--sender ...] [--from ...] [--to ...] [--cc ...] [--bcc ...][--subject ...] [--reply-to message] [--charset ...] [--signaturefile] [first_patch [last_patch]]Create mail messages from a specified range of patches, orall patches in the series file, and either store them in amailbox file, or send them immediately. The editor is openedwith a template for the introduction.Please see/usr/local/share/doc/quilt/README.MAIL for details.Whenspecifying a range of patches, a first patch name of `-'denotes the first, and a last patch name of `-' denotes thelast patch in the series.-m textText to use as the text in the introduction. When thisoption is used, the editor will not be invoked, and thepatches will be processed immediately.-M fileLike the -m option, but read the introduction from file.--prefix prefixUse an alternate prefix in the bracketed part of thesubjects generated. Defaults to `patch'.--mbox fileStore all messages in the specified file in mbox format.The mbox can later be sent using formail, for example.--sendSend the messages directly.--senderThe envelope sender address to use. The address must beof the form `user@domain.name'. No display name isallowed.--from, --subjectThe values for the From and Subject headers to use. If no--from option is given, the value of the --sender optionis used.--to, --cc, --bccAppend a recipient to the To, Cc, or Bcc header.--charsetSpecify a particular message encoding on systems whichdon't use UTF-8 or ISO-8859-15. This character encodingmust match the one used in the patches.--signature fileAppend the specified signature to messages (defaults to~/.signature if found; use `-' for no signature).--reply-to messageAdd the appropriate headers to reply to the specifiedmessage.new [-p n|-p ab] {patchname}Create a new patch with the specified file name, and insertit after the topmost patch. The name can be prefixed with asub-directory name, allowing for grouping related patchestogether.-p nCreate a -p n style patch (-p0 or -p1 are supported).-p abCreate a -p1 style patch, but use a/file and b/file asthe original and new filenames instead of the defaultdir.orig/file and dir/file names.Quilt can be used in sub-directories of a source tree. Itdetermines the root of a source tree by searching for adirectory above the current working directory. Create adirectory in the intended root directory if quilt choosesa top-level directory that is too high up in thedirectory tree.next [patch]Print the name of the next patch after the specified ortopmost patch in the series file.patches [-v] [--color[=always|auto|never]] {file} [files...]Print the list of patches that modify any of the specifiedfiles. (Uses a heuristic to determine which files aremodified by unapplied patches.Note that this heuristic ismuch slower than scanning applied patches.)-vVerbose, more user friendly output.--color[=always|auto|never]Use syntax coloring (auto activates it only if the outputis a tty).pop [-afRqv] [--refresh] [num|patch]Remove patch(es) from the stack of applied patches.Withoutoptions, the topmost patch is removed.When a number isspecified, remove the specified number of patches.When apatch name is specified, remove patches until the specifiedpatch end up on top of the stack.Patch names may includethe patches/ prefix, which means that filename completion canbe used.-aRemove all applied patches.-fForce remove. The state before the patch(es) were appliedwill be restored from backup files.-RAlways verify if the patch removes cleanly; don't rely ontimestamp checks.-qQuiet operation.-vVerbose operation.--refreshAutomatically refresh every patch before it getsunapplied.previous [patch]Print the name of the previous patch before the specified ortopmost patch in the series file.push [-afqvm] [--fuzz=N] [--merge[=merge|diff3]] [--leave-rejects] [--color[=always|auto|never]] [--refresh] [num|patch]Apply patch(es) from the series file.Without options, thenext patch in the series file is applied.When a number isspecified, apply the specified number of patches.When apatch name is specified, apply all patches up to andincluding the specified patch.Patch names may include thepatches/ prefix, which means that filename completion can beused.-aApply all patches in the series file.-qQuiet operation.-fForce apply, even if the patch has rejects.-vVerbose operation.--fuzz=NSet the maximum fuzz factor (default: 2).-m, --merge[=merge|diff3]Merge the patch file into the original files (seepatch(1)).--leave-rejectsLeave around the reject files patch produced, even if thepatch is not actually applied.--color[=always|auto|never]Use syntax coloring (auto activates it only if the outputis a tty).--refreshAutomatically refresh every patch after it wassuccessfully applied.refresh [-p n|-p ab] [-u|-U num|-c|-C num] [-z[new_name]] [-f][--no-timestamps] [--no-index] [--diffstat] [--sort] [--backup][--strip-trailing-whitespace] [patch]Refreshes the specified patch, or the topmost patch bydefault.Documentation that comes before the actual patch inthe patch file is retained.It is possible to refresh patches that are not on top.Ifany patches on top of the patch to refresh modify the samefiles, the script aborts by default.Patches can still berefreshed with -f.In that case this script will print awarning for each shadowed file, changes by more recentpatches will be ignored, and only changes in files that havenot been modified by any more recent patches will end up inthe specified patch.-p nCreate a -p n style patch (-p0 or -p1 supported).-p abCreate a -p1 style patch, but use a/file and b/file asthe original and new filenames instead of the defaultdir.orig/file and dir/file names.-u, -U num, -c, -C numCreate a unified diff (-u, -U) with num lines of context.Create a context diff (-c, -C) with num lines of context.The number of context lines defaults to 3.-z[new_name]Create a new patch containing the changes instead ofrefreshing the topmost patch. If no new name isspecified, `-2' is added to the original patch name, etc.(See the fork command.)--no-timestampsDo not include file timestamps in patch headers.--no-indexDo not output Index: lines.--diffstatAdd a diffstat section to the patch header, or replacethe existing diffstat section.-fEnforce refreshing of a patch that is not on top.--backupCreate a backup copy of the old version of a patch aspatch~.--sortSort files by their name instead of preserving theoriginal order.--strip-trailing-whitespaceStrip trailing whitespace at the end of lines.remove [-P patch] {file} ...Remove one or more files from the topmost or named patch.Files that are modified by patches on top of the specifiedpatch cannot be removed.-P patchRemove named files from the named patch.rename [-P patch] new_nameRename the topmost or named patch.-P patchPatch to rename.revert [-P patch] {file} ...Revert uncommitted changes to the topmost or named patch forthe specified file(s): after the revert, 'quilt diff -z' willshow no differences for those files. Changes to files thatare modified by patches on top of the specified patch cannotbe reverted.-P patchRevert changes in the named patch.series [--color[=always|auto|never]] [-v]Print the names of all patches in the series file.--color[=always|auto|never]Use syntax coloring (auto activates it only if the outputis a tty).-vVerbose, more user friendly output.setup [-d path-prefix] [-v] [--sourcedir dir] [--fuzz=N][--slow|--fast] {specfile|seriesfile}Initializes a source tree from an rpm spec file or a quiltseries file.-dOptional path prefix for the resulting source tree.--sourcedirDirectory that contains the package sources. Defaults to`.'.-vVerbose debug output.--fuzz=NSet the maximum fuzz factor (needs rpm 4.6 or later).--slowUse the original, slow method to process the spec file.In this mode, rpmbuild generates a working tree in atemporary directory while all its actions are recorded,and then everything is replayed from scratch in thetarget directory.--fastUse the new, faster method to process the spec file. Inthis mode, rpmbuild is told to generate a working treedirectly in the target directory. This is the default(since quilt version 0.67).The setup command is only guaranteed to work properly onspec files where applying all the patches is the lastthing done in the %prep section. This is a designlimitation due to the fact that quilt can only operate onpatches. If other commands in the %prep section modifythe patched files, this must happen first, otherwise youwon't be able to push the patch series.snapshot [-d]Take a snapshot of the current working state.After takingthe snapshot, the tree can be modified in the usual ways,including pushing and popping patches.A diff against thetree at the moment of the snapshot can be generated with`quilt diff --snapshot'.-dOnly remove current snapshot.topPrint the name of the topmost patch on the current stack ofapplied patches.unapplied [patch]Print a list of patches that are not applied, or all patchesthat follow the specified patch in the series file.upgradeUpgrade the meta-data in a working tree from an old versionof quilt to the current version. This command is only neededwhen the quilt meta-data format has changed, and the workingtree still contains old-format meta-data. In that case, quiltwill request to run `quilt upgrade'.",
        "name": "quilt - manage a series of patches",
        "section": 1
    },
    {
        "command": "quota",
        "description": "quota displays users' disk usage and limits.By default only theuser quotas are printed. By default space usage and limits areshown in kbytes (and are named blocks for historical reasons).quota reports the quotas of all the filesystems listed in/etc/mtab.For filesystems that are NFS-mounted a call to therpc.rquotad on the server machine is performed to get theinformation.",
        "name": "quota - display disk usage and limits",
        "section": 1
    },
    {
        "command": "quotasync",
        "description": "quotasync flushes file system usage and limits from kernel memoryto quota files stored in the file system. By default only theuser quotas are synchronized.This tool can be useful if you want to display accurate quotas bytools that parse quota files, like repquota(8).",
        "name": "quotasync - synchronize in-kernel file system usage and limits todisk format",
        "section": 1
    },
    {
        "command": "ranlib",
        "description": "ranlib generates an index to the contents of an archive andstores it in the archive.The index lists each symbol defined bya member of an archive that is a relocatable object file.You may use nm -s or nm --print-armap to list this index.An archive with such an index speeds up linking to the libraryand allows routines in the library to call each other withoutregard to their placement in the archive.The GNU ranlib program is another form of GNU ar; running ranlibis completely equivalent to executing ar -s.",
        "name": "ranlib - generate an index to an archive",
        "section": 1
    },
    {
        "command": "rcopy",
        "description": "Uses sockets over RDMA interface to copy a source file to thespecified destination.",
        "name": "rcopy - simple file copy over RDMA.",
        "section": 1
    },
    {
        "command": "rdma_client",
        "description": "Uses synchronous librdmam calls to establish an RDMA connectionbetween two nodes.This example is intended to provide a verysimple coding example of how to use RDMA.",
        "name": "rdma_client - simple RDMA CM connection and ping-pong test.",
        "section": 1
    },
    {
        "command": "rdma_server",
        "description": "Uses synchronous librdmam calls to establish an RDMA connectionsbetween two nodes.This example is intended to provide a verysimple coding example of how to use RDMA.",
        "name": "rdma_server - simple RDMA CM connection and ping-pong test.",
        "section": 1
    },
    {
        "command": "rdma_xclient",
        "description": "Uses synchronous librdmam calls to establish an RDMA connectionbetween two nodes.This example is intended to provide a verysimple coding example of how to use RDMA.",
        "name": "rdma_xclient - RDMA CM communication client test program",
        "section": 1
    },
    {
        "command": "rdma_xserver",
        "description": "Uses the librdmacm to establish various forms of communicationand exchange data.",
        "name": "rdma_xserver - RDMA CM communication server test program",
        "section": 1
    },
    {
        "command": "readelf",
        "description": "readelf displays information about one or more ELF format objectfiles.The options control what particular information todisplay.elffile... are the object files to be examined.32-bit and64-bit ELF files are supported, as are archives containing ELFfiles.This program performs a similar function to objdump but it goesinto more detail and it exists independently of the BFD library,so if there is a bug in BFD then readelf will not be affected.",
        "name": "readelf - display information about ELF files",
        "section": 1
    },
    {
        "command": "readlink",
        "description": "Note realpath(1) is the preferred command to use forcanonicalization functionality.Print value of a symbolic link or canonical file name-f, --canonicalizecanonicalize by following every symlink in every componentof the given name recursively; all but the last componentmust exist-e, --canonicalize-existingcanonicalize by following every symlink in every componentof the given name recursively, all components must exist-m, --canonicalize-missingcanonicalize by following every symlink in every componentof the given name recursively, without requirements oncomponents existence-n, --no-newlinedo not output the trailing delimiter-q, --quiet-s, --silentsuppress most error messages (on by default)-v, --verbosereport error messages-z, --zeroend each output line with NUL, not newline--help display this help and exit--versionoutput version information and exit",
        "name": "readlink - print resolved symbolic links or canonical file names",
        "section": 1
    },
    {
        "command": "realpath",
        "description": "Print the resolved absolute file name; all but the last componentmust exist-e, --canonicalize-existingall components of the path must exist-m, --canonicalize-missingno path components need exist or be a directory-L, --logicalresolve '..' components before symlinks-P, --physicalresolve symlinks as encountered (default)-q, --quietsuppress most error messages--relative-to=DIRprint the resolved path relative to DIR--relative-base=DIRprint absolute paths unless paths below DIR-s, --strip, --no-symlinksdon't expand symlinks-z, --zeroend each output line with NUL, not newline--help display this help and exit--versionoutput version information and exit",
        "name": "realpath - print the resolved path",
        "section": 1
    },
    {
        "command": "recode-sr-latin",
        "description": "Recode Serbian text from Cyrillic to Latin script.The inputtext is read from standard input.The converted text is outputto standard output.Informative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit",
        "name": "recode-sr-latin - convert Serbian text from Cyrillic to Latinscript",
        "section": 1
    },
    {
        "command": "refer",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "rename",
        "description": "rename will rename the specified files by replacing the firstoccurrence of expression in their name by replacement.",
        "name": "rename - rename files",
        "section": 1
    },
    {
        "command": "renice",
        "description": "renice alters the scheduling priority of one or more runningprocesses. The first argument is the priority value to be used.The other arguments are interpreted as process IDs (by default),process group IDs, user IDs, or user names. renice'ing a processgroup causes all processes in the process group to have theirscheduling priority altered. renice'ing a user causes allprocesses owned by the user to have their scheduling priorityaltered.If no -n, --priority or --relative option is used, then thepriority is set as absolute.",
        "name": "renice - alter priority of running processes",
        "section": 1
    },
    {
        "command": "replace",
        "description": "The replace utility program changes strings in place in files oron the standard input.Invoke replace in one of the following ways:shell> replace from to [from to] ... -- file_name [file_name] ...shell> replace from to [from to] ... < file_namefrom represents a string to look for and to represents itsreplacement. There can be one or more pairs of strings.Use the -- option to indicate where the string-replacement listends and the file names begin. In this case, any file named onthe command line is modified in place, so you may want to make acopy of the original before converting it.replace prints amessage indicating which of the input files it actually modifies.If the -- option is not given, replace reads the standard inputand writes to the standard output.replace uses a finite state machine to match longer stringsfirst. It can be used to swap strings. For example, the followingcommand swaps a and b in the given files, file1 and file2:shell> replace a b b a -- file1 file2 ...The replace program is used by msql2mysql. See msql2mysql(1).replace supports the following options.\u2022-?, -IDisplay a help message and exit.\u2022-#debug_optionsEnable debugging.\u2022-sSilent mode. Print less information what the program does.\u2022-vVerbose mode. Print more information about what the programdoes.\u2022-VDisplay version information and exit.",
        "name": "replace - a string-replacement utility",
        "section": 1
    },
    {
        "command": "repo-graph",
        "description": "repo-graph is a program that generates a full package dependencylist from a yum repository and outputs it in dot format.",
        "name": "repo-graph - output a full package dependency graph in dot format",
        "section": 1
    },
    {
        "command": "repo-rss",
        "description": "repo-rss is a program for generating RSS feeds for one or moreYum repositories.",
        "name": "repo-rss - generates an RSS feed from one or more Yumrepositories",
        "section": 1
    },
    {
        "command": "repoclosure",
        "description": "repoclosure is a program that reads package metadata from one ormore yum repositories, checks all dependencies, and displays alist of packages with unresolved dependencies.",
        "name": "repoclosure - display a list of unresolved dependencies for a yumrepository",
        "section": 1
    },
    {
        "command": "repodiff",
        "description": "repodiff is a program which will list differences between twosets of repositories.Note that by default only source packagesare compared.",
        "name": "repodiff - list differences between two or more Yum repositories",
        "section": 1
    },
    {
        "command": "repomanage",
        "description": "repomanage is a program to manage a directory of RPM packages. Itdisplays a list of the newest or oldest packages in a directoryfor easy piping to xargs or similar programs.",
        "name": "repomanage - list the newest or oldest RPM packages in adirectory",
        "section": 1
    },
    {
        "command": "repoquery",
        "description": "repoquery is a program for querying information from YUMrepositories similarly to rpm queries.",
        "name": "repoquery - query information from Yum repositories",
        "section": 1
    },
    {
        "command": "reposync",
        "description": "reposync is used to synchronize a remote yum repository to alocal directory, using yum to retrieve the packages.",
        "name": "reposync - synchronize yum repositories to a local directory",
        "section": 1
    },
    {
        "command": "repotrack",
        "description": "repotrack is a program for keeping track of a particular packageand its dependencies. It will download one or more packages andall dependencies.",
        "name": "repotrack - track a package and its dependencies and downloadthem",
        "section": 1
    },
    {
        "command": "reset",
        "description": "The @TPUT@ utility uses the terminfo database to make the valuesof terminal-dependent capabilities and information available tothe shell (see sh(1)), to initialize or reset the terminal, orreturn the long name of the requested terminal type.The resultdepends upon the capability's type:string@TPUT@ writes the string to the standard output.Notrailing newline is supplied.integer@TPUT@ writes the decimal value to the standard output,with a trailing newline.boolean@TPUT@ simply sets the exit code (0 for TRUE if theterminal has the capability, 1 for FALSE if it does not),and writes nothing to the standard output.Before using a value returned on the standard output, theapplication should test the exit code (e.g., $?, see sh(1)) to besure it is 0.(See the EXIT CODES and DIAGNOSTICS sections.)For a complete list of capabilities and the capname associatedwith each, see terminfo(5).Options-Sallows more than one capability per invocation of @TPUT@.The capabilities must be passed to @TPUT@ from thestandard input instead of from the command line (seeexample).Only one capname is allowed per line.The -Soption changes the meaning of the 0 and 1 boolean andstring exit codes (see the EXIT CODES section).Because some capabilities may use string parameters ratherthan numbers, @TPUT@ uses a table and the presence ofparameters in its input to decide whether to usetparm(3X), and how to interpret the parameters.-Ttype indicates the type of terminal.Normally this option isunnecessary, because the default is taken from theenvironment variable TERM.If -T is specified, then theshell variables LINES and COLUMNS will also be ignored.-Vreports the version of ncurses which was used in thisprogram, and exits.-xdo not attempt to clear the terminal's scrollback bufferusing the extended \u201cE3\u201d capability.CommandsA few commands (init, reset and longname) are special; they aredefined by the @TPUT@ program.The others are the names ofcapabilities from the terminal database (see terminfo(5) for alist).Although init and reset resemble capability names, @TPUT@uses several capabilities to perform these special functions.capnameindicates the capability from the terminal database.If the capability is a string that takes parameters, thearguments following the capability will be used asparameters for the string.Most parameters are numbers.Only a few terminalcapabilities require string parameters; @TPUT@ uses atable to decide which to pass as strings.Normally @TPUT@uses tparm(3X) to perform the substitution.If noparameters are given for the capability, @TPUT@ writes thestring without performing the substitution.initIf the terminal database is present and an entry for theuser's terminal exists (see -Ttype, above), the followingwill occur:(1)first, @TPUT@ retrieves the current terminal modesettings for your terminal.It does this bysuccessively testing\u2022the standard error,\u2022standard output,\u2022standard input and\u2022ultimately \u201c/dev/tty\u201dto obtain terminal settings.Having retrieved thesesettings, @TPUT@ remembers which file descriptor touse when updating settings.(2)if the window size cannot be obtained from theoperating system, but the terminal description (orenvironment, e.g., LINES and COLUMNS variablesspecify this), update the operating system's notionof the window size.(3)the terminal modes will be updated:\u2022any delays (e.g., newline) specified in the entrywill be set in the tty driver,\u2022tabs expansion will be turned on or off accordingto the specification in the entry, and\u2022if tabs are not expanded, standard tabs will beset (every 8 spaces).(4)if present, the terminal's initialization stringswill be output as detailed in the terminfo(5) sectionon Tabs and Initialization,(5)output is flushed.If an entry does not contain the information needed forany of these activities, that activity will silently beskipped.resetThis is similar to init, with two differences:(1)before any other initialization, the terminal modeswill be reset to a \u201csane\u201d state:\u2022set cooked and echo modes,\u2022turn off cbreak and raw modes,\u2022turn on newline translation and\u2022reset any unset special characters to theirdefault values(2)Instead of putting out initialization strings, theterminal's reset strings will be output if present(rs1, rs2, rs3, rf).If the reset strings are notpresent, but initialization strings are, theinitialization strings will be output.Otherwise, reset acts identically to init.longnameIf the terminal database is present and an entry for theuser's terminal exists (see -Ttype above), then the longname of the terminal will be put out.The long name isthe last name in the first line of the terminal'sdescription in the terminfo database [see term(5)].Aliases@TPUT@ handles the clear, init and reset commands specially: itallows for the possibility that it is invoked by a link withthose names.If @TPUT@ is invoked by a link named reset, this has the sameeffect as @TPUT@ reset.The @TSET@(1) utility also treats a linknamed reset specially.Before ncurses 6.1, the two utilities were different from eachother:\u2022@TSET@ utility reset the terminal modes and specialcharacters (not done with @TPUT@).\u2022On the other hand, @TSET@'s repertoire of terminalcapabilities for resetting the terminal was more limited,i.e., only reset_1string, reset_2string and reset_file incontrast to the tab-stops and margins which are set by thisutility.\u2022The reset program is usually an alias for @TSET@, because ofthis difference with resetting terminal modes and specialcharacters.With the changes made for ncurses 6.1, the reset feature of thetwo programs is (mostly) the same.A few differences remain:\u2022The @TSET@ program waits one second when resetting, in caseit happens to be a hardware terminal.\u2022The two programs write the terminal initialization strings todifferent streams (i.e., the standard error for @TSET@ andthe standard output for @TPUT@).Note: although these programs write to different streams,redirecting their output to a file will capture only part oftheir actions.The changes to the terminal modes are notaffected by redirecting the output.If @TPUT@ is invoked by a link named init, this has the sameeffect as @TPUT@ init.Again, you are less likely to use thatlink because another program named init has a more well-established use.Terminal SizeBesides the special commands (e.g., clear), @TPUT@ treats certainterminfo capabilities specially: lines and cols.@TPUT@ callssetupterm(3X) to obtain the terminal size:\u2022first, it gets the size from the terminal database (whichgenerally is not provided for terminal emulators which do nothave a fixed window size)\u2022then it asks the operating system for the terminal's size(which generally works, unless connecting via a serial linewhich does not support NAWS: negotiations about window size).\u2022finally, it inspects the environment variables LINES andCOLUMNS which may override the terminal size.If the -T option is given @TPUT@ ignores the environmentvariables by calling use_tioctl(TRUE), relying upon the operatingsystem (or finally, the terminal database).",
        "name": "@TPUT@, reset - initialize a terminal or query terminfo database",
        "section": 1
    },
    {
        "command": "resolvconf",
        "description": "resolvectl may be used to resolve domain names, IPv4 and IPv6addresses, DNS resource records and services with thesystemd-resolved.service(8) resolver service. By default, thespecified list of parameters will be resolved as hostnames,retrieving their IPv4 and IPv6 addresses. If the parametersspecified are formatted as IPv4 or IPv6 addresses the reverseoperation is done, and a hostname is retrieved for the specifiedaddresses.The program's output contains information about the protocol usedfor the look-up and on which network interface the data wasdiscovered. It also contains information on whether theinformation could be authenticated. All data for which localDNSSEC validation succeeds is considered authenticated. Moreoverall data originating from local, trusted sources is also reportedauthenticated, including resolution of the local host name, the\"localhost\" hostname or all data from /etc/hosts.",
        "name": "resolvectl, resolvconf - Resolve domain names, IPV4 and IPv6addresses, DNS resource records, and services; introspect andreconfigure the DNS resolver",
        "section": 1
    },
    {
        "command": "resolve_stack_dump",
        "description": "resolve_stack_dump resolves a numeric stack dump to symbols.Invoke resolve_stack_dump like this:shell> resolve_stack_dump [options] symbols_file [numeric_dump_file]The symbols file should include the output from the nm--numeric-sort mysqld command. The numeric dump file shouldcontain a numeric stack track from mysqld. If no numeric dumpfile is named on the command line, the stack trace is read fromthe standard input.resolve_stack_dump supports the following options.\u2022--help, -hDisplay a help message and exit.\u2022--numeric-dump-file=file_name, -n file_nameRead the stack trace from the given file.\u2022--symbols-file=file_name, -s file_nameUse the given symbols file.\u2022--version, -VDisplay version information and exit.",
        "name": "resolve_stack_dump - resolve numeric stack trace dump to symbols",
        "section": 1
    },
    {
        "command": "resolvectl",
        "description": "resolvectl may be used to resolve domain names, IPv4 and IPv6addresses, DNS resource records and services with thesystemd-resolved.service(8) resolver service. By default, thespecified list of parameters will be resolved as hostnames,retrieving their IPv4 and IPv6 addresses. If the parametersspecified are formatted as IPv4 or IPv6 addresses the reverseoperation is done, and a hostname is retrieved for the specifiedaddresses.The program's output contains information about the protocol usedfor the look-up and on which network interface the data wasdiscovered. It also contains information on whether theinformation could be authenticated. All data for which localDNSSEC validation succeeds is considered authenticated. Moreoverall data originating from local, trusted sources is also reportedauthenticated, including resolution of the local host name, the\"localhost\" hostname or all data from /etc/hosts.",
        "name": "resolvectl, resolvconf - Resolve domain names, IPV4 and IPv6addresses, DNS resource records, and services; introspect andreconfigure the DNS resolver",
        "section": 1
    },
    {
        "command": "resolveip",
        "description": "The resolveip utility resolves host names to IP addresses andvice versa.Invoke resolveip like this:shell> resolveip [options] {host_name|ip-addr} ...resolveip supports the following options.\u2022--help, --info, -?, -IDisplay a help message and exit.\u2022--silent, -sSilent mode. Produce less output.\u2022--version, -VDisplay version information and exit.",
        "name": "resolveip - resolve host name to IP address or vice versa",
        "section": 1
    },
    {
        "command": "rev",
        "description": "The rev utility copies the specified files to standard output,reversing the order of characters in every line. If no files arespecified, standard input is read.This utility is a line-oriented tool and it uses in-memoryallocated buffer for a whole wide-char line. If the input file ishuge and without line breaks then allocating the memory for thefile may be unsuccessful.",
        "name": "rev - reverse lines characterwise",
        "section": 1
    },
    {
        "command": "riostream",
        "description": "Uses the streaming over RDMA protocol (rsocket) to connect andexchange data between a client and server application.",
        "name": "riostream - zero-copy streaming over RDMA ping-pong test.",
        "section": 1
    },
    {
        "command": "rm",
        "description": "This manual page documents the GNU version of rm.rm removeseach specified file.By default, it does not remove directories.If the -I or --interactive=once option is given, and there aremore than three files or the -r, -R, or --recursive are given,then rm prompts the user for whether to proceed with the entireoperation.If the response is not affirmative, the entirecommand is aborted.Otherwise, if a file is unwritable, standard input is a terminal,and the -f or --force option is not given, or the -i or--interactive=always option is given, rm prompts the user forwhether to remove the file.If the response is not affirmative,the file is skipped.",
        "name": "rm - remove files or directories",
        "section": 1
    },
    {
        "command": "rmdir",
        "description": "Remove the DIRECTORY(ies), if they are empty.--ignore-fail-on-non-emptyignore each failure to remove a non-empty directory-p, --parentsremove DIRECTORY and its ancestors; e.g., 'rmdir -p a/b'is similar to 'rmdir a/b a'-v, --verboseoutput a diagnostic for every directory processed--help display this help and exit--versionoutput version information and exit",
        "name": "rmdir - remove empty directories",
        "section": 1
    },
    {
        "command": "rping",
        "description": "Establishes a reliable RDMA connection between two nodes usingthe librdmacm, optionally performs RDMA transfers between thenodes, then disconnects.",
        "name": "rping - RDMA CM connection and RDMA ping-pong test.",
        "section": 1
    },
    {
        "command": "rrsync",
        "description": "A user's ssh login can be restricted to only allow the running ofan rsync transfer in one of two easy ways:oforcing the running of the rrsync scriptoforcing the running of an rsync daemon-over-ssh command.Both of these setups use a feature of ssh that allows a commandto be forced to run instead of an interactive shell.However, ifthe user's home shell is bash, please see BASH SECURITY ISSUE fora potential issue.To use the rrsync script, edit the user's ~/.ssh/authorized_keysfile and add a prefix like one of the following (followed by aspace) in front of each ssh-key line that should be restricted:command=\"rrsync DIR\"command=\"rrsync -ro DIR\"command=\"rrsync -munge -no-del DIR\"Then, ensure that the rrsync script has your desired optionrestrictions. You may want to copy the script to a local bin dirwith a unique name if you want to have multiple configurations.One or more rrsync options can be specified prior to the DIR ifyou want to further restrict the transfer.To use an rsync daemon setup, edit the user's~/.ssh/authorized_keys file and add a prefix like one of thefollowing (followed by a space) in front of each ssh-key linethat should be restricted:command=\"rsync --server --daemon .\"command=\"rsync --server --daemon --config=/PATH/TO/rsyncd.conf .\"Then, ensure that the rsyncd.conf file is created with one ormore module names with the appropriate path and optionrestrictions.If rsync's --config option is omitted, it defaultsto ~/rsyncd.conf.See the rsyncd.conf(5) manpage for details ofhow to configure an rsync daemon.When using rrsync, there can be just one restricted dir perauthorized key.A daemon setup, on the other hand, allowsmultiple module names inside the config file, each one with itsown path setting.The remainder of this manpage is dedicated to using the rrsyncscript.",
        "name": "rrsync - a script to setup restricted rsync users via ssh logins",
        "section": 1
    },
    {
        "command": "rstream",
        "description": "Uses the streaming over RDMA protocol (rsocket) to connect andexchange data between a client and server application.",
        "name": "rstream - streaming over RDMA ping-pong test.",
        "section": 1
    },
    {
        "command": "rsync",
        "description": "Rsync is a fast and extraordinarily versatile file copying tool.It can copy locally, to/from another host over any remote shell,or to/from a remote rsync daemon.It offers a large number ofoptions that control every aspect of its behavior and permit veryflexible specification of the set of files to be copied.It isfamous for its delta-transfer algorithm, which reduces the amountof data sent over the network by sending only the differencesbetween the source files and the existing files in thedestination.Rsync is widely used for backups and mirroring andas an improved copy command for everyday use.Rsync finds files that need to be transferred using a \"quickcheck\" algorithm (by default) that looks for files that havechanged in size or in last-modified time.Any changes in theother preserved attributes (as requested by options) are made onthe destination file directly when the quick check indicates thatthe file's data does not need to be updated.Some of the additional features of rsync are:osupport for copying links, devices, owners, groups, andpermissionsoexclude and exclude-from options similar to GNU taroa CVS exclude mode for ignoring the same files that CVSwould ignoreocan use any transparent remote shell, including ssh or rshodoes not require super-user privilegesopipelining of file transfers to minimize latency costsosupport for anonymous or authenticated rsync daemons(ideal for mirroring)",
        "name": "rsync - a fast, versatile, remote (and local) file-copying tool",
        "section": 1
    },
    {
        "command": "rsync-ssl",
        "description": "The rsync-ssl script helps you to run an rsync copy to/from anrsync daemon that requires ssl connections.The script requires that you specify an rsync-daemon arg in thestyle of either hostname:: (with 2 colons) or rsync://hostname/.The default port used for connecting is 874 (one higher than thenormal 873) unless overridden in the environment.You canspecify an overriding port via --port or by including it in thenormal spot in the URL format, though both of those require yourrsync version to be at least 3.2.0.",
        "name": "rsync-ssl - a helper script for connecting to an ssl rsync daemon",
        "section": 1
    },
    {
        "command": "runcon",
        "description": "Run COMMAND with completely-specified CONTEXT, or with current ortransitioned security context modified by one or more of LEVEL,ROLE, TYPE, and USER.If none of -c, -t, -u, -r, or -l, is specified, the firstargument is used as the complete context.Any additionalarguments after COMMAND are interpreted as arguments to thecommand.Note that only carefully-chosen contexts are likely tosuccessfully run.Run a program in a different SELinux security context.Withneither CONTEXT nor COMMAND, print the current security context.Mandatory arguments to long options are mandatory for shortoptions too.CONTEXTComplete security context-c, --computecompute process transition context before modifying-t, --type=TYPEtype (for same role as parent)-u, --user=USERuser identity-r, --role=ROLErole-l, --range=RANGElevelrange--help display this help and exit--versionoutput version information and exitExit status:125if the runcon command itself fails126if COMMAND is found but cannot be invoked127if COMMAND cannot be found-the exit status of COMMAND otherwise",
        "name": "runcon - run command with specified security context",
        "section": 1
    },
    {
        "command": "runscript",
        "description": "runscript is a simple script interpreter that can be called fromwithin the minicom communications program to automate tasks likelogging in to a Unix system or your favorite BBS.",
        "name": "runscript - script interpreter for minicom",
        "section": 1
    },
    {
        "command": "runuser",
        "description": "runuser can be used to run commands with a substitute user andgroup ID. If the option -u is not given, runuser falls back tosu-compatible semantics and a shell is executed. The differencebetween the commands runuser and su is that runuser does not askfor a password (because it may be executed by the root user only)and it uses a different PAM configuration. The command runuserdoes not have to be installed with set-user-ID permissions.If the PAM session is not required, then the recommended solutionis to use the setpriv(1) command.When called without arguments, runuser defaults to running aninteractive shell as root.For backward compatibility, runuser defaults to not changing thecurrent directory and to setting only the environment variablesHOME and SHELL (plus USER and LOGNAME if the target user is notroot). This version of runuser uses PAM for session management.Note that runuser in all cases use PAM (pam_getenvlist()) to dothe final environment modification. Command-line options such as--login and --preserve-environment affect the environment beforeit is modified by PAM.Since version 2.38 runuser resets process resource limitsRLIMIT_NICE, RLIMIT_RTPRIO, RLIMIT_FSIZE, RLIMIT_AS andRLIMIT_NOFILE.",
        "name": "runuser - run a command with substitute user and group ID",
        "section": 1
    },
    {
        "command": "sadf",
        "description": "The sadf command is used for displaying the contents of datafiles created by the sar(1) command. But unlike sar, sadf canwrite its data in many different formats (CSV, XML, etc.)Thedefault format is one that can easily be handled by patternprocessing commands like awk (see option -p). The sadf commandcan also be used to draw graphs for the various activitiescollected by sar and display them as SVG (Scalable VectorGraphics) graphics in your web browser (see option -g).The sadf command extracts and writes to standard output recordssaved in the datafile file. This file must have been created by aversion of sar which is compatible with that of sadf. If datafileis omitted, sadf uses the standard system activity daily datafile.It is also possible to enter -1, -2 etc. as an argument tosadf to display data of that days ago. For example, -1 will pointat the standard system activity file of yesterday.The standard system activity daily data file is named saDD orsaYYYYMMDD, where YYYY stands for the current year, MM for thecurrent month and DD for the current day.sadf will look for themost recent of saDD and saYYYYMMDD, and use it. By default it islocated in the /var/log/sa directory. Yet it is possible tospecify an alternate location for it: If datafile is a directory(instead of a plain file) then it will be considered as thedirectory where the standard system activity daily data file islocated.The interval and count parameters are used to tell sadf to selectcount records at interval seconds apart. If the count parameteris not set, then all the records saved in the data file will bedisplayed.All the activity flags of sar may be entered on the command lineto indicate which activities are to be reported. Beforespecifying them, put a pair of dashes (--) on the command line inorder not to confuse the flags with those of sadf.Notspecifying any flags selects only CPU activity.",
        "name": "sadf - Display data collected by sar in multiple formats.",
        "section": 1
    },
    {
        "command": "sar",
        "description": "The sar command writes to standard output the contents ofselected cumulative activity counters in the operating system.The accounting system, based on the values in the count andinterval parameters, writes information the specified number oftimes spaced at the specified intervals in seconds.If theinterval parameter is set to zero, the sar command displays theaverage statistics for the time since the system was started. Ifthe interval parameter is specified without the count parameter,then reports are generated continuously.The collected data canalso be saved in the file specified by the -o filename flag, inaddition to being displayed onto the screen. If filename isomitted, sar uses the standard system activity daily data file(see below).By default all the data available from the kernelare saved in the data file.The sar command extracts and writes to standard output recordspreviously saved in a file. This file can be either the onespecified by the -f flag or, by default, the standard systemactivity daily data file.It is also possible to enter -1, -2etc. as an argument to sar to display data of that days ago. Forexample, -1 will point at the standard system activity file ofyesterday.Standard system activity daily data files are named saDD orsaYYYYMMDD, where YYYY stands for the current year, MM for thecurrent month and DD for the current day. They are the defaultfiles used by sar only when no filename has been explicitlyspecified.When used to write data to files (with its option-o), sar will use saYYYYMMDD if option -D has also beenspecified, else it will use saDD.When used to display therecords previously saved in a file, sar will look for the mostrecent of saDD and saYYYYMMDD, and use it.Standard system activity daily data files are located in the/var/log/sa directory by default. Yet it is possible to specifyan alternate location for them: If a directory (instead of aplain file) is used with options -f or -o then it will beconsidered as the directory containing the data files.Without the -P flag, the sar command reports system-wide (globalamong all processors) statistics, which are calculated asaverages for values expressed as percentages, and as sumsotherwise. If the -P flag is given, the sar command reportsactivity which relates to the specified processor or processors.If -P ALL is given, the sar command reports statistics for eachindividual processor and global statistics among all processors.Offline processors are not displayed.You can select information about specific system activities usingflags. Not specifying any flags selects only CPU activity.Specifying the -A flag selects all possible activities.The default version of the sar command (CPU utilization report)might be one of the first facilities the user runs to beginsystem activity investigation, because it monitors major systemresources. If CPU utilization is near 100 percent (user + nice +system), the workload sampled is CPU-bound.If multiple samples and multiple reports are desired, it isconvenient to specify an output file for the sar command. Run thesar command as a background process. The syntax for this is:sar -o datafile interval count >/dev/null 2>&1 &All data are captured in binary form and saved to a file(datafile).The data can then be selectively displayed with thesar command using the -f option. Set the interval and countparameters to select count records at interval second intervals.If the count parameter is not set, all the records saved in thefile will be selected.Collection of data in this manner isuseful to characterize system usage over a period of time anddetermine peak usage hours.Note: The sar command only reports on local activities.",
        "name": "sar - Collect, report, or save system activity information.",
        "section": 1
    },
    {
        "command": "sar2pcp",
        "description": "sar2pcp is intended to read a binary System Activity Reporting(sar) data file as created by sadc(1) (infile) and translate thisinto a Performance Co-Pilot (PCP) archive with the basenameoutfile.However, if infile has the suffix ``.xml'', then it will beconsidered already in XML format and sar2pcp will operatedirectly on it.The resultant PCP achive may be used with all the PCP clienttools to graph subsets of the data using pmchart(1), perform datareduction and reporting, filter with the PCP inference enginepmie(1), etc.A series of physical files will be created with the prefixoutfile.These are outfile.0 (the performance data),outfile.meta (the metadata that describes the performance data)and outfile.index (a temporal index to improve efficiency ofreplay operations for the archive).If any of these files existsalready, then sar2pcp will not overwrite them and will exit withan error message of the form__pmLogNewFile: ``blah.0'' already exists, not over-writtensar2pcp is a Perl script that uses the PCP::LogImport Perlwrapper around the PCP libpcp_import library, and as such couldbe used as an example to develop new tools to import other typesof performance data and create PCP archives.A Python wrappermodule is also available.",
        "name": "sar2pcp - import sar data and create a PCP archive",
        "section": 1
    },
    {
        "command": "scalar",
        "description": "Scalar is a repository management tool that optimizes Git for usein large repositories. Scalar improves performance by configuringadvanced Git settings, maintaining repositories in thebackground, and helping to reduce data sent across the network.An important Scalar concept is the enlistment: this is thetop-level directory of the project. It usually contains thesubdirectory src/ which is a Git worktree. This encourages theseparation between tracked files (inside src/) and untrackedfiles, such as build artifacts (outside src/). When registeringan existing Git worktree with Scalar whose name is not src, theenlistment will be identical to the worktree.The scalar command implements various subcommands, and differentoptions depending on the subcommand. With the exception of clone,list and reconfigure --all, all subcommands expect to be run inan enlistment.The following options can be specified before the subcommand:-C <directory>Before running the subcommand, change the working directory.This option imitates the same option of git(1).-c <key>=<value>For the duration of running the specified subcommand,configure this setting. This option imitates the same optionof git(1).",
        "name": "scalar - A tool for managing large Git repositories",
        "section": 1
    },
    {
        "command": "scmp_sys_resolver",
        "description": "This command resolves both system call names and numbers withrespect to the given architecture supplied in the optional ARCHargument.If the architecture is not supplied on the commandline then the native architecture is used.If the \"-t\" argumentis specified along with a system call name, then the system callwill be translated as necessary for the given architecture.The\"-t\" argument has no effect if a system call number is specified.In some combinations of architecture and system call, a negativesystem call number will be displayed.A negative system callnumber indicates that the system call is not defined for thegiven architecture and is treated in a special manner bylibseccomp depending on the operation.-a ARCHThe architecture to use for resolving the system call.Valid ARCH values are \"x86\", \"x86_64\", \"x32\", \"arm\",\"aarch64\", \"loongarch64\", \"m68k\", \"mips\", \"mipsel\",\"mips64\", \"mipsel64\", \"mips64n32\", \"mipsel64n32\",\"parisc\", \"parisc64\", \"ppc\", \"ppc64\", \"ppc64le\", \"s390\",\"s390x\", \"sheb\" and \"sh\".-tIf necessary, translate the system call name to the propersystem call number, even if the system call name isdifferent, e.g. socket(2) on x86.-hA simple one-line usage display.",
        "name": "scmp_sys_resolver - Resolve system calls",
        "section": 1
    },
    {
        "command": "scp",
        "description": "scp copies files between hosts on a network.scp uses the SFTP protocol over a ssh(1) connection for datatransfer, and uses the same authentication and provides the samesecurity as a login session.scp will ask for passwords or passphrases if they are needed forauthentication.The source and target may be specified as a local pathname, aremote host with optional path in the form [user@]host:[path], or aURI in the form scp://[user@]host[:port][/path].Local file namescan be made explicit using absolute or relative pathnames to avoidscp treating file names containing \u2018:\u2019 as host specifiers.When copying between two remote hosts, if the URI format is used, aport cannot be specified on the target if the -R option is used.The options are as follows:-3Copies between two remote hosts are transferred through thelocal host.Without this option the data is copieddirectly between the two remote hosts.Note that, whenusing the legacy SCP protocol (via the -O flag), thisoption selects batch mode for the second host as scp cannotask for passwords or passphrases for both hosts.This modeis the default.-4Forces scp to use IPv4 addresses only.-6Forces scp to use IPv6 addresses only.-AAllows forwarding of ssh-agent(1) to the remote system.The default is not to forward an authentication agent.-BSelects batch mode (prevents asking for passwords orpassphrases).-CCompression enable.Passes the -C flag to ssh(1) to enablecompression.-c cipherSelects the cipher to use for encrypting the data transfer.This option is directly passed to ssh(1).-D sftp_server_pathConnect directly to a local SFTP server program rather thana remote one via ssh(1).This option may be useful indebugging the client and server.-F ssh_configSpecifies an alternative per-user configuration file forssh.This option is directly passed to ssh(1).-i identity_fileSelects the file from which the identity (private key) forpublic key authentication is read.This option is directlypassed to ssh(1).-J destinationConnect to the target host by first making an scpconnection to the jump host described by destination andthen establishing a TCP forwarding to the ultimatedestination from there.Multiple jump hops may bespecified separated by comma characters.This is ashortcut to specify a ProxyJump configuration directive.This option is directly passed to ssh(1).-l limitLimits the used bandwidth, specified in Kbit/s.-OUse the legacy SCP protocol for file transfers instead ofthe SFTP protocol.Forcing the use of the SCP protocol maybe necessary for servers that do not implement SFTP, forbackwards-compatibility for particular filename wildcardpatterns and for expanding paths with a \u2018~\u2019 prefix forolder SFTP servers.-o ssh_optionCan be used to pass options to ssh in the format used inssh_config(5).This is useful for specifying options forwhich there is no separate scp command-line flag.For fulldetails of the options listed below, and their possiblevalues, see ssh_config(5).AddressFamilyBatchModeBindAddressBindInterfaceCanonicalDomainsCanonicalizeFallbackLocalCanonicalizeHostnameCanonicalizeMaxDotsCanonicalizePermittedCNAMEsCASignatureAlgorithmsCertificateFileCheckHostIPCiphersCompressionConnectionAttemptsConnectTimeoutControlMasterControlPathControlPersistGlobalKnownHostsFileGSSAPIAuthenticationGSSAPIDelegateCredentialsHashKnownHostsHostHostbasedAcceptedAlgorithmsHostbasedAuthenticationHostKeyAlgorithmsHostKeyAliasHostnameIdentitiesOnlyIdentityAgentIdentityFileIPQoSKbdInteractiveAuthenticationKbdInteractiveDevicesKexAlgorithmsKnownHostsCommandLogLevelMACsNoHostAuthenticationForLocalhostNumberOfPasswordPromptsPasswordAuthenticationPKCS11ProviderPortPreferredAuthenticationsProxyCommandProxyJumpPubkeyAcceptedAlgorithmsPubkeyAuthenticationRekeyLimitRequiredRSASizeSendEnvServerAliveIntervalServerAliveCountMaxSetEnvStrictHostKeyCheckingTCPKeepAliveUpdateHostKeysUserUserKnownHostsFileVerifyHostKeyDNS-P portSpecifies the port to connect to on the remote host.Notethat this option is written with a capital \u2018P\u2019, because -pis already reserved for preserving the times and mode bitsof the file.-pPreserves modification times, access times, and file modebits from the source file.-qQuiet mode: disables the progress meter as well as warningand diagnostic messages from ssh(1).-RCopies between two remote hosts are performed by connectingto the origin host and executing scp there.This requiresthat scp running on the origin host can authenticate to thedestination host without requiring a password.-rRecursively copy entire directories.Note that scp followssymbolic links encountered in the tree traversal.-S programName of program to use for the encrypted connection.Theprogram must understand ssh(1) options.-TDisable strict filename checking.By default when copyingfiles from a remote host to a local directory scp checksthat the received filenames match those requested on thecommand-line to prevent the remote end from sendingunexpected or unwanted files.Because of differences inhow various operating systems and shells interpret filenamewildcards, these checks may cause wanted files to berejected.This option disables these checks at the expenseof fully trusting that the server will not send unexpectedfilenames.-vVerbose mode.Causes scp and ssh(1) to print debuggingmessages about their progress.This is helpful indebugging connection, authentication, and configurationproblems.-X sftp_optionSpecify an option that controls aspects of SFTP protocolbehaviour.The valid options are:nrequests=valueControls how many concurrent SFTP read or writerequests may be in progress at any point in timeduring a download or upload.By default 64requests may be active concurrently.buffer=valueControls the maximum buffer size for a single SFTPread/write operation used during download orupload.By default a 32KB buffer is used.",
        "name": "scp \u2014 OpenSSH secure file copy",
        "section": 1
    },
    {
        "command": "screen",
        "description": "Screen is a full-screen window manager that multiplexes aphysical terminal between several processes (typicallyinteractive shells).Each virtual terminal provides thefunctions of a DEC VT100 terminal and, in addition, severalcontrol functions from the ISO 6429 (ECMA 48, ANSI X3.64) and ISO2022 standards (e.g. insert/delete line and support for multiplecharacter sets).There is a scrollback history buffer for eachvirtual terminal and a copy-and-paste mechanism that allowsmoving text regions between windows.When screen is called, it creates a single window with a shell init (or the specified command) and then gets out of your way sothat you can use the program as you normally would.Then, at anytime, you can create new (full-screen) windows with otherprograms in them (including more shells), kill existing windows,view a list of windows, turn output logging on and off, copy-and-paste text between windows, view the scrollback history, switchbetween windows in whatever manner you wish, etc. All windows runtheir programs completely independent of each other. Programscontinue to run when their window is currently not visible andeven when the whole screen session is detached from the user'sterminal.When a program terminates, screen (per default) killsthe window that contained it.If this window was in theforeground, the display switches to the previous window; if noneare left, screen exits. Shells usually distinguish betweenrunning as login-shell or sub-shell.Screen runs them as sub-shells, unless told otherwise (See \"shell\" .screenrc command).Everything you type is sent to the program running in the currentwindow.The only exception to this is the one keystroke that isused to initiate a command to the window manager.By default,each command begins with a control-a (abbreviated C-a from nowon), and is followed by one other keystroke.The commandcharacter and all the key bindings can be fully customized to beanything you like, though they are always two characters inlength.Screen does not understand the prefix \"C-\" to mean control,although this notation is used in this manual for readability.Please use the caret notation (\"^A\" instead of \"C-a\") asarguments to e.g. the escape command or the -e option.Screenwill also print out control characters in caret notation.The standard way to create a new window is to type \"C-a c\".Thiscreates a new window running a shell and switches to that windowimmediately, regardless of the state of the process running inthe current window.Similarly, you can create a new window witha custom command in it by first binding the command to akeystroke (in your .screenrc file or at the \"C-a :\" command line)and then using it just like the \"C-a c\" command.In addition,new windows can be created by running a command like:screen emacs prog.cfrom a shell prompt within a previously created window.Thiswill not run another copy of screen, but will instead supply thecommand name and its arguments to the window manager (specifiedin the $STY environment variable) who will use it to create thenew window.The above example would start the emacs editor(editing prog.c) and switch to its window. - Note that you cannottransport environment variables from the invoking shell to theapplication (emacs in this case), because it is forked from theparent screen process, not from the invoking shell.If \"/etc/utmp\" is writable by screen, an appropriate record willbe written to this file for each window, and removed when thewindow is terminated.This is useful for working with \"talk\",\"script\", \"shutdown\", \"rsend\", \"sccs\" and other similar programsthat use the utmp file to determine who you are. As long asscreen is active on your terminal, the terminal's own record isremoved from the utmp file. See also \"C-a L\".",
        "name": "screen - screen manager with VT100/ANSI terminal emulation",
        "section": 1
    },
    {
        "command": "script",
        "description": "script makes a typescript of everything on your terminal session.The terminal data are stored in raw form to the log file andinformation about timing to another (optional) structured logfile. The timing log file is necessary to replay the sessionlater by scriptreplay(1) and to store additional informationabout the session.Since version 2.35, script supports multiple streams and allowsthe logging of input and output to separate files or all the onefile. This version also supports a new timing file which recordsadditional information. The command scriptreplay --summary thenprovides all the information.If the argument file or option --log-out file is given, scriptsaves the dialogue in this file. If no filename is given, thedialogue is saved in the file typescript.Note that logging input using --log-in or --log-io may recordsecurity-sensitive information as the log file contains allterminal session input (e.g., passwords) independently of theterminal echo flag setting.",
        "name": "script - make typescript of terminal session",
        "section": 1
    },
    {
        "command": "scriptlive",
        "description": "This program re-runs a typescript, using stdin typescript andtiming information to ensure that input happens in the samerhythm as it originally appeared when the script was recorded.The session is executed in a newly created pseudoterminal withthe user\u2019s $SHELL (or defaults to /bin/bash).Be careful! Do not forget that the typescript may containsarbitrary commands. It is recommended to use \"scriptreplay--stream in --log-in typescript\" (or with --log-io instead of--log-in) to verify the typescript before it is executed byscriptlive.The timing information is what script(1) outputs to filespecified by --log-timing. The typescript has to contain stdininformation and it is what script1 outputs to file specified by--log-in or --log-io.",
        "name": "scriptlive - re-run session typescripts, using timing information",
        "section": 1
    },
    {
        "command": "scriptreplay",
        "description": "This program replays a typescript, using timing information toensure that output happens in the same rhythm as it originallyappeared when the script was recorded.The replay simply displays the information again; the programsthat were run when the typescript was being recorded are not runagain. Since the same information is simply being displayed,scriptreplay is only guaranteed to work properly if run on thesame type of terminal the typescript was recorded on. Otherwise,any escape characters in the typescript may be interpreteddifferently by the terminal to which scriptreplay is sending itsoutput.The timing information is what script(1) outputs to filespecified by --log-timing.By default, the typescript to display is assumed to be namedtypescript, but other filenames may be specified, as the secondparameter or with option --log-out.If the third parameter or --divisor is specified, it is used as aspeed-up multiplier. For example, a speed-up of 2 makesscriptreplay go twice as fast, and a speed-down of 0.1 makes itgo ten times slower than the original session.",
        "name": "scriptreplay - play back typescripts, using timing information",
        "section": 1
    },
    {
        "command": "sdiff",
        "description": "Side-by-side merge of differences between FILE1 and FILE2.Mandatory arguments to long options are mandatory for shortoptions too.-o, --output=FILEoperate interactively, sending output to FILE-i, --ignore-caseconsider upper- and lower-case to be the same-E, --ignore-tab-expansionignore changes due to tab expansion-Z, --ignore-trailing-spaceignore white space at line end-b, --ignore-space-changeignore changes in the amount of white space-W, --ignore-all-spaceignore all white space-B, --ignore-blank-linesignore changes whose lines are all blank-I, --ignore-matching-lines=REignore changes all whose lines match RE--strip-trailing-crstrip trailing carriage return on input-a, --texttreat all files as text-w, --width=NUMoutput at most NUM (default 130) print columns-l, --left-columnoutput only the left column of common lines-s, --suppress-common-linesdo not output common lines-t, --expand-tabsexpand tabs to spaces in output--tabsize=NUMtab stops at every NUM (default 8) print columns-d, --minimaltry hard to find a smaller set of changes-H, --speed-large-filesassume large files, many scattered small changes--diff-program=PROGRAMuse PROGRAM to compare files--help display this help and exit-v, --versionoutput version information and exitIf a FILE is '-', read standard input.Exit status is 0 ifinputs are the same, 1 if different, 2 if trouble.",
        "name": "sdiff - side-by-side merge of file differences",
        "section": 1
    },
    {
        "command": "secon",
        "description": "See a part of a context. The context is taken from a file, pid,user input or the context in which secon is originally executed.-V, --versionshows the current version of secon-h, --helpshows the usage information for secon-P, --promptoutputs data in a format suitable for a prompt-C, --coloroutputs data with the associated ANSI color codes(requires -P)-u, --usershow the user of the security context-r, --roleshow the role of the security context-t, --typeshow the type of the security context-s, --sensitivityshow the sensitivity level of the security context-c, --clearanceshow the clearance level of the security context-m, --mls-rangeshow the sensitivity level and clearance, as a range, ofthe security context-R, --rawoutputsthe sensitivity level and clearance in anuntranslated format.-f, --filegets the context from the specified file FILE-L, --linkgets the context from the specified file FILE (doesn'tfollow symlinks)-p, --pidgets the context from the specified process PID--pid-execgets the exec context from the specified process PID--pid-fsgets the fscreate context from the specified process PID--pid-keygets the key context from the specified process PID--current, --selfgets the context from the current process--current-exec, --self-execgets the exec context from the current process--current-fs, --self-fsgets the fscreate context from the current process--current-key, --self-keygets the key context from the current process--parentgets the context from the parent of the current process--parent-execgets the exec context from the parent of the currentprocess--parent-fsgets the fscreate context from the parent of the currentprocess--parent-keygets the key context from the parent of the currentprocessAdditional argument CONTEXT may be provided and will be used ifno options have been specified to make secon get its context fromanother source.If that argument is - then the context will beread from stdin.If there is no argument, secon will try reading a context fromstdin, if that is not a tty, otherwise secon will act as though--self had been passed.If none of --user, --role, --type, --level or --mls-range ispassed.Then all of them will be output.",
        "name": "secon - See an SELinux context, from a file, program or userinput.",
        "section": 1
    },
    {
        "command": "sed",
        "description": "Sed is a stream editor.A stream editor is used to perform basictext transformations on an input stream (a file or input from apipeline).While in some ways similar to an editor which permitsscripted edits (such as ed), sed works by making only one passover the input(s), and is consequently more efficient.But it issed's ability to filter text in a pipeline which particularlydistinguishes it from other types of editors.-n, --quiet, --silentsuppress automatic printing of pattern space--debugannotate program execution-e script, --expression=scriptadd the script to the commands to be executed-f script-file, --file=script-fileadd the contents of script-file to the commands to beexecuted--follow-symlinksfollow symlinks when processing in place-i[SUFFIX], --in-place[=SUFFIX]edit files in place (makes backup if SUFFIX supplied)-l N, --line-length=Nspecify the desired line-wrap length for the `l' command--posixdisable all GNU extensions.-E, -r, --regexp-extendeduse extended regular expressions in the script (forportability use POSIX -E).-s, --separateconsider files as separate rather than as a single,continuous long stream.--sandboxoperate in sandbox mode (disable e/r/w commands).-u, --unbufferedload minimal amounts of data from the input files andflush the output buffers more often-z, --null-dataseparate lines by NUL characters--helpdisplay this help and exit--versionoutput version information and exitIf no -e, --expression, -f, or --file option is given, then thefirst non-option argument is taken as the sed script tointerpret.All remaining arguments are names of input files; ifno input files are specified, then the standard input is read.GNU sed home page: <https://www.gnu.org/software/sed/>.Generalhelp using GNU software: <https://www.gnu.org/gethelp/>.E-mailbug reports to: <bug-sed@gnu.org>.",
        "name": "sed - stream editor for filtering and transforming text",
        "section": 1
    },
    {
        "command": "semind",
        "description": "semind is the simple to use cscope-like tool based onsparse/dissect.Unlike cscope it runs after pre-processor andthus it can't index the code filtered out by ifdef's, but otoh itunderstands how the symbol is used and it can track the usage ofstruct members.",
        "name": "semind - Semantic Indexer for C",
        "section": 1
    },
    {
        "command": "seq",
        "description": "Print numbers from FIRST to LAST, in steps of INCREMENT.Mandatory arguments to long options are mandatory for shortoptions too.-f, --format=FORMATuse printf style floating-point FORMAT-s, --separator=STRINGuse STRING to separate numbers (default: \\n)-w, --equal-widthequalize width by padding with leading zeroes--help display this help and exit--versionoutput version information and exitIf FIRST or INCREMENT is omitted, it defaults to 1.That is, anomitted INCREMENT defaults to 1 even when LAST is smaller thanFIRST.The sequence of numbers ends when the sum of the currentnumber and INCREMENT would become greater than LAST.FIRST,INCREMENT, and LAST are interpreted as floating point values.INCREMENT is usually positive if FIRST is smaller than LAST, andINCREMENT is usually negative if FIRST is greater than LAST.INCREMENT must not be 0; none of FIRST, INCREMENT and LAST may beNaN.FORMAT must be suitable for printing one argument of type'double'; it defaults to %.PRECf if FIRST, INCREMENT, and LASTare all fixed point decimal numbers with maximum precision PREC,and to %g otherwise.",
        "name": "seq - print a sequence of numbers",
        "section": 1
    },
    {
        "command": "setfacl",
        "description": "This utility sets Access Control Lists (ACLs) of files anddirectories.On the command line, a sequence of commands isfollowed by a sequence of files (which in turn can be followed byanother sequence of commands, ...).The -m and -x options expect an ACL on the command line. MultipleACL entries are separated by comma characters (`,'). The -M and-X options read an ACL from a file or from standard input. TheACL entry format is described in Section ACL ENTRIES.The --set and --set-file options set the ACL of a file or adirectory. The previous ACL is replaced.ACL entries for thisoperation must include permissions.The -m (--modify) and -M (--modify-file) options modify the ACLof a file or directory.ACL entries for this operation mustinclude permissions.The -x (--remove) and -X (--remove-file) options remove ACLentries. It is not an error to remove an entry which does notexist.Only ACL entries without the perms field are accepted asparameters, unless POSIXLY_CORRECT is defined.When reading from files using the -M and -X options, setfaclaccepts the output getfacl produces.There is at most one ACLentry per line. After a Pound sign (`#'), everything up to theend of the line is treated as a comment.If setfacl is used on a file system which does not support ACLs,setfacl operates on the file mode permission bits. If the ACLdoes not fit completely in the permission bits, setfacl modifiesthe file mode permission bits to reflect the ACL as closely aspossible, writes an error message to standard error, and returnswith an exit status greater than 0.PERMISSIONSThe file owner and processes capable of CAP_FOWNER are grantedthe right to modify ACLs of a file. This is analogous to thepermissions required for accessing the file mode. (On currentLinux systems, root is the only user with the CAP_FOWNERcapability.)",
        "name": "setfacl - set file access control lists",
        "section": 1
    },
    {
        "command": "setfattr",
        "description": "The setfattr command associates a new value with an extendedattribute name for each specified file.",
        "name": "setfattr - set extended attributes of filesystem objects",
        "section": 1
    },
    {
        "command": "setleds",
        "description": "Setleds reports and changes the led flag settings of a VT (namelyNumLock, CapsLock and ScrollLock).Without arguments, setledsprints the current settings.With arguments, it sets or clearsthe indicated flags (and leaves the others unchanged). Thesettings before and after the change are reported if the -v flagis given.The led flag settings are specific for each VT (and the VTcorresponding to stdin is used).By default (or with option -F), setleds will only change the VTflags (and their setting may be reflected by the keyboard leds).With option -D, setleds will change both the VT flags and theirdefault settings (so that a subsequent reset will not undo thechange).This might be useful for people who always want to havenumlock set.With option -L, setleds will not touch the VT flags, but onlychange the leds.From this moment on, the leds will no longerreflect the VT flags (but display whatever is put into them). Thecommand setleds -L (without further arguments) will restore thesituation in which the leds reflect the VT flags.One might use setleds in /etc/rc to define the initial anddefault state of NumLock, e.g. byINITTY=/dev/tty[1-8]for tty in $INITTY; dosetleds -D +num < $ttydone",
        "name": "setleds - set the keyboard leds",
        "section": 1
    },
    {
        "command": "setmetamode",
        "description": "Without argument, setmetamode prints the current Meta key mode.With argument, it sets the Meta key mode as indicated.Thesetting before and after the change are reported.The Meta key mode is specific for each VT (and the VTcorresponding to stdin is used).One might use setmetamode in/etc/rc to define the initial state of the Meta key mode, e.g. byINITTY=/dev/tty[1-8]for tty in $INITTY; dosetmetamode escprefix < $ttydone",
        "name": "setmetamode - define the keyboard meta key handling",
        "section": 1
    },
    {
        "command": "setpriv",
        "description": "Sets or queries various Linux privilege settings that areinherited across execve(2).In comparison to su(1) and runuser(1), setpriv neither uses PAM,nor does it prompt for a password. It is a simple,non-set-user-ID wrapper around execve(2), and can be used to dropprivileges in the same way as setuidgid(8) from daemontools,chpst(8) from runit, or similar tools shipped by other servicemanagers.",
        "name": "setpriv - run a program with different Linux privilege settings",
        "section": 1
    },
    {
        "command": "setsid",
        "description": "setsid runs a program in a new session. The command calls fork(2)if already a process group leader. Otherwise, it executes aprogram in the current process. This default behavior is possibleto override by the --fork option.",
        "name": "setsid - run a program in a new session",
        "section": 1
    },
    {
        "command": "setterm",
        "description": "setterm writes to standard output a character string that willinvoke the specified terminal capabilities. Where possibleterminfo is consulted to find the string to use. Some optionshowever (marked \"virtual consoles only\" below) do not correspondto a terminfo(5) capability. In this case, if the terminal typeis \"con\" or \"linux\" the string that invokes the specifiedcapabilities on the PC Minix virtual console driver is output.Options that are not implemented by the terminal are ignored.",
        "name": "setterm - set terminal attributes",
        "section": 1
    },
    {
        "command": "sftp",
        "description": "sftp is a file transfer program, similar to ftp(1), which performsall operations over an encrypted ssh(1) transport.It may also usemany features of ssh, such as public key authentication andcompression.The destination may be specified either as [user@]host[:path] or asa URI in the form sftp://[user@]host[:port][/path].If the destination includes a path and it is not a directory, sftpwill retrieve files automatically if a non-interactiveauthentication method is used; otherwise it will do so aftersuccessful interactive authentication.If no path is specified, or if the path is a directory, sftp willlog in to the specified host and enter interactive command mode,changing to the remote directory if one was specified.An optionaltrailing slash can be used to force the path to be interpreted as adirectory.Since the destination formats use colon characters to delimit hostnames from path names or port numbers, IPv6 addresses must beenclosed in square brackets to avoid ambiguity.The options are as follows:-4Forces sftp to use IPv4 addresses only.-6Forces sftp to use IPv6 addresses only.-AAllows forwarding of ssh-agent(1) to the remote system.The default is not to forward an authentication agent.-aAttempt to continue interrupted transfers rather thanoverwriting existing partial or complete copies of files.If the partial contents differ from those beingtransferred, then the resultant file is likely to becorrupt.-B buffer_sizeSpecify the size of the buffer that sftp uses whentransferring files.Larger buffers require fewer roundtrips at the cost of higher memory consumption.Thedefault is 32768 bytes.-b batchfileBatch mode reads a series of commands from an inputbatchfile instead of stdin.Since it lacks userinteraction, it should be used in conjunction with non-interactive authentication to obviate the need to enter apassword at connection time (see sshd(8) and ssh-keygen(1)for details).A batchfile of \u2018-\u2019 may be used to indicate standard input.sftp will abort if any of the following commands fail: get,put, reget, reput, rename, ln, rm, mkdir, chdir, ls,lchdir, copy, cp, chmod, chown, chgrp, lpwd, df, symlink,and lmkdir.Termination on error can be suppressed on a command bycommand basis by prefixing the command with a \u2018-\u2019 character(for example, -rm /tmp/blah*).Echo of the command may besuppressed by prefixing the command with a \u2018@\u2019 character.These two prefixes may be combined in any order, forexample -@ls /bsd.-CEnables compression (via ssh's -C flag).-c cipherSelects the cipher to use for encrypting the datatransfers.This option is directly passed to ssh(1).-D sftp_server_commandConnect directly to a local sftp server (rather than viassh(1)).A command and arguments may be specified, forexample \"/path/sftp-server -el debug3\".This option may beuseful in debugging the client and server.-F ssh_configSpecifies an alternative per-user configuration file forssh(1).This option is directly passed to ssh(1).-fRequests that files be flushed to disk immediately aftertransfer.When uploading files, this feature is onlyenabled if the server implements the \"fsync@openssh.com\"extension.-i identity_fileSelects the file from which the identity (private key) forpublic key authentication is read.This option is directlypassed to ssh(1).-J destinationConnect to the target host by first making an sftpconnection to the jump host described by destination andthen establishing a TCP forwarding to the ultimatedestination from there.Multiple jump hops may bespecified separated by comma characters.This is ashortcut to specify a ProxyJump configuration directive.This option is directly passed to ssh(1).-l limitLimits the used bandwidth, specified in Kbit/s.-NDisables quiet mode, e.g. to override the implicit quietmode set by the -b flag.-o ssh_optionCan be used to pass options to ssh in the format used inssh_config(5).This is useful for specifying options forwhich there is no separate sftp command-line flag.Forexample, to specify an alternate port use: sftp -oPort=24.For full details of the options listed below, and theirpossible values, see ssh_config(5).AddressFamilyBatchModeBindAddressBindInterfaceCanonicalDomainsCanonicalizeFallbackLocalCanonicalizeHostnameCanonicalizeMaxDotsCanonicalizePermittedCNAMEsCASignatureAlgorithmsCertificateFileCheckHostIPCiphersCompressionConnectionAttemptsConnectTimeoutControlMasterControlPathControlPersistGlobalKnownHostsFileGSSAPIAuthenticationGSSAPIDelegateCredentialsHashKnownHostsHostHostbasedAcceptedAlgorithmsHostbasedAuthenticationHostKeyAlgorithmsHostKeyAliasHostnameIdentitiesOnlyIdentityAgentIdentityFileIPQoSKbdInteractiveAuthenticationKbdInteractiveDevicesKexAlgorithmsKnownHostsCommandLogLevelMACsNoHostAuthenticationForLocalhostNumberOfPasswordPromptsPasswordAuthenticationPKCS11ProviderPortPreferredAuthenticationsProxyCommandProxyJumpPubkeyAcceptedAlgorithmsPubkeyAuthenticationRekeyLimitRequiredRSASizeSendEnvServerAliveIntervalServerAliveCountMaxSetEnvStrictHostKeyCheckingTCPKeepAliveUpdateHostKeysUserUserKnownHostsFileVerifyHostKeyDNS-P portSpecifies the port to connect to on the remote host.-pPreserves modification times, access times, and modes fromthe original files transferred.-qQuiet mode: disables the progress meter as well as warningand diagnostic messages from ssh(1).-R num_requestsSpecify how many requests may be outstanding at any onetime.Increasing this may slightly improve file transferspeed but will increase memory usage.The default is 64outstanding requests.-rRecursively copy entire directories when uploading anddownloading.Note that sftp does not follow symbolic linksencountered in the tree traversal.-S programName of the program to use for the encrypted connection.The program must understand ssh(1) options.-s subsystem | sftp_serverSpecifies the SSH2 subsystem or the path for an sftp serveron the remote host.A path is useful when the remotesshd(8) does not have an sftp subsystem configured.-vRaise logging level.This option is also passed to ssh.-X sftp_optionSpecify an option that controls aspects of SFTP protocolbehaviour.The valid options are:nrequests=valueControls how many concurrent SFTP read or writerequests may be in progress at any point in timeduring a download or upload.By default 64requests may be active concurrently.buffer=valueControls the maximum buffer size for a single SFTPread/write operation used during download orupload.By default a 32KB buffer is used.",
        "name": "sftp \u2014 OpenSSH secure file transfer",
        "section": 1
    },
    {
        "command": "sg",
        "description": "The sg command works similar to newgrp but accepts a command. Thecommand will be executed with the /bin/sh shell. With most shellsyou may run sg from, you need to enclose multi-word commands inquotes. Another difference between newgrp and sg is that someshells treat newgrp specially, replacing themselves with a newinstance of a shell that newgrp creates. This doesn't happen withsg, so upon exit from a sg command you are returned to yourprevious group ID.",
        "name": "sg - execute command as different group ID",
        "section": 1
    },
    {
        "command": "sha1sum",
        "description": "Print or check SHA1 (160-bit) checksums.With no FILE, or when FILE is -, read standard input.-b, --binaryread in binary mode-c, --checkread checksums from the FILEs and check them--tagcreate a BSD-style checksum-t, --textread in text mode (default)-z, --zeroend each output line with NUL, not newline, and disablefile name escapingThe following five options are useful only when verifying checksums:--ignore-missingdon't fail or report status for missing files--quietdon't print OK for each successfully verified file--statusdon't output anything, status code shows success--strictexit non-zero for improperly formatted checksum lines-w, --warnwarn about improperly formatted checksum lines--help display this help and exit--versionoutput version information and exitThe sums are computed as described in FIPS-180-1.When checking,the input should be a former output of this program.The defaultmode is to print a line with: checksum, a space, a characterindicating input mode ('*' for binary, ' ' for text or wherebinary is insignificant), and name for each FILE.Note: There is no difference between binary mode and text mode onGNU systems.",
        "name": "sha1sum - compute and check SHA1 message digest",
        "section": 1
    },
    {
        "command": "sha224sum",
        "description": "Print or check SHA224 (224-bit) checksums.With no FILE, or when FILE is -, read standard input.-b, --binaryread in binary mode-c, --checkread checksums from the FILEs and check them--tagcreate a BSD-style checksum-t, --textread in text mode (default)-z, --zeroend each output line with NUL, not newline, and disablefile name escapingThe following five options are useful only when verifying checksums:--ignore-missingdon't fail or report status for missing files--quietdon't print OK for each successfully verified file--statusdon't output anything, status code shows success--strictexit non-zero for improperly formatted checksum lines-w, --warnwarn about improperly formatted checksum lines--help display this help and exit--versionoutput version information and exitThe sums are computed as described in RFC 3874.When checking,the input should be a former output of this program.The defaultmode is to print a line with: checksum, a space, a characterindicating input mode ('*' for binary, ' ' for text or wherebinary is insignificant), and name for each FILE.Note: There is no difference between binary mode and text mode onGNU systems.",
        "name": "sha224sum - compute and check SHA224 message digest",
        "section": 1
    },
    {
        "command": "sha256sum",
        "description": "Print or check SHA256 (256-bit) checksums.With no FILE, or when FILE is -, read standard input.-b, --binaryread in binary mode-c, --checkread checksums from the FILEs and check them--tagcreate a BSD-style checksum-t, --textread in text mode (default)-z, --zeroend each output line with NUL, not newline, and disablefile name escapingThe following five options are useful only when verifying checksums:--ignore-missingdon't fail or report status for missing files--quietdon't print OK for each successfully verified file--statusdon't output anything, status code shows success--strictexit non-zero for improperly formatted checksum lines-w, --warnwarn about improperly formatted checksum lines--help display this help and exit--versionoutput version information and exitThe sums are computed as described in FIPS-180-2.When checking,the input should be a former output of this program.The defaultmode is to print a line with: checksum, a space, a characterindicating input mode ('*' for binary, ' ' for text or wherebinary is insignificant), and name for each FILE.Note: There is no difference between binary mode and text mode onGNU systems.",
        "name": "sha256sum - compute and check SHA256 message digest",
        "section": 1
    },
    {
        "command": "sha384sum",
        "description": "Print or check SHA384 (384-bit) checksums.With no FILE, or when FILE is -, read standard input.-b, --binaryread in binary mode-c, --checkread checksums from the FILEs and check them--tagcreate a BSD-style checksum-t, --textread in text mode (default)-z, --zeroend each output line with NUL, not newline, and disablefile name escapingThe following five options are useful only when verifying checksums:--ignore-missingdon't fail or report status for missing files--quietdon't print OK for each successfully verified file--statusdon't output anything, status code shows success--strictexit non-zero for improperly formatted checksum lines-w, --warnwarn about improperly formatted checksum lines--help display this help and exit--versionoutput version information and exitThe sums are computed as described in FIPS-180-2.When checking,the input should be a former output of this program.The defaultmode is to print a line with: checksum, a space, a characterindicating input mode ('*' for binary, ' ' for text or wherebinary is insignificant), and name for each FILE.Note: There is no difference between binary mode and text mode onGNU systems.",
        "name": "sha384sum - compute and check SHA384 message digest",
        "section": 1
    },
    {
        "command": "sha512sum",
        "description": "Print or check SHA512 (512-bit) checksums.With no FILE, or when FILE is -, read standard input.-b, --binaryread in binary mode-c, --checkread checksums from the FILEs and check them--tagcreate a BSD-style checksum-t, --textread in text mode (default)-z, --zeroend each output line with NUL, not newline, and disablefile name escapingThe following five options are useful only when verifying checksums:--ignore-missingdon't fail or report status for missing files--quietdon't print OK for each successfully verified file--statusdon't output anything, status code shows success--strictexit non-zero for improperly formatted checksum lines-w, --warnwarn about improperly formatted checksum lines--help display this help and exit--versionoutput version information and exitThe sums are computed as described in FIPS-180-2.When checking,the input should be a former output of this program.The defaultmode is to print a line with: checksum, a space, a characterindicating input mode ('*' for binary, ' ' for text or wherebinary is insignificant), and name for each FILE.Note: There is no difference between binary mode and text mode onGNU systems.",
        "name": "sha512sum - compute and check SHA512 message digest",
        "section": 1
    },
    {
        "command": "sheet2pcp",
        "description": "sheet2pcp is intended to read a data spreadsheet (infile)translate this into a Performance Co-Pilot (PCP) archive with thebasename outfile.The input spreadsheet can be in any of the common formats,provided the appropriate Perl modules have been installed (seethe CAVEATS section below).The spreadsheet must be``normalized'' so that each row contains data for the same timeinterval, and one of the columns contains the date and time forthe data in each row.The resultant PCP archive may be used with all the PCP clienttools to graph subsets of the data using pmchart(1), perform datareduction and reporting, filter with the PCP inference enginepmie(1), etc.The mapfile controls the import process and defines the datamapping from the spreadsheet columns onto the PCP data model.The file is written in XML and conforms to the syntax defined inthe MAPPING CONFIGURATION section below.A series of physical files will be created with the prefixoutfile.These are outfile.0 (the performance data),outfile.meta (the metadata that describes the performance data)and outfile.index (a temporal index to improve efficiency ofreplay operations for the archive).If any of these files existsalready, then sheet2pcp will not overwrite them and will exitwith an error message.The -h option is an alternate to the hostname attribute of the<sheet> element in mapfile described below.If both arespecified, the value from mapfile is used.The -V option specifies the version for the output PCP archive.By default the archive version $PCP_ARCHIVE_VERSION (set to 2 incurrent PCP releases) is used, and the only values currentlysupported for version are 2 or 3.The -Z option is an alternate to the timezone attribute of the<sheet> element in mapfile described below.If both arespecified, the value from mapfile is used.sheet2pcp is a Perl script that uses the PCP::LogImport Perlwrapper around the PCP libpcp_import library, and as such couldbe used as an example to develop new tools to import other typesof performance data and create PCP archives.",
        "name": "sheet2pcp - import spreadsheet data and create a PCP archive",
        "section": 1
    },
    {
        "command": "show-changed-rco",
        "description": "show-changed-rco gives a compact description of the changes to apackages Requires, Conflicts and Obsoletes data from theinstalled (or old) to a specified rpm file.",
        "name": "show-changed-rco - show changes in an RPM package",
        "section": 1
    },
    {
        "command": "show-installed",
        "description": "show-installed gives a compact description of the packagesinstalled (or given) making use of the comps groups found in therepositories.",
        "name": "show-installed - show installed RPM packages and descriptions",
        "section": 1
    },
    {
        "command": "showkey",
        "description": "showkey prints to standard output either the scan codes or thekeycode or the `ascii' code of each key pressed.In the firsttwo modes the program runs until 10 seconds have elapsed sincethe last key press or release event, or until it receives asuitable signal, like SIGTERM, from another process.In `ascii'mode the program terminates when the user types ^D.When in scancode dump mode, showkey prints in hexadecimal formateach byte received from the keyboard to the standard output. Anew line is printed when an interval of about 0.1 seconds occursbetween the bytes received, or when the internal receive bufferfills up. This can be used to determine roughly, what bytesequences the keyboard sends at once on a given key press. Thescan code dumping mode is primarily intended for debugging thekeyboard driver or other low level interfaces. As such itshouldn't be of much interest to the regular end-user. However,some modern keyboards have keys or buttons that produce scancodesto which the kernel does not associate a keycode, and, afterfinding out what these are, the user can assign keycodes withsetkeycodes(8).When in the default keycode dump mode, showkey prints to thestandard output the keycode number or each key pressed orreleased. The kind of the event, press or release, is alsoreported.Keycodes are numbers assigned by the kernel to eachindividual physical key. Every key has always only one associatedkeycode number, whether the keyboard sends single or multiplescan codes when pressing it. Using showkey in this mode, you canfind out what numbers to use in your personalized keymap files.When in `ascii' dump mode, showkey prints to the standard outputthe decimal, octal, and hexadecimal value(s) of the key pressed,according to he present keymap.",
        "name": "showkey - examine the codes sent by the keyboard",
        "section": 1
    },
    {
        "command": "shred",
        "description": "Overwrite the specified FILE(s) repeatedly, in order to make itharder for even very expensive hardware probing to recover thedata.If FILE is -, shred standard output.Mandatory arguments to long options are mandatory for shortoptions too.-f, --forcechange permissions to allow writing if necessary-n, --iterations=Noverwrite N times instead of the default (3)--random-source=FILEget random bytes from FILE-s, --size=Nshred this many bytes (suffixes like K, M, G accepted)-udeallocate and remove file after overwriting--remove[=HOW]like -u but give control on HOW to delete;See below-v, --verboseshow progress-x, --exactdo not round file sizes up to the next full block;this is the default for non-regular files-z, --zeroadd a final overwrite with zeros to hide shredding--help display this help and exit--versionoutput version information and exitDelete FILE(s) if --remove (-u) is specified.The default is notto remove the files because it is common to operate on devicefiles like /dev/hda, and those files usually should not beremoved.The optional HOW parameter indicates how to remove adirectory entry: 'unlink' => use a standard unlink call.'wipe'=> also first obfuscate bytes in the name.'wipesync' => alsosync each obfuscated byte to the device.The default mode is'wipesync', but note it can be expensive.CAUTION: shred assumes the file system and hardware overwritedata in place.Although this is common, many platforms operateotherwise.Also, backups and mirrors may contain unremovablecopies that will let a shredded file be recovered later.See theGNU coreutils manual for details.",
        "name": "shred - overwrite a file to hide its contents, and optionallydelete it",
        "section": 1
    },
    {
        "command": "shuf",
        "description": "Write a random permutation of the input lines to standard output.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-e, --echotreat each ARG as an input line-i, --input-range=LO-HItreat each number LO through HI as an input line-n, --head-count=COUNToutput at most COUNT lines-o, --output=FILEwrite result to FILE instead of standard output--random-source=FILEget random bytes from FILE-r, --repeatoutput lines can be repeated-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exit",
        "name": "shuf - generate random permutations",
        "section": 1
    },
    {
        "command": "size",
        "description": "The GNU size utility lists the section sizes and the total sizefor each of the binary files objfile on its argument list.Bydefault, one line of output is generated for each file or eachmodule if the file is an archive.objfile... are the files to be examined.If none are specified,the file \"a.out\" will be used instead.",
        "name": "size - list section sizes and total size of binary files",
        "section": 1
    },
    {
        "command": "skill",
        "description": "These tools are obsolete and unportable.The command syntax ispoorly defined.Consider using the killall, pkill, and pgrepcommands instead.The default signal for skill is TERM.Use -l or -L to listavailable signals.Particularly useful signals include HUP, INT,KILL, STOP, CONT, and 0.Alternate signals may be specified inthree ways: -9 -SIGKILL -KILL.The default priority for snice is +4.Priority numbers rangefrom +20 (slowest) to -20 (fastest).Negative priority numbersare restricted to administrative users.",
        "name": "skill, snice - send a signal or report process status",
        "section": 1
    },
    {
        "command": "slabtop",
        "description": "slabtop displays detailed kernel slab cache information in realtime.It displays a listing of the top caches sorted by one ofthe listed sort criteria.It also displays a statistics headerfilled with slab layer information.",
        "name": "slabtop - display kernel slab cache information in real time",
        "section": 1
    },
    {
        "command": "sleep",
        "description": "Pause for NUMBER seconds.SUFFIX may be 's' for seconds (thedefault), 'm' for minutes, 'h' for hours or 'd' for days.NUMBERneed not be an integer.Given two or more arguments, pause forthe amount of time specified by the sum of their values.--help display this help and exit--versionoutput version information and exit",
        "name": "sleep - delay for a specified amount of time",
        "section": 1
    },
    {
        "command": "smtp",
        "description": "The smtp utility is a Simple Mail Transfer Protocol (SMTP) clientwhich can be used to run an SMTP transaction against an SMTPserver.By default, smtp reads the mail content from the standard input,establishes an SMTP session, and runs an SMTP transaction for allthe specified recipients.The content is sent unaltered as maildata.The options are as follows:-a authfilePerform a login before sending the message.The usernameand password are read from authfile and need to be on thefirst and second line respectively.This option requires aTLS or STARTTLS server.-CDo not require server certificate to be valid.This flagis deprecated.Use \u201c-T noverify\u201d instead.-F fromSet the return-path (MAIL FROM) for the SMTP transaction.Default to the current username.-H heloDefine the hostname to advertise (HELO) when establishingthe SMTP session.-hDisplay usage.-nDo not actually execute a transaction, just try toestablish an SMTP session and quit.When this option isgiven, no message is read from the standard input.-s serverSpecify the server to connect to and connection parameters.The format is [proto://[user:pass@]]host[:port].Thefollowing protocols are available:smtpNormal SMTP session with opportunisticSTARTTLS.smtp+tlsNormal SMTP session with mandatory STARTTLS.smtp+notlsPlain text SMTP session without TLS.lmtpLMTP session with opportunistic STARTTLS.lmtp+tlsLMTP session with mandatory STARTTLS.lmtp+notlsPlain text LMTP session without TLS.smtpsSMTP session with forced TLS on connection.Defaults to \u201csmtp://localhost:25\u201d.-T paramsSet specific parameters for TLS sessions.The paramsstring is a comma or space separated list of options.Theavailable options are:cafile=filenameUse filename as root certificates file instead ofthe system default.ciphers=valueSpecify the allowed ciphers.Refer totls_config_set_ciphers(3) for value.nosniDisable Server Name Indication (SNI).noverifyDo not require server certificate to be valid.protocols=valueSpecify the protocols to use.Refer totls_config_parse_protocols(3) for value.servername=valueUse value for Server Name Indication (SNI).Defaults to the specified server hostname.-vBe more verbose.This option can be specified multipletimes.",
        "name": "smtp \u2014 Simple Mail Transfer Protocol client",
        "section": 1
    },
    {
        "command": "snice",
        "description": "These tools are obsolete and unportable.The command syntax ispoorly defined.Consider using the killall, pkill, and pgrepcommands instead.The default signal for skill is TERM.Use -l or -L to listavailable signals.Particularly useful signals include HUP, INT,KILL, STOP, CONT, and 0.Alternate signals may be specified inthree ways: -9 -SIGKILL -KILL.The default priority for snice is +4.Priority numbers rangefrom +20 (slowest) to -20 (fastest).Negative priority numbersare restricted to administrative users.",
        "name": "skill, snice - send a signal or report process status",
        "section": 1
    },
    {
        "command": "soelim",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "sort",
        "description": "Write sorted concatenation of all FILE(s) to standard output.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.Ordering options:-b, --ignore-leading-blanksignore leading blanks-d, --dictionary-orderconsider only blanks and alphanumeric characters-f, --ignore-casefold lower case to upper case characters-g, --general-numeric-sortcompare according to general numerical value-i, --ignore-nonprintingconsider only printable characters-M, --month-sortcompare (unknown) < 'JAN' < ... < 'DEC'-h, --human-numeric-sortcompare human readable numbers (e.g., 2K 1G)-n, --numeric-sortcompare according to string numerical value-R, --random-sortshuffle, but group identical keys.See shuf(1)--random-source=FILEget random bytes from FILE-r, --reversereverse the result of comparisons--sort=WORDsort according to WORD: general-numeric -g, human-numeric-h, month -M, numeric -n, random -R, version -V-V, --version-sortnatural sort of (version) numbers within textOther options:--batch-size=NMERGEmerge at most NMERGE inputs at once; for more use tempfiles-c, --check, --check=diagnose-firstcheck for sorted input; do not sort-C, --check=quiet, --check=silentlike -c, but do not report first bad line--compress-program=PROGcompress temporaries with PROG; decompress them with PROG-d--debugannotate the part of the line used to sort, and warn aboutquestionable usage to stderr--files0-from=Fread input from the files specified by NUL-terminatednames in file F; If F is - then read names from standardinput-k, --key=KEYDEFsort via a key; KEYDEF gives location and type-m, --mergemerge already sorted files; do not sort-o, --output=FILEwrite result to FILE instead of standard output-s, --stablestabilize sort by disabling last-resort comparison-S, --buffer-size=SIZEuse SIZE for main memory buffer-t, --field-separator=SEPuse SEP instead of non-blank to blank transition-T, --temporary-directory=DIRuse DIR for temporaries, not $TMPDIR or /tmp; multipleoptions specify multiple directories--parallel=Nchange the number of sorts run concurrently to N-u, --uniquewith -c, check for strict ordering; without -c, outputonly the first of an equal run-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exitKEYDEF is F[.C][OPTS][,F[.C][OPTS]] for start and stop position,where F is a field number and C a character position in thefield; both are origin 1, and the stop position defaults to theline's end.If neither -t nor -b is in effect, characters in afield are counted from the beginning of the preceding whitespace.OPTS is one or more single-letter ordering options [bdfgiMhnRrV],which override global ordering options for that key.If no keyis given, use the entire line as the key.Use --debug todiagnose incorrect key usage.SIZE may be followed by the following multiplicative suffixes: %1% of memory, b 1, K 1024 (default), and so on for M, G, T, P, E,Z, Y, R, Q.*** WARNING *** The locale specified by the environment affectssort order.Set LC_ALL=C to get the traditional sort order thatuses native byte values.",
        "name": "sort - sort lines of text files",
        "section": 1
    },
    {
        "command": "sparse",
        "description": "Sparse parses C source and looks for errors, producing warningson standard error.Sparse accepts options controlling the set of warnings togenerate.To turn on warnings Sparse does not issue by default,use the corresponding warning option -Wsomething.Sparse issuessome warnings by default; to turn off those warnings, pass thenegation of the associated warning option, -Wno-something.",
        "name": "sparse - Semantic Parser for C",
        "section": 1
    },
    {
        "command": "split",
        "description": "Output pieces of FILE to PREFIXaa, PREFIXab, ...; default size is1000 lines, and default PREFIX is 'x'.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-a, --suffix-length=Ngenerate suffixes of length N (default 2)--additional-suffix=SUFFIXappend an additional SUFFIX to file names-b, --bytes=SIZEput SIZE bytes per output file-C, --line-bytes=SIZEput at most SIZE bytes of records per output file-duse numeric suffixes starting at 0, not alphabetic--numeric-suffixes[=FROM]same as -d, but allow setting the start value-xuse hex suffixes starting at 0, not alphabetic--hex-suffixes[=FROM]same as -x, but allow setting the start value-e, --elide-empty-filesdo not generate empty output files with '-n'--filter=COMMANDwrite to shell COMMAND; file name is $FILE-l, --lines=NUMBERput NUMBER lines/records per output file-n, --number=CHUNKSgenerate CHUNKS output files; see explanation below-t, --separator=SEPuse SEP instead of newline as the record separator; '\\0'(zero) specifies the NUL character-u, --unbufferedimmediately copy input to output with '-n r/...'--verboseprint a diagnostic just before each output file is opened--help display this help and exit--versionoutput version information and exitThe SIZE argument is an integer and optional unit (example: 10Kis 10*1024).Units are K,M,G,T,P,E,Z,Y,R,Q (powers of 1024) orKB,MB,... (powers of 1000).Binary prefixes can be used, too:KiB=K, MiB=M, and so on.CHUNKS may be:Nsplit into N files based on size of inputK/Noutput Kth of N to stdoutl/Nsplit into N files without splitting lines/recordsl/K/Noutput Kth of N to stdout without splitting lines/recordsr/Nlike 'l' but use round robin distributionr/K/Nlikewise but only output Kth of N to stdout",
        "name": "split - split a file into pieces",
        "section": 1
    },
    {
        "command": "sprof",
        "description": "The sprof command displays a profiling summary for the sharedobject (shared library) specified as its first command-lineargument.The profiling summary is created using previouslygenerated profiling data in the (optional) second command-lineargument.If the profiling data pathname is omitted, then sprofwill attempt to deduce it using the soname of the shared object,looking for a file with the name <soname>.profile in the currentdirectory.",
        "name": "sprof - read and display shared object profiling data",
        "section": 1
    },
    {
        "command": "sqlhist",
        "description": "The sqlhist(1) will take an SQL like statement to create tracefshistograms and synthetic events that can perform various actionsfor various handling of the data.The tracefs file system interfaces with the Linux tracinginfrastructure that has various dynamic and static events throughout the kernel. Each of these events can have a \"histogram\"attached to it, where the fields of the event will define thebuckets of the histogram.A synthetic event is a way to attach two separate events and usethe fields and time stamps of those events to create a newdynamic event. This new dynamic event is call a synthetic event.The fields of each event can have simple calculations done onthem where, for example, the delta between a field of one eventto a field of the other event can be taken. This also works forthe time stamps of the events where the time delta between thetwo events can also be extracted and placed into the syntheticevent.Other actions can be done from the fields of the events. Asnapshot can be taken of the kernel ring buffer a variable usedin the synthetic event creating hits a max, or simply changes.The commands to create histograms and synthetic events arecomplex and not easy to remember. sqlhist is used to convert SQLsyntax into the commands needed to create the histogram orsynthetic event.The SQL-select-command is a SQL string defined by tracefs_sql(3).Note, this must be run as root (or sudo) as interacting with thetracefs directory requires root privilege, unless the -t optionis given with a copy of the tracefs directory and its events.The sqlhist is a simple program where its code actual exists inthe tracefs_sql(3) man page.",
        "name": "sqlhist - Tool that uses SQL language to create / show creationof tracefs histograms and synthetic events.",
        "section": 1
    },
    {
        "command": "srptool",
        "description": "Simple program that emulates the programs in the Stanford SRP(Secure Remote Password) libraries using GnuTLS.It is intendedfor use inplaces where you don't expect SRP authentication tobe the used for system users.Inbrief,to use SRP you need to create two files. These arethe password file that holds the users and the verifiersassociated withthemandthe configuration file to hold thegroup parameters (called tpasswd.conf).",
        "name": "srptool - GnuTLS SRP tool",
        "section": 1
    },
    {
        "command": "ssh",
        "description": "ssh (SSH client) is a program for logging into a remote machine andfor executing commands on a remote machine.It is intended toprovide secure encrypted communications between two untrusted hostsover an insecure network.X11 connections, arbitrary TCP ports andUNIX-domain sockets can also be forwarded over the secure channel.ssh connects and logs into the specified destination, which may bespecified as either [user@]hostname or a URI of the formssh://[user@]hostname[:port].The user must prove their identityto the remote machine using one of several methods (see below).If a command is specified, it will be executed on the remote hostinstead of a login shell.A complete command line may be specifiedas command, or it may have additional arguments.If supplied, thearguments will be appended to the command, separated by spaces,before it is sent to the server to be executed.The options are as follows:-4Forces ssh to use IPv4 addresses only.-6Forces ssh to use IPv6 addresses only.-AEnables forwarding of connections from an authenticationagent such as ssh-agent(1).This can also be specified ona per-host basis in a configuration file.Agent forwarding should be enabled with caution.Userswith the ability to bypass file permissions on the remotehost (for the agent's UNIX-domain socket) can access thelocal agent through the forwarded connection.An attackercannot obtain key material from the agent, however they canperform operations on the keys that enable them toauthenticate using the identities loaded into the agent.Asafer alternative may be to use a jump host (see -J).-aDisables forwarding of the authentication agent connection.-B bind_interfaceBind to the address of bind_interface before attempting toconnect to the destination host.This is only useful onsystems with more than one address.-b bind_addressUse bind_address on the local machine as the source addressof the connection.Only useful on systems with more thanone address.-CRequests compression of all data (including stdin, stdout,stderr, and data for forwarded X11, TCP and UNIX-domainconnections).The compression algorithm is the same usedby gzip(1).Compression is desirable on modem lines andother slow connections, but will only slow down things onfast networks.The default value can be set on a host-by-host basis in the configuration files; see the Compressionoption in ssh_config(5).-c cipher_specSelects the cipher specification for encrypting thesession.cipher_spec is a comma-separated list of cipherslisted in order of preference.See the Ciphers keyword inssh_config(5) for more information.-D [bind_address:]portSpecifies a local \u201cdynamic\u201d application-level portforwarding.This works by allocating a socket to listen toport on the local side, optionally bound to the specifiedbind_address.Whenever a connection is made to this port,the connection is forwarded over the secure channel, andthe application protocol is then used to determine where toconnect to from the remote machine.Currently the SOCKS4and SOCKS5 protocols are supported, and ssh will act as aSOCKS server.Only root can forward privileged ports.Dynamic port forwardings can also be specified in theconfiguration file.IPv6 addresses can be specified by enclosing the address insquare brackets.Only the superuser can forward privilegedports.By default, the local port is bound in accordancewith the GatewayPorts setting.However, an explicitbind_address may be used to bind the connection to aspecific address.The bind_address of \u201clocalhost\u201dindicates that the listening port be bound for local useonly, while an empty address or \u2018*\u2019 indicates that the portshould be available from all interfaces.-E log_fileAppend debug logs to log_file instead of standard error.-e escape_charSets the escape character for sessions with a pty (default:\u2018~\u2019).The escape character is only recognized at thebeginning of a line.The escape character followed by adot (\u2018.\u2019) closes the connection; followed by control-Zsuspends the connection; and followed by itself sends theescape character once.Setting the character to \u201cnone\u201ddisables any escapes and makes the session fullytransparent.-F configfileSpecifies an alternative per-user configuration file.If aconfiguration file is given on the command line, thesystem-wide configuration file (/etc/ssh/ssh_config) willbe ignored.The default for the per-user configurationfile is ~/.ssh/config.If set to \u201cnone\u201d, no configurationfiles will be read.-fRequests ssh to go to background just before commandexecution.This is useful if ssh is going to ask forpasswords or passphrases, but the user wants it in thebackground.This implies -n.The recommended way to startX11 programs at a remote site is with something like ssh -fhost xterm.If the ExitOnForwardFailure configuration option is set to\u201cyes\u201d, then a client started with -f will wait for allremote port forwards to be successfully established beforeplacing itself in the background.Refer to the descriptionof ForkAfterAuthentication in ssh_config(5) for details.-GCauses ssh to print its configuration after evaluating Hostand Match blocks and exit.-gAllows remote hosts to connect to local forwarded ports.If used on a multiplexed connection, then this option mustbe specified on the master process.-I pkcs11Specify the PKCS#11 shared library ssh should use tocommunicate with a PKCS#11 token providing keys for userauthentication.-i identity_fileSelects a file from which the identity (private key) forpublic key authentication is read.You can also specify apublic key file to use the corresponding private key thatis loaded in ssh-agent(1) when the private key file is notpresent locally.The default is ~/.ssh/id_rsa,~/.ssh/id_ecdsa, ~/.ssh/id_ecdsa_sk, ~/.ssh/id_ed25519,~/.ssh/id_ed25519_sk and ~/.ssh/id_dsa.Identity files mayalso be specified on a per-host basis in the configurationfile.It is possible to have multiple -i options (andmultiple identities specified in configuration files).Ifno certificates have been explicitly specified by theCertificateFile directive, ssh will also try to loadcertificate information from the filename obtained byappending -cert.pub to identity filenames.-J destinationConnect to the target host by first making a ssh connectionto the jump host described by destination and thenestablishing a TCP forwarding to the ultimate destinationfrom there.Multiple jump hops may be specified separatedby comma characters.This is a shortcut to specify aProxyJump configuration directive.Note that configurationdirectives supplied on the command-line generally apply tothe destination host and not any specified jump hosts.Use~/.ssh/config to specify configuration for jump hosts.-KEnables GSSAPI-based authentication and forwarding(delegation) of GSSAPI credentials to the server.-kDisables forwarding (delegation) of GSSAPI credentials tothe server.-L [bind_address:]port:host:hostport-L [bind_address:]port:remote_socket-L local_socket:host:hostport-L local_socket:remote_socketSpecifies that connections to the given TCP port or Unixsocket on the local (client) host are to be forwarded tothe given host and port, or Unix socket, on the remoteside.This works by allocating a socket to listen toeither a TCP port on the local side, optionally bound tothe specified bind_address, or to a Unix socket.Whenevera connection is made to the local port or socket, theconnection is forwarded over the secure channel, and aconnection is made to either host port hostport, or theUnix socket remote_socket, from the remote machine.Port forwardings can also be specified in the configurationfile.Only the superuser can forward privileged ports.IPv6 addresses can be specified by enclosing the address insquare brackets.By default, the local port is bound in accordance with theGatewayPorts setting.However, an explicit bind_addressmay be used to bind the connection to a specific address.The bind_address of \u201clocalhost\u201d indicates that thelistening port be bound for local use only, while an emptyaddress or \u2018*\u2019 indicates that the port should be availablefrom all interfaces.-l login_nameSpecifies the user to log in as on the remote machine.This also may be specified on a per-host basis in theconfiguration file.-MPlaces the ssh client into \u201cmaster\u201d mode for connectionsharing.Multiple -M options places ssh into \u201cmaster\u201d modebut with confirmation required using ssh-askpass(1) beforeeach operation that changes the multiplexing state (e.g.opening a new session).Refer to the description ofControlMaster in ssh_config(5) for details.-m mac_specA comma-separated list of MAC (message authentication code)algorithms, specified in order of preference.See the MACskeyword in ssh_config(5) for more information.-NDo not execute a remote command.This is useful for justforwarding ports.Refer to the description of SessionTypein ssh_config(5) for details.-nRedirects stdin from /dev/null (actually, prevents readingfrom stdin).This must be used when ssh is run in thebackground.A common trick is to use this to run X11programs on a remote machine.For example, ssh -nshadows.cs.hut.fi emacs & will start an emacs onshadows.cs.hut.fi, and the X11 connection will beautomatically forwarded over an encrypted channel.The sshprogram will be put in the background.(This does not workif ssh needs to ask for a password or passphrase; see alsothe -f option.)Refer to the description of StdinNull inssh_config(5) for details.-O ctl_cmdControl an active connection multiplexing master process.When the -O option is specified, the ctl_cmd argument isinterpreted and passed to the master process.Validcommands are: \u201ccheck\u201d (check that the master process isrunning), \u201cforward\u201d (request forwardings without commandexecution), \u201ccancel\u201d (cancel forwardings), \u201cexit\u201d (requestthe master to exit), and \u201cstop\u201d (request the master to stopaccepting further multiplexing requests).-o optionCan be used to give options in the format used in theconfiguration file.This is useful for specifying optionsfor which there is no separate command-line flag.For fulldetails of the options listed below, and their possiblevalues, see ssh_config(5).AddKeysToAgentAddressFamilyBatchModeBindAddressCanonicalDomainsCanonicalizeFallbackLocalCanonicalizeHostnameCanonicalizeMaxDotsCanonicalizePermittedCNAMEsCASignatureAlgorithmsCertificateFileCheckHostIPCiphersClearAllForwardingsCompressionConnectionAttemptsConnectTimeoutControlMasterControlPathControlPersistDynamicForwardEnableEscapeCommandlineEscapeCharExitOnForwardFailureFingerprintHashForkAfterAuthenticationForwardAgentForwardX11ForwardX11TimeoutForwardX11TrustedGatewayPortsGlobalKnownHostsFileGSSAPIAuthenticationGSSAPIDelegateCredentialsHashKnownHostsHostHostbasedAcceptedAlgorithmsHostbasedAuthenticationHostKeyAlgorithmsHostKeyAliasHostnameIdentitiesOnlyIdentityAgentIdentityFileIPQoSKbdInteractiveAuthenticationKbdInteractiveDevicesKexAlgorithmsKnownHostsCommandLocalCommandLocalForwardLogLevelMACsMatchNoHostAuthenticationForLocalhostNumberOfPasswordPromptsPasswordAuthenticationPermitLocalCommandPermitRemoteOpenPKCS11ProviderPortPreferredAuthenticationsProxyCommandProxyJumpProxyUseFdpassPubkeyAcceptedAlgorithmsPubkeyAuthenticationRekeyLimitRemoteCommandRemoteForwardRequestTTYRequiredRSASizeSendEnvServerAliveIntervalServerAliveCountMaxSessionTypeSetEnvStdinNullStreamLocalBindMaskStreamLocalBindUnlinkStrictHostKeyCheckingTCPKeepAliveTunnelTunnelDeviceUpdateHostKeysUserUserKnownHostsFileVerifyHostKeyDNSVisualHostKeyXAuthLocation-p portPort to connect to on the remote host.This can bespecified on a per-host basis in the configuration file.-Q query_optionQueries for the algorithms supported by one of thefollowing features: cipher (supported symmetric ciphers),cipher-auth (supported symmetric ciphers that supportauthenticated encryption), help (supported query terms foruse with the -Q flag), mac (supported message integritycodes), kex (key exchange algorithms), key (key types),key-cert (certificate key types), key-plain (non-certificate key types), key-sig (all key types andsignature algorithms), protocol-version (supported SSHprotocol versions), and sig (supported signaturealgorithms).Alternatively, any keyword from ssh_config(5)or sshd_config(5) that takes an algorithm list may be usedas an alias for the corresponding query_option.-qQuiet mode.Causes most warning and diagnostic messages tobe suppressed.-R [bind_address:]port:host:hostport-R [bind_address:]port:local_socket-R remote_socket:host:hostport-R remote_socket:local_socket-R [bind_address:]portSpecifies that connections to the given TCP port or Unixsocket on the remote (server) host are to be forwarded tothe local side.This works by allocating a socket to listen to either a TCPport or to a Unix socket on the remote side.Whenever aconnection is made to this port or Unix socket, theconnection is forwarded over the secure channel, and aconnection is made from the local machine to either anexplicit destination specified by host port hostport, orlocal_socket, or, if no explicit destination was specified,ssh will act as a SOCKS 4/5 proxy and forward connectionsto the destinations requested by the remote SOCKS client.Port forwardings can also be specified in the configurationfile.Privileged ports can be forwarded only when loggingin as root on the remote machine.IPv6 addresses can bespecified by enclosing the address in square brackets.By default, TCP listening sockets on the server will bebound to the loopback interface only.This may beoverridden by specifying a bind_address.An emptybind_address, or the address \u2018*\u2019, indicates that the remotesocket should listen on all interfaces.Specifying aremote bind_address will only succeed if the server'sGatewayPorts option is enabled (see sshd_config(5)).If the port argument is \u20180\u2019, the listen port will bedynamically allocated on the server and reported to theclient at run time.When used together with -O forward,the allocated port will be printed to the standard output.-S ctl_pathSpecifies the location of a control socket for connectionsharing, or the string \u201cnone\u201d to disable connectionsharing.Refer to the description of ControlPath andControlMaster in ssh_config(5) for details.-sMay be used to request invocation of a subsystem on theremote system.Subsystems facilitate the use of SSH as asecure transport for other applications (e.g. sftp(1)).The subsystem is specified as the remote command.Refer tothe description of SessionType in ssh_config(5) fordetails.-TDisable pseudo-terminal allocation.-tForce pseudo-terminal allocation.This can be used toexecute arbitrary screen-based programs on a remotemachine, which can be very useful, e.g. when implementingmenu services.Multiple -t options force tty allocation,even if ssh has no local tty.-VDisplay the version number and exit.-vVerbose mode.Causes ssh to print debugging messages aboutits progress.This is helpful in debugging connection,authentication, and configuration problems.Multiple -voptions increase the verbosity.The maximum is 3.-W host:portRequests that standard input and output on the client beforwarded to host on port over the secure channel.Implies-N, -T, ExitOnForwardFailure and ClearAllForwardings,though these can be overridden in the configuration file orusing -o command line options.-w local_tun[:remote_tun]Requests tunnel device forwarding with the specified tun(4)devices between the client (local_tun) and the server(remote_tun).The devices may be specified by numerical ID or the keyword\u201cany\u201d, which uses the next available tunnel device.Ifremote_tun is not specified, it defaults to \u201cany\u201d.Seealso the Tunnel and TunnelDevice directives inssh_config(5).If the Tunnel directive is unset, it will be set to thedefault tunnel mode, which is \u201cpoint-to-point\u201d.If adifferent Tunnel forwarding mode it desired, then it shouldbe specified before -w.-XEnables X11 forwarding.This can also be specified on aper-host basis in a configuration file.X11 forwarding should be enabled with caution.Users withthe ability to bypass file permissions on the remote host(for the user's X authorization database) can access thelocal X11 display through the forwarded connection.Anattacker may then be able to perform activities such askeystroke monitoring.For this reason, X11 forwarding is subjected to X11SECURITY extension restrictions by default.Refer to thessh -Y option and the ForwardX11Trusted directive inssh_config(5) for more information.-xDisables X11 forwarding.-YEnables trusted X11 forwarding.Trusted X11 forwardingsare not subjected to the X11 SECURITY extension controls.-ySend log information using the syslog(3) system module.Bydefault this information is sent to stderr.ssh may additionally obtain configuration data from a per-userconfiguration file and a system-wide configuration file.The fileformat and configuration options are described in ssh_config(5).",
        "name": "ssh \u2014 OpenSSH remote login client",
        "section": 1
    },
    {
        "command": "ssh-add",
        "description": "ssh-add adds private key identities to the authentication agent,ssh-agent(1).When run without arguments, it adds the files~/.ssh/id_rsa, ~/.ssh/id_ecdsa, ~/.ssh/id_ecdsa_sk,~/.ssh/id_ed25519, ~/.ssh/id_ed25519_sk, and ~/.ssh/id_dsa.Afterloading a private key, ssh-add will try to load correspondingcertificate information from the filename obtained by appending-cert.pub to the name of the private key file.Alternative filenames can be given on the command line.If any file requires a passphrase, ssh-add asks for the passphrasefrom the user.The passphrase is read from the user's tty.ssh-add retries the last passphrase if multiple identity files aregiven.The authentication agent must be running and the SSH_AUTH_SOCKenvironment variable must contain the name of its socket forssh-add to work.The options are as follows:-cIndicates that added identities should be subject toconfirmation before being used for authentication.Confirmation is performed by ssh-askpass(1).Successfulconfirmation is signaled by a zero exit status fromssh-askpass(1), rather than text entered into therequester.-DDeletes all identities from the agent.-dInstead of adding identities, removes identities from theagent.If ssh-add has been run without arguments, the keysfor the default identities and their correspondingcertificates will be removed.Otherwise, the argument listwill be interpreted as a list of paths to public key filesto specify keys and certificates to be removed from theagent.If no public key is found at a given path, ssh-addwill append .pub and retry.If the argument list consistsof \u201c-\u201d then ssh-add will read public keys to be removedfrom standard input.-E fingerprint_hashSpecifies the hash algorithm used when displaying keyfingerprints.Valid options are: \u201cmd5\u201d and \u201csha256\u201d.Thedefault is \u201csha256\u201d.-e pkcs11Remove keys provided by the PKCS#11 shared library pkcs11.-H hostkey_fileSpecifies a known hosts file to look up hostkeys when usingdestination-constrained keys via the -h flag.This optionmay be specified multiple times to allow multiple files tobe searched.If no files are specified, ssh-add will usethe default ssh_config(5) known hosts files:~/.ssh/known_hosts, ~/.ssh/known_hosts2,/etc/ssh/ssh_known_hosts, and /etc/ssh/ssh_known_hosts2.-h destination_constraintWhen adding keys, constrain them to be usable only throughspecific hosts or to specific destinations.Destination constraints of the form \u2018[user@]dest-hostname\u2019permit use of the key only from the origin host (the onerunning ssh-agent(1)) to the listed destination host, withoptional user name.Constraints of the form \u2018src-hostname>[user@]dst-hostname\u2019allow a key available on a forwarded ssh-agent(1) to beused through a particular host (as specified by\u2018src-hostname\u2019) to authenticate to a further host,specified by \u2018dst-hostname\u2019.Multiple destination constraints may be added when loadingkeys.When attempting authentication with a key that hasdestination constraints, the whole connection path,including ssh-agent(1) forwarding, is tested against thoseconstraints and each hop must be permitted for the attemptto succeed.For example, if key is forwarded to a remotehost, \u2018host-b\u2019, and is attempting authentication to anotherhost, \u2018host-c\u2019, then the operation will be successful onlyif \u2018host-b\u2019 was permitted from the origin host and thesubsequent \u2018host-b>host-c\u2019 hop is also permitted bydestination constraints.Hosts are identified by their host keys, and are looked upfrom known hosts files by ssh-add.Wildcards patterns maybe used for hostnames and certificate host keys aresupported.By default, keys added by ssh-add are notdestination constrained.Destination constraints were added in OpenSSH release 8.9.Support in both the remote SSH client and server isrequired when using destination-constrained keys over aforwarded ssh-agent(1) channel.It is also important to note that destination constraintscan only be enforced by ssh-agent(1) when a key is used, orwhen it is forwarded by a cooperating ssh(1).Specifically, it does not prevent an attacker with accessto a remote SSH_AUTH_SOCK from forwarding it again andusing it on a different host (but only to a permitteddestination).-KLoad resident keys from a FIDO authenticator.-kWhen loading keys into or deleting keys from the agent,process plain private keys only and skip certificates.-LLists public key parameters of all identities currentlyrepresented by the agent.-lLists fingerprints of all identities currently representedby the agent.-qBe quiet after a successful operation.-S providerSpecifies a path to a library that will be used when addingFIDO authenticator-hosted keys, overriding the default ofusing the internal USB HID support.-s pkcs11Add keys provided by the PKCS#11 shared library pkcs11.-T pubkey ...Tests whether the private keys that correspond to thespecified pubkey files are usable by performing sign andverify operations on each.-t lifeSet a maximum lifetime when adding identities to an agent.The lifetime may be specified in seconds or in a timeformat specified in sshd_config(5).-vVerbose mode.Causes ssh-add to print debugging messagesabout its progress.This is helpful in debugging problems.Multiple -v options increase the verbosity.The maximum is3.-XUnlock the agent.-xLock the agent with a password.",
        "name": "ssh-add \u2014 adds private key identities to the OpenSSH authenticationagent",
        "section": 1
    },
    {
        "command": "ssh-agent",
        "description": "ssh-agent is a program to hold private keys used for public keyauthentication.Through use of environment variables the agent canbe located and automatically used for authentication when loggingin to other machines using ssh(1).The options are as follows:-a bind_addressBind the agent to the UNIX-domain socket bind_address.Thedefault is $TMPDIR/ssh-XXXXXXXXXX/agent.<ppid>.-cGenerate C-shell commands on stdout.This is the defaultif SHELL looks like it's a csh style of shell.-DForeground mode.When this option is specified, ssh-agentwill not fork.-dDebug mode.When this option is specified, ssh-agent willnot fork and will write debug information to standarderror.-E fingerprint_hashSpecifies the hash algorithm used when displaying keyfingerprints.Valid options are: \u201cmd5\u201d and \u201csha256\u201d.Thedefault is \u201csha256\u201d.-kKill the current agent (given by the SSH_AGENT_PIDenvironment variable).-O optionSpecify an option when starting ssh-agent.Currently onlyone option is supported: no-restrict-websafe.Thisinstructs ssh-agent to permit signatures using FIDO keysthat might be web authentication requests.By default,ssh-agent refuses signature requests for FIDO keys wherethe key application string does not start with \u201cssh:\u201d andwhen the data to be signed does not appear to be a ssh(1)user authentication request or a ssh-keygen(1) signature.The default behaviour prevents forwarded access to a FIDOkey from also implicitly forwarding the ability toauthenticate to websites.-P allowed_providersSpecify a pattern-list of acceptable paths for PKCS#11provider and FIDO authenticator middleware shared librariesthat may be used with the -S or -s options to ssh-add(1).Libraries that do not match the pattern list will berefused.See PATTERNS in ssh_config(5) for a descriptionof pattern-list syntax.The default list is\u201c/usr/lib/*,/usr/local/lib/*\u201d.-sGenerate Bourne shell commands on stdout.This is thedefault if SHELL does not look like it's a csh style ofshell.-t lifeSet a default value for the maximum lifetime of identitiesadded to the agent.The lifetime may be specified inseconds or in a time format specified in sshd_config(5).Alifetime specified for an identity with ssh-add(1)overrides this value.Without this option the defaultmaximum lifetime is forever.command [arg ...]If a command (and optional arguments) is given, this isexecuted as a subprocess of the agent.The agent exitsautomatically when the command given on the command lineterminates.There are two main ways to get an agent set up.The first is atthe start of an X session, where all other windows or programs arestarted as children of the ssh-agent program.The agent starts acommand under which its environment variables are exported, forexample ssh-agent xterm &.When the command terminates, so doesthe agent.The second method is used for a login session.When ssh-agent isstarted, it prints the shell commands required to set itsenvironment variables, which in turn can be evaluated in thecalling shell, for example eval `ssh-agent -s`.In both cases, ssh(1) looks at these environment variables and usesthem to establish a connection to the agent.The agent initially does not have any private keys.Keys are addedusing ssh-add(1) or by ssh(1) when AddKeysToAgent is set inssh_config(5).Multiple identities may be stored in ssh-agentconcurrently and ssh(1) will automatically use them if present.ssh-add(1) is also used to remove keys from ssh-agent and to querythe keys that are held in one.Connections to ssh-agent may be forwarded from further remote hostsusing the -A option to ssh(1) (but see the caveats documentedtherein), avoiding the need for authentication data to be stored onother machines.Authentication passphrases and private keys nevergo over the network: the connection to the agent is forwarded overSSH remote connections and the result is returned to the requester,allowing the user access to their identities anywhere in thenetwork in a secure fashion.",
        "name": "ssh-agent \u2014 OpenSSH authentication agent",
        "section": 1
    },
    {
        "command": "ssh-keygen",
        "description": "ssh-keygen generates, manages and converts authentication keys forssh(1).ssh-keygen can create keys for use by SSH protocol version2.The type of key to be generated is specified with the -t option.If invoked without any arguments, ssh-keygen will generate an RSAkey.ssh-keygen is also used to generate groups for use in Diffie-Hellman group exchange (DH-GEX).See the MODULI GENERATION sectionfor details.Finally, ssh-keygen can be used to generate and update KeyRevocation Lists, and to test whether given keys have been revokedby one.See the KEY REVOCATION LISTS section for details.Normally each user wishing to use SSH with public keyauthentication runs this once to create the authentication key in~/.ssh/id_dsa, ~/.ssh/id_ecdsa, ~/.ssh/id_ecdsa_sk,~/.ssh/id_ed25519, ~/.ssh/id_ed25519_sk or ~/.ssh/id_rsa.Additionally, the system administrator may use this to generatehost keys, as seen in /etc/rc.Normally this program generates the key and asks for a file inwhich to store the private key.The public key is stored in a filewith the same name but \u201c.pub\u201d appended.The program also asks fora passphrase.The passphrase may be empty to indicate nopassphrase (host keys must have an empty passphrase), or it may bea string of arbitrary length.A passphrase is similar to apassword, except it can be a phrase with a series of words,punctuation, numbers, whitespace, or any string of characters youwant.Good passphrases are 10-30 characters long, are not simplesentences or otherwise easily guessable (English prose has only 1-2bits of entropy per character, and provides very bad passphrases),and contain a mix of upper and lowercase letters, numbers, and non-alphanumeric characters.The passphrase can be changed later byusing the -p option.There is no way to recover a lost passphrase.If the passphrase islost or forgotten, a new key must be generated and thecorresponding public key copied to other machines.ssh-keygen will by default write keys in an OpenSSH-specificformat.This format is preferred as it offers better protectionfor keys at rest as well as allowing storage of key comments withinthe private key file itself.The key comment may be useful to helpidentify the key.The comment is initialized to \u201cuser@host\u201d whenthe key is created, but can be changed using the -c option.It is still possible for ssh-keygen to write the previously-usedPEM format private keys using the -m flag.This may be used whengenerating new keys, and existing new-format keys may be convertedusing this option in conjunction with the -p (change passphrase)flag.After a key is generated, ssh-keygen will ask where the keys shouldbe placed to be activated.The options are as follows:-AGenerate host keys of all default key types (rsa, ecdsa,and ed25519) if they do not already exist.The host keysare generated with the default key file path, an emptypassphrase, default bits for the key type, and defaultcomment.If -f has also been specified, its argument isused as a prefix to the default path for the resulting hostkey files.This is used by /etc/rc to generate new hostkeys.-a roundsWhen saving a private key, this option specifies the numberof KDF (key derivation function, currently bcrypt_pbkdf(3))rounds used.Higher numbers result in slower passphraseverification and increased resistance to brute-forcepassword cracking (should the keys be stolen).The defaultis 16 rounds.-BShow the bubblebabble digest of specified private or publickey file.-b bitsSpecifies the number of bits in the key to create.For RSAkeys, the minimum size is 1024 bits and the default is 3072bits.Generally, 3072 bits is considered sufficient.DSAkeys must be exactly 1024 bits as specified by FIPS 186-2.For ECDSA keys, the -b flag determines the key length byselecting from one of three elliptic curve sizes: 256, 384or 521 bits.Attempting to use bit lengths other thanthese three values for ECDSA keys will fail.ECDSA-SK,Ed25519 and Ed25519-SK keys have a fixed length and the -bflag will be ignored.-C commentProvides a new comment.-cRequests changing the comment in the private and public keyfiles.The program will prompt for the file containing theprivate keys, for the passphrase if the key has one, andfor the new comment.-D pkcs11Download the public keys provided by the PKCS#11 sharedlibrary pkcs11.When used in combination with -s, thisoption indicates that a CA key resides in a PKCS#11 token(see the CERTIFICATES section for details).-E fingerprint_hashSpecifies the hash algorithm used when displaying keyfingerprints.Valid options are: \u201cmd5\u201d and \u201csha256\u201d.Thedefault is \u201csha256\u201d.-eThis option will read a private or public OpenSSH key fileand print to stdout a public key in one of the formatsspecified by the -m option.The default export format is\u201cRFC4716\u201d.This option allows exporting OpenSSH keys foruse by other programs, including several commercial SSHimplementations.-F hostname | [hostname]:portSearch for the specified hostname (with optional portnumber) in a known_hosts file, listing any occurrencesfound.This option is useful to find hashed host names oraddresses and may also be used in conjunction with the -Hoption to print found keys in a hashed format.-f filenameSpecifies the filename of the key file.-gUse generic DNS format when printing fingerprint resourcerecords using the -r command.-HHash a known_hosts file.This replaces all hostnames andaddresses with hashed representations within the specifiedfile; the original content is moved to a file with a .oldsuffix.These hashes may be used normally by ssh and sshd,but they do not reveal identifying information should thefile's contents be disclosed.This option will not modifyexisting hashed hostnames and is therefore safe to use onfiles that mix hashed and non-hashed names.-hWhen signing a key, create a host certificate instead of auser certificate.See the CERTIFICATES section fordetails.-I certificate_identitySpecify the key identity when signing a public key.Seethe CERTIFICATES section for details.-iThis option will read an unencrypted private (or public)key file in the format specified by the -m option and printan OpenSSH compatible private (or public) key to stdout.This option allows importing keys from other software,including several commercial SSH implementations.Thedefault import format is \u201cRFC4716\u201d.-KDownload resident keys from a FIDO authenticator.Publicand private key files will be written to the currentdirectory for each downloaded key.If multiple FIDOauthenticators are attached, keys will be downloaded fromthe first touched authenticator.See the FIDOAUTHENTICATOR section for more information.-kGenerate a KRL file.In this mode, ssh-keygen willgenerate a KRL file at the location specified via the -fflag that revokes every key or certificate presented on thecommand line.Keys/certificates to be revoked may bespecified by public key file or using the format describedin the KEY REVOCATION LISTS section.-LPrints the contents of one or more certificates.-lShow fingerprint of specified public key file.For RSA andDSA keys ssh-keygen tries to find the matching public keyfile and prints its fingerprint.If combined with -v, avisual ASCII art representation of the key is supplied withthe fingerprint.-M generateGenerate candidate Diffie-Hellman Group Exchange (DH-GEX)parameters for eventual use by the\u2018diffie-hellman-group-exchange-*\u2019 key exchange methods.The numbers generated by this operation must be furtherscreened before use.See the MODULI GENERATION section formore information.-M screenScreen candidate parameters for Diffie-Hellman GroupExchange.This will accept a list of candidate numbers andtest that they are safe (Sophie Germain) primes withacceptable group generators.The results of this operationmay be added to the /etc/moduli file.See the MODULIGENERATION section for more information.-m key_formatSpecify a key format for key generation, the -i (import),-e (export) conversion options, and the -p changepassphrase operation.The latter may be used to convertbetween OpenSSH private key and PEM private key formats.The supported key formats are: \u201cRFC4716\u201d (RFC 4716/SSH2public or private key), \u201cPKCS8\u201d (PKCS8 public or privatekey) or \u201cPEM\u201d (PEM public key).By default OpenSSH willwrite newly-generated private keys in its own format, butwhen converting public keys for export the default formatis \u201cRFC4716\u201d.Setting a format of \u201cPEM\u201d when generating orupdating a supported private key type will cause the key tobe stored in the legacy PEM private key format.-N new_passphraseProvides the new passphrase.-n principalsSpecify one or more principals (user or host names) to beincluded in a certificate when signing a key.Multipleprincipals may be specified, separated by commas.See theCERTIFICATES section for details.-O optionSpecify a key/value option.These are specific to theoperation that ssh-keygen has been requested to perform.When signing certificates, one of the options listed in theCERTIFICATES section may be specified here.When performing moduli generation or screening, one of theoptions listed in the MODULI GENERATION section may bespecified.When generating FIDO authenticator-backed keys, the optionslisted in the FIDO AUTHENTICATOR section may be specified.When performing signature-related options using the -Yflag, the following options are accepted:hashalg=algorithmSelects the hash algorithm to use for hashing themessage to be signed.Valid algorithms are\u201csha256\u201d and \u201csha512.\u201d The default is \u201csha512.\u201dprint-pubkeyPrint the full public key to standard output aftersignature verification.verify-time=timestampSpecifies a time to use when validating signaturesinstead of the current time.The time may bespecified as a date or time in the YYYYMMDD[Z] orin YYYYMMDDHHMM[SS][Z] formats.Dates and timeswill be interpreted in the current system time zoneunless suffixed with a Z character, which causesthem to be interpreted in the UTC time zone.When generating SSHFP DNS records from public keys usingthe -r flag, the following options are accepted:hashalg=algorithmSelects a hash algorithm to use when printing SSHFPrecords using the -D flag.Valid algorithms are\u201csha1\u201d and \u201csha256\u201d.The default is to print both.The -O option may be specified multiple times.-P passphraseProvides the (old) passphrase.-pRequests changing the passphrase of a private key fileinstead of creating a new private key.The program willprompt for the file containing the private key, for the oldpassphrase, and twice for the new passphrase.-QTest whether keys have been revoked in a KRL.If the -loption is also specified then the contents of the KRL willbe printed.-qSilence ssh-keygen.-R hostname | [hostname]:portRemoves all keys belonging to the specified hostname (withoptional port number) from a known_hosts file.This optionis useful to delete hashed hosts (see the -H option above).-r hostnamePrint the SSHFP fingerprint resource record named hostnamefor the specified public key file.-s ca_keyCertify (sign) a public key using the specified CA key.See the CERTIFICATES section for details.When generating a KRL, -s specifies a path to a CA publickey file used to revoke certificates directly by key ID orserial number.See the KEY REVOCATION LISTS section fordetails.-t dsa | ecdsa | ecdsa-sk | ed25519 | ed25519-sk | rsaSpecifies the type of key to create.The possible valuesare \u201cdsa\u201d, \u201cecdsa\u201d, \u201cecdsa-sk\u201d, \u201ced25519\u201d, \u201ced25519-sk\u201d, or\u201crsa\u201d.This flag may also be used to specify the desired signaturetype when signing certificates using an RSA CA key.Theavailable RSA signature variants are \u201cssh-rsa\u201d (SHA1signatures, not recommended), \u201crsa-sha2-256\u201d, and\u201crsa-sha2-512\u201d (the default).-UWhen used in combination with -s or -Y sign, this optionindicates that a CA key resides in a ssh-agent(1).See theCERTIFICATES section for more information.-uUpdate a KRL.When specified with -k, keys listed via thecommand line are added to the existing KRL rather than anew KRL being created.-V validity_intervalSpecify a validity interval when signing a certificate.Avalidity interval may consist of a single time, indicatingthat the certificate is valid beginning now and expiring atthat time, or may consist of two times separated by a colonto indicate an explicit time interval.The start time may be specified as:\u2022The string \u201calways\u201d to indicate the certificate has nospecified start time.\u2022A date or time in the system time zone formatted asYYYYMMDD or YYYYMMDDHHMM[SS].\u2022A date or time in the UTC time zone as YYYYMMDDZ orYYYYMMDDHHMM[SS]Z.\u2022A relative time before the current system timeconsisting of a minus sign followed by an interval inthe format described in the TIME FORMATS section ofsshd_config(5).\u2022A raw seconds since epoch (Jan 1 1970 00:00:00 UTC) asa hexadecimal number beginning with \u201c0x\u201d.The end time may be specified similarly to the start time:\u2022The string \u201cforever\u201d to indicate the certificate has nospecified end time.\u2022A date or time in the system time zone formatted asYYYYMMDD or YYYYMMDDHHMM[SS].\u2022A date or time in the UTC time zone as YYYYMMDDZ orYYYYMMDDHHMM[SS]Z.\u2022A relative time after the current system timeconsisting of a plus sign followed by an interval inthe format described in the TIME FORMATS section ofsshd_config(5).\u2022A raw seconds since epoch (Jan 1 1970 00:00:00 UTC) asa hexadecimal number beginning with \u201c0x\u201d.For example:+52w1dValid from now to 52 weeks and one day from now.-4w:+4wValid from four weeks ago to four weeks from now.20100101123000:20110101123000Valid from 12:30 PM, January 1st, 2010 to 12:30 PM,January 1st, 2011.20100101123000Z:20110101123000ZSimilar, but interpreted in the UTC time zonerather than the system time zone.-1d:20110101Valid from yesterday to midnight, January 1st,2011.0x1:0x2000000000Valid from roughly early 1970 to May 2033.-1m:foreverValid from one minute ago and never expiring.-vVerbose mode.Causes ssh-keygen to print debuggingmessages about its progress.This is helpful for debuggingmoduli generation.Multiple -v options increase theverbosity.The maximum is 3.-w providerSpecifies a path to a library that will be used whencreating FIDO authenticator-hosted keys, overriding thedefault of using the internal USB HID support.-Y find-principalsFind the principal(s) associated with the public key of asignature, provided using the -s flag in an authorizedsigners file provided using the -f flag.The format of theallowed signers file is documented in the ALLOWED SIGNERSsection below.If one or more matching principals arefound, they are returned on standard output.-Y match-principalsFind principal matching the principal name provided usingthe -I flag in the authorized signers file specified usingthe -f flag.If one or more matching principals are found,they are returned on standard output.-Y check-novalidateChecks that a signature generated using ssh-keygen -Y signhas a valid structure.This does not validate if asignature comes from an authorized signer.When testing asignature, ssh-keygen accepts a message on standard inputand a signature namespace using -n.A file containing thecorresponding signature must also be supplied using the -sflag.Successful testing of the signature is signalled byssh-keygen returning a zero exit status.-Y signCryptographically sign a file or some data using a SSH key.When signing, ssh-keygen accepts zero or more files to signon the command-line - if no files are specified thenssh-keygen will sign data presented on standard input.Signatures are written to the path of the input file with\u201c.sig\u201d appended, or to standard output if the message to besigned was read from standard input.The key used for signing is specified using the -f optionand may refer to either a private key, or a public key withthe private half available via ssh-agent(1).An additionalsignature namespace, used to prevent signature confusionacross different domains of use (e.g. file signing vs emailsigning) must be provided via the -n flag.Namespaces arearbitrary strings, and may include: \u201cfile\u201d for filesigning, \u201cemail\u201d for email signing.For custom uses, it isrecommended to use names following a NAMESPACE@YOUR.DOMAINpattern to generate unambiguous namespaces.-Y verifyRequest to verify a signature generated using ssh-keygen -Ysign as described above.When verifying a signature,ssh-keygen accepts a message on standard input and asignature namespace using -n.A file containing thecorresponding signature must also be supplied using the -sflag, along with the identity of the signer using -I and alist of allowed signers via the -f flag.The format of theallowed signers file is documented in the ALLOWED SIGNERSsection below.A file containing revoked keys can bepassed using the -r flag.The revocation file may be a KRLor a one-per-line list of public keys.Successfulverification by an authorized signer is signalled byssh-keygen returning a zero exit status.-yThis option will read a private OpenSSH format file andprint an OpenSSH public key to stdout.-Z cipherSpecifies the cipher to use for encryption when writing anOpenSSH-format private key file.The list of availableciphers may be obtained using \"ssh -Q cipher\".The defaultis \u201caes256-ctr\u201d.-z serial_numberSpecifies a serial number to be embedded in the certificateto distinguish this certificate from others from the sameCA.If the serial_number is prefixed with a \u2018+\u2019 character,then the serial number will be incremented for eachcertificate signed on a single command-line.The defaultserial number is zero.When generating a KRL, the -z flag is used to specify a KRLversion number.",
        "name": "ssh-keygen \u2014 OpenSSH authentication key utility",
        "section": 1
    },
    {
        "command": "ssh-keyscan",
        "description": "ssh-keyscan is a utility for gathering the public SSH host keys ofa number of hosts.It was designed to aid in building andverifying ssh_known_hosts files, the format of which is documentedin sshd(8).ssh-keyscan provides a minimal interface suitable foruse by shell and perl scripts.ssh-keyscan uses non-blocking socket I/O to contact as many hostsas possible in parallel, so it is very efficient.The keys from adomain of 1,000 hosts can be collected in tens of seconds, evenwhen some of those hosts are down or do not run sshd(8).Forscanning, one does not need login access to the machines that arebeing scanned, nor does the scanning process involve anyencryption.Hosts to be scanned may be specified by hostname, address or byCIDR network range (e.g. 192.168.16/28).If a network range isspecified, then all addresses in that range will be scanned.The options are as follows:-4Force ssh-keyscan to use IPv4 addresses only.-6Force ssh-keyscan to use IPv6 addresses only.-cRequest certificates from target hosts instead of plainkeys.-DPrint keys found as SSHFP DNS records.The default is toprint keys in a format usable as a ssh(1) known_hosts file.-f fileRead hosts or \u201caddrlist namelist\u201d pairs from file, one perline.If \u2018-\u2019 is supplied instead of a filename,ssh-keyscan will read from the standard input.Names readfrom a file must start with an address, hostname or CIDRnetwork range to be scanned.Addresses and hostnames mayoptionally be followed by comma-separated name or addressaliases that will be copied to the output.For example:192.168.11.0/2410.20.1.1happy.example.org10.0.0.1,sad.example.org-HHash all hostnames and addresses in the output.Hashednames may be used normally by ssh(1) and sshd(8), but theydo not reveal identifying information should the file'scontents be disclosed.-O optionSpecify a key/value option.At present, only a singleoption is supported:hashalg=algorithmSelects a hash algorithm to use when printing SSHFPrecords using the -D flag.Valid algorithms are\u201csha1\u201d and \u201csha256\u201d.The default is to print both.-p portConnect to port on the remote host.-T timeoutSet the timeout for connection attempts.If timeoutseconds have elapsed since a connection was initiated to ahost or since the last time anything was read from thathost, the connection is closed and the host in questionconsidered unavailable.The default is 5 seconds.-t typeSpecify the type of the key to fetch from the scannedhosts.The possible values are \u201cdsa\u201d, \u201cecdsa\u201d, \u201ced25519\u201d,\u201cecdsa-sk\u201d, \u201ced25519-sk\u201d, or \u201crsa\u201d.Multiple values may bespecified by separating them with commas.The default isto fetch \u201crsa\u201d, \u201cecdsa\u201d, \u201ced25519\u201d, \u201cecdsa-sk\u201d, and\u201ced25519-sk\u201d keys.-vVerbose mode: print debugging messages about progress.If an ssh_known_hosts file is constructed using ssh-keyscan withoutverifying the keys, users will be vulnerable to man in the middleattacks.On the other hand, if the security model allows such arisk, ssh-keyscan can help in the detection of tampered keyfiles orman in the middle attacks which have begun after thessh_known_hosts file was created.",
        "name": "ssh-keyscan \u2014 gather SSH public keys from servers",
        "section": 1
    },
    {
        "command": "sshfs",
        "description": "SSHFS allows you to mount a remote filesystem using SSH (moreprecisely, the SFTP subsystem). Most SSH servers support andenable this SFTP access by default, so SSHFS is very simple touse - there's nothing to do on the server-side.By default, file permissions are ignored by SSHFS. Any user thatcan access the filesystem will be able to perform any operationthat the remote server permits - based on the credentials thatwere used to connect to the server. If this is undesired, localpermission checking can be enabled with -o default_permissions.By default, only the mounting user will be able to access thefilesystem. Access for other users can be enabled by passing -oallow_other. In this case you most likely also want to use -odefault_permissions.It is recommended to run SSHFS as regular user (not as root).For this to work the mountpoint must be owned by the user.Ifusername is omitted SSHFS will use the local username. If thedirectory is omitted, SSHFS will mount the (remote) homedirectory.If you need to enter a password sshfs will ask for it(actually it just runs ssh which ask for the password if needed).",
        "name": "SSHFS - filesystem client based on SSH",
        "section": 1
    },
    {
        "command": "stap",
        "description": "The stap program is the front-end to the Systemtap tool.Itaccepts probing instructions written in a simple domain-specificlanguage, translates those instructions into C code, compilesthis C code, and loads the resulting module into a running Linuxkernel or a Dyninst user-space mutator, to perform the requestedsystem trace/probe functions.You can supply the script in anamed file (FILENAME), from standard input (use - instead ofFILENAME), or from the command line (using -e SCRIPT).Theprogram runs until it is interrupted by the user, or if thescript voluntarily invokes the exit() function, or by sufficientnumber of soft errors.The language, which is described the SCRIPT LANGUAGE sectionbelow, is strictly typed, expressive, declaration free,procedural, prototyping-friendly, and inspired by awk and C.Itallows source code points or events in the system to beassociated with handlers, which are subroutines that are executedsynchronously.It is somewhat similar conceptually to\"breakpoint command lists\" in the gdb debugger.",
        "name": "stap - systemtap script translator/driver",
        "section": 1
    },
    {
        "command": "stap-jupyter",
        "description": "ISystemtap is an interactive jupyter interface for theincremental writing and running of Systemtap scripts.The stap-jupyter-install program can be used to locally installthe ISystemtap jupyter kernel, language-server and jupyter-labextension in ~/.systemtap/jupyter.Once installed, the kernelcan be used with jupyter-lab.Alternatively the stap-jupyter-container program can be used torun ISystemtap within a container, preventing the need for anylocal jupyter kernel installation.",
        "name": "stap-jupyter-install- locally install isystemtapstap-jupyter-container - manage an isystemtap container image",
        "section": 1
    },
    {
        "command": "stap-jupyter-container",
        "description": "ISystemtap is an interactive jupyter interface for theincremental writing and running of Systemtap scripts.The stap-jupyter-install program can be used to locally installthe ISystemtap jupyter kernel, language-server and jupyter-labextension in ~/.systemtap/jupyter.Once installed, the kernelcan be used with jupyter-lab.Alternatively the stap-jupyter-container program can be used torun ISystemtap within a container, preventing the need for anylocal jupyter kernel installation.",
        "name": "stap-jupyter-install- locally install isystemtapstap-jupyter-container - manage an isystemtap container image",
        "section": 1
    },
    {
        "command": "stap-jupyter-install",
        "description": "ISystemtap is an interactive jupyter interface for theincremental writing and running of Systemtap scripts.The stap-jupyter-install program can be used to locally installthe ISystemtap jupyter kernel, language-server and jupyter-labextension in ~/.systemtap/jupyter.Once installed, the kernelcan be used with jupyter-lab.Alternatively the stap-jupyter-container program can be used torun ISystemtap within a container, preventing the need for anylocal jupyter kernel installation.",
        "name": "stap-jupyter-install- locally install isystemtapstap-jupyter-container - manage an isystemtap container image",
        "section": 1
    },
    {
        "command": "stap-merge",
        "description": "The stap-merge executable applies when the -b option has beenused while running a stap script.The -b option will generatefiles per-cpu, based on the timestamp field. Then stap-merge willmerge and sort through the per-cpu files based on the timestampfield.",
        "name": "stap-merge - systemtap per-cpu binary merger",
        "section": 1
    },
    {
        "command": "stap-prep",
        "description": "The stap-prep executable prepares the system for systemtap use byinstalling kernel headers, debug symbols and build tools thatmatch the currently running kernel or optionally the kernelversion given by the user.If the debuginfod-find tool is installed and is able to fetchdebuginfo for a kernel component, it is assumed to remainavailable later.In this case, no debug symbols will bedownloaded during stap-prep.The exact behavior of stap-prep may be customized by thedistribution maintainers. It might for example only givesuggestions and not actually install the required packages ifthat is difficult to automate.",
        "name": "stap-prep - prepare system for systemtap use",
        "section": 1
    },
    {
        "command": "stap-report",
        "description": "The stap-report executable collects system information that isuseful for debugging systemtap bugs. It is a good idea to includesuch a report in bug reports especially if you send them directlyto the upstream. stap-report can be run either as a normal useror as root. The report will be more complete if stap-report isrun as root.",
        "name": "stap-report - collect system information that is useful fordebugging systemtap bugs",
        "section": 1
    },
    {
        "command": "stapref",
        "description": "The reference for the systemtap scripting language.",
        "name": "stapref - systemtap language reference",
        "section": 1
    },
    {
        "command": "stapvirt",
        "description": "The stapvirt program can be used to add ports to domains managedby libvirt (see <http://libvirt.org/>).These ports can then beused by stap to run scripts inside the domains (see the'--remote' option in stap(1) for more information).Ports are added to the definition of the domain using the port-add command.These ports can later be removed using the port-remove command.Note that there can only be as many simultaneousstap sessions as there are ports.Starting from libvirt v1.1.1 and QEMU v0.10.0, SystemTap portscan be hotplugged and thus do not need to be added first usingthe port-add command.However, you need to ensure that there isa virtio-serial controller in place so that hotplugged ports canbe connected. If creating a domain using virt-install, you can dothis by adding this option:$ virt-install [...] --controller=virtio-serialIf the domain has already been created, you can simply do port-add followed immediately by port-remove, and then power off andrestart the domain. The port will be removed, but the controllerwill remain.",
        "name": "stapvirt - prepare libvirt domains for systemtap probing",
        "section": 1
    },
    {
        "command": "stat",
        "description": "Display file or file system status.Mandatory arguments to long options are mandatory for shortoptions too.-L, --dereferencefollow links-f, --file-systemdisplay file system status instead of file status--cached=MODEspecify how to use cached attributes; useful on remotefile systems. See MODE below-c--format=FORMATuse the specified FORMAT instead of the default; output anewline after each use of FORMAT--printf=FORMATlike --format, but interpret backslash escapes, and do notoutput a mandatory trailing newline; if you want anewline, include \\n in FORMAT-t, --terseprint the information in terse form--help display this help and exit--versionoutput version information and exitThe MODE argument of --cached can be: always, never, or default.'always' will use cached attributes if available, while 'never'will try to synchronize with the latest attributes, and 'default'will leave it up to the underlying file system.The valid format sequences for files (without --file-system):%apermission bits in octal (note '#' and '0' printf flags)%Apermission bits and file type in human readable form%bnumber of blocks allocated (see %B)%Bthe size in bytes of each block reported by %b%CSELinux security context string%ddevice number in decimal (st_dev)%Ddevice number in hex (st_dev)%Hdmajor device number in decimal%Ldminor device number in decimal%fraw mode in hex%Ffile type%ggroup ID of owner%Ggroup name of owner%hnumber of hard links%iinode number%mmount point%nfile name%Nquoted file name with dereference if symbolic link%ooptimal I/O transfer size hint%stotal size, in bytes%rdevice type in decimal (st_rdev)%Rdevice type in hex (st_rdev)%Hrmajor device type in decimal, for character/block devicespecial files%Lrminor device type in decimal, for character/block devicespecial files%tmajor device type in hex, for character/block devicespecial files%Tminor device type in hex, for character/block devicespecial files%uuser ID of owner%Uuser name of owner%wtime of file birth, human-readable; - if unknown%Wtime of file birth, seconds since Epoch; 0 if unknown%xtime of last access, human-readable%Xtime of last access, seconds since Epoch%ytime of last data modification, human-readable%Ytime of last data modification, seconds since Epoch%ztime of last status change, human-readable%Ztime of last status change, seconds since EpochValid format sequences for file systems:%afree blocks available to non-superuser%btotal data blocks in file system%ctotal file nodes in file system%dfree file nodes in file system%ffree blocks in file system%ifile system ID in hex%lmaximum length of filenames%nfile name%sblock size (for faster transfers)%Sfundamental block size (for block counts)%tfile system type in hex%Tfile system type in human readable form--terse is equivalent to the following FORMAT:%n %s %b %f %u %g %D %i %h %t %T %X %Y %Z %W %o %C--terse --file-system is equivalent to the following FORMAT:%n %i %l %t %s %S %b %f %a %c %dNOTE: your shell may have its own version of stat, which usuallysupersedes the version described here.Please refer to yourshell's documentation for details about the options it supports.",
        "name": "stat - display file or file system status",
        "section": 1
    },
    {
        "command": "stdbuf",
        "description": "Run COMMAND, with modified buffering operations for its standardstreams.Mandatory arguments to long options are mandatory for shortoptions too.-i, --input=MODEadjust standard input stream buffering-o, --output=MODEadjust standard output stream buffering-e, --error=MODEadjust standard error stream buffering--help display this help and exit--versionoutput version information and exitIf MODE is 'L' the corresponding stream will be line buffered.This option is invalid with standard input.If MODE is '0' the corresponding stream will be unbuffered.Otherwise MODE is a number which may be followed by one of thefollowing: KB 1000, K 1024, MB 1000*1000, M 1024*1024, and so onfor G,T,P,E,Z,Y,R,Q.Binary prefixes can be used, too: KiB=K,MiB=M, and so on.In this case the corresponding stream will befully buffered with the buffer size set to MODE bytes.NOTE: If COMMAND adjusts the buffering of its standard streams('tee' does for example) then that will override correspondingchanges by 'stdbuf'.Also some filters (like 'dd' and 'cat'etc.) don't use streams for I/O, and are thus unaffected by'stdbuf' settings.Exit status:125if the stdbuf command itself fails126if COMMAND is found but cannot be invoked127if COMMAND cannot be found-the exit status of COMMAND otherwise",
        "name": "stdbuf - Run COMMAND, with modified buffering operations for itsstandard streams.",
        "section": 1
    },
    {
        "command": "stg",
        "description": "StGit (Stacked Git) is an application that provides a convenientway to maintain a patch stack on top of a Git branch:\u2022The topmost (most recent) commits of a branch are givennames. Such a named commit is called a patch.\u2022After making changes to the worktree, you can incorporate thechanges into an existing patch; this is called refreshing.You may refresh any patch, not just the topmost one.\u2022You can pop a patch: temporarily putting it aside, so thatthe patch below it becomes the topmost patch. Later you maypush it onto the stack again. Pushing and popping can be usedto reorder patches.\u2022You can easily rebase your patch stack on top of any otherGit commit. (The base of a patch stack is the most recent Gitcommit that is not an StGit patch.) For example, if youstarted making patches on top of someone else\u2019s branch, andthat person publishes an updated branch, you can take allyour patches and apply them on top of the updated branch.\u2022As you would expect, changing what is below a patch can causethat patch to no longer apply cleanly \u2014 this can occur whenyou reorder patches, rebase patches, or refresh a non-topmostpatch. StGit uses Git\u2019s rename-aware three-way mergecapability to automatically fix up what it can; if it stillfails, it lets you manually resolve the conflict just likeyou would resolve a merge conflict in Git.\u2022The patch stack is just some extra metadata attached toregular Git commits, so you can continue to use most Gittools along with StGit.Typical usesTracking branchTracking changes from a remote branch, while maintaininglocal modifications against that branch, possibly with theintent of sending some patches upstream. You can modify yourpatch stack as much as you want, and when your patches arefinally accepted upstream, the permanent recorded Git historywill contain just the final sequence of patches, and not themessy sequence of edits that produced them.Commands of interest in this workflow are e.g. rebase andmail.Development branchEven if you have no \"upstream\" to send patches to, you canuse StGit as a convenient way to modify the recent history ofa Git branch. For example, instead of first committing changeA, then change B, and then A2 to fix A because it wasn\u2019tquite right, you could incorporate the fix directly into A.This way of working results in a much more readable Githistory than if you had immortalized every misstep you madeon your way to the right solution.Commands of interest in this workflow are e.g. uncommit,which can be used to move the patch stack base downwards \u2014i.e., turn Git commits into StGit patches after the fact \u2014and commit, its inverse.For more information, see the tutorial[1].Specifying patchesMost StGit commands have patch arguments. Patches in the stackmay be specified in a variety of ways. A patch in the currentbranch may simply referred to by its name, or, alternatively, belocated by a relative offset from the topmost patch (e.g. +3), asan absolute index into the stack (e.g. 7), or as an offset fromthe last visible patch (e.g. ^1).Some commands allow you to specify a patch in another branch ofthe repository; this is done by prefixing the patch name with thebranch name and a colon (e.g. otherbranch:thatpatch).Commands that take multiple patch arguments may be supplied withpatch ranges of the form patch1..patchN as an alternative tospecifying each patch individually. For example, stg deletep0..p4 would be equivalent to stg delete p0 p1 p2 p3 p4. Patchranges may be open on either or both ends. For example, stgdelete ..p2 would delete the first applied patch up to andincluding patch p2. Alternatively, stg delete p2.. would deletepatch p2 up to and including the topmost applied patch. And stgdelete .. would delete all applied patches.The complete syntax for locating patches follows:<patchname>, e.g. patchThe name of a patch.@Refers to the topmost applied patch, or the base of the stackif no patches are applied.[<patchname>]~[<n>], e.g. ~2, patch~, patch~3The <n>th previous patch from the named patch. If <patchname>is not supplied, @ is implied. A single ~ represents thefirst previous patch. Multiple ~ may be specified, e.g.patch~~~ is the same as patch~3. This is similar to git\u2019srevision syntax where <rev>~[<n>] means the <n>th ancestorcommit from <rev> following first parents.[<patchname>]+[<n>], e.g. +, +3, patch+, patch+3The <n>th next patch from the named patch. If <patchname> isnot supplied, @ is implied. A single + represents the nextpatch in the series. Multiple + may be specified, e.g.patch+++ is the same as patch+3.-[<n>], e.g. -3, -References the <n>th previously applied patch. This issimilar to ~<n>, except it is only valid without a patch nameprefix. Note that certain commands with other options takingnumeric values may require escaping - with \\-, e.g.\\-10.<n>, e.g. 3The patch at absolute index <n> in the stack. This is azero-based index, so 0 refers to the bottommost patch in thestack.^[<n>], e.g. ^, ^3The patch at offset <n> from the last visible patch in thestack. This is a zero-based offset, so ^0 refers to the lastvisible patch in the stack, which is equivalent to just ^.Negative values of <n> are allowed and refer to hiddenpatches which are after the last visible patch in the stack.{base}+[<n>], e.g. {base}+, {base}+3The patch at offset <n> from the stack\u2019s base commit. Sincethe stack base is not a commit, a positive offset isrequired.Take note that numeric patch locations of the form <n>, -<n>, and+<n>, e.g. 3, -3, or +3 are also valid patch names. I.e. it ispossible (but not recommended) to name a patch, for example,\"-3\". In the case where a patch name could also be interpreted asa numeric index or offset, the literal patch name will takeprecidence when resolving the patch location.Specifying commitsSome StGit commands take Git commits as arguments. StGit acceptsall revision specifications that Git does (see gitrevisions(7));and additionally, the patch specifiers from above. The usual Gitmodifiers, including ^, are also allowed; e.g.some-branch:a-patch^^ refers to the grandparent of the committhat is patch a-patch on branch some-branch.If you need to pass a given StGit reference to a Git command,stg-id(1) will convert it to a Git commit id for you.",
        "name": "stg - Manage stacks of patches using the Git content tracker",
        "section": 1
    },
    {
        "command": "stg-branch",
        "description": "Create, clone, switch, rename, or delete StGit-enabled branches.With no arguments, the current branch is printed to stdout.With a single argument, switch to the named branch.StGit supports specifying a branch using the @{-<n>} syntaxsupported by git, including - as a synonym for @{-1}. Thus stgbranch - may be used to switch to the last checked-out HEAD. Notethat @{-<n>} refers to the <n>th last HEAD, which is notnecessarily a local branch. Using an @{-<n>} value that refers toanything but a local branch will result in an error.",
        "name": "stg-branch - Branch operations: switch, list, create, rename,delete, ...",
        "section": 1
    },
    {
        "command": "stg-clean",
        "description": "Delete the empty patches from the entire series by default, oronly empty patches from the applied or unapplied patches. A patchis considered empty if its tree is the same as its parent.",
        "name": "stg-clean - Delete empty patches from the series",
        "section": 1
    },
    {
        "command": "stg-commit",
        "description": "Finalize one or more patches into the base of the current stackand remove them from the series. This is the opposite ofstg-uncommit(1). Use this command when a patch is completed andno longer needs to be managed with StGit.By default, the bottommost patch is committed. If patch names aregiven, the stack is rearranged so that those patches are at thebottom, and then they are committed.The -n/--number option specifies the number of applied patches tocommit (counting from the bottom of the stack). If -a/--all isgiven, all applied patches are committed.",
        "name": "stg-commit - Finalize patches to the stack base",
        "section": 1
    },
    {
        "command": "stg-completion",
        "description": "Support completions for bash, fish, and zsh. Also provides stgcompletion list command for dynamically introspecting StGit\u2019scommands and aliases.",
        "name": "stg-completion - Support for shell completions",
        "section": 1
    },
    {
        "command": "stg-delete",
        "description": "Delete patches",
        "name": "stg-delete - Delete patches",
        "section": 1
    },
    {
        "command": "stg-diff",
        "description": "Show the diff (default) or diffstat between the current workingcopy or a tree-ish object and another tree-ish object (defaultingto HEAD). File names can also be given to restrict the diffoutput. The tree-ish object has the format accepted by thestg-id(1) command.",
        "name": "stg-diff - Show a diff",
        "section": 1
    },
    {
        "command": "stg-edit",
        "description": "Edit a patch. Various aspects of a patch may be edited, includingthe message, author, patch name, or even the patch\u2019s diff.By default, the topmost patch is edited.With no options or when --edit is specified, the patch detailsare edited interactively. Alternatively, command line options maybe used to modify the patch non-interactively.The --diff option causes the patch\u2019s diff to be appended to thepatch description when editing interactively. This diff may beedited interactively (or just used as a reference when editingthe patch\u2019s message). The StGit attempts to apply the modifieddiff to the patch\u2019s parent tree. If the updated diff does notapply, no changes are made to the patch and the edited patch issaved to a file which may be corrected and then fed-back into stgedit --file.",
        "name": "stg-edit - Edit a patch",
        "section": 1
    },
    {
        "command": "stg-email",
        "description": "Format and send patches as email.A typical workflow is to first generate email files for eachpatch along with an optional cover letter using \u2018stg emailformat. Then, after checking the email files\u2019 contents, sendingthe emails using `stg email send. This workflow may be condensedto one step by specifying patch names to stg email send insteadof email files.The format and send subcommands are thin wrappers over gitformat-patch and git send-email, respectively. Refer to thegit-format-patch(1) and git-send-email(1) manpages for moredetails about configuration and options.",
        "name": "stg-email - Format and send patches as email",
        "section": 1
    },
    {
        "command": "stg-export",
        "description": "Export a range of patches to a given directory in unified diffformat. All applied patches are exported by default.Patches are exported to patches-<branch> by default. The --diroption may be used to specify a different output directory.The patch file output may be customized via a template file foundat \"$GIT_DIR/patchexport.tmpl\",\"~/.stgit/templates/patchexport.tmpl\", or\"$(prefix)/share/stgit/templates\". The following variables aresupported in the template file:%(description)s - patch description%(shortdescr)s- the first line of the patch description%(longdescr)s- the rest of the patch description, after the first line%(diffstat)s- the diff statistics%(authname)s- author name%(authemail)s- author email%(authdate)s- patch creation date (ISO-8601 format)%(commname)s- committer name%(commemail)s- committer email",
        "name": "stg-export - Export patches to a directory",
        "section": 1
    },
    {
        "command": "stg-files",
        "description": "Show the files modified by a patch. The files of the topmostpatch are shown by default. Passing the --stat option shows thediff statistics for the given patch. Note that this command doesnot show the files modified in the working tree and not yetincluded in the patch by a refresh command. Use the diff orstatus commands to show these files.",
        "name": "stg-files - Show files modified by a patch",
        "section": 1
    },
    {
        "command": "stg-float",
        "description": "Push patches to the top, even if applied.Float one or more patches to be the topmost applied patches. Thepatches to be floated may currently be either applied orunapplied. The necessary pop and push operations will beperformed to float the named patches. Patches not specified willremain applied or unapplied as they were prior to the floatoperation.",
        "name": "stg-float - Push patches to the top, even if applied",
        "section": 1
    },
    {
        "command": "stg-fold",
        "description": "Fold diff file into the current patch. The given GNU diff file(or standard input) is applied onto the current patch.With the --threeway option, the diff is applied onto the bottomof the current patch and a three-way merge is performed with thecurrent top. With the --base option, the diff is applied onto thespecified base and a three-way merge is performed with thecurrent top.",
        "name": "stg-fold - Fold diff file into the current patch",
        "section": 1
    },
    {
        "command": "stg-goto",
        "description": "Go to patch by pushing or popping as necessary",
        "name": "stg-goto - Go to patch by pushing or popping as necessary",
        "section": 1
    },
    {
        "command": "stg-help",
        "description": "Print this message or the help of the given subcommand(s)",
        "name": "stg-help - Print this message or the help of the givensubcommand(s)",
        "section": 1
    },
    {
        "command": "stg-hide",
        "description": "Hide patches in the series.Hidden patches are no longer shown in the plain series output.",
        "name": "stg-hide - Hide patches in the series",
        "section": 1
    },
    {
        "command": "stg-id",
        "description": "Print the hash (object id) of a StGit revision.In addition to standard Git revision specifiers (revspecs),patches may be of a stack. If no branch is specified, the currentbranch is used by default. The parent of a patch may be specifiedwith [<branch>:]<patch>^.",
        "name": "stg-id - Print git hash of a StGit revision",
        "section": 1
    },
    {
        "command": "stg-import",
        "description": "Import patches from various sources to the stack.The simplest usage is to import a diff/patch file into the stackfrom a local file. By default, the file name is used as the patchname, but this can be overridden with --name. The patch caneither be a normal file with the description at the top, or itcan have standard mail format. The \"Subject\", \"From\", and \"Date\"headers will be used for the imported patch\u2019s author details.Patches may also be imported from a mail file (-m/--mail), anmbox (-M/--mbox), or a series (-S/--series). Furthermore, the-u/--url option allows the patches source to be fetched from aurl instead of from a local file.If a patch does not apply cleanly, the failed diff is written toa .stgit-failed.patch file and an empty patch is added to thestack.The patch description must be separated from the diff with a\"---\" line.",
        "name": "stg-import - Import patches to stack",
        "section": 1
    },
    {
        "command": "stg-init",
        "description": "Initialize a StGit stack on a branch.Initializing a branch with a StGit stack commits initial, emptystack state for the branch to the repository. Theses stackmetadata commits are tracked by the refs/stacks/<branch>reference. Updated stack state is committed by each StGit commandthat modifies the stack. StGit users do not have to do anythingwith the refs/stacks/<branch> ref directly.Some StGit commands, such as stg new and stg uncommit, willautomatically initialize the stack, so it is often not necessaryto explicitly initialize the stack on a branch. Also, branchescreated with stg branch --create are automatically initialized.The branch must already exist and point to a commit beforeinitializing a StGit stack.StGit stack metadata can be deinitialized from a branch using stgbranch --cleanup. See stg-branch(1) for more details.",
        "name": "stg-init - Initialize a StGit stack on a branch",
        "section": 1
    },
    {
        "command": "stg-log",
        "description": "Show the history of changes to the stack. If one or more patchnames are given, only the changes affecting those patches areshown.The stg-undo(1) and stg-redo(1) commands may be used to step backand forth through historical stack states. The stg-reset(1)command may be used to reset the stack directly to a historicstate.The --clear option may be used to delete the stack\u2019s changehistory. Undo and redo are unavailable on a stack without changehistory. Clearing the stack state history cannot be undone.",
        "name": "stg-log - Display or optionally clear the stack changelog",
        "section": 1
    },
    {
        "command": "stg-new",
        "description": "Create a new, empty patch on the current stack. The new patch iscreated on top of the currently applied patches, and is made thenew top of the stack. Uncommitted changes in the work tree arenot included in the patch \u2014 that is handled by stg-refresh.The given patch name must be unique in the stack. If no name isgiven, one is generated from the first line of the patch\u2019s commitmessage.Patch names follow the rules for Git references with theadditional constraint that patch names may not contain the /character. See git-check-ref-format(1) for details.Patch names may start with a leading -. When specifying such apatch name on the command line, the leading - may be escaped witha single backslash as in \\-patch-name to disambiguate the patchname from command line options.An editor will be launched to edit the commit message to be usedfor the patch, unless the --message flag already specified one.The patchdescr.tmpl template file (if available) is used topre-fill the editor.",
        "name": "stg-new - Create a new patch at top of the stack",
        "section": 1
    },
    {
        "command": "stg-next",
        "description": "Print the name of the next patch.The next patch is the unapplied patch that follows the current,topmost patch. An error message will be printed if there are nounapplied patches.",
        "name": "stg-next - Print the name of the next patch",
        "section": 1
    },
    {
        "command": "stg-patches",
        "description": "Show the applied patches modifying the given paths. Without patharguments, the files modified in the working tree are used as thepaths.",
        "name": "stg-patches - Show patches that modify files",
        "section": 1
    },
    {
        "command": "stg-pick",
        "description": "Import one or more patches from another branch or commit objectinto the current series.By default, the imported patch\u2019s name is reused, but may beoverridden with the --name option. A commit object can bereverted with the --revert option.When using the --expose option, the format of the commit messageis determined by the stgit.pick.expose-format configurationoption. This option is a format string as may be supplied to the--pretty option of git-show(1). The default is\"format:%B%n(imported from commit %H)\", which appends the commithash of the picked commit to the patch\u2019s commit message.",
        "name": "stg-pick - Import a patch from another branch or a commit object",
        "section": 1
    },
    {
        "command": "stg-pop",
        "description": "Pop (unapply) one or more applied patches.By default, the topmost applied patch is popped.If ranges of patches are specified, pop and push operations areperformed such that only the patches specified on the commandline are unapplied at the end of the operation. It is possiblefor some of these intermediate push operations to fail due toconflicts if patches are popped out of last-pushed first-poppedorder.",
        "name": "stg-pop - Pop (unapply) one or more applied patches",
        "section": 1
    },
    {
        "command": "stg-prev",
        "description": "Print the name of the previous patch.The previous patch is the applied patch preceding the current,topmost patch. An error message will be printed if not enoughpatches are applied.",
        "name": "stg-prev - Print the name of the previous patch",
        "section": 1
    },
    {
        "command": "stg-pull",
        "description": "Pull the latest changes from a remote repository.The remote repository may be specified on the command line, butdefaults to branch.<name>.remote from the git configuration, or\"origin\" if not configured.This command works by popping all currently applied patches fromthe stack, pulling the changes from the remote repository,updating the stack base to the new remote HEAD, and finallypushing all formerly applied patches back onto the stack. Mergeconflicts may occur during the final push step. Those conflictsneed to be resolved manually.See git-fetch(1) for the format of remote repository argument.",
        "name": "stg-pull - Pull changes from a remote repository",
        "section": 1
    },
    {
        "command": "stg-push",
        "description": "Push one or more unapplied patches from the series onto thestack.By default, the first unapplied patch is pushed.Unapplied patches may be pushed in arbitrary order, but out oforder pushes may result in merge conflicts. If there areconflicts while pushing a patch, the conflicts are written to thework tree and the push command halts. Conflicts may then beresolved using the normal Git methods, or alternatively the pushmay be undone using stg-undo(1).",
        "name": "stg-push - Push (apply) one or more unapplied patches",
        "section": 1
    },
    {
        "command": "stg-rebase",
        "description": "Pop all patches from the current stack, move the stack base tothe given new base and push the patches back.Merge conflicts may arise when patches are being pushed-back ontothe stack. If this occurs, resolve the conflicts and thencontinue the rebase with the following sequence:stg add --updatestg refreshstg goto top-patchOr to skip the conflicting patch:stg undo --hardstg push next-patch..top-patch",
        "name": "stg-rebase - Move the stack base to another point in history",
        "section": 1
    },
    {
        "command": "stg-redo",
        "description": "If the last command was an undo, the patch stack state will bereset to its state before the undo. Consecutive redos will undothe effects of consecutive invocations of stg-undo(1).It is an error to redo if the last stack-modifying command wasnot an undo.",
        "name": "stg-redo - Undo the last undo operation",
        "section": 1
    },
    {
        "command": "stg-refresh",
        "description": "Include the latest work tree and index changes in the currentpatch. This command generates a new git commit object for thepatch; the old commit is no longer visible.Refresh will warn if the index is dirty, and require use ofeither the --index or --force options to override this check.This is to prevent accidental full refresh when only some changeswere staged using git add interactive mode.You may optionally list one or more files or directories relativeto the current working directory; if you do, only matching fileswill be updated.Behind the scenes, stg refresh first creates a new temporarypatch with your updates, and then merges that patch into thepatch you asked to have refreshed. If you asked to refresh apatch other than the topmost patch, there can be conflicts; inthat case, the temporary patch will be left for you to take careof, for example with stg squash.The creation of the temporary patch is recorded in a separateentry in the patch stack log; this means that one undo step willundo the merge between the other patch and the temp patch, andtwo undo steps will additionally get rid of the temp patch.",
        "name": "stg-refresh - Incorporate worktree changes into current patch",
        "section": 1
    },
    {
        "command": "stg-rename",
        "description": "Rename [old-patch] to <new-patch>. If [old-patch] is not given,the topmost patch will be renamed.",
        "name": "stg-rename - Rename a patch",
        "section": 1
    },
    {
        "command": "stg-repair",
        "description": "If a branch with a StGit stack is modified with certain gitcommands such as git-commit(1), git-pull(1), git-merge(1), orgit-rebase(1), the StGit stack metadata will become inconsistentwith the branch state. There are a few options for resolving thiskind of situation:1. Use stg-undo(1) to undo the effect of the git commands. Orsimilarly use stg-reset(1) to reset the stack/branch to anyprevious stack state.2. Use stg repair. This will repair the StGit stack metadata toaccommodate the modifications to the branch made by the gitcommands. Specifically, it will do the following:\u2022If regular git commits were made on top of the stack ofStGit patches (i.e. by using plain git commit), stgrepair will convert those commits to StGit patches,preserving their content.\u2022However, merge commits cannot become patches. So if amerge was committed on top of the stack, stg repair willmark all patches below the merge commit as unapplied,since they are no longer reachable. An alternative whenthis is not the desired behavior is to use stg undo tofirst get rid of the offending merge and then run stgrepair again.\u2022The applied patches are supposed to be precisely thosethat are reachable from the branch head. If, for example,git-reset(1) was used to move the head, some appliedpatches may no longer be reachable and some unappliedpatches may have become reachable. In this case, stgrepair will correct the applied/unapplied state of suchpatches.stg repair will repair these inconsistencies reliably, so thereare valid workflows where git commands are used followed by stgrepair. For example, new patches can be created by first makingcommits with a graphical commit tool and then running stg repairto convert those commits into patches.",
        "name": "stg-repair - Repair stack after branch is modified with gitcommands",
        "section": 1
    },
    {
        "command": "stg-reset",
        "description": "Reset the patch stack to an earlier state. If no state isspecified, reset only the changes in the worktree.The state is specified with a commit id from the stack log, whichmay be viewed with stg-log(1). Patch name arguments mayoptionally be provided to limit which patches are reset.",
        "name": "stg-reset - Reset the patch stack to an earlier state",
        "section": 1
    },
    {
        "command": "stg-series",
        "description": "Show all the patches in the series, or just those in the givenrange, ordered from bottom to top.The topmost applied patch is prefixed with >. All other appliedpatches are prefixed with +. Unapplied patches are prefixed with- and hidden patches with !.The --reverse option may be used to reverse the order in whichpatches are displayed. The reversed order is more stack-like,with the base of the stack appearing at the bottom of of thedisplay.Empty patches are prefixed with a * when the --empty option isused.",
        "name": "stg-series - Display the patch series",
        "section": 1
    },
    {
        "command": "stg-show",
        "description": "Show the commit log and diff corresponding to the given patches.The topmost patch is shown by default, or HEAD if no patches areapplied. The output is similar to git-show(1).",
        "name": "stg-show - Show patch commits",
        "section": 1
    },
    {
        "command": "stg-sink",
        "description": "Move the specified patches down the stack.If no patch is specified on the command line, the current(topmost) patch is sunk. By default, patches are sunk to thebottom of the stack, but the --to option may be used to placethem under any applied patch.Internally, sinking involves popping all patches to the bottom(or to the target patch if --to is used), then pushing thepatches to sink, and then, unless --nopush is specified, pushingback any other formerly applied patches.Sinking may be useful, for example, to group stable patches atthe bottom of the stack where they less likely to be impacted bythe push of another patch, and from where they can be more easilycommitted or pushed to another repository.",
        "name": "stg-sink - Move patches deeper in the stack",
        "section": 1
    },
    {
        "command": "stg-spill",
        "description": "Spill changes from the topmost patch. Changes are removed fromthe patch, but remain in the index and worktree.Spilling a patch may be useful for reselecting the files/hunks tobe included in the patch.",
        "name": "stg-spill - Spill changes from the topmost patch",
        "section": 1
    },
    {
        "command": "stg-squash",
        "description": "Squash two or more patches, creating one patch with theircombined changes.The squash process, at a high level:1. Pop all the given patches, plus any other patches on top ofthem.2. Push the given patches in the order they were given on thecommand line. This establishes a tree containing the combinedchanges from the given patches.3. Replace given patches with a new, squashed patch.4. Allow the user to interactively edit the commit message ofthe new, squashed patch.5. Push other patches that were popped in step (1), if any.Conflicts can occur whenever a patch is pushed; this is, in steps(2) and (5). If conflicts occur, the squash command will haltsuch that the conflicts may be resolved manually.",
        "name": "stg-squash - Squash two or more patches into one",
        "section": 1
    },
    {
        "command": "stg-sync",
        "description": "For each of the specified patches, perform a three-way merge withthe same patch in the specified branch or series. The command canbe used for keeping patches on several branches in sync. Notethat the operation may fail for some patches because ofconflicts. The patches in the series must apply cleanly.",
        "name": "stg-sync - Synchronize patches with a branch or a series",
        "section": 1
    },
    {
        "command": "stg-top",
        "description": "Print the name of the top patch.The topmost patch is the currently applied patch. An errormessage will be printed if no patches are applied.",
        "name": "stg-top - Print the name of the top patch",
        "section": 1
    },
    {
        "command": "stg-uncommit",
        "description": "Convert one or more Git commits from the base of the currentstack into StGit patches. The original Git commits are notmodified; the StGit stack extends to incorporate these commits asthe bottommost applied patches. This is the opposite ofstg-commit(1).By default, the number of patches to uncommit is determined bythe number of patch names provided on the command line. The firstprovided name is used for the first patch to uncommit, i.e. forthe newest patch.The -n/--number option specifies the number of patches touncommit. In this case, at most one patch name may be specified.It is used as prefix to which the patch number is appended. If nopatch names are provided on the command line, StGit automaticallygenerates names based on the first lines of the commit messages.The -t/--to option specifies that all commits up to and includingthe given commit should be uncommitted. The -x/--exclusive optionmay be used to exclude the \"to\" commit.Only commits with exactly one parent can be uncommitted; in otherwords, merge commits may not be uncommitted.",
        "name": "stg-uncommit - Convert regular Git commits into StGit patches",
        "section": 1
    },
    {
        "command": "stg-undo",
        "description": "Reset the patch stack to the state before the last operation.Consecutive undos will go back to yet older stack states.",
        "name": "stg-undo - Undo the last command",
        "section": 1
    },
    {
        "command": "stg-unhide",
        "description": "Unhide hidden patches in the series.Hidden patches are no longer shown in the plain series output.",
        "name": "stg-unhide - Unhide hidden patches",
        "section": 1
    },
    {
        "command": "stg-version",
        "description": "Print version information and exit",
        "name": "stg-version - Print version information and exit",
        "section": 1
    },
    {
        "command": "strace",
        "description": "In the simplest case strace runs the specified command until itexits.It intercepts and records the system calls which arecalled by a process and the signals which are received by aprocess.The name of each system call, its arguments and itsreturn value are printed on standard error or to the filespecified with the -o option.strace is a useful diagnostic, instructional, and debugging tool.System administrators, diagnosticians and trouble-shooters willfind it invaluable for solving problems with programs for whichthe source is not readily available since they do not need to berecompiled in order to trace them.Students, hackers and theoverly-curious will find that a great deal can be learned about asystem and its system calls by tracing even ordinary programs.And programmers will find that since system calls and signals areevents that happen at the user/kernel interface, a closeexamination of this boundary is very useful for bug isolation,sanity checking and attempting to capture race conditions.Each line in the trace contains the system call name, followed byits arguments in parentheses and its return value.An examplefrom stracing the command \"cat /dev/null\" is:open(\"/dev/null\", O_RDONLY) = 3Errors (typically a return value of -1) have the errno symbol anderror string appended.open(\"/foo/bar\", O_RDONLY) = -1 ENOENT (No such file or directory)Signals are printed as signal symbol and decoded siginfostructure.An excerpt from stracing and interrupting the command\"sleep 666\" is:sigsuspend([] <unfinished ...>--- SIGINT {si_signo=SIGINT, si_code=SI_USER, si_pid=...} ---+++ killed by SIGINT +++If a system call is being executed and meanwhile another one isbeing called from a different thread/process then strace will tryto preserve the order of those events and mark the ongoing callas being unfinished.When the call returns it will be marked asresumed.[pid 28772] select(4, [3], NULL, NULL, NULL <unfinished ...>[pid 28779] clock_gettime(CLOCK_REALTIME, {tv_sec=1130322148, tv_nsec=3977000}) = 0[pid 28772] <... select resumed> )= 1 (in [3])Interruption of a (restartable) system call by a signal deliveryis processed differently as kernel terminates the system call andalso arranges its immediate reexecution after the signal handlercompletes.read(0, 0x7ffff72cf5cf, 1)= ? ERESTARTSYS (To be restarted)--- SIGALRM {si_signo=SIGALRM, si_code=SI_KERNEL} ---rt_sigreturn({mask=[]})= 0read(0, \"\", 1)= 0Arguments are printed in symbolic form with passion.Thisexample shows the shell performing \">>xyzzy\" output redirection:open(\"xyzzy\", O_WRONLY|O_APPEND|O_CREAT, 0666) = 3Here, the second and the third argument of open(2) are decoded bybreaking down the flag argument into its three bitwise-ORconstituents and printing the mode value in octal by tradition.Where the traditional or native usage differs from ANSI or POSIX,the latter forms are preferred.In some cases, strace output isproven to be more readable than the source.Structure pointers are dereferenced and the members are displayedas appropriate.In most cases, arguments are formatted in themost C-like fashion possible.For example, the essence of thecommand \"ls -l /dev/null\" is captured as:lstat(\"/dev/null\", {st_mode=S_IFCHR|0666, st_rdev=makedev(0x1, 0x3), ...}) = 0Notice how the 'struct stat' argument is dereferenced and howeach member is displayed symbolically.In particular, observehow the st_mode member is carefully decoded into a bitwise-OR ofsymbolic and numeric values.Also notice in this example thatthe first argument to lstat(2) is an input to the system call andthe second argument is an output.Since output arguments are notmodified if the system call fails, arguments may not always bedereferenced.For example, retrying the \"ls -l\" example with anon-existent file produces the following line:lstat(\"/foo/bar\", 0xb004) = -1 ENOENT (No such file or directory)In this case the porch light is on but nobody is home.Syscalls unknown to strace are printed raw, with the unknownsystem call number printed in hexadecimal form and prefixed with\"syscall_\":syscall_0xbad(0x1, 0x2, 0x3, 0x4, 0x5, 0x6) = -1 ENOSYS (Function not implemented)Character pointers are dereferenced and printed as C strings.Non-printing characters in strings are normally represented byordinary C escape codes.Only the first strsize (32 by default)bytes of strings are printed; longer strings have an ellipsisappended following the closing quote.Here is a line from \"ls-l\" where the getpwuid(3) library routine is reading the passwordfile:read(3, \"root::0:0:System Administrator:/\"..., 1024) = 422While structures are annotated using curly braces, pointers tobasic types and arrays are printed using square brackets withcommas separating the elements.Here is an example from thecommand id(1) on a system with supplementary group ids:getgroups(32, [100, 0]) = 2On the other hand, bit-sets are also shown using square brackets,but set elements are separated only by a space.Here is theshell, preparing to execute an external command:sigprocmask(SIG_BLOCK, [CHLD TTOU], []) = 0Here, the second argument is a bit-set of two signals, SIGCHLDand SIGTTOU.In some cases, the bit-set is so full that printingout the unset elements is more valuable.In that case, the bit-set is prefixed by a tilde like this:sigprocmask(SIG_UNBLOCK, ~[], NULL) = 0Here, the second argument represents the full set of all signals.",
        "name": "strace - trace system calls and signals",
        "section": 1
    },
    {
        "command": "strace-log-merge",
        "description": "strace-log-merge merges the output of strace -ff -tt[t] command,prepending PID to each line and sorting the result using timestamp as a key.",
        "name": "strace-log-merge - merge strace -ff -tt output",
        "section": 1
    },
    {
        "command": "strings",
        "description": "For each file given, GNU strings prints the printable charactersequences that are at least 4 characters long (or the numbergiven with the options below) and are followed by an unprintablecharacter.Depending upon how the strings program was configured it willdefault to either displaying all the printable sequences that itcan find in each file, or only those sequences that are inloadable, initialized data sections.If the file type isunrecognizable, or if strings is reading from stdin then it willalways display all of the printable sequences that it can find.For backwards compatibility any file that occurs after a command-line option of just - will also be scanned in full, regardless ofthe presence of any -d option.strings is mainly useful for determining the contents of non-textfiles.",
        "name": "strings - print the sequences of printable characters in files",
        "section": 1
    },
    {
        "command": "strip",
        "description": "GNU strip discards all symbols from object files objfile.Thelist of object files may include archives.At least one objectfile must be given.strip modifies the files named in its argument, rather thanwriting modified copies under different names.",
        "name": "strip - discard symbols and other data from object files",
        "section": 1
    },
    {
        "command": "stty",
        "description": "Print or change terminal characteristics.Mandatory arguments to long options are mandatory for shortoptions too.-a, --allprint all current settings in human-readable form-g, --saveprint all current settings in a stty-readable form-F, --file=DEVICEopen and use the specified DEVICE instead of stdin--help display this help and exit--versionoutput version information and exitOptional - before SETTING indicates negation.An * marksnon-POSIX settings.The underlying system defines which settingsare available.Special characters:* discard CHARCHAR will toggle discarding of outputeof CHARCHAR will send an end of file (terminate the input)eol CHARCHAR will end the line* eol2 CHARalternate CHAR for ending the lineerase CHARCHAR will erase the last character typedintr CHARCHAR will send an interrupt signalkill CHARCHAR will erase the current line* lnext CHARCHAR will enter the next character quotedquit CHARCHAR will send a quit signal* rprnt CHARCHAR will redraw the current linestart CHARCHAR will restart the output after stopping itstop CHARCHAR will stop the outputsusp CHARCHAR will send a terminal stop signal* swtch CHARCHAR will switch to a different shell layer* werase CHARCHAR will erase the last word typedSpecial settings:Nset the input and output speeds to N bauds* cols Ntell the kernel that the terminal has N columns* columns Nsame as cols N* [-]drainwait for transmission before applying settings (on bydefault)ispeed Nset the input speed to N* line Nuse line discipline Nmin Nwith -icanon, set N characters minimum for a completedreadospeed Nset the output speed to N* rows Ntell the kernel that the terminal has N rows* size print the number of rows and columns according to thekernelspeedprint the terminal speedtime N with -icanon, set read timeout of N tenths of a secondControl settings:[-]clocaldisable modem control signals[-]creadallow input to be received* [-]crtsctsenable RTS/CTS handshakingcsNset character size to N bits, N in [5..8][-]cstopbuse two stop bits per character (one with '-')[-]hup send a hangup signal when the last process closes the tty[-]hupclsame as [-]hup[-]parenbgenerate parity bit in output and expect parity bit ininput[-]paroddset odd parity (or even parity with '-')* [-]cmsparuse \"stick\" (mark/space) parityInput settings:[-]brkintbreaks cause an interrupt signal[-]icrnltranslate carriage return to newline[-]ignbrkignore break characters[-]igncrignore carriage return[-]ignparignore characters with parity errors* [-]imaxbelbeep and do not flush a full input buffer on a character[-]inlcrtranslate newline to carriage return[-]inpckenable input parity checking[-]istripclear high (8th) bit of input characters* [-]iutf8assume input characters are UTF-8 encoded* [-]iuclctranslate uppercase characters to lowercase* [-]ixanylet any character restart output, not only start character[-]ixoffenable sending of start/stop characters[-]ixonenable XON/XOFF flow control[-]parmrkmark parity errors (with a 255-0-character sequence)[-]tandemsame as [-]ixoffOutput settings:* bsNbackspace delay style, N in [0..1]* crNcarriage return delay style, N in [0..3]* ffNform feed delay style, N in [0..1]* nlNnewline delay style, N in [0..1]* [-]ocrnltranslate carriage return to newline* [-]ofdeluse delete characters for fill instead of NUL characters* [-]ofilluse fill (padding) characters instead of timing for delays* [-]olcuctranslate lowercase characters to uppercase* [-]onlcrtranslate newline to carriage return-newline* [-]onlretnewline performs a carriage return* [-]onocrdo not print carriage returns in the first column[-]opostpostprocess output* tabN horizontal tab delay style, N in [0..3]* tabs same as tab0* -tabssame as tab3* vtNvertical tab delay style, N in [0..1]Local settings:[-]crteraseecho erase characters as backspace-space-backspace* crtkillkill all line by obeying the echoprt and echoe settings* -crtkillkill all line by obeying the echoctl and echok settings* [-]ctlechoecho control characters in hat notation ('^c')[-]echoecho input characters* [-]echoctlsame as [-]ctlecho[-]echoesame as [-]crterase[-]echokecho a newline after a kill character* [-]echokesame as [-]crtkill[-]echonlecho newline even if not echoing other characters* [-]echoprtecho erased characters backward, between '\\' and '/'* [-]extprocenable \"LINEMODE\"; useful with high latency links* [-]flushodiscard output[-]icanonenable special characters: erase, kill, werase, rprnt[-]iextenenable non-POSIX special characters[-]isigenable interrupt, quit, and suspend special characters[-]noflshdisable flushing after interrupt and quit specialcharacters* [-]prterasesame as [-]echoprt* [-]tostopstop background jobs that try to write to the terminal* [-]xcasewith icanon, escape with '\\' for uppercase charactersCombination settings:* [-]LCASEsame as [-]lcasecbreak same as -icanon-cbreaksame as icanoncooked same as brkint ignpar istrip icrnl ixon opost isig icanon,eof and eol characters to their default values-cookedsame as rawcrtsame as echoe echoctl echokedecsame as echoe echoctl echoke -ixany intr ^c erase 0177kill ^u* [-]decctlqsame as [-]ixanyekerase and kill characters to their default valuesevenpsame as parenb -parodd cs7-evenp same as -parenb cs8* [-]lcasesame as xcase iuclc olcuclitout same as -parenb -istrip -opost cs8-litoutsame as parenb istrip opost cs7nlsame as -icrnl -onlcr-nlsame as icrnl -inlcr -igncr onlcr -ocrnl -onlretoddpsame as parenb parodd cs7-oddpsame as -parenb cs8[-]paritysame as [-]evenppass8same as -parenb -istrip cs8-pass8 same as parenb istrip cs7rawsame as -ignbrk -brkint -ignpar -parmrk -inpck -istrip-inlcr -igncr -icrnl -ixon -ixoff -icanon -opost -isig-iuclc -ixany -imaxbel -xcase min 1 time 0-rawsame as cookedsanesame as cread -ignbrk brkint -inlcr -igncr icrnl icanoniexten echo echoe echok -echonl -noflsh -ixoff -iutf8-iuclc -ixany imaxbel -xcase -olcuc -ocrnl opost -ofillonlcr -onocr -onlret nl0 cr0 tab0 bs0 vt0 ff0 isig -tostop-ofdel -echoprt echoctl echoke -extproc -flusho, allspecial characters to their default valuesHandle the tty line connected to standard input.Withoutarguments, prints baud rate, line discipline, and deviations fromstty sane.In settings, CHAR is taken literally, or coded as in^c, 0x37, 0177 or 127; special values ^- or undef used to disablespecial characters.",
        "name": "stty - change and print terminal line settings",
        "section": 1
    },
    {
        "command": "su",
        "description": "su allows commands to be run with a substitute user and group ID.When called with no user specified, su defaults to running aninteractive shell as root. When user is specified, additionalarguments can be supplied, in which case they are passed to theshell.For backward compatibility, su defaults to not change the currentdirectory and to only set the environment variables HOME andSHELL (plus USER and LOGNAME if the target user is not root). Itis recommended to always use the --login option (instead of itsshortcut -) to avoid side effects caused by mixing environments.This version of su uses PAM for authentication, account andsession management. Some configuration options found in other suimplementations, such as support for a wheel group, have to beconfigured via PAM.su is mostly designed for unprivileged users, the recommendedsolution for privileged users (e.g., scripts executed by root) isto use non-set-user-ID command runuser(1) that does not requireauthentication and provides separate PAM configuration. If thePAM session is not required at all then the recommended solutionis to use command setpriv(1).Note that su in all cases uses PAM (pam_getenvlist(3)) to do thefinal environment modification. Command-line options such as--login and --preserve-environment affect the environment beforeit is modified by PAM.Since version 2.38 su resets process resource limits RLIMIT_NICE,RLIMIT_RTPRIO, RLIMIT_FSIZE, RLIMIT_AS and RLIMIT_NOFILE.",
        "name": "su - run a command with substitute user and group ID",
        "section": 1
    },
    {
        "command": "sum",
        "description": "Print or check BSD (16-bit) checksums.With no FILE, or when FILE is -, read standard input.-ruse BSD sum algorithm (the default), use 1K blocks-s, --sysvuse System V sum algorithm, use 512 bytes blocks--help display this help and exit--versionoutput version information and exit",
        "name": "sum - checksum and count the blocks in a file",
        "section": 1
    },
    {
        "command": "sync",
        "description": "Synchronize cached writes to persistent storageIf one or more files are specified, sync only them, or theircontaining file systems.-d, --datasync only file data, no unneeded metadata-f, --file-systemsync the file systems that contain the files--help display this help and exit--versionoutput version information and exit",
        "name": "sync - Synchronize cached writes to persistent storage",
        "section": 1
    },
    {
        "command": "systemctl",
        "description": "systemctl may be used to introspect and control the state of the\"systemd\" system and service manager. Please refer to systemd(1)for an introduction into the basic concepts and functionalitythis tool manages.",
        "name": "systemctl - Control the systemd system and service manager",
        "section": 1
    },
    {
        "command": "systemd",
        "description": "systemd is a system and service manager for Linux operatingsystems. When run as first process on boot (as PID 1), it acts asinit system that brings up and maintains userspace services.Separate instances are started for logged-in users to start theirservices.systemd is usually not invoked directly by the user, but isinstalled as the /sbin/init symlink and started during earlyboot. The user manager instances are started automaticallythrough the user@.service(5) service.For compatibility with SysV, if the binary is called as init andis not the first process on the machine (PID is not 1), it willexecute telinit and pass all command line arguments unmodified.That means init and telinit are mostly equivalent when invokedfrom normal login sessions. See telinit(8) for more information.When run as a system instance, systemd interprets theconfiguration file system.conf and the files in system.conf.ddirectories; when run as a user instance, systemd interprets theconfiguration file user.conf and the files in user.conf.ddirectories. See systemd-system.conf(5) for more information.",
        "name": "systemd, init - systemd system and service manager",
        "section": 1
    },
    {
        "command": "systemd-ac-power",
        "description": "systemd-ac-power may be used to check whether the system isrunning on AC power or not. By default it will simply returnsuccess (if we can detect that we are running on AC power) orfailure, with no output. This can be useful for example to debugConditionACPower= (see systemd.unit(5)).",
        "name": "systemd-ac-power - Report whether we are connected to an externalpower source",
        "section": 1
    },
    {
        "command": "systemd-analyze",
        "description": "systemd-analyze may be used to determine system boot-upperformance statistics and retrieve other state and tracinginformation from the system and service manager, and to verifythe correctness of unit files. It is also used to access specialfunctions useful for advanced system manager debugging.If no command is passed, systemd-analyze time is implied.systemd-analyze timeThis command prints the time spent in the kernel before userspacehas been reached, the time spent in the initrd before normalsystem userspace has been reached, and the time normal systemuserspace took to initialize. Note that these measurements simplymeasure the time passed up to the point where all system serviceshave been spawned, but not necessarily until they fully finishedinitialization or the disk is idle.Example 1. Show how long the boot took# in a container$ systemd-analyze timeStartup finished in 296ms (userspace)multi-user.target reached after 275ms in userspace# on a real machine$ systemd-analyze timeStartup finished in 2.584s (kernel) + 19.176s (initrd) + 47.847s (userspace) = 1min 9.608smulti-user.target reached after 47.820s in userspacesystemd-analyze blameThis command prints a list of all running units, ordered by thetime they took to initialize. This information may be used tooptimize boot-up times. Note that the output might be misleadingas the initialization of one service might be slow simply becauseit waits for the initialization of another service to complete.Also note: systemd-analyze blame doesn't display results forservices with Type=simple, because systemd considers suchservices to be started immediately, hence no measurement of theinitialization delays can be done. Also note that this commandonly shows the time units took for starting up, it does not showhow long unit jobs spent in the execution queue. In particular itshows the time units spent in \"activating\" state, which is notdefined for units such as device units that transition directlyfrom \"inactive\" to \"active\". This command hence gives animpression of the performance of program code, but cannotaccurately reflect latency introduced by waiting for hardware andsimilar events.Example 2. Show which units took the most time during boot$ systemd-analyze blame32.875s pmlogger.service20.905s systemd-networkd-wait-online.service13.299s dev-vda1.device...23ms sysroot.mount11ms initrd-udevadm-cleanup-db.service3ms sys-kernel-config.mountsystemd-analyze critical-chain [UNIT...]This command prints a tree of the time-critical chain of units(for each of the specified UNITs or for the default targetotherwise). The time after the unit is active or started isprinted after the \"@\" character. The time the unit takes to startis printed after the \"+\" character. Note that the output might bemisleading as the initialization of services might depend onsocket activation and because of the parallel execution of units.Also, similarly to the blame command, this only takes intoaccount the time units spent in \"activating\" state, and hencedoes not cover units that never went through an \"activating\"state (such as device units that transition directly from\"inactive\" to \"active\"). Moreover it does not show information onjobs (and in particular not jobs that timed out).Example 3. systemd-analyze critical-chain$ systemd-analyze critical-chainmulti-user.target @47.820s\u2514\u2500pmie.service @35.968s +548ms\u2514\u2500pmcd.service @33.715s +2.247s\u2514\u2500network-online.target @33.712s\u2514\u2500systemd-networkd-wait-online.service @12.804s +20.905s\u2514\u2500systemd-networkd.service @11.109s +1.690s\u2514\u2500systemd-udevd.service @9.201s +1.904s\u2514\u2500systemd-tmpfiles-setup-dev.service @7.306s +1.776s\u2514\u2500kmod-static-nodes.service @6.976s +177ms\u2514\u2500systemd-journald.socket\u2514\u2500system.slice\u2514\u2500-.slicesystemd-analyze dump [pattern...]Without any parameter, this command outputs a (usually very long)human-readable serialization of the complete service managerstate. Optional glob pattern may be specified, causing the outputto be limited to units whose names match one of the patterns. Theoutput format is subject to change without notice and should notbe parsed by applications. This command is rate limited forunprivileged users.Example 4. Show the internal state of user manager$ systemd-analyze --user dumpTimestamp userspace: Thu 2019-03-14 23:28:07 CETTimestamp finish: Thu 2019-03-14 23:28:07 CETTimestamp generators-start: Thu 2019-03-14 23:28:07 CETTimestamp generators-finish: Thu 2019-03-14 23:28:07 CETTimestamp units-load-start: Thu 2019-03-14 23:28:07 CETTimestamp units-load-finish: Thu 2019-03-14 23:28:07 CET-> Unit proc-timer_list.mount:Description: /proc/timer_list...-> Unit default.target:Description: Main user target...systemd-analyze malloc [D-Bus service...]This command can be used to request the output of the internalmemory state (as returned by malloc_info(3)) of a D-Bus service.If no service is specified, the query will be sent toorg.freedesktop.systemd1 (the system or user service manager).The output format is not guaranteed to be stable and should notbe parsed by applications.The service must implement the org.freedesktop.MemoryAllocation1interface. In the systemd suite, it is currently only implementedby the manager.systemd-analyze plotThis command prints either an SVG graphic, detailing which systemservices have been started at what time, highlighting the timethey spent on initialization, or the raw time data in JSON ortable format.Example 5. Plot a bootchart$ systemd-analyze plot >bootup.svg$ eog bootup.svg&Note that this plot is based on the most recent per-unit timingdata of loaded units. This means that if a unit gets started,then stopped and then started again the information shown willcover the most recent start cycle, not the first one. Thus it'srecommended to consult this information only shortly after boot,so that this distinction doesn't matter. Moreover, units that arenot referenced by any other unit through a dependency might beunloaded by the service manager once they terminate (and did notfail). Such units will not show up in the plot.systemd-analyze dot [pattern...]This command generates textual dependency graph description indot format for further processing with the GraphViz dot(1) tool.Use a command line like systemd-analyze dot | dot -Tsvg>systemd.svg to generate a graphical dependency tree. Unless--order or --require is passed, the generated graph will showboth ordering and requirement dependencies. Optional patternglobbing style specifications (e.g.*.target) may be given atthe end. A unit dependency is included in the graph if any ofthese patterns match either the origin or destination node.Example 6. Plot all dependencies of any unit whose name startswith \"avahi-daemon\"$ systemd-analyze dot 'avahi-daemon.*' | dot -Tsvg >avahi.svg$ eog avahi.svgExample 7. Plot the dependencies between all known target units$ systemd-analyze dot --to-pattern='*.target' --from-pattern='*.target' \\| dot -Tsvg >targets.svg$ eog targets.svgsystemd-analyze unit-pathsThis command outputs a list of all directories from which unitfiles, .d overrides, and .wants, .requires symlinks may beloaded. Combine with --user to retrieve the list for the usermanager instance, and --global for the global configuration ofuser manager instances.Example 8. Show all paths for generated units$ systemd-analyze unit-paths | grep '^/run'/run/systemd/system.control/run/systemd/transient/run/systemd/generator.early/run/systemd/system/run/systemd/system.attached/run/systemd/generator/run/systemd/generator.lateNote that this verb prints the list that is compiled intosystemd-analyze itself, and does not communicate with the runningmanager. Usesystemctl [--user] [--global] show -p UnitPath --valueto retrieve the actual list that the manager uses, with any emptydirectories omitted.systemd-analyze exit-status [STATUS...]This command prints a list of exit statuses along with their\"class\", i.e. the source of the definition (one of \"glibc\",\"systemd\", \"LSB\", or \"BSD\"), see the Process Exit Codes sectionin systemd.exec(5). If no additional arguments are specified, allknown statuses are shown. Otherwise, only the definitions for thespecified codes are shown.Example 9. Show some example exit status names$ systemd-analyze exit-status 0 1 {63..65}NAMESTATUS CLASSSUCCESS 0glibcFAILURE 1glibc-63-USAGE64BSDDATAERR 65BSDsystemd-analyze capability [CAPABILITY...]This command prints a list of Linux capabilities along with theirnumeric IDs. See capabilities(7) for details. If no argument isspecified the full list of capabilities known to the servicemanager and the kernel is shown. Capabilities defined by thekernel but not known to the service manager are shown as\"cap_???\". Optionally, if arguments are specified they may referto specific cabilities by name or numeric ID, in which case onlythe indicated capabilities are shown in the table.Example 10. Show some example capability names$ systemd-analyze capability 0 1 {30..32}NAMENUMBERcap_chown0cap_dac_override1cap_audit_control30cap_setfcap31cap_mac_override32systemd-analyze condition CONDITION...This command will evaluate Condition*=...and Assert*=...assignments, and print their values, and the resulting value ofthe combined condition set. See systemd.unit(5) for a list ofavailable conditions and asserts.Example 11. Evaluate conditions that check kernel versions$ systemd-analyze condition 'ConditionKernelVersion = ! <4.0' \\'ConditionKernelVersion = >=5.1' \\'ConditionACPower=|false' \\'ConditionArchitecture=|!arm' \\'AssertPathExists=/etc/os-release'test.service: AssertPathExists=/etc/os-release succeeded.Asserts succeeded.test.service: ConditionArchitecture=|!arm succeeded.test.service: ConditionACPower=|false failed.test.service: ConditionKernelVersion=>=5.1 succeeded.test.service: ConditionKernelVersion=!<4.0 succeeded.Conditions succeeded.systemd-analyze syscall-filter [SET...]This command will list system calls contained in the specifiedsystem call set SET, or all known sets if no sets are specified.Argument SET must include the \"@\" prefix.systemd-analyze filesystems [SET...]This command will list filesystems in the specified filesystemset SET, or all known sets if no sets are specified. Argument SETmust include the \"@\" prefix.systemd-analyze calendar EXPRESSION...This command will parse and normalize repetitive calendar timeevents, and will calculate when they elapse next. This takes thesame input as the OnCalendar= setting in systemd.timer(5),following the syntax described in systemd.time(7). By default,only the next time the calendar expression will elapse is shown;use --iterations= to show the specified number of next times theexpression elapses. Each time the expression elapses forms atimestamp, see the timestamp verb below.Example 12. Show leap days in the near future$ systemd-analyze calendar --iterations=5 '*-2-29 0:0:0'Original form: *-2-29 0:0:0Normalized form: *-02-29 00:00:00Next elapse: Sat 2020-02-29 00:00:00 UTCFrom now: 11 months 15 days leftIter. #2: Thu 2024-02-29 00:00:00 UTCFrom now: 4 years 11 months leftIter. #3: Tue 2028-02-29 00:00:00 UTCFrom now: 8 years 11 months leftIter. #4: Sun 2032-02-29 00:00:00 UTCFrom now: 12 years 11 months leftIter. #5: Fri 2036-02-29 00:00:00 UTCFrom now: 16 years 11 months leftsystemd-analyze timestamp TIMESTAMP...This command parses a timestamp (i.e. a single point in time) andoutputs the normalized form and the difference between thistimestamp and now. The timestamp should adhere to the syntaxdocumented in systemd.time(7), section \"PARSING TIMESTAMPS\".Example 13. Show parsing of timestamps$ systemd-analyze timestamp yesterday now tomorrowOriginal form: yesterdayNormalized form: Mon 2019-05-20 00:00:00 CEST(in UTC): Sun 2019-05-19 22:00:00 UTCUNIX seconds: @15583032000From now: 1 day 9h agoOriginal form: nowNormalized form: Tue 2019-05-21 09:48:39 CEST(in UTC): Tue 2019-05-21 07:48:39 UTCUNIX seconds: @1558424919.659757From now: 43us agoOriginal form: tomorrowNormalized form: Wed 2019-05-22 00:00:00 CEST(in UTC): Tue 2019-05-21 22:00:00 UTCUNIX seconds: @15584760000From now: 14h leftsystemd-analyze timespan EXPRESSION...This command parses a time span (i.e. a difference between twotimestamps) and outputs the normalized form and the equivalentvalue in microseconds. The time span should adhere to the syntaxdocumented in systemd.time(7), section \"PARSING TIME SPANS\".Values without units are parsed as seconds.Example 14. Show parsing of timespans$ systemd-analyze timespan 1s 300s '1year 0.000001s'Original: 1s\u03bcs: 1000000Human: 1sOriginal: 300s\u03bcs: 300000000Human: 5minOriginal: 1year 0.000001s\u03bcs: 31557600000001Human: 1y 1ussystemd-analyze cat-config NAME|PATH...This command is similar to systemctl cat, but operates on configfiles. It will copy the contents of a config file and anydrop-ins to standard output, using the usual systemd set ofdirectories and rules for precedence. Each argument must beeither an absolute path including the prefix (such as/etc/systemd/logind.conf or /usr/lib/systemd/logind.conf), or aname relative to the prefix (such as systemd/logind.conf).Example 15. Showing logind configuration$ systemd-analyze cat-config systemd/logind.conf# /etc/systemd/logind.conf...[Login]NAutoVTs=8...# /usr/lib/systemd/logind.conf.d/20-test.conf... some override from another package# /etc/systemd/logind.conf.d/50-override.conf... some administrator overridesystemd-analyze compare-versions VERSION1 [OP] VERSION2This command has two distinct modes of operation, depending onwhether the operator OP is specified.In the first mode \u2014 when OP is not specified \u2014 it will comparethe two version strings and print either \"VERSION1 < VERSION2\",or \"VERSION1 == VERSION2\", or \"VERSION1 > VERSION2\" asappropriate.The exit status is 0 if the versions are equal, 11 if the versionof the right is smaller, and 12 if the version of the left issmaller. (This matches the convention used by rpmdev-vercmp.)In the second mode \u2014 when OP is specified \u2014 it will compare thetwo version strings using the operation OP and return 0 (success)if they condition is satisfied, and 1 (failure) otherwise.OPmay be lt, le, eq, ne, ge, gt. In this mode, no output isprinted. (This matches the convention used by dpkg(1)--compare-versions.)Example 16. Compare versions of a package$ systemd-analyze compare-versions systemd-250~rc1.fc36.aarch64 systemd-251.fc36.aarch64systemd-250~rc1.fc36.aarch64 < systemd-251.fc36.aarch64$ echo $?12$ systemd-analyze compare-versions 1 lt 2; echo $?0$ systemd-analyze compare-versions 1 ge 2; echo $?1systemd-analyze verify FILE...This command will load unit files and print warnings if anyerrors are detected. Files specified on the command line will beloaded, but also any other units referenced by them. A unit'sname on disk can be overridden by specifying an alias after acolon; see below for an example. The full unit search path isformed by combining the directories for all command linearguments, and the usual unit load paths. The variable$SYSTEMD_UNIT_PATH is supported, and may be used to replace oraugment the compiled in set of unit load paths; seesystemd.unit(5). All units files present in the directoriescontaining the command line arguments will be used in preferenceto the other paths.The following errors are currently detected:\u2022unknown sections and directives,\u2022missing dependencies which are required to start the givenunit,\u2022man pages listed in Documentation= which are not found in thesystem,\u2022commands listed in ExecStart= and similar which are not foundin the system or not executable.Example 17. Misspelt directives$ cat ./user.slice[Unit]WhatIsThis=11Documentation=man:nosuchfile(1)Requires=different.service[Service]Description=x$ systemd-analyze verify ./user.slice[./user.slice:9] Unknown lvalue 'WhatIsThis' in section 'Unit'[./user.slice:13] Unknown section 'Service'. Ignoring.Error: org.freedesktop.systemd1.LoadFailed:Unit different.service failed to load:No such file or directory.Failed to create user.slice/start: Invalid argumentuser.slice: man nosuchfile(1) command failed with code 16Example 18. Missing service units$ tail ./a.socket ./b.socket==> ./a.socket <==[Socket]ListenStream=100==> ./b.socket <==[Socket]ListenStream=100Accept=yes$ systemd-analyze verify ./a.socket ./b.socketService a.service not loaded, a.socket cannot be started.Service b@0.service not loaded, b.socket cannot be started.Example 19. Aliasing a unit$ cat /tmp/source[Unit]Description=Hostname printer[Service]Type=simpleExecStart=/usr/bin/echo %HMysteryKey=true$ systemd-analyze verify /tmp/sourceFailed to prepare filename /tmp/source: Invalid argument$ systemd-analyze verify /tmp/source:alias.servicealias.service:7: Unknown key name 'MysteryKey' in section 'Service', ignoring.systemd-analyze security [UNIT...]This command analyzes the security and sandboxing settings of oneor more specified service units. If at least one unit name isspecified the security settings of the specified service unitsare inspected and a detailed analysis is shown. If no unit nameis specified, all currently loaded, long-running service unitsare inspected and a terse table with results shown. The commandchecks for various security-related service settings, assigningeach a numeric \"exposure level\" value, depending on how importanta setting is. It then calculates an overall exposure level forthe whole unit, which is an estimation in the range 0.0...10.0indicating how exposed a service is security-wise. High exposurelevels indicate very little applied sandboxing. Low exposurelevels indicate tight sandboxing and strongest securityrestrictions. Note that this only analyzes the per-servicesecurity features systemd itself implements. This means that anyadditional security mechanisms applied by the service code itselfare not accounted for. The exposure level determined this wayshould not be misunderstood: a high exposure level neither meansthat there is no effective sandboxing applied by the service codeitself, nor that the service is actually vulnerable to remote orlocal attacks. High exposure levels do indicate however that mostlikely the service might benefit from additional settings appliedto them.Please note that many of the security and sandboxing settingsindividually can be circumvented \u2014 unless combined with others.For example, if a service retains the privilege to establish orundo mount points many of the sandboxing options can be undone bythe service code itself. Due to that is essential that eachservice uses the most comprehensive and strict sandboxing andsecurity settings possible. The tool will take into account someof these combinations and relationships between the settings, butnot all. Also note that the security and sandboxing settingsanalyzed here only apply to the operations executed by theservice code itself. If a service has access to an IPC system(such as D-Bus) it might request operations from other servicesthat are not subject to the same restrictions. Any comprehensivesecurity and sandboxing analysis is hence incomplete if the IPCaccess policy is not validated too.Example 20. Analyze systemd-logind.service$ systemd-analyze security --no-pager systemd-logind.serviceNAMEDESCRIPTIONEXPOSURE\u2717 PrivateNetwork=Service has access to the host's network0.5\u2717 User=/DynamicUser=Service runs as root user0.4\u2717 DeviceAllow=Service has no device ACL0.2\u2713 IPAddressDeny=Service blocks all IP address ranges...\u2192 Overall exposure level for systemd-logind.service: 4.1 OK \ud83d\ude42systemd-analyze inspect-elf FILE...This command will load the specified files, and if they are ELFobjects (executables, libraries, core files, etc.) it will parsethe embedded packaging metadata, if any, and print it in a tableor json format. See the Packaging Metadata[1] documentation formore information.Example 21. Print information about a core file as JSON$ systemd-analyze inspect-elf --json=pretty \\core.fsverity.1000.f77dac5dc161402aa44e15b7dd9dcf97.58561.1637106137000000{\"elfType\" : \"coredump\",\"elfArchitecture\" : \"AMD x86-64\",\"/home/bluca/git/fsverity-utils/fsverity\" : {\"type\" : \"deb\",\"name\" : \"fsverity-utils\",\"version\" : \"1.3-1\",\"buildId\" : \"7c895ecd2a271f93e96268f479fdc3c64a2ec4ee\"},\"/home/bluca/git/fsverity-utils/libfsverity.so.0\" : {\"type\" : \"deb\",\"name\" : \"fsverity-utils\",\"version\" : \"1.3-1\",\"buildId\" : \"b5e428254abf14237b0ae70ed85fffbb98a78f88\"}}systemd-analyze fdstore [UNIT...]Lists the current contents of the specified service unit's filedescriptor store. This shows names, inode types, device numbers,inode numbers, paths and open modes of the open file descriptors.The specified units must have FileDescriptorStoreMax= enabled,see systemd.service(5) for details.Example 22. Table output$ systemd-analyze fdstore systemd-journald.serviceFDNAME TYPE DEVNOINODE RDEVNO PATHFLAGSstored sock 0:84218620 -socket:[4218620] rostored sock 0:84213198 -socket:[4213198] rostored sock 0:84213190 -socket:[4213190] ro...Note: the \"DEVNO\" column refers to the major/minor numbers of thedevice node backing the file system the file descriptor's inodeis on. The \"RDEVNO\" column refers to the major/minor numbers ofthe device node itself if the file descriptor refers to one.Compare with corresponding .st_dev and .st_rdev fields in structstat (see stat(2) for details). The listed inode numbers in the\"INODE\" column are on the file system indicated by \"DEVNO\".systemd-analyze image-policy [POLICY...]This command analyzes the specified image policy string, as persystemd.image-policy(7). The policy is normalized and simplified.For each currently defined partition identifier (as per theDiscoverable Partitions Specification[2] the effect of the imagepolicy string is shown in tabular form.Example 23. Example Output$ systemd-analyze image-policy swap=encrypted:usr=read-only-on+verity:root=encryptedAnalyzing policy: root=encrypted:usr=verity+read-only-on:swap=encryptedLong form: root=encrypted:usr=verity+read-only-on:swap=encrypted:=unused+absentPARTITIONMODEREAD-ONLY GROWFSrootencrypted--usrverityyes-homeignore--srvignore--espignore--xbootldrignore--swapencrypted--root-verityignore--usr-verityunprotected yes-root-verity-sig ignore--usr-verity-sigignore--tmpignore--varignore--defaultignore--",
        "name": "systemd-analyze - Analyze and debug system manager",
        "section": 1
    },
    {
        "command": "systemd-ask-password",
        "description": "systemd-ask-password may be used to query a system password orpassphrase from the user, using a question message specified onthe command line. When run from a TTY it will query a password onthe TTY and print it to standard output. When run with no TTY orwith --no-tty it will use the system-wide query mechanism, whichallows active users to respond via several agents, listed below.The purpose of this tool is to query system-wide passwords \u2014 thatis passwords not attached to a specific user account. Examplesinclude: unlocking encrypted hard disks when they are plugged inor at boot, entering an SSL certificate passphrase for web andVPN servers.Existing agents are:\u2022A boot-time password agent asking the user for passwordsusing plymouth(8),\u2022A boot-time password agent querying the user directly on theconsole \u2014 systemd-ask-password-console.service(8),\u2022An agent requesting password input via a wall(1) message \u2014systemd-ask-password-wall.service(8),\u2022A TTY agent that is temporarily spawned during systemctl(1)invocations,\u2022A command line agent which can be started temporarily toprocess queued password requests \u2014systemd-tty-ask-password-agent --query.Answering system-wide password queries is a privileged operation,hence all the agents listed above (except for the last one), runas privileged system services. The last one also needs elevatedprivileges, so should be run through sudo(8) or similar.Additional password agents may be implemented according to thesystemd Password Agent Specification[1].If a password is queried on a TTY, the user may press TAB to hidethe asterisks normally shown for each character typed. PressingBackspace as first key achieves the same effect.",
        "name": "systemd-ask-password - Query the user for a system password",
        "section": 1
    },
    {
        "command": "systemd-cat",
        "description": "systemd-cat may be used to connect the standard input and outputof a process to the journal, or as a filter tool in a shellpipeline to pass the output the previous pipeline elementgenerates to the journal.If no parameter is passed, systemd-cat will write everything itreads from standard input (stdin) to the journal.If parameters are passed, they are executed as command line withstandard output (stdout) and standard error output (stderr)connected to the journal, so that all it writes is stored in thejournal.",
        "name": "systemd-cat - Connect a pipeline or program's output with thejournal",
        "section": 1
    },
    {
        "command": "systemd-cgls",
        "description": "systemd-cgls recursively shows the contents of the selected Linuxcontrol group hierarchy in a tree. If arguments are specified,shows all member processes of the specified control groups plusall their subgroups and their members. The control groups mayeither be specified by their full file paths or are assumed inthe systemd control group hierarchy. If no argument is specifiedand the current working directory is beneath the control groupmount point /sys/fs/cgroup/, shows the contents of the controlgroup the working directory refers to. Otherwise, the fullsystemd control group hierarchy is shown.By default, empty control groups are not shown.",
        "name": "systemd-cgls - Recursively show control group contents",
        "section": 1
    },
    {
        "command": "systemd-cgtop",
        "description": "systemd-cgtop shows the top control groups of the local Linuxcontrol group hierarchy, ordered by their CPU, memory, or diskI/O load. The display is refreshed in regular intervals (bydefault every 1s), similar in style to top(1). If a control grouppath is specified, shows only the services of the specifiedcontrol group.If systemd-cgtop is not connected to a tty, no column headers areprinted and the default is to only run one iteration. The--iterations= argument, if given, is honored. This mode issuitable for scripting.Resource usage is only accounted for control groups with theappropriate controllers turned on: \"cpu\" controller for CPUusage, \"memory\" controller for memory usage, and \"io\" controllerfor disk I/O consumption. If resource monitoring for theseresources is required, it is recommended to add theCPUAccounting=1, MemoryAccounting=1 and IOAccounting=1 settingsin the unit files in question. See systemd.resource-control(5)for details.The CPU load value can be between 0 and 100 times the number ofprocessors the system has. For example, if the system has 8processors, the CPU load value is going to be between 0% and800%. The number of processors can be found in \"/proc/cpuinfo\".To emphasize: unless \"CPUAccounting=1\", \"MemoryAccounting=1\", and\"IOAccounting=1\" are enabled for the services in question, noresource accounting will be available for system services and thedata shown by systemd-cgtop will be incomplete.",
        "name": "systemd-cgtop - Show top control groups by their resource usage",
        "section": 1
    },
    {
        "command": "systemd-creds",
        "description": "systemd-creds is a tool for listing, showing, encrypting anddecrypting unit credentials. Credentials are limited-size binaryor textual objects that may be passed to unit processes. They areprimarily used for passing cryptographic keys (both public andprivate) or certificates, user account information or identityinformation from the host to services.Credentials are configured in unit files via theImportCredential>, LoadCredential=, SetCredential=,LoadCredentialEncrypted= and SetCredentialEncrypted= settings,see systemd.exec(5) for details.For further information see System and Service Credentials[1]documentation.",
        "name": "systemd-creds - Lists, shows, encrypts and decrypts servicecredentials",
        "section": 1
    },
    {
        "command": "systemd-delta",
        "description": "systemd-delta may be used to identify and compare configurationfiles that override other configuration files. Files in /etc/have highest priority, files in /run/ have the second highestpriority, ..., files in /usr/lib/ have lowest priority. Files ina directory with higher priority override files with the samename in directories of lower priority. In addition, certainconfiguration files can have \".d\" directories which contain\"drop-in\" files with configuration snippets which augment themain configuration file. \"Drop-in\" files can be overridden in thesame way by placing files with the same name in a directory ofhigher priority (except that, in case of \"drop-in\" files, boththe \"drop-in\" file name and the name of the containing directory,which corresponds to the name of the main configuration file,must match). For a fuller explanation, see systemd.unit(5).The command line argument will be split into a prefix and asuffix. Either is optional. The prefix must be one of thedirectories containing configuration files (/etc/, /run/,/usr/lib/, ...). If it is given, only overriding files containedin this directory will be shown. Otherwise, all overriding fileswill be shown. The suffix must be a name of a subdirectorycontaining configuration files like tmpfiles.d, sysctl.d orsystemd/system. If it is given, only configuration files in thissubdirectory (across all configuration paths) will be analyzed.Otherwise, all configuration files will be analyzed. If thecommand line argument is not given at all, all configurationfiles will be analyzed. See below for some examples.",
        "name": "systemd-delta - Find overridden configuration files",
        "section": 1
    },
    {
        "command": "systemd-detect-virt",
        "description": "systemd-detect-virt detects execution in a virtualizedenvironment. It identifies the virtualization technology and candistinguish full machine virtualization from containervirtualization.systemd-detect-virt exits with a return value of0 (success) if a virtualization technology is detected, andnon-zero (error) otherwise. By default, any type ofvirtualization is detected, and the options --container and --vmcan be used to limit what types of virtualization are detected.When executed without --quiet will print a short identifier forthe detected virtualization technology. The followingtechnologies are currently identified:Table 1. Known virtualization technologies (both VM, i.e. fullhardware virtualization, and container, i.e. shared kernelvirtualization)\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502Type\u2502 ID\u2502 Product\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502VM\u2502 qemu\u2502 QEMU software\u2502\u2502\u2502\u2502 virtualization,\u2502\u2502\u2502\u2502 without KVM\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 kvm\u2502 Linux KVM kernel\u2502\u2502\u2502\u2502 virtual machine,\u2502\u2502\u2502\u2502 in combination\u2502\u2502\u2502\u2502 with QEMU. Not\u2502\u2502\u2502\u2502 used for other\u2502\u2502\u2502\u2502 virtualizers using\u2502\u2502\u2502\u2502 the KVM\u2502\u2502\u2502\u2502 interfaces, such\u2502\u2502\u2502\u2502 as Oracle\u2502\u2502\u2502\u2502 VirtualBox or\u2502\u2502\u2502\u2502 Amazon EC2 Nitro,\u2502\u2502\u2502\u2502 see below.\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 amazon\u2502 Amazon EC2 Nitro\u2502\u2502\u2502\u2502 using Linux KVM\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 zvm\u2502 s390 z/VM\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 vmware\u2502 VMware Workstation\u2502\u2502\u2502\u2502 or Server, and\u2502\u2502\u2502\u2502 related products\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 microsoft\u2502 Hyper-V, also\u2502\u2502\u2502\u2502 known as Viridian\u2502\u2502\u2502\u2502 or Windows Server\u2502\u2502\u2502\u2502 Virtualization\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 oracle\u2502 Oracle VM\u2502\u2502\u2502\u2502 VirtualBox\u2502\u2502\u2502\u2502 (historically\u2502\u2502\u2502\u2502 marketed by\u2502\u2502\u2502\u2502 innotek and Sun\u2502\u2502\u2502\u2502 Microsystems), for\u2502\u2502\u2502\u2502 legacy and KVM\u2502\u2502\u2502\u2502 hypervisor\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 powervm\u2502 IBM PowerVM\u2502\u2502\u2502\u2502 hypervisor \u2014 comes\u2502\u2502\u2502\u2502 as firmware with\u2502\u2502\u2502\u2502 some IBM POWER\u2502\u2502\u2502\u2502 servers\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 xen\u2502 Xen hypervisor\u2502\u2502\u2502\u2502 (only domU, not\u2502\u2502\u2502\u2502 dom0)\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 bochs\u2502 Bochs Emulator\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 uml\u2502 User-mode Linux\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 parallels\u2502 Parallels Desktop,\u2502\u2502\u2502\u2502 Parallels Server\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 bhyve\u2502 bhyve, FreeBSD\u2502\u2502\u2502\u2502 hypervisor\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 qnx\u2502 QNX hypervisor\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 acrn\u2502 ACRN hypervisor[1]\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 apple\u2502 Apple\u2502\u2502\u2502\u2502 Virtualization.framework[2] \u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 sre\u2502 LMHS SRE hypervisor[3]\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502Container \u2502 openvz\u2502 OpenVZ/Virtuozzo\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 lxc\u2502 Linux container\u2502\u2502\u2502\u2502 implementation by LXC\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 lxc-libvirt\u2502 Linux container\u2502\u2502\u2502\u2502 implementation by libvirt\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 systemd-nspawn \u2502 systemd's minimal container \u2502\u2502\u2502\u2502 implementation, see\u2502\u2502\u2502\u2502 systemd-nspawn(1)\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 docker\u2502 Docker container manager\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 podman\u2502 Podman[4] container manager \u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 rkt\u2502 rkt app container runtime\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 wsl\u2502 Windows Subsystem for\u2502\u2502\u2502\u2502 Linux[5]\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 proot\u2502 proot[6] userspace\u2502\u2502\u2502\u2502 chroot/bind mount emulation \u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502 pouch\u2502 Pouch[7] Container Engine\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518If multiple virtualization solutions are used, only the\"innermost\" is detected and identified. That means if bothmachine and container virtualization are used in conjunction,only the latter will be identified (unless --vm is passed).Windows Subsystem for Linux is not a Linux container, but anenvironment for running Linux userspace applications on top ofthe Windows kernel using a Linux-compatible interface. WSL iscategorized as a container for practical purposes. Multiple WSLenvironments share the same kernel and services should generallybehave like when being run in a container.",
        "name": "systemd-detect-virt - Detect execution in a virtualizedenvironment",
        "section": 1
    },
    {
        "command": "systemd-dissect",
        "description": "systemd-dissect is a tool for introspecting and interacting withfile system OS disk images, specifically Discoverable Disk Images(DDIs). It supports four different operations:1. Show general OS image information, including the image'sos-release(5) data, machine ID, partition information andmore.2. Mount an OS image to a local directory. In this mode it willdissect the OS image and mount the included partitionsaccording to their designation onto a directory and possiblysub-directories.3. Unmount an OS image from a local directory. In this mode itwill recursively unmount the mounted partitions and removethe underlying loop device, including all the partitionsub-devices.4. Copy files and directories in and out of an OS image.The tool may operate on three types of OS images:1. OS disk images containing a GPT partition table envelope,with partitions marked according to the DiscoverablePartitions Specification[1].2. OS disk images containing just a plain file-system without anenveloping partition table. (This file system is assumed tobe the root file system of the OS.)3. OS disk images containing a GPT or MBR partition table, witha single partition only. (This partition is assumed tocontain the root file system of the OS.)OS images may use any kind of Linux-supported file systems. Inaddition they may make use of LUKS disk encryption, and containVerity integrity information. Note that qualifying OS images maybe booted with systemd-nspawn(1)'s --image= switch, and be usedas root file system for system service using the RootImage= unitfile setting, see systemd.exec(5).Note that the partition table shown when invoked without commandswitch (as listed below) does not necessarily show all partitionsincluded in the image, but just the partitions that areunderstood and considered part of an OS disk image. Specifically,partitions of unknown types are ignored, as well as duplicatepartitions (i.e. more than one per partition type), as are rootand /usr/ partitions of architectures not compatible with thelocal system. In other words: this tool will display what itoperates with when mounting the image. To display the completelist of partitions use a tool such as fdisk(8).The systemd-dissect command may be invoked as mount.ddi in whichcase it implements the mount(8) \"external helper\" interface. Thisensures disk images compatible with systemd-dissect can bemounted directly by mount and fstab(5). For details see below.",
        "name": "systemd-dissect, mount.ddi - Dissect Discoverable Disk Images(DDIs)",
        "section": 1
    },
    {
        "command": "systemd-escape",
        "description": "systemd-escape may be used to escape strings for inclusion insystemd unit names. The command may be used to escape and to undoescaping of strings.The command takes any number of strings on the command line, andwill process them individually, one after another. It will outputthem separated by spaces to stdout.By default, this command will escape the strings passed, unless--unescape is passed which results in the inverse operation beingapplied. If --mangle is given, a special mode of escaping isapplied instead, which assumes the string is already escaped butwill escape everything that appears obviously non-escaped.For details on the escaping and unescaping algorithms see therelevant section in systemd.unit(5).",
        "name": "systemd-escape - Escape strings for usage in systemd unit names",
        "section": 1
    },
    {
        "command": "systemd-firstboot",
        "description": "systemd-firstboot initializes basic system settings interactivelyduring the first boot, or non-interactively on an offline systemimage. The service is started during boot ifConditionFirstBoot=yes is met, which essentially means that /etc/is empty, see systemd.unit(5) for details.The following settings may be configured:\u2022The machine ID of the system\u2022The system locale, more specifically the two locale variablesLANG= and LC_MESSAGES\u2022The system keyboard map\u2022The system time zone\u2022The system hostname\u2022The kernel command line used when installing kernel images\u2022The root user's password and shellEach of the fields may either be queried interactively by users,set non-interactively on the tool's command line, or be copiedfrom a host system that is used to set up the system image.If a setting is already initialized, it will not be overwrittenand the user will not be prompted for the setting.Note that this tool operates directly on the file system and doesnot involve any running system services, unlike localectl(1),timedatectl(1) or hostnamectl(1). This allows systemd-firstbootto operate on mounted but not booted disk images and in earlyboot. It is not recommended to use systemd-firstboot on therunning system after it has been set up.",
        "name": "systemd-firstboot, systemd-firstboot.service - Initialize basicsystem settings on or before the first boot-up of a system",
        "section": 1
    },
    {
        "command": "systemd-firstboot.service",
        "description": "systemd-firstboot initializes basic system settings interactivelyduring the first boot, or non-interactively on an offline systemimage. The service is started during boot ifConditionFirstBoot=yes is met, which essentially means that /etc/is empty, see systemd.unit(5) for details.The following settings may be configured:\u2022The machine ID of the system\u2022The system locale, more specifically the two locale variablesLANG= and LC_MESSAGES\u2022The system keyboard map\u2022The system time zone\u2022The system hostname\u2022The kernel command line used when installing kernel images\u2022The root user's password and shellEach of the fields may either be queried interactively by users,set non-interactively on the tool's command line, or be copiedfrom a host system that is used to set up the system image.If a setting is already initialized, it will not be overwrittenand the user will not be prompted for the setting.Note that this tool operates directly on the file system and doesnot involve any running system services, unlike localectl(1),timedatectl(1) or hostnamectl(1). This allows systemd-firstbootto operate on mounted but not booted disk images and in earlyboot. It is not recommended to use systemd-firstboot on therunning system after it has been set up.",
        "name": "systemd-firstboot, systemd-firstboot.service - Initialize basicsystem settings on or before the first boot-up of a system",
        "section": 1
    },
    {
        "command": "systemd-id128",
        "description": "id128 may be used to conveniently print sd-id128(3) UUIDs. Whatidentifier is printed depends on the specific verb.With new, a new random identifier will be generated.With machine-id, the identifier of the current machine will beprinted. See machine-id(5).With boot-id, the identifier of the current boot will be printed.Both machine-id and boot-id may be combined with the--app-specific=app-id switch to generate application-specificIDs. See sd_id128_get_machine(3) for the discussion when this isuseful.With invocation-id, the identifier of the current serviceinvocation will be printed. This is available in systemdservices. See systemd.exec(5).With show, well-known IDs are printed (for now, only GPTpartition type UUIDs), along with brief identifier strings. Whenno arguments are specified, all known IDs are shown. Whenarguments are specified, they must be the identifiers or IDvalues of one or more known IDs, which are then printed. Combinewith --uuid to list the IDs in UUID style, i.e. the way GPTpartition type UUIDs are usually shown.",
        "name": "systemd-id128 - Generate and print sd-128 identifiers",
        "section": 1
    },
    {
        "command": "systemd-inhibit",
        "description": "systemd-inhibit may be used to execute a program with a shutdown,sleep, or idle inhibitor lock taken. The lock will be acquiredbefore the specified command line is executed and releasedafterwards.Inhibitor locks may be used to block or delay system sleep andshutdown requests from the user, as well as automatic idlehandling of the OS. This is useful to avoid system suspends whilean optical disc is being recorded, or similar operations thatshould not be interrupted.For more information see the Inhibitor Lock DeveloperDocumentation[1].",
        "name": "systemd-inhibit - Execute a program with an inhibition lock taken",
        "section": 1
    },
    {
        "command": "systemd-machine-id-setup",
        "description": "systemd-machine-id-setup may be used by system installer tools toinitialize the machine ID stored in /etc/machine-id at installtime, with a provisioned or randomly generated ID. Seemachine-id(5) for more information about this file.If the tool is invoked without the --commit switch,/etc/machine-id is initialized with a valid, new machine ID if itis missing or empty. The new machine ID will be acquired in thefollowing fashion:1. If a valid D-Bus machine ID is already configured for thesystem, the D-Bus machine ID is copied and used to initializethe machine ID in /etc/machine-id.2. If run inside a KVM virtual machine and a UUID is configured(via the -uuid option), this UUID is used to initialize themachine ID. The caller must ensure that the UUID passed issufficiently unique and is different for every bootedinstance of the VM.3. Similarly, if run inside a Linux container environment and aUUID is configured for the container, this is used toinitialize the machine ID. For details, see the documentationof the Container Interface[1].4. Otherwise, a new ID is randomly generated.The --commit switch may be used to commit a transient machined IDto disk, making it persistent. For details, see below.Use systemd-firstboot(1) to initialize the machine ID on mounted(but not booted) system images.",
        "name": "systemd-machine-id-setup - Initialize the machine ID in/etc/machine-id",
        "section": 1
    },
    {
        "command": "systemd-mount",
        "description": "systemd-mount may be used to create and start a transient .mountor .automount unit of the file system WHAT on the mount pointWHERE.In many ways, systemd-mount is similar to the lower-levelmount(8) command, however instead of executing the mountoperation directly and immediately, systemd-mount schedules itthrough the service manager job queue, so that it may pull infurther dependencies (such as parent mounts, or a file systemchecker to execute a priori), and may make use of theauto-mounting logic.The command takes either one or two arguments. If only oneargument is specified it should refer to a block device orregular file containing a file system (e.g.\"/dev/sdb1\" or\"/path/to/disk.img\"). The block device or image file is thenprobed for a file system label and other metadata, and is mountedto a directory below /run/media/system/ whose name is generatedfrom the file system label. In this mode the block device orimage file must exist at the time of invocation of the command,so that it may be probed. If the device is found to be aremovable block device (e.g. a USB stick), an automount point iscreated instead of a regular mount point (i.e. the --automount=option is implied, see below).If two arguments are specified, the first indicates the mountsource (the WHAT) and the second indicates the path to mount iton (the WHERE). In this mode no probing of the source isattempted, and a backing device node doesn't have to exist.However, if this mode is combined with --discover, device nodeprobing for additional metadata is enabled, and \u2013 much like inthe single-argument case discussed above \u2013 the specified devicehas to exist at the time of invocation of the command.Use the --list command to show a terse table of all local, knownblock devices with file systems that may be mounted with thiscommand.systemd-umount can be used to unmount a mount or automount point.It is the same as systemd-mount --umount.",
        "name": "systemd-mount, systemd-umount - Establish and destroy transientmount or auto-mount points",
        "section": 1
    },
    {
        "command": "systemd-notify",
        "description": "systemd-notify may be called by service scripts to notify theinvoking service manager about status changes. It can be used tosend arbitrary information, encoded in an environment-block-likelist of strings. Most importantly, it can be used for start-upcompletion notification.This is mostly just a wrapper around sd_notify() and makes thisfunctionality available to shell scripts. For details seesd_notify(3).The command line may carry a list of environment variables tosend as part of the status update.Note that systemd will refuse reception of status updates fromthis command unless NotifyAccess= is appropriately set for theservice unit this command is called from. See systemd.service(5)for details.Note that sd_notify() notifications may be attributed to unitscorrectly only if either the sending process is still around atthe time the service manager processes the message, or if thesending process is explicitly runtime-tracked by the servicemanager. The latter is the case if the service manager originallyforked off the process, i.e. on all processes that matchNotifyAccess=main or NotifyAccess=exec. Conversely, if anauxiliary process of the unit sends an sd_notify() message andimmediately exits, the service manager might not be able toproperly attribute the message to the unit, and thus will ignoreit, even if NotifyAccess=all is set for it. To address thissystemd-notify will wait until the notification message has beenprocessed by the service manager. When --no-block is used, thissynchronization for reception of notifications is disabled, andhence the aforementioned race may occur if the invoking processis not the service manager or spawned by the service manager.systemd-notify will first attempt to invoke sd_notify()pretending to have the PID of the parent process ofsystemd-notify (i.e. the invoking process). This will onlysucceed when invoked with sufficient privileges. On failure, itwill then fall back to invoking it under its own PID. Thisbehaviour is useful in order that when the tool is invoked from ashell script the shell process \u2014 and not the systemd-notifyprocess \u2014 appears as sender of the message, which in turn ishelpful if the shell process is the main process of a service,due to the limitations of NotifyAccess=all. Use the --pid= switchto tweak this behaviour.",
        "name": "systemd-notify - Notify service manager about start-up completionand other daemon status changes",
        "section": 1
    },
    {
        "command": "systemd-nspawn",
        "description": "systemd-nspawn may be used to run a command or OS in alight-weight namespace container. In many ways it is similar tochroot(1), but more powerful since it fully virtualizes the filesystem hierarchy, as well as the process tree, the various IPCsubsystems and the host and domain name.systemd-nspawn may be invoked on any directory tree containing anoperating system tree, using the --directory= command lineoption. By using the --machine= option an OS tree isautomatically searched for in a couple of locations, mostimportantly in /var/lib/machines/, the suggested directory toplace OS container images installed on the system.In contrast to chroot(1) systemd-nspawn may be used to boot fullLinux-based operating systems in a container.systemd-nspawn limits access to various kernel interfaces in thecontainer to read-only, such as /sys/, /proc/sys/ or/sys/fs/selinux/. The host's network interfaces and the systemclock may not be changed from within the container. Device nodesmay not be created. The host system cannot be rebooted and kernelmodules may not be loaded from within the container.Use a tool like dnf(8), debootstrap(8), or pacman(8) to set up anOS directory tree suitable as file system hierarchy forsystemd-nspawn containers. See the Examples section below fordetails on suitable invocation of these commands.As a safety check systemd-nspawn will verify the existence of/usr/lib/os-release or /etc/os-release in the container treebefore booting a container (see os-release(5)). It might benecessary to add this file to the container tree manually if theOS of the container is too old to contain this fileout-of-the-box.systemd-nspawn may be invoked directly from the interactivecommand line or run as system service in the background. In thismode each container instance runs as its own service instance; adefault template unit file systemd-nspawn@.service is provided tomake this easy, taking the container name as instance identifier.Note that different default options apply when systemd-nspawn isinvoked by the template unit file than interactively on thecommand line. Most importantly the template unit file makes useof the --boot option which is not the default in casesystemd-nspawn is invoked from the interactive command line.Further differences with the defaults are documented along withthe various supported options below.The machinectl(1) tool may be used to execute a number ofoperations on containers. In particular it provides easy-to-usecommands to run containers as system services using thesystemd-nspawn@.service template unit file.Along with each container a settings file with the .nspawn suffixmay exist, containing additional settings to apply when runningthe container. See systemd.nspawn(5) for details. Settings filesoverride the default options used by the systemd-nspawn@.servicetemplate unit file, making it usually unnecessary to alter thistemplate file directly.Note that systemd-nspawn will mount file systems private to thecontainer to /dev/, /run/ and similar. These will not be visibleoutside of the container, and their contents will be lost whenthe container exits.Note that running two systemd-nspawn containers from the samedirectory tree will not make processes in them see each other.The PID namespace separation of the two containers is completeand the containers will share very few runtime objects except forthe underlying file system. Rather use machinectl(1)'s login orshell commands to request an additional login session in arunning container.systemd-nspawn implements the Container Interface[1]specification.While running, containers invoked with systemd-nspawn areregistered with the systemd-machined(8) service that keeps trackof running containers, and provides programming interfaces tointeract with them.",
        "name": "systemd-nspawn - Spawn a command or OS in a light-weightcontainer",
        "section": 1
    },
    {
        "command": "systemd-path",
        "description": "systemd-path may be used to query system and user paths. The toolmakes many of the paths described in file-hierarchy(7) availablefor querying.When invoked without arguments, a list of known paths and theircurrent values is shown. When at least one argument is passed,the path with this name is queried and its value shown. Thevariables whose name begins with \"search-\" do not refer toindividual paths, but instead to a list of colon-separated searchpaths, in their order of precedence.",
        "name": "systemd-path - List and query system and user paths",
        "section": 1
    },
    {
        "command": "systemd-run",
        "description": "systemd-run may be used to create and start a transient .serviceor .scope unit and run the specified COMMAND in it. It may alsobe used to create and start a transient .path, .socket, or .timerunit, that activates a .service unit when elapsing.If a command is run as transient service unit, it will be startedand managed by the service manager like any other service, andthus shows up in the output of systemctl list-units like anyother unit. It will run in a clean and detached executionenvironment, with the service manager as its parent process. Inthis mode, systemd-run will start the service asynchronously inthe background and return after the command has begun execution(unless --no-block or --wait are specified, see below).If a command is run as transient scope unit, it will be executedby systemd-run itself as parent process and will thus inherit theexecution environment of the caller. However, the processes ofthe command are managed by the service manager similarly tonormal services, and will show up in the output of systemctllist-units. Execution in this case is synchronous, and willreturn only when the command finishes. This mode is enabled viathe --scope switch (see below).If a command is run with path, socket, or timer options such as--on-calendar= (see below), a transient path, socket, or timerunit is created alongside the service unit for the specifiedcommand. Only the transient path, socket, or timer unit isstarted immediately, the transient service unit will be triggeredby the path, socket, or timer unit. If the --unit= option isspecified, the COMMAND may be omitted. In this case, systemd-runcreates only a .path, .socket, or .timer unit that triggers thespecified unit.By default, services created with systemd-run default to thesimple type, see the description of Type= in systemd.service(5)for details. Note that when this type is used, the servicemanager (and thus the systemd-run command) considers servicestart-up successful as soon as the fork() for the main serviceprocess succeeded, i.e. before the execve() is invoked, and thuseven if the specified command cannot be started. Consider usingthe exec service type (i.e.--property=Type=exec) to ensure thatsystemd-run returns successfully only if the specified commandline has been successfully started.After systemd-run passes the command to the service manager, themanager performs variable expansion. This means that dollarcharacters (\"$\") which should not be expanded need to be escapedas \"$$\". Expansion can also be disabled using--expand-environment=no.",
        "name": "systemd-run - Run programs in transient scope units, serviceunits, or path-, socket-, or timer-triggered service units",
        "section": 1
    },
    {
        "command": "systemd-socket-activate",
        "description": "systemd-socket-activate may be used to launch a socket-activatedservice program from the command line for testing purposes. Itmay also be used to launch individual instances of the serviceprogram per connection.The daemon to launch and its options should be specified afteroptions intended for systemd-socket-activate.If the --inetd option is given, the socket file descriptor willbe used as the standard input and output of the launched process.Otherwise, standard input and output will be inherited, andsockets will be passed through file descriptors 3 and higher.Sockets passed through $LISTEN_FDS to systemd-socket-activatewill be passed through to the daemon, in the original positions.Other sockets specified with --listen= will use consecutivedescriptors. By default, systemd-socket-activate listens on astream socket, use --datagram and --seqpacket to listen ondatagram or sequential packet sockets instead (see below).",
        "name": "systemd-socket-activate - Test socket activation of daemons",
        "section": 1
    },
    {
        "command": "systemd-stdio-bridge",
        "description": "systemd-stdio-bridge implements a proxy between STDIN/STDOUT anda D-Bus bus. It expects to receive an open connection viaSTDIN/STDOUT when started, and will create a new connection tothe specified bus. It will then forward messages between the twoconnections. This program is suitable for socket activation: thefirst connection may be a pipe or a socket and must be passed aseither standard input, or as an open file descriptor according tothe protocol described in sd_listen_fds(3). The second connectionwill be made by default to the local system bus, but this can beinfluenced by the --user, --system, --machine=, and --bus-path=options described below.sd-bus(3) uses systemd-stdio-bridge to forward D-Bus connectionsover ssh(1), or to connect to the bus of a different user, seesd_bus_set_address(3).",
        "name": "systemd-stdio-bridge - D-Bus proxy",
        "section": 1
    },
    {
        "command": "systemd-tty-ask-password-agent",
        "description": "systemd-tty-ask-password-agent is a password agent that handlespassword requests of the system, for example for hard diskencryption passwords or SSL certificate passwords that need to bequeried at boot-time or during runtime.systemd-tty-ask-password-agent implements the Password AgentsSpecification[1], and is one of many possible response agentswhich answer to queries formulated with systemd-ask-password(1).",
        "name": "systemd-tty-ask-password-agent - List or process pending systemdpassword requests",
        "section": 1
    },
    {
        "command": "systemd-umount",
        "description": "systemd-mount may be used to create and start a transient .mountor .automount unit of the file system WHAT on the mount pointWHERE.In many ways, systemd-mount is similar to the lower-levelmount(8) command, however instead of executing the mountoperation directly and immediately, systemd-mount schedules itthrough the service manager job queue, so that it may pull infurther dependencies (such as parent mounts, or a file systemchecker to execute a priori), and may make use of theauto-mounting logic.The command takes either one or two arguments. If only oneargument is specified it should refer to a block device orregular file containing a file system (e.g.\"/dev/sdb1\" or\"/path/to/disk.img\"). The block device or image file is thenprobed for a file system label and other metadata, and is mountedto a directory below /run/media/system/ whose name is generatedfrom the file system label. In this mode the block device orimage file must exist at the time of invocation of the command,so that it may be probed. If the device is found to be aremovable block device (e.g. a USB stick), an automount point iscreated instead of a regular mount point (i.e. the --automount=option is implied, see below).If two arguments are specified, the first indicates the mountsource (the WHAT) and the second indicates the path to mount iton (the WHERE). In this mode no probing of the source isattempted, and a backing device node doesn't have to exist.However, if this mode is combined with --discover, device nodeprobing for additional metadata is enabled, and \u2013 much like inthe single-argument case discussed above \u2013 the specified devicehas to exist at the time of invocation of the command.Use the --list command to show a terse table of all local, knownblock devices with file systems that may be mounted with thiscommand.systemd-umount can be used to unmount a mount or automount point.It is the same as systemd-mount --umount.",
        "name": "systemd-mount, systemd-umount - Establish and destroy transientmount or auto-mount points",
        "section": 1
    },
    {
        "command": "tabs",
        "description": "The @TABS@ program clears and sets tab-stops on the terminal.This uses the terminfo clear_all_tabs and set_tab capabilities.If either is absent, @TABS@ is unable to clear/set tab-stops.The terminal should be configured to use hard tabs, e.g.,stty tab0Like @CLEAR@(1), @TABS@ writes to the standard output.You canredirect the standard output to a file (which prevents @TABS@from actually changing the tabstops), and later cat the file tothe screen, setting tabstops at that point.These are hardware tabs, which cannot be queried rapidly byapplications running in the terminal, if at all.Curses andother full-screen applications may use hardware tabs inoptimizing their output to the terminal.If the hardwaretabstops differ from the information in the terminal database,the result is unpredictable.Before running curses programs, youshould either reset tab-stops to the standard intervaltabs -8or use the @RESET@ program, since the normal initializationsequences do not ensure that tab-stops are reset.",
        "name": "@TABS@ - set tabs on a terminal",
        "section": 1
    },
    {
        "command": "tac",
        "description": "Write each FILE to standard output, last line first.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-b, --beforeattach the separator before instead of after-r, --regexinterpret the separator as a regular expression-s, --separator=STRINGuse STRING as the separator instead of newline--help display this help and exit--versionoutput version information and exit",
        "name": "tac - concatenate and print files in reverse",
        "section": 1
    },
    {
        "command": "tail",
        "description": "Print the last 10 lines of each FILE to standard output.Withmore than one FILE, precede each with a header giving the filename.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-c, --bytes=[+]NUMoutput the last NUM bytes; or use -c +NUM to outputstarting with byte NUM of each file-f, --follow[={name|descriptor}]output appended data as the file grows;an absent option argument means 'descriptor'-Fsame as --follow=name --retry-n, --lines=[+]NUMoutput the last NUM lines, instead of the last 10; or use-n +NUM to output starting with line NUM--max-unchanged-stats=Nwith --follow=name, reopen a FILE which has notchanged size after N (default 5) iterations to see if ithas been unlinked or renamed (this is the usual case ofrotated log files); with inotify, this option is rarelyuseful--pid=PIDwith -f, terminate after process ID, PID dies-q, --quiet, --silentnever output headers giving file names--retrykeep trying to open a file if it is inaccessible-s, --sleep-interval=Nwith -f, sleep for approximately N seconds (default 1.0)between iterations; with inotify and --pid=P, checkprocess P at least once every N seconds-v, --verbosealways output headers giving file names-z, --zero-terminatedline delimiter is NUL, not newline--help display this help and exit--versionoutput version information and exitNUM may have a multiplier suffix: b 512, kB 1000, K 1024, MB1000*1000, M 1024*1024, GB 1000*1000*1000, G 1024*1024*1024, andso on for T, P, E, Z, Y, R, Q.Binary prefixes can be used, too:KiB=K, MiB=M, and so on.With --follow (-f), tail defaults to following the filedescriptor, which means that even if a tail'ed file is renamed,tail will continue to track its end.This default behavior isnot desirable when you really want to track the actual name ofthe file, not the file descriptor (e.g., log rotation).Use--follow=name in that case.That causes tail to track the namedfile in a way that accommodates renaming, removal and creation.",
        "name": "tail - output the last part of files",
        "section": 1
    },
    {
        "command": "tapestat",
        "description": "The tapestat command is used for monitoring the activity of tapedrives connected to a system.The first report generated by the tapestat command providesstatistics concerning the time since the system was booted,unless the -y option is used, when this first report is omitted.Each subsequent report covers the time since the previous report.The interval parameter specifies the amount of time in secondsbetween each report. The count parameter can be specified inconjunction with the interval parameter. If the count parameteris specified, the value of count determines the number of reportsgenerated at interval seconds apart. If the interval parameter isspecified without the count parameter, the tapestat commandgenerates reports continuously.",
        "name": "tapestat - Report tape statistics.",
        "section": 1
    },
    {
        "command": "tar",
        "description": "GNU tar is an archiving program designed to store multiple filesin a single file (an archive), and to manipulate such archives.The archive can be either a regular file or a device (e.g. a tapedrive, hence the name of the program, which stands for tapearchiver), which can be located either on the local or on aremote machine.Option stylesOptions to GNU tar can be given in three different styles.Intraditional style, the first argument is a cluster of optionletters and all subsequent arguments supply arguments to thoseoptions that require them.The arguments are read in the sameorder as the option letters.Any command line words that remainafter all options has been processed are treated as non-optionalarguments: file or archive member names.For example, the c option requires creating the archive, the voption requests the verbose operation, and the f option takes anargument that sets the name of the archive to operate upon.Thefollowing command, written in the traditional style, instructstar to store all files from the directory /etc into the archivefile etc.tar verbosely listing the files being archived:tar cfv etc.tar /etcIn UNIX or short-option style, each option letter is prefixedwith a single dash, as in other command line utilities.If anoption takes argument, the argument follows it, either as aseparate command line word, or immediately following the option.However, if the option takes an optional argument, the argumentmust follow the option letter without any intervening whitespace,as in -g/tmp/snar.db.Any number of options not taking arguments can be clusteredtogether after a single dash, e.g. -vkp.Options that takearguments (whether mandatory or optional), can appear at the endof such a cluster, e.g. -vkpf a.tar.The example command above written in the short-option style couldlook like:tar -cvf etc.tar /etcortar -c -v -f etc.tar /etcIn GNU or long-option style, each option begins with two dashesand has a meaningful name, consisting of lower-case letters anddashes.When used, the long option can be abbreviated to itsinitial letters, provided that this does not create ambiguity.Arguments to long options are supplied either as a separatecommand line word, immediately following the option, or separatedfrom the option by an equals sign with no intervening whitespace.Optional arguments must always use the latter method.Here are several ways of writing the example command in thisstyle:tar --create --file etc.tar --verbose /etcor (abbreviating some options):tar --cre --file=etc.tar --verb /etcThe options in all three styles can be intermixed, although doingso with old options is not encouraged.Operation modeThe options listed in the table below tell GNU tar what operationit is to perform.Exactly one of them must be given.Meaning ofnon-optional arguments depends on the operation mode requested.-A, --catenate, --concatenateAppend archive to the end of another archive.Thearguments are treated as the names of archives to append.All archives must be of the same format as the archivethey are appended to, otherwise the resulting archivemight be unusable with non-GNU implementations of tar.Notice also that when more than one archive is given, themembers from archives other than the first one will beaccessible in the resulting archive only if using the -i(--ignore-zeros) option.Compressed archives cannot be concatenated.-c, --createCreate a new archive.Arguments supply the names of thefiles to be archived.Directories are archivedrecursively, unless the --no-recursion option is given.-d, --diff, --compareFind differences between archive and file system.Thearguments are optional and specify archive members tocompare.If not given, the current working directory isassumed.--deleteDelete from the archive.The arguments supply names ofthe archive members to be removed.At least one argumentmust be given.This option does not operate on compressed archives.There is no short option equivalent.-r, --appendAppend files to the end of an archive.Arguments have thesame meaning as for -c (--create).-t, --listList the contents of an archive.Arguments are optional.When given, they specify the names of the members to list.--test-labelTest the archive volume label and exit.When used withoutarguments, it prints the volume label (if any) and exitswith status 0.When one or more command line argumentsare given.tar compares the volume label with eachargument.It exits with code 0 if a match is found, andwith code 1 otherwise.No output is displayed, unlessused together with the -v (--verbose) option.There is no short option equivalent for this option.-u, --updateAppend files which are newer than the corresponding copyin the archive.Arguments have the same meaning as with-c and -r options.Notice, that newer files don't replacetheir old archive copies, but instead are appended to theend of archive.The resulting archive can thus containseveral members of the same name, corresponding to variousversions of the same file.-x, --extract, --getExtract files from an archive.Arguments are optional.When given, they specify names of the archive members tobe extracted.--show-defaultsShow built-in defaults for various tar options and exit.No arguments are allowed.-?, --helpDisplay a short option summary and exit.No argumentsallowed.--usageDisplay a list of available options and exit.Noarguments allowed.--versionPrint program version and copyright information and exit.",
        "name": "tar - an archiving utility",
        "section": 1
    },
    {
        "command": "taskset",
        "description": "The taskset command is used to set or retrieve the CPU affinityof a running process given its pid, or to launch a new commandwith a given CPU affinity. CPU affinity is a scheduler propertythat \"bonds\" a process to a given set of CPUs on the system. TheLinux scheduler will honor the given CPU affinity and the processwill not run on any other CPUs. Note that the Linux scheduleralso supports natural CPU affinity: the scheduler attempts tokeep processes on the same CPU as long as practical forperformance reasons. Therefore, forcing a specific CPU affinityis useful only in certain applications. The affinity of someprocesses like kernel per-CPU threads cannot be set.The CPU affinity is represented as a bitmask, with the lowestorder bit corresponding to the first logical CPU and the highestorder bit corresponding to the last logical CPU. Not all CPUs mayexist on a given system but a mask may specify more CPUs than arepresent. A retrieved mask will reflect only the bits thatcorrespond to CPUs physically on the system. If an invalid maskis given (i.e., one that corresponds to no valid CPUs on thecurrent system) an error is returned. The masks may be specifiedin hexadecimal (with or without a leading \"0x\"), or as a CPU listwith the --cpu-list option. For example,0x00000001is processor #0,0x00000003is processors #0 and #1,FFFFFFFFis processors #0 through #31,0x32is processors #1, #4, and #5,--cpu-list 0-2,6is processors #0, #1, #2, and #6.--cpu-list 0-10:2is processors #0, #2, #4, #6, #8 and #10. The suffix \":N\"specifies stride in the range, for example 0-10:3 isinterpreted as 0,3,6,9 list.When taskset returns, it is guaranteed that the given program hasbeen scheduled to a legal CPU.",
        "name": "taskset - set or retrieve a process's CPU affinity",
        "section": 1
    },
    {
        "command": "tbl",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "tcpdump",
        "description": "Tcpdump prints out a description of the contents of packets on anetwork interface that match the Boolean expression (seepcap-filter(@MAN_MISC_INFO@) for the expression syntax); thedescription is preceded by a time stamp, printed, by default, ashours, minutes, seconds, and fractions of a second sincemidnight.It can also be run with the -w flag, which causes itto save the packet data to a file for later analysis, and/or withthe -r flag, which causes it to read from a saved packet filerather than to read packets from a network interface.It canalso be run with the -V flag, which causes it to read a list ofsaved packet files. In all cases, only packets that matchexpression will be processed by tcpdump.Tcpdump will, if not run with the -c flag, continue capturingpackets until it is interrupted by a SIGINT signal (generated,for example, by typing your interrupt character, typicallycontrol-C) or a SIGTERM signal (typically generated with thekill(1) command); if run with the -c flag, it will capturepackets until it is interrupted by a SIGINT or SIGTERM signal orthe specified number of packets have been processed.When tcpdump finishes capturing packets, it will report countsof:packets ``captured'' (this is the number of packets thattcpdump has received and processed);packets ``received by filter'' (the meaning of thisdepends on the OS on which you're running tcpdump, andpossibly on the way the OS was configured - if a filterwas specified on the command line, on some OSes it countspackets regardless of whether they were matched by thefilter expression and, even if they were matched by thefilter expression, regardless of whether tcpdump has readand processed them yet, on other OSes it counts onlypackets that were matched by the filter expressionregardless of whether tcpdump has read and processed themyet, and on other OSes it counts only packets that werematched by the filter expression and were processed bytcpdump);packets ``dropped by kernel'' (this is the number ofpackets that were dropped, due to a lack of buffer space,by the packet capture mechanism in the OS on which tcpdumpis running, if the OS reports that information toapplications; if not, it will be reported as 0).On platforms that support the SIGINFO signal, such as most BSDs(including macOS) and Digital/Tru64 UNIX, it will report thosecounts when it receives a SIGINFO signal (generated, for example,by typing your ``status'' character, typically control-T,although on some platforms, such as macOS, the ``status''character is not set by default, so you must set it with stty(1)in order to use it) and will continue capturing packets. Onplatforms that do not support the SIGINFO signal, the same can beachieved by using the SIGUSR1 signal.Using the SIGUSR2 signal along with the -w flag will forciblyflush the packet buffer into the output file.Reading packets from a network interface may require that youhave special privileges; see the pcap(3PCAP) man page fordetails.Reading a saved packet file doesn't require specialprivileges.",
        "name": "tcpdump - dump traffic on a network",
        "section": 1
    },
    {
        "command": "tee",
        "description": "Copy standard input to each FILE, and also to standard output.-a, --appendappend to the given FILEs, do not overwrite-i, --ignore-interruptsignore interrupt signals-poperate in a more appropriate MODE with pipes.--output-error[=MODE]set behavior on write error.See MODE below--help display this help and exit--versionoutput version information and exitMODE determines behavior with write errors on the outputs:warndiagnose errors writing to any outputwarn-nopipediagnose errors writing to any output not a pipeexitexit on error writing to any outputexit-nopipeexit on error writing to any output not a pipeThe default MODE for the -p option is 'warn-nopipe'.With\"nopipe\" MODEs, exit immediately if all outputs become brokenpipes.The default operation when --output-error is notspecified, is to exit immediately on error writing to a pipe, anddiagnose errors writing to non pipe outputs.",
        "name": "tee - read from standard input and write to standard output andfiles",
        "section": 1
    },
    {
        "command": "telnet-probe",
        "description": "telnet-probe allows the pmdashping(1) daemons to establishconnections to arbitrary local and remote service-providingdaemons so that response time and service availabilityinformation can be obtained.The required host and port number arguments have the same meaningas their telnet(1) equivalents.The -c option causes telnet-probe to perform a connect(2) only.This skips the read(2) and write(2) exercise that would otherwisebe done after connecting (see below).Once the telnet connection has been established, telnet-probereads from stdin until end-of-file, and writes all the input datato the telnet connection.Next, telnet-probe will read from thetelnet connection until end-of-file, discarding whatever data itreceives.Then telnet-probe exits.To operate successfully, the input passed via telnet-probe to theremote service must be sufficient to cause the remote service toclose the connection when the last line of input has beenprocessed, e.g. ending with ``quit'' when probing SMTP on port25.By default telnet-probe will not produce any output, unless thereis an error in which case a diagnostic message can be displayed(in verbose mode only) and the exit status will be non-zeroindicating a failure.",
        "name": "telnet-probe - lightweight telnet-like port probe",
        "section": 1
    },
    {
        "command": "test",
        "description": "Exit with the status determined by EXPRESSION.--help display this help and exit--versionoutput version information and exitAn omitted EXPRESSION defaults to false.Otherwise, EXPRESSIONis true or false and sets exit status.It is one of:( EXPRESSION )EXPRESSION is true! EXPRESSIONEXPRESSION is falseEXPRESSION1 -a EXPRESSION2both EXPRESSION1 and EXPRESSION2 are trueEXPRESSION1 -o EXPRESSION2either EXPRESSION1 or EXPRESSION2 is true-n STRINGthe length of STRING is nonzeroSTRING equivalent to -n STRING-z STRINGthe length of STRING is zeroSTRING1 = STRING2the strings are equalSTRING1 != STRING2the strings are not equalINTEGER1 -eq INTEGER2INTEGER1 is equal to INTEGER2INTEGER1 -ge INTEGER2INTEGER1 is greater than or equal to INTEGER2INTEGER1 -gt INTEGER2INTEGER1 is greater than INTEGER2INTEGER1 -le INTEGER2INTEGER1 is less than or equal to INTEGER2INTEGER1 -lt INTEGER2INTEGER1 is less than INTEGER2INTEGER1 -ne INTEGER2INTEGER1 is not equal to INTEGER2FILE1 -ef FILE2FILE1 and FILE2 have the same device and inode numbersFILE1 -nt FILE2FILE1 is newer (modification date) than FILE2FILE1 -ot FILE2FILE1 is older than FILE2-b FILEFILE exists and is block special-c FILEFILE exists and is character special-d FILEFILE exists and is a directory-e FILEFILE exists-f FILEFILE exists and is a regular file-g FILEFILE exists and is set-group-ID-G FILEFILE exists and is owned by the effective group ID-h FILEFILE exists and is a symbolic link (same as -L)-k FILEFILE exists and has its sticky bit set-L FILEFILE exists and is a symbolic link (same as -h)-N FILEFILE exists and has been modified since it was last read-O FILEFILE exists and is owned by the effective user ID-p FILEFILE exists and is a named pipe-r FILEFILE exists and the user has read access-s FILEFILE exists and has a size greater than zero-S FILEFILE exists and is a socket-t FDfile descriptor FD is opened on a terminal-u FILEFILE exists and its set-user-ID bit is set-w FILEFILE exists and the user has write access-x FILEFILE exists and the user has execute (or search) accessExcept for -h and -L, all FILE-related tests dereference symboliclinks.Beware that parentheses need to be escaped (e.g., bybackslashes) for shells.INTEGER may also be -l STRING, whichevaluates to the length of STRING.NOTE: Binary -a and -o are inherently ambiguous.Use 'test EXPR1&& test EXPR2' or 'test EXPR1 || test EXPR2' instead.NOTE: [ honors the --help and --version options, but test doesnot.test treats each of those as it treats any other nonemptySTRING.NOTE: your shell may have its own version of test and/or [, whichusually supersedes the version described here.Please refer toyour shell's documentation for details about the options itsupports.",
        "name": "test - check file types and compare values",
        "section": 1
    },
    {
        "command": "tfmtodit",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "time",
        "description": "The time command runs the specified program command with thegiven arguments.When command finishes, time writes a message tostandard error giving timing statistics about this program run.These statistics consist of (i) the elapsed real time betweeninvocation and termination, (ii) the user CPU time (the sum ofthe tms_utime and tms_cutime values in a struct tms as returnedby times(2)), and (iii) the system CPU time (the sum of thetms_stime and tms_cstime values in a struct tms as returned bytimes(2)).Note: some shells (e.g., bash(1)) have a built-in time commandthat provides similar information on the usage of time andpossibly other resources.To access the real command, you mayneed to specify its pathname (something like /usr/bin/time).",
        "name": "time - time a simple command or give resource usage",
        "section": 1
    },
    {
        "command": "timedatectl",
        "description": "timedatectl may be used to query and change the system clock andits settings, and enable or disable time synchronizationservices.Use systemd-firstboot(1) to initialize the system time zone formounted (but not booted) system images.timedatectl may be used to show the current status of timesynchronization services, for examplesystemd-timesyncd.service(8).",
        "name": "timedatectl - Control the system time and date",
        "section": 1
    },
    {
        "command": "timeout",
        "description": "Start COMMAND, and kill it if still running after DURATION.Mandatory arguments to long options are mandatory for shortoptions too.--preserve-statusexit with the same status as COMMAND, even when thecommand times out--foregroundwhen not running timeout directly from a shell prompt,allow COMMAND to read from the TTY and get TTY signals; inthis mode, children of COMMAND will not be timed out-k, --kill-after=DURATIONalso send a KILL signal if COMMAND is still runningthis long after the initial signal was sent-s, --signal=SIGNALspecify the signal to be sent on timeout;SIGNAL may be a name like 'HUP' or a number; see 'kill -l'for a list of signals-v, --verbosediagnose to stderr any signal sent upon timeout--help display this help and exit--versionoutput version information and exitDURATION is a floating point number with an optional suffix: 's'for seconds (the default), 'm' for minutes, 'h' for hours or 'd'for days.A duration of 0 disables the associated timeout.Upon timeout, send the TERM signal to COMMAND, if no other SIGNALspecified.The TERM signal kills any process that does not blockor catch that signal.It may be necessary to use the KILLsignal, since this signal can't be caught.Exit status:124if COMMAND times out, and --preserve-status is notspecified125if the timeout command itself fails126if COMMAND is found but cannot be invoked127if COMMAND cannot be found137if COMMAND (or timeout itself) is sent the KILL (9) signal(128+9)-the exit status of COMMAND otherwise",
        "name": "timeout - run a command with a time limit",
        "section": 1
    },
    {
        "command": "tload",
        "description": "tload prints a graph of the current system load average to thespecified tty (or the tty of the tload process if none isspecified).",
        "name": "tload - graphic representation of system load average",
        "section": 1
    },
    {
        "command": "tmux",
        "description": "tmux is a terminal multiplexer: it enables a number of terminals tobe created, accessed, and controlled from a single screen.tmuxmay be detached from a screen and continue running in thebackground, then later reattached.When tmux is started, it creates a new session with a single windowand displays it on screen.A status line at the bottom of thescreen shows information on the current session and is used toenter interactive commands.A session is a single collection of pseudo terminals under themanagement of tmux.Each session has one or more windows linked toit.A window occupies the entire screen and may be split intorectangular panes, each of which is a separate pseudo terminal (thepty(4) manual page documents the technical details of pseudoterminals).Any number of tmux instances may connect to the samesession, and any number of windows may be present in the samesession.Once all sessions are killed, tmux exits.Each session is persistent and will survive accidentaldisconnection (such as ssh(1) connection timeout) or intentionaldetaching (with the \u2018C-b d\u2019 key strokes).tmux may be reattachedusing:$ tmux attachIn tmux, a session is displayed on screen by a client and allsessions are managed by a single server.The server and eachclient are separate processes which communicate through a socket in/tmp.The options are as follows:-2Force tmux to assume the terminal supports 256colours.This is equivalent to -T 256.-CStart in control mode (see the CONTROL MODE section).Given twice (-CC) disables echo.-c shell-commandExecute shell-command using the default shell.Ifnecessary, the tmux server will be started toretrieve the default-shell option.This option isfor compatibility with sh(1) when tmux is used as alogin shell.-DDo not start the tmux server as a daemon.This alsoturns the exit-empty option off.With -D, commandmay not be specified.-f fileSpecify an alternative configuration file.Bydefault, tmux loads the system configuration filefrom @SYSCONFDIR@/tmux.conf, if present, then looksfor a user configuration file at ~/.tmux.conf,$XDG_CONFIG_HOME/tmux/tmux.conf or~/.config/tmux/tmux.conf.The configuration file is a set of tmux commandswhich are executed in sequence when the server isfirst started.tmux loads configuration files oncewhen the server process has started.The source-filecommand may be used to load a file later.tmux shows any error messages from commands inconfiguration files in the first session created, andcontinues to process the rest of the configurationfile.-L socket-nametmux stores the server socket in a directory underTMUX_TMPDIR or /tmp if it is unset.The defaultsocket is named default.This option allows adifferent socket name to be specified, allowingseveral independent tmux servers to be run.Unlike-S a full path is not necessary: the sockets are allcreated in a directory tmux-UID under the directorygiven by TMUX_TMPDIR or in /tmp.The tmux-UIDdirectory is created by tmux and must not be worldreadable, writable or executable.If the socket is accidentally removed, the SIGUSR1signal may be sent to the tmux server process torecreate it (note that this will fail if any parentdirectories are missing).-lBehave as a login shell.This flag currently has noeffect and is for compatibility with other shellswhen using tmux as a login shell.-NDo not start the server even if the command wouldnormally do so (for example new-session orstart-server).-S socket-pathSpecify a full alternative path to the server socket.If -S is specified, the default socket directory isnot used and any -L flag is ignored.-T featuresSet terminal features for the client.This is acomma-separated list of features.See theterminal-features option.-uWrite UTF-8 output to the terminal even if the firstenvironment variable of LC_ALL, LC_CTYPE, or LANGthat is set does not contain \"UTF-8\" or \"UTF8\".-VReport the tmux version.-vRequest verbose logging.Log messages will be savedinto tmux-client-PID.log and tmux-server-PID.logfiles in the current directory, where PID is the PIDof the server or client process.If -v is specifiedtwice, an additional tmux-out-PID.log file isgenerated with a copy of everything tmux writes tothe terminal.The SIGUSR2 signal may be sent to the tmux serverprocess to toggle logging between on (as if -v wasgiven) and off.command [flags]This specifies one of a set of commands used tocontrol tmux, as described in the following sections.If no commands are specified, the new-session commandis assumed.",
        "name": "tmux \u2014 terminal multiplexer",
        "section": 1
    },
    {
        "command": "top",
        "description": "The top program provides a dynamic real-time view of a runningsystem.It can display system summary information as well as alist of processes or threads currently being managed by the Linuxkernel.The types of system summary information shown and thetypes, order and size of information displayed for processes areall user configurable and that configuration can be madepersistent across restarts.The program provides a limited interactive interface for processmanipulation as well as a much more extensive interface forpersonal configuration--encompassing every aspect of itsoperation.And while top is referred to throughout thisdocument, you are free to name the program anything you wish.That new name, possibly an alias, will then be reflected on top'sdisplay and used when reading and writing a configuration file.",
        "name": "top - display Linux processes",
        "section": 1
    },
    {
        "command": "touch",
        "description": "Update the access and modification times of each FILE to thecurrent time.A FILE argument that does not exist is created empty, unless -cor -h is supplied.A FILE argument string of - is handled specially and causes touchto change the times of the file associated with standard output.Mandatory arguments to long options are mandatory for shortoptions too.-achange only the access time-c, --no-createdo not create any files-d, --date=STRINGparse STRING and use it instead of current time-f(ignored)-h, --no-dereferenceaffect each symbolic link instead of any referenced file(useful only on systems that can change the timestamps ofa symlink)-mchange only the modification time-r, --reference=FILEuse this file's times instead of current time-t STAMPuse [[CC]YY]MMDDhhmm[.ss] instead of current time--time=WORDchange the specified time: WORD is access, atime, or use:equivalent to -a WORD is modify or mtime: equivalent to -m--help display this help and exit--versionoutput version information and exitNote that the -d and -t options accept different time-dateformats.",
        "name": "touch - change file timestamps",
        "section": 1
    },
    {
        "command": "tpmtool",
        "description": "Program that allows handling cryptographic data from the TPMchip.",
        "name": "tpmtool - GnuTLS TPM tool",
        "section": 1
    },
    {
        "command": "tput",
        "description": "The @TPUT@ utility uses the terminfo database to make the valuesof terminal-dependent capabilities and information available tothe shell (see sh(1)), to initialize or reset the terminal, orreturn the long name of the requested terminal type.The resultdepends upon the capability's type:string@TPUT@ writes the string to the standard output.Notrailing newline is supplied.integer@TPUT@ writes the decimal value to the standard output,with a trailing newline.boolean@TPUT@ simply sets the exit code (0 for TRUE if theterminal has the capability, 1 for FALSE if it does not),and writes nothing to the standard output.Before using a value returned on the standard output, theapplication should test the exit code (e.g., $?, see sh(1)) to besure it is 0.(See the EXIT CODES and DIAGNOSTICS sections.)For a complete list of capabilities and the capname associatedwith each, see terminfo(5).Options-Sallows more than one capability per invocation of @TPUT@.The capabilities must be passed to @TPUT@ from thestandard input instead of from the command line (seeexample).Only one capname is allowed per line.The -Soption changes the meaning of the 0 and 1 boolean andstring exit codes (see the EXIT CODES section).Because some capabilities may use string parameters ratherthan numbers, @TPUT@ uses a table and the presence ofparameters in its input to decide whether to usetparm(3X), and how to interpret the parameters.-Ttype indicates the type of terminal.Normally this option isunnecessary, because the default is taken from theenvironment variable TERM.If -T is specified, then theshell variables LINES and COLUMNS will also be ignored.-Vreports the version of ncurses which was used in thisprogram, and exits.-xdo not attempt to clear the terminal's scrollback bufferusing the extended \u201cE3\u201d capability.CommandsA few commands (init, reset and longname) are special; they aredefined by the @TPUT@ program.The others are the names ofcapabilities from the terminal database (see terminfo(5) for alist).Although init and reset resemble capability names, @TPUT@uses several capabilities to perform these special functions.capnameindicates the capability from the terminal database.If the capability is a string that takes parameters, thearguments following the capability will be used asparameters for the string.Most parameters are numbers.Only a few terminalcapabilities require string parameters; @TPUT@ uses atable to decide which to pass as strings.Normally @TPUT@uses tparm(3X) to perform the substitution.If noparameters are given for the capability, @TPUT@ writes thestring without performing the substitution.initIf the terminal database is present and an entry for theuser's terminal exists (see -Ttype, above), the followingwill occur:(1)first, @TPUT@ retrieves the current terminal modesettings for your terminal.It does this bysuccessively testing\u2022the standard error,\u2022standard output,\u2022standard input and\u2022ultimately \u201c/dev/tty\u201dto obtain terminal settings.Having retrieved thesesettings, @TPUT@ remembers which file descriptor touse when updating settings.(2)if the window size cannot be obtained from theoperating system, but the terminal description (orenvironment, e.g., LINES and COLUMNS variablesspecify this), update the operating system's notionof the window size.(3)the terminal modes will be updated:\u2022any delays (e.g., newline) specified in the entrywill be set in the tty driver,\u2022tabs expansion will be turned on or off accordingto the specification in the entry, and\u2022if tabs are not expanded, standard tabs will beset (every 8 spaces).(4)if present, the terminal's initialization stringswill be output as detailed in the terminfo(5) sectionon Tabs and Initialization,(5)output is flushed.If an entry does not contain the information needed forany of these activities, that activity will silently beskipped.resetThis is similar to init, with two differences:(1)before any other initialization, the terminal modeswill be reset to a \u201csane\u201d state:\u2022set cooked and echo modes,\u2022turn off cbreak and raw modes,\u2022turn on newline translation and\u2022reset any unset special characters to theirdefault values(2)Instead of putting out initialization strings, theterminal's reset strings will be output if present(rs1, rs2, rs3, rf).If the reset strings are notpresent, but initialization strings are, theinitialization strings will be output.Otherwise, reset acts identically to init.longnameIf the terminal database is present and an entry for theuser's terminal exists (see -Ttype above), then the longname of the terminal will be put out.The long name isthe last name in the first line of the terminal'sdescription in the terminfo database [see term(5)].Aliases@TPUT@ handles the clear, init and reset commands specially: itallows for the possibility that it is invoked by a link withthose names.If @TPUT@ is invoked by a link named reset, this has the sameeffect as @TPUT@ reset.The @TSET@(1) utility also treats a linknamed reset specially.Before ncurses 6.1, the two utilities were different from eachother:\u2022@TSET@ utility reset the terminal modes and specialcharacters (not done with @TPUT@).\u2022On the other hand, @TSET@'s repertoire of terminalcapabilities for resetting the terminal was more limited,i.e., only reset_1string, reset_2string and reset_file incontrast to the tab-stops and margins which are set by thisutility.\u2022The reset program is usually an alias for @TSET@, because ofthis difference with resetting terminal modes and specialcharacters.With the changes made for ncurses 6.1, the reset feature of thetwo programs is (mostly) the same.A few differences remain:\u2022The @TSET@ program waits one second when resetting, in caseit happens to be a hardware terminal.\u2022The two programs write the terminal initialization strings todifferent streams (i.e., the standard error for @TSET@ andthe standard output for @TPUT@).Note: although these programs write to different streams,redirecting their output to a file will capture only part oftheir actions.The changes to the terminal modes are notaffected by redirecting the output.If @TPUT@ is invoked by a link named init, this has the sameeffect as @TPUT@ init.Again, you are less likely to use thatlink because another program named init has a more well-established use.Terminal SizeBesides the special commands (e.g., clear), @TPUT@ treats certainterminfo capabilities specially: lines and cols.@TPUT@ callssetupterm(3X) to obtain the terminal size:\u2022first, it gets the size from the terminal database (whichgenerally is not provided for terminal emulators which do nothave a fixed window size)\u2022then it asks the operating system for the terminal's size(which generally works, unless connecting via a serial linewhich does not support NAWS: negotiations about window size).\u2022finally, it inspects the environment variables LINES andCOLUMNS which may override the terminal size.If the -T option is given @TPUT@ ignores the environmentvariables by calling use_tioctl(TRUE), relying upon the operatingsystem (or finally, the terminal database).",
        "name": "@TPUT@, reset - initialize a terminal or query terminfo database",
        "section": 1
    },
    {
        "command": "tr",
        "description": "Translate, squeeze, and/or delete characters from standard input,writing to standard output.STRING1 and STRING2 specify arraysof characters ARRAY1 and ARRAY2 that control the action.-c, -C, --complementuse the complement of ARRAY1-d, --deletedelete characters in ARRAY1, do not translate-s, --squeeze-repeatsreplace each sequence of a repeated character that islisted in the last specified ARRAY, with a singleoccurrence of that character-t, --truncate-set1first truncate ARRAY1 to length of ARRAY2--help display this help and exit--versionoutput version information and exitARRAYs are specified as strings of characters.Most representthemselves.Interpreted sequences are:\\NNNcharacter with octal value NNN (1 to 3 octal digits)\\\\backslash\\aaudible BEL\\bbackspace\\fform feed\\nnew line\\rreturn\\thorizontal tab\\vvertical tabCHAR1-CHAR2all characters from CHAR1 to CHAR2 in ascending order[CHAR*]in ARRAY2, copies of CHAR until length of ARRAY1[CHAR*REPEAT]REPEAT copies of CHAR, REPEAT octal if starting with 0[:alnum:]all letters and digits[:alpha:]all letters[:blank:]all horizontal whitespace[:cntrl:]all control characters[:digit:]all digits[:graph:]all printable characters, not including space[:lower:]all lower case letters[:print:]all printable characters, including space[:punct:]all punctuation characters[:space:]all horizontal or vertical whitespace[:upper:]all upper case letters[:xdigit:]all hexadecimal digits[=CHAR=]all characters which are equivalent to CHARTranslation occurs if -d is not given and both STRING1 andSTRING2 appear.-t is only significant when translating.ARRAY2is extended to length of ARRAY1 by repeating its last characteras necessary.Excess characters of ARRAY2 are ignored.Character classes expand in unspecified order; while translating,[:lower:] and [:upper:] may be used in pairs to specify caseconversion.Squeezing occurs after translation or deletion.",
        "name": "tr - translate or delete characters",
        "section": 1
    },
    {
        "command": "trace-cmd",
        "description": "The trace-cmd(1) command interacts with the Ftrace tracer that isbuilt inside the Linux kernel. It interfaces with the Ftracespecific files found in the debugfs file system under the tracingdirectory. A COMMAND must be specified to tell trace-cmd what todo.",
        "name": "trace-cmd - interacts with Ftrace Linux kernel internal tracer",
        "section": 1
    },
    {
        "command": "trace-cmd-agent",
        "description": "The trace-cmd(1) agent listens over a vsocket (for virtualmachines) or a TCP port for connections to control the tracing ofthe machine. The agent will then start tracing on the localmachine and pass the data to the controlling connection.",
        "name": "trace-cmd-agent - Run as an agent on a machine (to be controlledby another machine)",
        "section": 1
    },
    {
        "command": "trace-cmd-attach",
        "description": "The trace-cmd(1) attach is used to take a trace.dat file createdon a guest and attach it to a trace.dat file that was created onthe host. In most cases, trace-cmd-agent(1) can be used toautomate this, but if for some reason, the agent isn\u2019tappropriate, it may be required to start trace-cmd recording onthe guest with trace-cmd-record(1). If the host recording isactivated at the same time, one can use trace-cmd attach(1) toconnect the guest and host files as if they were created by thetrace-cmd agent.host-trace-fileThe trace.dat file created by the host. Must have kvm_exitand kvm_entry events, and use the \"tsc2nsec\" clock.guest-trace-fileThe trace.dat file created by the guest. Must use the\"x86-tsc\" clock. For now, this is only supported on x86, itmay support other achitectures later.guest-pidThe process ID of the host thread that represents the gueststhreads. Each process ID that represents all of the guestvCPUs should be listed. Note, you can add more than just thethreads that represent the guest vCPUs, as the tool willsearch the host-trace-file for kvm_exit and kvm_entry eventsto match these PIDs with the vCPUs that they represent.",
        "name": "trace-cmd-attach - attach a guest trace.dat file to a hosttrace.dat file",
        "section": 1
    },
    {
        "command": "trace-cmd-check-events",
        "description": "The trace-cmd(1) check-events parses format strings for all theevents on the local system. It returns whether all the formatstrings can be parsed correctly. It will load plugins unlessspecified otherwise.This is useful to check for any trace event format strings whichmay contain some internal kernel function references which cannotbe decoded outside of the kernel. This may mean that either theunparsed format strings of the trace events need to be changed orthat a plugin needs to be created to parse them.",
        "name": "trace-cmd-check-events - parse the event formats on local system",
        "section": 1
    },
    {
        "command": "trace-cmd-clear",
        "description": "The trace-cmd(1) clear clears the content of the Ftrace ringbuffer.",
        "name": "trace-cmd-clear - clear the Ftrace buffer.",
        "section": 1
    },
    {
        "command": "trace-cmd-convert",
        "description": "The trace-cmd(1) convert command converts trace file. It readsthe input file and copies the data into an output file. Theoutput file may be in different format, depending on the commandline arguments. The default output is in version 7 and compressed(if compiled with compression support).",
        "name": "trace-cmd-convert - convert trace files",
        "section": 1
    },
    {
        "command": "trace-cmd-dump",
        "description": "The trace-cmd(1) dump command will display the meta data from atrace file created by trace-cmd record.",
        "name": "trace-cmd-dump - show a meta data from a trace file, created bytrace-cmd record",
        "section": 1
    },
    {
        "command": "trace-cmd-extract",
        "description": "The trace-cmd(1) extract is usually used after trace-cmd-start(1)and trace-cmd-stop(1). It can be used after the Ftrace tracer hasbeen started manually through the Ftrace pseudo file system.The extract command creates a trace.dat file that can be used bytrace-cmd-report(1) to read from. It reads the kernel internalring buffer to produce the trace.dat file.",
        "name": "trace-cmd-extract - extract out the data from the Ftrace Linuxtracer.",
        "section": 1
    },
    {
        "command": "trace-cmd-hist",
        "description": "The trace-cmd(1) hist displays a histogram form from thetrace.dat file. Instead of showing the events as they wereordered, it creates a histogram that can be displayed per task orfor all tasks where the most common events appear first. It usesthe function tracer and call stacks that it finds to try to puttogether a call graph of the events.",
        "name": "trace-cmd-hist - show histogram of events in trace.dat file",
        "section": 1
    },
    {
        "command": "trace-cmd-list",
        "description": "The trace-cmd(1) list displays the available plugins, events orFtrace options that are configured on the current machine. If nooption is given, then it lists all plugins, event systems, eventsand Ftrace options to standard output.",
        "name": "trace-cmd-list - list available plugins, events or options forFtrace.",
        "section": 1
    },
    {
        "command": "trace-cmd-listen",
        "description": "The trace-cmd(1) listen sets up a port to listen to waiting forconnections from other hosts that run trace-cmd-record(1) withthe -N option. When a connection is made, and the remote hostsends data, it will create a file called trace.HOST:PORT.dat.Where HOST is the name of the remote host, and PORT is the portthat the remote host used to connect with.",
        "name": "trace-cmd-listen - listen for incoming connection to recordtracing.",
        "section": 1
    },
    {
        "command": "trace-cmd-mem",
        "description": "The trace-cmd(1) mem requires a trace-cmd record that enabled thefollowing events:kmallockmalloc_nodekfreekmem_cache_allockmem_cache_alloc_nodekmem_cache_alloc_freeIt then reads the amount requested and the ammount freed as wellas the functions that called the allocation. It then reports thefinal amount of bytes requested and allocated, along with thetotal amount allocated and requested, as well as the maxallocation and requested during the run. It reports the amount ofwasted bytes (allocated - requested) that was not freed, as wellas the max wasted amount during the run. The list is sorted bydescending order of wasted bytes after the run.FunctionWasteAllocreqTotAllocTotReqMaxAllocMaxReqMaxWaste---------------------------------------------------------rb_allocate_cpu_buffer768230415362304153623041536768alloc_pipe_info400115275211527521152752400instance_mkdir252544292544292544292252__d_alloc2151086560 10863451087208108699310865601086345215get_empty_filp72230422324864471248644712152mm_alloc4096092096092096092040prepare_creds321921601728144017281440288tracing_buffers_open83224322432248do_brk0003683683683680journal_add_journal_head06048604860486048604860480journal_start0001224122448480__rb_allocate_pages03289856 328985632898563289856328985632898560anon_vma_alloc0009369368648640[...]",
        "name": "trace-cmd-mem - show memory usage of certain kmem events",
        "section": 1
    },
    {
        "command": "trace-cmd-options",
        "description": "The trace-cmd(1) options command will examine all the trace-cmdplugins that are used by trace-cmd report(1) and list them.",
        "name": "trace-cmd-options - list available options from trace-cmd plugins",
        "section": 1
    },
    {
        "command": "trace-cmd-profile",
        "description": "The trace-cmd(1) profile will start tracing just liketrace-cmd-record(1), with the --profile option, except that itdoes not write to a file, but instead, it will read the events asthey happen and will update the accounting of the events. Whenthe trace is finished, it will report the results just liketrace-cmd-report(1) would do with its --profile option. In otherwords, the profile command does the work of trace-cmd record--profile, and trace-cmd report --profile without having torecord the data to disk, in between.The advantage of using the profile command is that the profilingcan be done over a long period of time where recording all eventswould take up too much disk space.This will enable several events as well as the function graphtracer with a depth of one (if the kernel supports it). This isto show where tasks enter and exit the kernel and how long theywere in the kernel.To disable calling function graph, use the -p option to enableanother tracer. To not enable any tracer, use -p nop.All timings are currently in nanoseconds.",
        "name": "trace-cmd-profile - profile tasks running live",
        "section": 1
    },
    {
        "command": "trace-cmd-record",
        "description": "The trace-cmd(1) record command will set up the Ftrace Linuxkernel tracer to record the specified plugins or events thathappen while the command executes. If no command is given, thenit will record until the user hits Ctrl-C.The record command of trace-cmd will set up the Ftrace tracer tostart tracing the various events or plugins that are given on thecommand line. It will then create a number of tracing processes(one per CPU) that will start recording from the kernel ringbuffer straight into temporary files. When the command iscomplete (or Ctrl-C is hit) all the files will be combined into atrace.dat file that can later be read (see trace-cmd-report(1)).",
        "name": "trace-cmd-record - record a trace from the Ftrace Linux internaltracer",
        "section": 1
    },
    {
        "command": "trace-cmd-report",
        "description": "The trace-cmd(1) report command will output a human readablereport of a trace created by trace-cmd record.",
        "name": "trace-cmd-report - show in ASCII a trace created by trace-cmdrecord",
        "section": 1
    },
    {
        "command": "trace-cmd-reset",
        "description": "The trace-cmd(1) reset command turns off all tracing of Ftrace.This will bring back the performance of the system before tracingwas enabled. This is necessary since trace-cmd-record(1),trace-cmd-stop(1) and trace-cmd-extract(1) do not disable thetracer, event after the data has been pulled from the buffers.The rational is that the user may want to manually enable thetracer with the Ftrace pseudo file system, or examine other partsof Ftrace to see what trace-cmd did. After the reset commandhappens, the data in the ring buffer, and the options that wereused are all lost.",
        "name": "trace-cmd-reset - turn off all Ftrace tracing to bring back fullperformance",
        "section": 1
    },
    {
        "command": "trace-cmd-restore",
        "description": "The trace-cmd(1) restore command will restore a crashedtrace-cmd-record(1) file. If for some reason a trace-cmd recordfails, it will leave a the per-cpu data files and not create thefinal trace.dat file. The trace-cmd restore will append the filesto create a working trace.dat file that can be read withtrace-cmd-report(1).When trace-cmd record runs, it spawns off a process per CPU andwrites to a per cpu file usually called trace.dat.cpuX, where Xrepresents the CPU number that it is tracing. If the -o optionwas used in the trace-cmd record, then the CPU data files willhave that name instead of the trace.dat name. If a unexpectedcrash occurs before the tracing is finished, then the per CPUfiles will still exist but there will not be any trace.dat fileto read from. trace-cmd restore will allow you to create atrace.dat file with the existing data files.",
        "name": "trace-cmd-restore - restore a failed trace record",
        "section": 1
    },
    {
        "command": "trace-cmd-set",
        "description": "The trace-cmd(1) set command will set a configuration parameterof the Ftrace Linux kernel tracer. The specified command will berun after the ftrace state is set. The configured ftrace statecan be restored to default using the trace-cmd-reset(1) command.",
        "name": "trace-cmd-set - set a configuration parameter of the Ftrace Linuxinternal tracer",
        "section": 1
    },
    {
        "command": "trace-cmd-show",
        "description": "The trace-cmd(1) show displays the contents of one of the FtraceLinux kernel tracing files: trace, snapshot, or trace_pipe. It isbasically the equivalent of doing:cat /sys/kernel/debug/tracing/trace",
        "name": "trace-cmd-show - show the contents of the Ftrace Linux kerneltracing buffer.",
        "section": 1
    },
    {
        "command": "trace-cmd-snapshot",
        "description": "The trace-cmd(1) snapshot controls or displays the Ftrace Linuxkernel snapshot feature (if the kernel supports it). This isuseful to \"freeze\" an instance of a live trace but withoutstopping the trace.trace-cmd start -p functiontrace-cmd snapshot -strace-cmd snapshot[ dumps the content of buffer at 'trace-cmd snapshot -s' ]trace-cmd snapshot -strace-cmd snapshot[ dumps the new content of the buffer at the last -s operation ]",
        "name": "trace-cmd-snapshot - take, reset, free, or show a Ftrace kernelsnapshot",
        "section": 1
    },
    {
        "command": "trace-cmd-split",
        "description": "The trace-cmd(1) split is used to break up a trace.dat into smallfiles. The start-time specifies where the new file will start at.Using trace-cmd-report(1) and copying the time stamp given at aparticular event, can be used as input for either start-time orend-time. The split will stop creating files when it reaches anevent after end-time. If only the end-time is needed, use 0.0 asthe start-time.If start-time is left out, then the split will start at thebeginning of the file. If end-time is left out, then split willcontinue to the end unless it meets one of the requirementsspecified by the options.",
        "name": "trace-cmd-split - split a trace.dat file into smaller files",
        "section": 1
    },
    {
        "command": "trace-cmd-stack",
        "description": "The trace-cmd(1) stack enables the Ftrace stack tracer within thekernel. The stack tracer enables the function tracer and at eachfunction call within the kernel, the stack is checked. When a newmaximum usage stack is discovered, it is recorded.When no option is used, the current stack is displayed.To enable the stack tracer, use the option --start, and todisable the stack tracer, use the option --stop. The output willbe the maximum stack found since the start was enabled.Use --reset to reset the stack counter to zero.User --verbose[=level] to set the log level. Supported log levelsare \"none\", \"critical\", \"error\", \"warning\", \"info\", \"debug\",\"all\" or their identifiers \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\".Setting the log level to specific value enables all logs fromthat and all previous levels. The level will default to \"info\" ifone is not specified.",
        "name": "trace-cmd-stack - read, enable or disable Ftrace Linux kernelstack tracing.",
        "section": 1
    },
    {
        "command": "trace-cmd-start",
        "description": "The trace-cmd(1) start enables all the Ftrace tracing the sameway trace-cmd-record(1) does. The difference is that it does notrun threads to create a trace.dat file. This is useful just toenable Ftrace and you are only interested in the trace after someevent has occurred and the trace is stopped. Then the trace canbe read straight from the Ftrace pseudo file system or can beextracted with trace-cmd-extract(1).",
        "name": "trace-cmd-start - start the Ftrace Linux kernel tracer withoutrecording",
        "section": 1
    },
    {
        "command": "trace-cmd-stat",
        "description": "The trace-cmd(1) stat displays the various status of the tracing(ftrace) system. The status that it shows is:Instances: List all configured ftrace instances.Tracer: if one of the tracers (like function_graph) is active.Otherwise nothing is displayed.Events: Lists the events that are enable.Event filters: Shows any filters that are set for any eventsFunction filters: Shows any filters for the function tracersGraph functions: Shows any functions that the function graphtracer should graphBuffers: Shows the trace buffer size if they have been expanded.By default, tracing buffers are in a compressed format until theyare used. If they are compressed, the buffer display will not beshown.Trace clock: If the tracing clock is anything other than thedefault \"local\" it will be displayed.Trace CPU mask: If not all available CPUs are in the tracing CPUmask, then the tracing CPU mask will be displayed.Trace max latency: Shows the value of the trace max latency if itis other than zero.Kprobes: Shows any kprobes that are defined for tracing.Uprobes: Shows any uprobes that are defined for tracing.Error log: Dump the content of ftrace error_log file.",
        "name": "trace-cmd-stat - show the status of the tracing (ftrace) system",
        "section": 1
    },
    {
        "command": "trace-cmd-stop",
        "description": "The trace-cmd(1) stop is a complement to trace-cmd-start(1). Thiswill disable Ftrace from writing to the ring buffer. This doesnot stop the overhead that the tracing may incur. Only theupdating of the ring buffer is disabled, the Ftrace tracing maystill be inducing overhead.After stopping the trace, the trace-cmd-extract(1) may strip outthe data from the ring buffer and create a trace.dat file. TheFtrace pseudo file system may also be examined.To disable the tracing completely to remove the overhead itcauses, use trace-cmd-reset(1). But after a reset is performed,the data that has been recorded is lost.",
        "name": "trace-cmd-stop - stop the Ftrace Linux kernel tracer from writingto the ring buffer.",
        "section": 1
    },
    {
        "command": "trace-cmd-stream",
        "description": "The trace-cmd(1) stream will start tracing just liketrace-cmd-record(1), except it will not record to a file andinstead it will read the binary buffer as it is happening,convert it to a human readable format and write it to stdout.This is basically the same as trace-cmd-start(1) and then doing atrace-cmd-show(1) with the -p option. trace-cmd-stream is not asefficient as reading from the pipe file as most of the streamwork is done in userspace. This is useful if it is needed to dothe work mostly in userspace instead of the kernel, and streamalso helps to debug trace-cmd-profile(1) which uses the streamcode to perform the live data analysis for the profile.",
        "name": "trace-cmd-stream - stream a trace to stdout as it is happening",
        "section": 1
    },
    {
        "command": "troff",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "true",
        "description": "Exit with a status code indicating success.--help display this help and exit--versionoutput version information and exitNOTE: your shell may have its own version of true, which usuallysupersedes the version described here.Please refer to yourshell's documentation for details about the options it supports.",
        "name": "true - do nothing, successfully",
        "section": 1
    },
    {
        "command": "truncate",
        "description": "Shrink or extend the size of each FILE to the specified sizeA FILE argument that does not exist is created.If a FILE is larger than the specified size, the extra data islost.If a FILE is shorter, it is extended and the sparseextended part (hole) reads as zero bytes.Mandatory arguments to long options are mandatory for shortoptions too.-c, --no-createdo not create any files-o, --io-blockstreat SIZE as number of IO blocks instead of bytes-r, --reference=RFILEbase size on RFILE-s, --size=SIZEset or adjust the file size by SIZE bytes--help display this help and exit--versionoutput version information and exitThe SIZE argument is an integer and optional unit (example: 10Kis 10*1024).Units are K,M,G,T,P,E,Z,Y,R,Q (powers of 1024) orKB,MB,... (powers of 1000).Binary prefixes can be used, too:KiB=K, MiB=M, and so on.SIZE may also be prefixed by one of the following modifyingcharacters: '+' extend by, '-' reduce by, '<' at most, '>' atleast, '/' round down to multiple of, '%' round up to multipleof.",
        "name": "truncate - shrink or extend the size of a file to the specifiedsize",
        "section": 1
    },
    {
        "command": "tset",
        "description": "tset - initializationThis program initializes terminals.First, @TSET@ retrieves the current terminal mode settings foryour terminal.It does this by successively testing\u2022the standard error,\u2022standard output,\u2022standard input and\u2022ultimately \u201c/dev/tty\u201dto obtain terminal settings.Having retrieved these settings,@TSET@ remembers which file descriptor to use when updatingsettings.Next, @TSET@ determines the type of terminal that you are using.This determination is done as follows, using the first terminaltype found.1. The terminal argument specified on the command line.2. The value of the TERM environmental variable.3. (BSD systems only.) The terminal type associated with thestandard error output device in the /etc/ttys file.(OnSystem-V-like UNIXes and systems using that convention, getty(1)does this job by setting TERM according to the type passed to itby /etc/inittab.)4. The default terminal type, \u201cunknown\u201d.If the terminal type was not specified on the command-line, the-m option mappings are then applied (see the section TERMINALTYPE MAPPING for more information).Then, if the terminal typebegins with a question mark (\u201c?\u201d), the user is prompted forconfirmation of the terminal type.An empty response confirmsthe type, or, another type can be entered to specify a new type.Once the terminal type has been determined, the terminaldescription for the terminal is retrieved.If no terminaldescription is found for the type, the user is prompted foranother terminal type.Once the terminal description is retrieved,\u2022if the \u201c-w\u201d option is enabled, @TSET@ may update theterminal's window size.If the window size cannot be obtained from the operatingsystem, but the terminal description (or environment, e.g.,LINES and COLUMNS variables specify this), use this to setthe operating system's notion of the window size.\u2022if the \u201c-c\u201d option is enabled, the backspace, interrupt andline kill characters (among many other things) are set\u2022unless the \u201c-I\u201d option is enabled, the terminal and tabinitialization strings are sent to the standard error output,and @TSET@ waits one second (in case a hardware reset wasissued).\u2022Finally, if the erase, interrupt and line kill charactershave changed, or are not set to their default values, theirvalues are displayed to the standard error output.reset - reinitializationWhen invoked as @RESET@, @TSET@ sets the terminal modes to \u201csane\u201dvalues:\u2022sets cooked and echo modes,\u2022turns off cbreak and raw modes,\u2022turns on newline translation and\u2022resets any unset special characters to their default valuesbefore doing the terminal initialization described above.Also,rather than using the terminal initialization strings, it usesthe terminal reset strings.The @RESET@ command is useful after a program dies leaving aterminal in an abnormal state:\u2022you may have to type<LF>@RESET@<LF>(the line-feed character is normally control-J) to get theterminal to work, as carriage-return may no longer work inthe abnormal state.\u2022Also, the terminal will often not echo the command.",
        "name": "@TSET@, @RESET@ - terminal initialization",
        "section": 1
    },
    {
        "command": "tsort",
        "description": "Write totally ordered list consistent with the partial orderingin FILE.With no FILE, or when FILE is -, read standard input.--help display this help and exit--versionoutput version information and exit",
        "name": "tsort - perform topological sort",
        "section": 1
    },
    {
        "command": "tty",
        "description": "Print the file name of the terminal connected to standard input.-s, --silent, --quietprint nothing, only return an exit status--help display this help and exit--versionoutput version information and exit",
        "name": "tty - print the file name of the terminal connected to standardinput",
        "section": 1
    },
    {
        "command": "txrecord",
        "description": "pmdatxmon is an example Performance Metrics Domain Agent (PMDA)which exports a small number of performance metrics from asimulated transaction monitor.The txmon PMDA is shipped as both binary and source code and isdesigned to be an aid for PMDA developers; the txmon PMDAdemonstrates how performance data can be exported from anapplication (in this case txrecord) to the PCP infrastructure viaa shared memory segment.As a matter of convenience, pmdatxmoncreates (and destroys on exit) the shared memory segment.The tx_type arguments are arbitrary unique tags used to identifydifferent transaction types.The txrecord application simulates the processing of one or moretransactions identified by tx_type and with an observed servicetime of servtime .With the -l option, txrecord displays the current summary of thetransaction activity from the shared memory segment.genload is a shell and awk(1) script that acts as a front-end totxrecord to generate a constant load of simulated transactionactivity.A brief description of the pmdatxmon command line optionsfollows:-dIt is absolutely crucial that the performance metrics domainnumber specified here is unique and consistent.That is,domain should be different for every PMDA on the one host,and the same domain number should be used for the same PMDAon all hosts.-lLocation of the log file.By default, a log file namedtxmon.log is written in the current directory of pmcd(1)when pmdatxmon is started, i.e.$PCP_LOG_DIR/pmcd.If thelog file cannot be created or is not writable, output iswritten to the standard error instead.-UUser account under which to run the agent.The default isthe unprivileged \"pcp\" account in current versions of PCP,but in older versions the superuser account (\"root\") wasused by default.",
        "name": "pmdatxmon, txrecord, genload - txmon performance metrics domainagent (PMDA)",
        "section": 1
    },
    {
        "command": "uclampset",
        "description": "uclampset sets or retrieves the utilization clamping attributesof an existing PID, or runs command with the given attributes.Utilization clamping is a new feature added in v5.3. It gives ahint to the scheduler about the allowed range of utilization thetask should be operating at.The utilization of the task affects frequency selection and taskplacement. Only schedutil cpufreq governor understands handlingutil clamp hints at the time of writing. Consult your kernel docsfor further info about other cpufreq governors support.If you\u2019re running on asymmetric heterogeneous system like Arm\u2019sbig.LITTLE. Utilization clamping can help bias task placement. Ifthe task is boosted such that util_min value is higher than thelittle cores' capacity, then the scheduler will do its best toplace it on a big core.Similarly, if util_max is smaller than or equal the capacity ofthe little cores, then the scheduler can still choose to place itthere even if the actual utilization of the task is at max.Setting a task\u2019s uclamp_min to a none zero value will effectivelyboost the task as when it runs it\u2019ll always start from thisutilization value.By setting a task\u2019s uclamp_max below 1024, this will effectivelycap the task as when it runs it\u2019ll never be able to go above thisutilization value.The full utilization range is: [0:1024]. The special value -1 isused to reset to system\u2019s default.",
        "name": "uclampset - manipulate the utilization clamping attributes of thesystem or a process",
        "section": 1
    },
    {
        "command": "ucmatose",
        "description": "Establishes a set of reliable RDMA connections between two nodesusing the librdmacm, optionally transfers data between the nodes,then disconnects.",
        "name": "ucmatose - RDMA CM connection and simple ping-pong test.",
        "section": 1
    },
    {
        "command": "uconv",
        "description": "uconv converts, or transcodes, each given file (or its standardinput if no file is specified) from one encoding to another.Thetranscoding is done using Unicode as a pivot encoding (i.e. thedata are first transcoded from their original encoding toUnicode, and then from Unicode to the destination encoding).If an encoding is not specified or is -, the default encoding isused. Thus, calling uconv with no encoding provides an easy wayto validate and sanitize data files for further consumption bytools requiring data in the default encoding.When calling uconv, it is possible to specify callbacks that areused to handle invalid characters in the input, or charactersthat cannot be transcoded to the destination encoding. Someencodings, for example, offer a default substitution characterthat can be used to represent the occurrence of such charactersin the input. Other callbacks offer a useful visualrepresentation of the invalid data.uconv can also run the specified transliteration on thetranscoded data, in which case transliteration will happen as anintermediate step, after the data have been transcoded toUnicode.The transliteration can be either a list of semicolon-separated transliterator names, or an arbitrarily complex set ofrules in the ICU transliteration rules format.For transcoding purposes, uconv options are compatible with thoseof iconv(1), making it easy to replace it in scripts. It is notnecessarily the case, however, that the encoding names used byuconv and ICU are the same as the ones used by iconv(1).Also,options that provide informational data, such as the -l, --listone offered by some iconv(1) variants such as GNU's, produce datain a slightly different and easier to parse format.",
        "name": "uconv - convert data from one encoding to another",
        "section": 1
    },
    {
        "command": "udaddy",
        "description": "Establishes a set of unreliable RDMA datagram communication pathsbetween two nodes using the librdmacm, optionally transfersdatagrams between the nodes, then tears down the communication.",
        "name": "udaddy - RDMA CM datagram setup and simple ping-pong test.",
        "section": 1
    },
    {
        "command": "udfinfo",
        "description": "udfinfo shows various information about a UDF filesystem storedeither on the block device or in the disk file image. The outputfrom the udfinfo is suitable for parsing by external applicationsor scripts.",
        "name": "udfinfo \u2014 show information about UDF filesystem",
        "section": 1
    },
    {
        "command": "udpong",
        "description": "Uses unreliable datagram streaming over RDMA protocol (rsocket)to connect and exchange data between a client and serverapplication.",
        "name": "udpong - unreliable datagram streaming over RDMA ping-pong test.",
        "section": 1
    },
    {
        "command": "ul",
        "description": "ul reads the named files (or standard input if none are given)and translates occurrences of underscores to the sequence whichindicates underlining for the terminal in use, as specified bythe environment variable TERM. The terminfo database is read todetermine the appropriate sequences for underlining. If theterminal is incapable of underlining but is capable of a standoutmode, then that is used instead. If the terminal can overstrike,or handles underlining automatically, ul degenerates to cat(1).If the terminal cannot underline, underlining is ignored.",
        "name": "ul - do underlining",
        "section": 1
    },
    {
        "command": "uname",
        "description": "Print certain system information.With no OPTION, same as -s.-a, --allprint all information, in the following order, except omit-p and -i if unknown:-s, --kernel-nameprint the kernel name-n, --nodenameprint the network node hostname-r, --kernel-releaseprint the kernel release-v, --kernel-versionprint the kernel version-m, --machineprint the machine hardware name-p, --processorprint the processor type (non-portable)-i, --hardware-platformprint the hardware platform (non-portable)-o, --operating-systemprint the operating system--help display this help and exit--versionoutput version information and exit",
        "name": "uname - print system information",
        "section": 1
    },
    {
        "command": "unexpand",
        "description": "Convert blanks in each FILE to tabs, writing to standard output.With no FILE, or when FILE is -, read standard input.Mandatory arguments to long options are mandatory for shortoptions too.-a, --allconvert all blanks, instead of just initial blanks--first-onlyconvert only leading sequences of blanks (overrides -a)-t, --tabs=Nhave tabs N characters apart instead of 8 (enables -a)-t, --tabs=LISTuse comma separated list of tab positions.The lastspecified position can be prefixed with '/' to specify atab size to use after the last explicitly specified tabstop.Also a prefix of '+' can be used to align remainingtab stops relative to the last specified tab stop insteadof the first column--help display this help and exit--versionoutput version information and exit",
        "name": "unexpand - convert spaces to tabs",
        "section": 1
    },
    {
        "command": "unicode_start",
        "description": "The unicode_start command will put the keyboard and console intoUnicode (UTF-8) mode.For the keyboard this means that one can attach 16-bit U+xxxxvalues to keyboard keys using loadkeys(1), and have these appearas UTF-8 input to user programs.Also, that one can typehexadecimal Alt-xxxx using the numeric keypad, and again produceUTF-8.For the console this means that the kernel expects UTF-8 outputfrom user programs, and displays the output accordingly.The parameter font is a font that is loaded. It should have abuilt-in Unicode map, or, if it hasn't, such a map can be givenexplicitly as second parameter.When no font was specified, thecurrent font is kept.",
        "name": "unicode_start - put keyboard and console in unicode mode",
        "section": 1
    },
    {
        "command": "unicode_stop",
        "description": "The unicode_stop command will more-or-less undo the effect ofunicode_start.It puts the keyboard in ASCII (XLATE) mode, andclears the console UTF-8 mode.",
        "name": "unicode_stop - revert keyboard and console from unicode mode",
        "section": 1
    },
    {
        "command": "uniq",
        "description": "Filter adjacent matching lines from INPUT (or standard input),writing to OUTPUT (or standard output).With no options, matching lines are merged to the firstoccurrence.Mandatory arguments to long options are mandatory for shortoptions too.-c, --countprefix lines by the number of occurrences-d, --repeatedonly print duplicate lines, one for each group-Dprint all duplicate lines--all-repeated[=METHOD]like -D, but allow separating groups with an empty line;METHOD={none(default),prepend,separate}-f, --skip-fields=Navoid comparing the first N fields--group[=METHOD]show all items, separating groups with an empty line;METHOD={separate(default),prepend,append,both}-i, --ignore-caseignore differences in case when comparing-s, --skip-chars=Navoid comparing the first N characters-u, --uniqueonly print unique lines-z, --zero-terminatedline delimiter is NUL, not newline-w, --check-chars=Ncompare no more than N characters in lines--help display this help and exit--versionoutput version information and exitA field is a run of blanks (usually spaces and/or TABs), thennon-blank characters.Fields are skipped before chars.Note: 'uniq' does not detect repeated lines unless they areadjacent.You may want to sort the input first, or use 'sort -u'without 'uniq'.",
        "name": "uniq - report or omit repeated lines",
        "section": 1
    },
    {
        "command": "unlink",
        "description": "Call the unlink function to remove the specified FILE.--help display this help and exit--versionoutput version information and exit",
        "name": "unlink - call the unlink function to remove the specified file",
        "section": 1
    },
    {
        "command": "unshare",
        "description": "The unshare command creates new namespaces (as specified by thecommand-line options described below) and then executes thespecified program. If program is not given, then \"${SHELL}\" isrun (default: /bin/sh).By default, a new namespace persists only as long as it hasmember processes. A new namespace can be made persistent evenwhen it has no member processes by bind mounting/proc/pid/ns/type files to a filesystem path. A namespace thathas been made persistent in this way can subsequently be enteredwith nsenter(1) even after the program terminates (except PIDnamespaces where a permanently running init process is required).Once a persistent namespace is no longer needed, it can beunpersisted by using umount(8) to remove the bind mount. See theEXAMPLES section for more details.unshare since util-linux version 2.36 uses/proc/[pid]/ns/pid_for_children and/proc/[pid]/ns/time_for_children files for persistent PID andTIME namespaces. This change requires Linux kernel 4.17 or newer.The following types of namespaces can be created with unshare:mount namespaceMounting and unmounting filesystems will not affect the restof the system, except for filesystems which are explicitlymarked as shared (with mount --make-shared; see/proc/self/mountinfo or findmnt -o+PROPAGATION for the sharedflags). For further details, see mount_namespaces(7).unshare since util-linux version 2.27 automatically setspropagation to private in a new mount namespace to make surethat the new namespace is really unshared. It\u2019s possible todisable this feature with option --propagation unchanged.Note that private is the kernel default.UTS namespaceSetting hostname or domainname will not affect the rest ofthe system. For further details, see uts_namespaces(7).IPC namespaceThe process will have an independent namespace for POSIXmessage queues as well as System V message queues, semaphoresets and shared memory segments. For further details, seeipc_namespaces(7).network namespaceThe process will have independent IPv4 and IPv6 stacks, IProuting tables, firewall rules, the /proc/net and/sys/class/net directory trees, sockets, etc. For furtherdetails, see network_namespaces(7).PID namespaceChildren will have a distinct set of PID-to-process mappingsfrom their parent. For further details, seepid_namespaces(7).cgroup namespaceThe process will have a virtualized view of/proc/self/cgroup, and new cgroup mounts will be rooted atthe namespace cgroup root. For further details, seecgroup_namespaces(7).user namespaceThe process will have a distinct set of UIDs, GIDs andcapabilities. For further details, see user_namespaces(7).time namespaceThe process can have a distinct view of CLOCK_MONOTONICand/or CLOCK_BOOTTIME which can be changed using/proc/self/timens_offsets. For further details, seetime_namespaces(7).",
        "name": "unshare - run program in new namespaces",
        "section": 1
    },
    {
        "command": "update-alternatives",
        "description": "update-alternatives creates, removes, maintains and displaysinformation about the symbolic links comprising the Debianalternatives system.It is possible for several programs fulfilling the same orsimilar functions to be installed on a single system at the sametime.For example, many systems have several text editorsinstalled at once.This gives choice to the users of a system,allowing each to use a different editor, if desired, but makes itdifficult for a program to make a good choice for an editor toinvoke if the user has not specified a particular preference.Debian's alternatives system aims to solve this problem.Ageneric name in the filesystem is shared by all files providinginterchangeable functionality.The alternatives system and thesystem administrator together determine which actual file isreferenced by this generic name.For example, if the texteditors ed(1) and nvi(1) are both installed on the system, thealternatives system will cause the generic name /usr/bin/editorto refer to /usr/bin/nvi by default. The system administrator canoverride this and cause it to refer to /usr/bin/ed instead, andthe alternatives system will not alter this setting untilexplicitly requested to do so.The generic name is not a direct symbolic link to the selectedalternative.Instead, it is a symbolic link to a name in thealternatives directory, which in turn is a symbolic link to theactual file referenced.This is done so that the systemadministrator's changes can be confined within the /usr/local/etcdirectory: the FHS (q.v.) gives reasons why this is a Good Thing.When each package providing a file with a particularfunctionality is installed, changed or removed, update-alternatives is called to update information about that file inthe alternatives system.update-alternatives is usually calledfrom the following Debian package maintainer scripts, postinst(configure) to install the alternative and from prerm and postrm(remove) to remove the alternative.Note: in most (if not all)cases no other maintainer script actions should call update-alternatives, in particular neither of upgrade nor disappear, asany other such action can lose the manual state of analternative, or make the alternative temporarily flip-flop, orcompletely switch when several of them have the same priority.It is often useful for a number of alternatives to besynchronized, so that they are changed as a group; for example,when several versions of the vi(1) editor are installed, themanual page referenced by /usr/share/man/man1/vi.1 shouldcorrespond to the executable referenced by /usr/bin/vi.update-alternatives handles this by means of master and slave links;when the master is changed, any associated slaves are changedtoo.A master link and its associated slaves make up a linkgroup.Each link group is, at any given time, in one of two modes:automatic or manual.When a group is in automatic mode, thealternatives system will automatically decide, as packages areinstalled and removed, whether and how to update the links.Inmanual mode, the alternatives system will retain the choice ofthe administrator and avoid changing the links (except whensomething is broken).Link groups are in automatic mode when they are first introducedto the system.If the system administrator makes changes to thesystem's automatic settings, this will be noticed the next timeupdate-alternatives is run on the changed link's group, and thegroup will automatically be switched to manual mode.Each alternative has a priority associated with it.When a linkgroup is in automatic mode, the alternatives pointed to bymembers of the group will be those which have the highestpriority.When using the --config option, update-alternatives will list allof the choices for the link group of which given name is themaster alternative name.The current choice is marked with a\u2018*\u2019.You will then be prompted for your choice regarding thislink group.Depending on the choice made, the link group mightno longer be in auto mode. You will need to use the --auto optionin order to return to the automatic mode (or you can rerun--config and select the entry marked as automatic).If you want to configure non-interactively you can use the --setoption instead (see below).Different packages providing the same file need to do socooperatively.In other words, the usage of update-alternativesis mandatory for all involved packages in such case. It is notpossible to override some file in a package that does not employthe update-alternatives mechanism.",
        "name": "update-alternatives - maintain symbolic links determining defaultcommands",
        "section": 1
    },
    {
        "command": "updatedb",
        "description": "This manual page documents the GNU version of updatedb, whichupdates file name databases used by GNU locate.The file namedatabases contain lists of files that were in particulardirectory trees when the databases were last updated.The filename of the default database is determined when locate andupdatedb are configured and installed.The frequency with whichthe databases are updated and the directories for which theycontain entries depend on how often updatedb is run, and withwhich arguments.In networked environments, it often makes sense to build adatabase at the root of each filesystem, containing the entriesfor that filesystem.updatedb is then run for each filesystem onthe fileserver where that filesystem is on a local disk, toprevent thrashing the network.Users can select which databaseslocate searches using an environment variable or command lineoption; see locate(1).Databases cannot be concatenatedtogether.The LOCATGE02 database format was introduced in GNU findutilsversion 4.0 in order to allow machines with different byteorderings to share the databases.GNU locate can read both theold and LOCATE02 database formats, though support for the oldpre-4.0 database format will be removed shortly.",
        "name": "updatedb - update a file name database",
        "section": 1
    },
    {
        "command": "uptime",
        "description": "uptime gives a one line display of the following information.The current time, how long the system has been running, how manyusers are currently logged on, and the system load averages forthe past 1, 5, and 15 minutes.This is the same information contained in the header linedisplayed by w(1).System load averages is the average number of processes that areeither in a runnable or uninterruptable state.A process in arunnable state is either using the CPU or waiting to use the CPU.A process in uninterruptable state is waiting for some I/Oaccess, eg waiting for disk.The averages are taken over thethree time intervals.Load averages are not normalized for thenumber of CPUs in a system, so a load average of 1 means a singleCPU system is loaded all the time while on a 4 CPU system itmeans it was idle 75% of the time.",
        "name": "uptime - Tell how long the system has been running.",
        "section": 1
    },
    {
        "command": "usb-devices",
        "description": "usb-devices is a shell script that can be used to display detailsof USB buses in the system and the devices connected to them.The output of the script is similar to the usb/devices fileavailable either under /proc/bus (if usbfs is mounted), or under/sys/kernel/debug (if debugfs is mounted there). The script isprimarily intended to be used if the file is not available.In contrast to the usb/devices file, this script only listsactive interfaces (those marked with a \"*\" in the usb/devicesfile) and their endpoints.Be advised that there can be differences in the way informationis sorted, as well as in the format of the output.",
        "name": "usb-devices - print USB device details",
        "section": 1
    },
    {
        "command": "userdbctl",
        "description": "userdbctl may be used to inspect user and groups (as well asgroup memberships) of the system. This client utility inquiresuser/group information provided by various system services, bothoperating on JSON user/group records (as defined by the JSON UserRecords[1] and JSON Group Records[2] definitions), and classicUNIX NSS/glibc user and group records. This tool is primarily aclient to the User/Group Record Lookup API via Varlink[3], andmay also pick up drop-in JSON user and group records from/etc/userdb/, /run/userdb/, /run/host/userdb/, /usr/lib/userdb/.",
        "name": "userdbctl - Inspect users, groups and group memberships",
        "section": 1
    },
    {
        "command": "users",
        "description": "Output who is currently logged in according to FILE.If FILE isnot specified, use /var/run/utmp./var/log/wtmp as FILE iscommon.--help display this help and exit--versionoutput version information and exit",
        "name": "users - print the user names of users currently logged in to thecurrent host",
        "section": 1
    },
    {
        "command": "utmpdump",
        "description": "utmpdump is a simple program to dump UTMP and WTMP files in rawformat, so they can be examined. utmpdump reads from stdin unlessa filename is passed.",
        "name": "utmpdump - dump UTMP and WTMP files in raw format",
        "section": 1
    },
    {
        "command": "uuidgen",
        "description": "The uuidgen program creates (and prints) a new universally uniqueidentifier (UUID) using the libuuid(3) library. The new UUID canreasonably be considered unique among all UUIDs created on thelocal system, and among UUIDs created on other systems in thepast and in the future.There are three types of UUIDs which uuidgen can generate:time-based UUIDs, random-based UUIDs, and hash-based UUIDs. Bydefault uuidgen will generate a random-based UUID if ahigh-quality random number generator is present. Otherwise, itwill choose a time-based UUID. It is possible to force thegeneration of one of these first two UUID types by using the--random or --time options.The third type of UUID is generated with the --md5 or --sha1options, followed by --namespace namespace and --name name. Thenamespace may either be a well-known UUID, or else an alias toone of the well-known UUIDs defined in RFC 4122, that is @dns,@url, @oid, or @x500. The name is an arbitrary string value. Thegenerated UUID is the digest of the concatenation of thenamespace UUID and the name value, hashed with the MD5 or SHA1algorithms. It is, therefore, a predictable value which may beuseful when UUIDs are being used as handles or nonces for morecomplex values or values which shouldn\u2019t be disclosed directly.See the RFC for more information.",
        "name": "uuidgen - create a new UUID value",
        "section": 1
    },
    {
        "command": "uuidparse",
        "description": "This command will parse unique identifier inputs from eithercommand line arguments or standard input. The inputs arewhite-space separated.",
        "name": "uuidparse - a utility to parse unique identifiers",
        "section": 1
    },
    {
        "command": "valgrind",
        "description": "Valgrind is a flexible program for debugging and profiling Linuxexecutables. It consists of a core, which provides a syntheticCPU in software, and a series of debugging and profiling tools.The architecture is modular, so that new tools can be createdeasily and without disturbing the existing structure.Some of the options described below work with all Valgrind tools,and some only work with a few or one. The section MEMCHECKOPTIONS and those below it describe tool-specific options.This manual page covers only basic usage and options. For morecomprehensive information, please see the HTML documentation onyour system: $INSTALL/share/doc/valgrind/html/index.html, oronline: http://www.valgrind.org/docs/manual/index.html.",
        "name": "valgrind - a suite of tools for debugging and profiling programs",
        "section": 1
    },
    {
        "command": "valgrind-di-server",
        "description": "valgrind-di-server accepts (multiple) connections from valgrindprocesses that use the--debuginfo-server option on the specified port and serves(compressed) debuginfo files (in chunks) from the current workingdirectory.",
        "name": "valgrind-di-server - Experimental debuginfo server for valgrind",
        "section": 1
    },
    {
        "command": "valgrind-listener",
        "description": "valgrind-listener accepts (multiple) connections from valgrindprocesses that use the --log-socket option on the specified portand copies the commentary it is sent to stdout.",
        "name": "valgrind-listener - listens on a socket for Valgrind commentary",
        "section": 1
    },
    {
        "command": "vdir",
        "description": "List information about the FILEs (the current directory bydefault).Sort entries alphabetically if none of -cftuvSUX nor--sort is specified.Mandatory arguments to long options are mandatory for shortoptions too.-a, --alldo not ignore entries starting with .-A, --almost-alldo not list implied . and ..--authorwith -l, print the author of each file-b, --escapeprint C-style escapes for nongraphic characters--block-size=SIZEwith -l, scale sizes by SIZE when printing them; e.g.,'--block-size=M'; see SIZE format below-B, --ignore-backupsdo not list implied entries ending with ~-cwith -lt: sort by, and show, ctime (time of last change offile status information); with -l: show ctime and sort byname; otherwise: sort by ctime, newest first-Clist entries by columns--color[=WHEN]color the output WHEN; more info below-d, --directorylist directories themselves, not their contents-D, --diredgenerate output designed for Emacs' dired mode-flist all entries in directory order-F, --classify[=WHEN]append indicator (one of */=>@|) to entries WHEN--file-typelikewise, except do not append '*'--format=WORDacross -x, commas -m, horizontal -x, long -l,single-column -1, verbose -l, vertical -C--full-timelike -l --time-style=full-iso-glike -l, but do not list owner--group-directories-firstgroup directories before files; can be augmented with a--sort option, but any use of --sort=none (-U) disablesgrouping-G, --no-groupin a long listing, don't print group names-h, --human-readablewith -l and -s, print sizes like 1K 234M 2G etc.--silikewise, but use powers of 1000 not 1024-H, --dereference-command-linefollow symbolic links listed on the command line--dereference-command-line-symlink-to-dirfollow each command line symbolic link that points to adirectory--hide=PATTERNdo not list implied entries matching shell PATTERN(overridden by -a or -A)--hyperlink[=WHEN]hyperlink file names WHEN--indicator-style=WORDappend indicator with style WORD to entry names: none(default), slash (-p), file-type (--file-type), classify(-F)-i, --inodeprint the index number of each file-I, --ignore=PATTERNdo not list implied entries matching shell PATTERN-k, --kibibytesdefault to 1024-byte blocks for file system usage; usedonly with -s and per directory totals-luse a long listing format-L, --dereferencewhen showing file information for a symbolic link, showinformation for the file the link references rather thanfor the link itself-mfill width with a comma separated list of entries-n, --numeric-uid-gidlike -l, but list numeric user and group IDs-N, --literalprint entry names without quoting-olike -l, but do not list group information-p, --indicator-style=slashappend / indicator to directories-q, --hide-control-charsprint ? instead of nongraphic characters--show-control-charsshow nongraphic characters as-is (the default, unlessprogram is 'ls' and output is a terminal)-Q, --quote-nameenclose entry names in double quotes--quoting-style=WORDuse quoting style WORD for entry names: literal, locale,shell, shell-always, shell-escape, shell-escape-always, c,escape (overrides QUOTING_STYLE environment variable)-r, --reversereverse order while sorting-R, --recursivelist subdirectories recursively-s, --sizeprint the allocated size of each file, in blocks-Ssort by file size, largest first--sort=WORDsort by WORD instead of name: none (-U), size (-S), time(-t), version (-v), extension (-X), width--time=WORDselect which timestamp used to display or sort; accesstime (-u): atime, access, use; metadata change time (-c):ctime, status; modified time (default): mtime,modification; birth time: birth, creation;with -l, WORD determines which time to show; with--sort=time, sort by WORD (newest first)--time-style=TIME_STYLEtime/date format with -l; see TIME_STYLE below-tsort by time, newest first; see --time-T, --tabsize=COLSassume tab stops at each COLS instead of 8-uwith -lt: sort by, and show, access time; with -l: showaccess time and sort by name; otherwise: sort by accesstime, newest first-Udo not sort; list entries in directory order-vnatural sort of (version) numbers within text-w, --width=COLSset output width to COLS.0 means no limit-xlist entries by lines instead of by columns-Xsort alphabetically by entry extension-Z, --contextprint any security context of each file--zero end each output line with NUL, not newline-1list one file per line--help display this help and exit--versionoutput version information and exitThe SIZE argument is an integer and optional unit (example: 10Kis 10*1024).Units are K,M,G,T,P,E,Z,Y,R,Q (powers of 1024) orKB,MB,... (powers of 1000).Binary prefixes can be used, too:KiB=K, MiB=M, and so on.The TIME_STYLE argument can be full-iso, long-iso, iso, locale,or +FORMAT.FORMAT is interpreted like in date(1).If FORMAT isFORMAT1<newline>FORMAT2, then FORMAT1 applies to non-recent filesand FORMAT2 to recent files.TIME_STYLE prefixed with 'posix-'takes effect only outside the POSIX locale.Also the TIME_STYLEenvironment variable sets the default style to use.The WHEN argument defaults to 'always' and can also be 'auto' or'never'.Using color to distinguish file types is disabled both by defaultand with --color=never.With --color=auto, ls emits color codesonly when standard output is connected to a terminal.TheLS_COLORS environment variable can change the settings.Use thedircolors(1) command to set it.Exit status:0if OK,1if minor problems (e.g., cannot access subdirectory),2if serious trouble (e.g., cannot access command-lineargument).",
        "name": "vdir - list directory contents",
        "section": 1
    },
    {
        "command": "verify_blkparse",
        "description": "Verifies an output file from blkparse. All it does is check ifthe events in the file are correctly time ordered. If an entry isfound that isn't ordered, it's dumped to stdout.",
        "name": "verify_blkparse - verifies an output file produced by blkparse",
        "section": 1
    },
    {
        "command": "verifytree",
        "description": "verifytree is a program that verifies whether a local yumrepository is consistent.",
        "name": "verifytree - verify that a local yum repository is consistent",
        "section": 1
    },
    {
        "command": "vgdb",
        "description": "vgdb (\"Valgrind to GDB\") is used as an intermediary betweenValgrind and GDB or a shell. It has three usage modes:1. As a standalone utility, it is used from a shell command lineto send monitor commands to a process running under Valgrind.For this usage, the vgdb OPTION(s) must be followed by themonitor command to send. To send more than one command,separate them with the -c option.2. In combination with GDB \"target remote |\" command, it is usedas the relay application between GDB and the Valgrindgdbserver. For this usage, only OPTION(s) can be given, butno COMMAND can be given.3. In the --multi mode, vgdb uses the extended remote protocolto communicate with GDB. This allows you to view output fromboth valgrind and GDB in the GDB session. This isaccomplished via the \"target extended-remote | vgdb --multi\".In this mode you no longer need to start valgrind yourself.vgdb will start up valgrind when gdb tells it to run a newprogram. For this usage, the vgdb OPTIONS(s) can also include--valgrind and --vargs to describe how valgrind should bestarted.",
        "name": "vgdb - intermediary between Valgrind and GDB or a shell",
        "section": 1
    },
    {
        "command": "vlock",
        "description": "vlock is a program to lock one or more sessions on the Linuxconsole.This is especially useful for Linux machines which havemultiple users with access to the console.One user may lock hisor her session(s) while still allowing other users to use thesystem on other virtual consoles.If desired, the entire consolemay be locked and virtual console switching disabled.By default, only the current VC (virtual console) is locked.With the -a,-all option all VCs are locked.The locked VCscannot be unlocked without the invoker's password.And, for theparanoid, vlock makes it a trying experience for those attemptingto guess the password, so unauthorized access to session(s) ishighly unlikely.Please note that it is entirely possible to completely lockyourself out of the console with the -a,--all option if youcannot remember your password!Unless you are able to kill vlockby logging in remotely via a serial terminal or network, a hardreset is the only method of ``unlocking'' the display.vlock works for console sessions primarily.However, there issupport for trying to lock non-console sessions as well, but thatsupport has not been well tested.",
        "name": "vlock - Virtual Console lock program",
        "section": 1
    },
    {
        "command": "w",
        "description": "w displays information about the users currently on the machine,and their processes.The header shows, in this order, thecurrent time, how long the system has been running, how manyusers are currently logged on, and the system load averages forthe past 1, 5, and 15 minutes.The following entries are displayed for each user: login name,the tty name, the remote host, login time, idle time, JCPU, PCPU,and the command line of their current process.The JCPU time is the time used by all processes attached to thetty.It does not include past background jobs, but does includecurrently running background jobs.The PCPU time is the time used by the current process, named inthe \"what\" field.",
        "name": "w - Show who is logged on and what they are doing.",
        "section": 1
    },
    {
        "command": "waitpid",
        "description": "waitpid is a simple command to wait for arbitrary non-childprocesses.It exits after all processes whose PIDs have been passed asarguments have exited.",
        "name": "waitpid - utility to wait for arbitrary processes",
        "section": 1
    },
    {
        "command": "wall",
        "description": "wall displays a message, or the contents of a file, or otherwiseits standard input, on the terminals of all currently logged inusers. The command will wrap lines that are longer than 79characters. Short lines are whitespace padded to have 79characters. The command will always put a carriage return and newline at the end of each line.Only the superuser can write on the terminals of users who havechosen to deny messages or are using a program whichautomatically denies messages.Reading from a file is refused when the invoker is not superuserand the program is set-user-ID or set-group-ID.",
        "name": "wall - write a message to all users",
        "section": 1
    },
    {
        "command": "watch",
        "description": "watch runs command repeatedly, displaying its output and errors(the first screenfull).This allows you to watch the programoutput change over time.By default, command is run every 2seconds and watch will run until interrupted.",
        "name": "watch - execute a program periodically, showing output fullscreen",
        "section": 1
    },
    {
        "command": "wc",
        "description": "Print newline, word, and byte counts for each FILE, and a totalline if more than one FILE is specified.A word is anon-zero-length sequence of printable characters delimited bywhite space.With no FILE, or when FILE is -, read standard input.The options below may be used to select which counts are printed,always in the following order: newline, word, character, byte,maximum line length.-c, --bytesprint the byte counts-m, --charsprint the character counts-l, --linesprint the newline counts--files0-from=Fread input from the files specified by NUL-terminatednames in file F; If F is - then read names from standardinput-L, --max-line-lengthprint the maximum display width-w, --wordsprint the word counts--total=WHENwhen to print a line with total counts; WHEN can be: auto,always, only, never--help display this help and exit--versionoutput version information and exit",
        "name": "wc - print newline, word, and byte counts for each file",
        "section": 1
    },
    {
        "command": "wget",
        "description": "GNU Wget is a free utility for non-interactive download of filesfrom the Web.It supports HTTP, HTTPS, and FTP protocols, aswell as retrieval through HTTP proxies.Wget is non-interactive, meaning that it can work in thebackground, while the user is not logged on.This allows you tostart a retrieval and disconnect from the system, letting Wgetfinish the work.By contrast, most of the Web browsers requireconstant user's presence, which can be a great hindrance whentransferring a lot of data.Wget can follow links in HTML, XHTML, and CSS pages, to createlocal versions of remote web sites, fully recreating thedirectory structure of the original site.This is sometimesreferred to as \"recursive downloading.\"While doing that, Wgetrespects the Robot Exclusion Standard (/robots.txt).Wget can beinstructed to convert the links in downloaded files to point atthe local files, for offline viewing.Wget has been designed for robustness over slow or unstablenetwork connections; if a download fails due to a networkproblem, it will keep retrying until the whole file has beenretrieved.If the server supports regetting, it will instructthe server to continue the download from where it left off.",
        "name": "Wget - The non-interactive network downloader.",
        "section": 1
    },
    {
        "command": "whatis",
        "description": "Each manual page has a short description available within it.whatis searches the manual page names and displays the manualpage descriptions of any name matched.name may contain wildcards (-w) or be a regular expression (-r).Using these options, it may be necessary to quote the name orescape (\\) the special characters to stop the shell frominterpreting them.index databases are used during the search, and are updated bythe mandb program.Depending on your installation, this may berun by a periodic cron job, or may need to be run manually afternew manual pages have been installed.To produce an old styletext whatis database from the relative index database, issue thecommand:whatis -M manpath -w '*' | sort > manpath/whatiswhere manpath is a manual page hierarchy such as /usr/man.",
        "name": "whatis - display one-line manual page descriptions",
        "section": 1
    },
    {
        "command": "whereis",
        "description": "whereis locates the binary, source and manual files for thespecified command names. The supplied names are first stripped ofleading pathname components. Prefixes of s. resulting from use ofsource code control are also dealt with. whereis then attempts tolocate the desired program in the standard Linux places, and inthe places specified by $PATH and $MANPATH.The search restrictions (options -b, -m and -s) are cumulativeand apply to the subsequent name patterns on the command line.Any new search restriction resets the search mask. For example,whereis -bm ls tr -m gccsearches for \"ls\" and \"tr\" binaries and man pages, and for \"gcc\"man pages only.The options -B, -M and -S reset search paths for the subsequentname patterns. For example,whereis -m ls -M /usr/share/man/man1 -f calsearches for \"ls\" man pages in all default paths, but for \"cal\"in the /usr/share/man/man1 directory only.",
        "name": "whereis - locate the binary, source, and manual page files for acommand",
        "section": 1
    },
    {
        "command": "who",
        "description": "Print information about users who are currently logged in.-a, --allsame as -b -d --login -p -r -t -T -u-b, --boottime of last system boot-d, --deadprint dead processes-H, --headingprint line of column headings-l, --loginprint system login processes--lookupattempt to canonicalize hostnames via DNS-monly hostname and user associated with stdin-p, --processprint active processes spawned by init-q, --countall login names and number of users logged on-r, --runlevelprint current runlevel-s, --shortprint only name, line, and time (default)-t, --timeprint last system clock change-T, -w, --mesgadd user's message status as +, - or ?-u, --userslist users logged in--messagesame as -T--writablesame as -T--help display this help and exit--versionoutput version information and exitIf FILE is not specified, use /var/run/utmp./var/log/wtmp asFILE is common.If ARG1 ARG2 given, -m presumed: 'am i' or 'momlikes' are usual.",
        "name": "who - show who is logged on",
        "section": 1
    },
    {
        "command": "whoami",
        "description": "Print the user name associated with the current effective userID.Same as id -un.--help display this help and exit--versionoutput version information and exit",
        "name": "whoami - print effective user name",
        "section": 1
    },
    {
        "command": "windmc",
        "description": "windmc reads message definitions from an input file (.mc) andtranslate them into a set of output files.The output files maybe of four kinds:\"h\" A C header file containing the message definitions.\"rc\"A resource file compilable by the windres tool.\"bin\"One or more binary files containing the resource data for aspecific message language.\"dbg\"A C include file that maps message id's to their symbolicname.The exact description of these different formats is available indocumentation from Microsoft.When windmc converts from the \"mc\" format to the \"bin\" format,\"rc\", \"h\", and optional \"dbg\" it is acting like the WindowsMessage Compiler.",
        "name": "windmc - generates Windows message resources",
        "section": 1
    },
    {
        "command": "windres",
        "description": "windres reads resources from an input file and copies them intoan output file.Either file may be in one of three formats:\"rc\"A text format read by the Resource Compiler.\"res\"A binary format generated by the Resource Compiler.\"coff\"A COFF object or executable.The exact description of these different formats is available indocumentation from Microsoft.When windres converts from the \"rc\" format to the \"res\" format,it is acting like the Windows Resource Compiler.When windresconverts from the \"res\" format to the \"coff\" format, it is actinglike the Windows \"CVTRES\" program.When windres generates an \"rc\" file, the output is similar butnot identical to the format expected for the input.When aninput \"rc\" file refers to an external filename, an output \"rc\"file will instead include the file contents.If the input or output format is not specified, windres willguess based on the file name, or, for the input file, the filecontents.A file with an extension of .rc will be treated as an\"rc\" file, a file with an extension of .res will be treated as a\"res\" file, and a file with an extension of .o or .exe will betreated as a \"coff\" file.If no output file is specified, windres will print the resourcesin \"rc\" format to standard output.The normal use is for you to write an \"rc\" file, use windres toconvert it to a COFF object file, and then link the COFF fileinto your application.This will make the resources described inthe \"rc\" file available to Windows.",
        "name": "windres - manipulate Windows resources",
        "section": 1
    },
    {
        "command": "wrudf",
        "description": "wrudf provides an interactive shell with operations on existingUDF filesystem: cp, rm, mkdir, rmdir, ls, cd.COMMANDScpcopyrmremovemkdirmake directoryrmdirremove directorylsclist files (Compact disc version)lshlist files (Hard disc version)cdcchange working directory (Compact disc)cdhchange working directory (Hard disc)quitquit wrudfexitquit wrudf",
        "name": "wrudf - Maintain a UDF filesystem.",
        "section": 1
    },
    {
        "command": "wsrep_sst_common",
        "description": "Use: Common command line parser to be sourced by other SSTscripts.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "wsrep_sst_common - common command line parser to be sourced byother SST scripts",
        "section": 1
    },
    {
        "command": "wsrep_sst_mariabackup",
        "description": "Use: mariabackup-based state snapshot transfer.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "wsrep_sst_mariabackup - mariabackup-based state snapshot transfer",
        "section": 1
    },
    {
        "command": "wsrep_sst_mysqldump",
        "description": "Use: mysqldump-based state snapshot transfer.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "wsrep_sst_mysqldump - mysqldump-based state snapshot transfer",
        "section": 1
    },
    {
        "command": "wsrep_sst_rsync",
        "description": "Use: rsync-based state snapshot transfer.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "wsrep_sst_rsync - rsync-based state snapshot transfer",
        "section": 1
    },
    {
        "command": "wsrep_sst_rsync_wan",
        "description": "Use: rsync_wan-based state snapshot transfer.For more information, please refer to the MariaDB Knowledge Base,available online at https://mariadb.com/kb/",
        "name": "wsrep_sst_rsync_wan - rsync_wan (rsync with deltatransfers)-based state snapshot transfer",
        "section": 1
    },
    {
        "command": "xargs",
        "description": "This manual page documents the GNU version of xargs.xargs readsitems from the standard input, delimited by blanks (which can beprotected with double or single quotes or a backslash) ornewlines, and executes the command (default is echo) one or moretimes with any initial-arguments followed by items read fromstandard input.Blank lines on the standard input are ignored.The command line for command is built up until it reaches asystem-defined limit (unless the -n and -L options are used).The specified command will be invoked as many times as necessaryto use up the list of input items.In general, there will bemany fewer invocations of command than there were items in theinput.This will normally have significant performance benefits.Some commands can usefully be executed in parallel too; see the-P option.Because Unix filenames can contain blanks and newlines, thisdefault behaviour is often problematic; filenames containingblanks and/or newlines are incorrectly processed by xargs.Inthese situations it is better to use the -0 option, whichprevents such problems.When using this option you will need toensure that the program which produces the input for xargs alsouses a null character as a separator.If that program is GNUfind for example, the -print0 option does this for you.If any invocation of the command exits with a status of 255,xargs will stop immediately without reading any further input.An error message is issued on stderr when this happens.",
        "name": "xargs - build and execute command lines from standard input",
        "section": 1
    },
    {
        "command": "xgettext",
        "description": "Extract translatable strings from given input files.Mandatory arguments to long options are mandatory for shortoptions too.Similarly for optional arguments.Input file location:INPUTFILE ...input files-f, --files-from=FILEget list of input files from FILE-D, --directory=DIRECTORYadd DIRECTORY to list for input files searchIf input file is -, standard input is read.Output file location:-d, --default-domain=NAMEuse NAME.po for output (instead of messages.po)-o, --output=FILEwrite output to specified file-p, --output-dir=DIRoutput files will be placed in directory DIRIf output file is -, output is written to standard output.Choice of input file language:-L, --language=NAMErecognise the specified language (C, C++, ObjectiveC, PO,Shell, Python, Lisp, EmacsLisp, librep, Scheme, Smalltalk,Java, JavaProperties, C#, awk, YCP, Tcl, Perl, PHP, Ruby,GCC-source, NXStringTable, RST, RSJ, Glade, Lua,JavaScript, Vala, Desktop)-C, --c++shorthand for --language=C++By default the language is guessed depending on the input filename extension.Input file interpretation:--from-code=NAMEencoding of input files (except for Python, Tcl, Glade)By default the input files are assumed to be in ASCII.Operation mode:-j, --join-existingjoin messages with existing file-x, --exclude-file=FILE.poentries from FILE.po are not extracted-cTAG, --add-comments=TAGplace comment blocks starting with TAG and precedingkeyword lines in output file-c, --add-commentsplace all comment blocks preceding keyword lines in outputfile--check=NAMEperform syntax check on messages (ellipsis-unicode,space-ellipsis,quote-unicode, bullet-unicode)--sentence-end=TYPEtype describing the end of sentence (single-space, whichis the default,or double-space)Language specific options:-a, --extract-allextract all strings (only languages C, C++, ObjectiveC,Shell, Python, Lisp, EmacsLisp, librep, Scheme, Java, C#,awk, Tcl, Perl, PHP, GCC-source, Glade, Lua, JavaScript,Vala)-kWORD, --keyword=WORDlook for WORD as an additional keyword-k, --keyworddo not to use default keywords (only languages C, C++,ObjectiveC, Shell, Python, Lisp, EmacsLisp, librep,Scheme, Java, C#, awk, Tcl, Perl, PHP, GCC-source, Glade,Lua, JavaScript, Vala, Desktop)--flag=WORD:ARG:FLAGadditional flag for strings inside the argument number ARGof keyword WORD(only languages C, C++, ObjectiveC, Shell,Python, Lisp, EmacsLisp, librep, Scheme, Java, C#, awk,YCP, Tcl, Perl, PHP, GCC-source, Lua, JavaScript, Vala)-T, --trigraphsunderstand ANSI C trigraphs for input (only languages C,C++, ObjectiveC)--its=FILEapply ITS rules from FILE (only XML based languages)--qtrecognize Qt format strings (only language C++)--kderecognize KDE 4 format strings (only language C++)--boostrecognize Boost format strings (only language C++)--debugmore detailed formatstring recognition resultOutput details:--coloruse colors and other text attributes always--color=WHENuse colors and other text attributes if WHEN.WHEN may be'always', 'never', 'auto', or 'html'.--style=STYLEFILEspecify CSS style rule file for --color-e, --no-escapedo not use C escapes in output (default)-E, --escapeuse C escapes in output, no extended chars--force-powrite PO file even if empty-i, --indentwrite the .po file using indented style--no-locationdo not write '#: filename:line' lines-n, --add-locationgenerate '#: filename:line' lines (default)--strictwrite out strict Uniforum conforming .po file--properties-outputwrite out a Java .properties file--stringtable-outputwrite out a NeXTstep/GNUstep .strings file--itstoolwrite out itstool comments-w, --width=NUMBERset output page width--no-wrapdo not break long message lines, longer than the outputpage width, into several lines-s, --sort-outputgenerate sorted output (deprecated)-F, --sort-by-filesort output by file location--omit-headerdon't write header with 'msgid \"\"' entry--copyright-holder=STRINGset copyright holder in output--foreign-useromit FSF copyright in output for foreign user--package-name=PACKAGEset package name in output--package-version=VERSIONset package version in output--msgid-bugs-address=EMAIL@ADDRESSset report address for msgid bugs-m[STRING], --msgstr-prefix[=STRING]use STRING or \"\" as prefix for msgstr values-M[STRING], --msgstr-suffix[=STRING]use STRING or \"\" as suffix for msgstr valuesInformative output:-h, --helpdisplay this help and exit-V, --versionoutput version information and exit-v, --verboseincrease verbosity level",
        "name": "xgettext - extract gettext strings from source",
        "section": 1
    },
    {
        "command": "xminicom",
        "description": "Xminicom is a script wrapper around minicom.It tries to find acolor capable xterm or rxvt on your system, and then runs minicomwith the -c (color) flag in a terminal session.",
        "name": "xminicom - friendly serial communication program",
        "section": 1
    },
    {
        "command": "xtotroff",
        "description": null,
        "name": null,
        "section": 1
    },
    {
        "command": "yes",
        "description": "Repeatedly output a line with all specified STRING(s), or 'y'.--help display this help and exit--versionoutput version information and exit",
        "name": "yes - output a string repeatedly until killed",
        "section": 1
    },
    {
        "command": "ypdomainname",
        "description": "Hostname is the program that is used to either set or display thecurrent host, domain or node name of the system.These names areused by many of the networking programs to identify the machine.The domain name is also used by NIS/YP.GET NAMEWhen called without any arguments, the program displays thecurrent names:hostname will print the name of the system as returned by thegethostname(2) function.domainname, nisdomainname, ypdomainname will print the name ofthe system as returned by the getdomainname(2) function. This isalso known as the YP/NIS domain name of the system.nodename will print the DECnet node name of the system asreturned by the getnodename(2) function.dnsdomainname will print the domain part of the FQDN (FullyQualified Domain Name). The complete FQDN of the system isreturned with hostname --fqdn.SET NAMEWhen called with one argument or with the --file option, thecommands set the host name, the NIS/YP domain name or the nodename.Note, that only the super-user can change the names.It is not possible to set the FQDN or the DNS domain name withthe dnsdomainname command (see THE FQDN below).The host name is usually set once at system startup by readingthe contents of a file which contains the host name, e.g./etc/hostname).THE FQDNYou can't change the FQDN (as returned by hostname --fqdn) or theDNS domain name (as returned by dnsdomainname) with this command.The FQDN of the system is the name that the resolver(3) returnsfor the host name.Technically: The FQDN is the canonical name returned bygethostbyname2(2) when resolving the result of the gethostname(2)name. The DNS domain name is the part after the first dot.Therefore it depends on the configuration (usually in/etc/host.conf) how you can change it. If hosts is the firstlookup method, you can change the FQDN in /etc/hosts.",
        "name": "hostname - show or set the system's host namednsdomainname - show the system's DNS domain namedomainname - show or set the system's NIS/YP domain namenisdomainname - show or set system's NIS/YP domain namenodename - show or set the system's DECnet node nameypdomainname - show or set the system's NIS/YP domain name",
        "section": 1
    },
    {
        "command": "yum-aliases",
        "description": "DNF is the next upcoming major version of YUM, a package managerfor RPM-based Linux distributions. It roughly maintains CLIcompatibility with YUM and defines a strict API for extensionsand plugins.Plugins can modify or extend features of DNF or provideadditional CLI commands on top of those mentioned below. If youknow the name of such a command (including commands mentionedbelow), you may find/install the package which provides it usingthe appropriate virtual provide in the form ofdnf-command(<alias>), where <alias> is the name of the command;e.g.``dnf install 'dnf-command(versionlock)'`` installs aversionlock plugin. This approach also applies to specifyingdependencies of packages that require a particular DNF command.Return values:\u2022 0: Operation was successful.\u2022 1: An error occurred, which was handled by dnf.\u2022 3: An unknown unhandled error occurred during operation.\u2022 100: See check-update\u2022 200: There was a problem with acquiring or releasing of locks.Available commands:\u2022 alias\u2022 autoremove\u2022 check\u2022 check-update\u2022 clean\u2022 deplist\u2022 distro-sync\u2022 downgrade\u2022 group\u2022 help\u2022 history\u2022 info\u2022 install\u2022 list\u2022 makecache\u2022 mark\u2022 module\u2022 provides\u2022 reinstall\u2022 remove\u2022 repoinfo\u2022 repolist\u2022 repoquery\u2022 repository-packages\u2022 search\u2022 shell\u2022 swap\u2022 updateinfo\u2022 upgrade\u2022 upgrade-minimalAdditional information:\u2022 Options\u2022 Specifying Packages\u2022 Specifying Provides\u2022 Specifying File Provides\u2022 Specifying Groups\u2022 Specifying Transactions\u2022 Metadata Synchronization\u2022 Configuration Files Replacement Policy\u2022 Files\u2022 See Also",
        "name": "yum-aliases - redirecting to DNF Command Reference",
        "section": 1
    },
    {
        "command": "yum-builddep",
        "description": "yum-builddep is a program which installs the RPMs needed to buildthe specified package.The source RPM for the specified packagemust be available in a Yum repository (which will beautomatically enabled, if it is disabled) or it can be a localsource RPM or a spec file.Note, that only the BuildRequires information within the SRPMheader information is used to determine build dependencies. Thiswill specifically omit any dependencies that are required onlyfor specific architectures.",
        "name": "yum-builddep - install missing dependencies for building an RPMpackage",
        "section": 1
    },
    {
        "command": "yum-changelog",
        "description": "yum-changelog(1) is a Yum plugin for viewing package changelogsbefore/after updating.yum will invoke yum-changelog(1) pluginif the --changelog option or the changelog command is used withyum.",
        "name": "yum-changelog",
        "section": 1
    },
    {
        "command": "yum-config-manager",
        "description": "yum-config-manager is a program that can manage main yumconfiguration options, toggle which repositories are enabled ordisabled, and add new repositories.Unless --add-repo is used, the program will output the currentconfiguration of the selected sections, and optionally save itback to the corresponding files.By default, if no positional arguments are specified, the programwill select the [main] section and each enabled repository.Youcan override this by specifying your own list of sections asarguments (these may also include disabled repositories).Asection can either be main or a repoid.",
        "name": "yum-config-manager - manage yum configuration options and yumrepositories",
        "section": 1
    },
    {
        "command": "yum-debug-dump",
        "description": "yum-debug-dump is a program which creates a gzipped filecontaining a lot of information useful to developers trying todebug a problem.By default it will output a file to the current working directorynamed yum_debug_dump-<hostname>-<time>.txt.gz. This file containsno private information but does contain a complete list of allpackages you have installed, all packages available in anyrepository, important configuration and system information.Youcan view this file using the 'zless' command.You can use the coresponding program yum-debug-restore to act onthis file and restore a set of packages (much like dump/restore).",
        "name": "yum-debug-dump - write system RPM configuration to a debug-dumpfile",
        "section": 1
    },
    {
        "command": "yum-debug-restore",
        "description": "yum-debug-restore is a program which takes a gzipped file createdby yum-debug-dump and acts on the information about installedpackages contained within.",
        "name": "yum-debug-restore - replay Yum transactions captured in a debug-dump file",
        "section": 1
    },
    {
        "command": "yum-filter-data",
        "description": "This plugin extends yum with some options, currently just for\"update\" and \"list update\" type commands, to allow filters to beplaced on which packages should be used based on the data inthose packages. Note that due to some of the data being unknown,and thus could possibly match, all unknown data is treated as amatch.",
        "name": "yum filter data plugin",
        "section": 1
    },
    {
        "command": "yum-fs-snapshot",
        "description": "yum-fs-snapshot(1) is a Yum plugin for taking snapshots of yourfilesystems before running a yum transaction.By default it willtake a snapshot of any filesystem that can be snapshotted, whichcurrently is limited to BTRFS filesystems.However, allfilesystems built on LVM logical volumes may be snapshotted atthe block level using LVM snapshots.LVM snapshot support isprovided for the purpose of system rollback.As such LVMsnapshots will only be created if the kernel supports the\"snapshot-merge\" DM target.",
        "name": "yum-fs-snapshot",
        "section": 1
    },
    {
        "command": "yum-groups-manager",
        "description": "yum-groups-manager is used to create or edit a group metadatafile for a yum repository. This is often much easier thanwriting/editing the XML by hand.The yum-groups-manager can loadan entire file of groups metadata and either create a new groupor edit an existing group and then write all of the groupsmetadata back out.",
        "name": "yum-groups-manager - create and edit yum's group metadata",
        "section": 1
    },
    {
        "command": "yum-list-data",
        "description": "This plugin extends yum for some commands that give aggregatepackage data based on lists of packagesadded yum commands are:* list-vendors* info-vendors* list-rpm-groups* info-rpm-groups* list-packagers* info-packagers* list-licenses* info-licenses* list-arches* info-arches* list-committers* info-committers* list-buildhosts* info-buildhosts* list-baseurls* info-baseurls* list-package-sizes* info-package-sizes* list-archive-sizes* info-archive-sizes* list-installed-sizes* info-installed-sizes* list-groups* info-groupsall of which take the same arguments as the list and info yumcommands. The difference between the list and info varieties isthat the info versions lists all the packages under eachaggregation.list-vendors, info-vendorsIs used to list the aggregate of the vendor attribute onthe packages, examples are \"Fedora Project\" and \"Red Hat,Inc.\".list-rpm-groups, info-rpm-groupsIs used to list the aggregate of the group attribute onthe packages, examples are \"Applications/System\",\"Development/Tools\" and \"System Environment/Base\"list-packagers, info-packagersIs used to list the aggregate of the packager attribute onthe packages, examples are \"Fedora Project\" and \"Red Hat,Inc.\".list-licenses, info-licensesIs used to list the aggregate of the license attribute onthe packages, examples are \"GPL\" and \"MIT\"list-arches, info-archesIs used to list the aggregate of the arch attribute on thepackages, examples are \"i386\" and \"x86_64\"list-committers, info-committersIs used to list the aggregate of the committer attributeon the packages, this is taken from the most recentchangelog entry of the package.list-buildhosts, info-buildhostsIs used to list the aggregate of the buildhost attributeon the packages, examples are \"mybuilder.example.com\" and\"xenbuilder1.fedora.redhat.com\"list-baseurls, info-baseurlsIs used to list the aggregate of the url attribute on thepackages after discarding the path of the URL, examplesare \"http://yum.baseurl.org/\" and \"http://www.and.org/\"list-package-sizes, info-package-sizesIs used to list the aggregate of specified ranges thepackagesize attribute on the packages, examples are \"[1B -10KB ]\" and \"[ 750KB -1MB ]\".list-archive-sizes, info-archive-sizesIs used to list the aggregate of specified ranges thearchivesize attribute on the packages, examples are \"[1B -10KB ]\" and \"[ 750KB -1MB ]\".list-installed-sizes, info-installed-sizesIs used to list the aggregate of specified ranges theinstalledsize attribute on the packages, examples are \"[1B -10KB ]\" and \"[ 750KB -1MB ]\".list-groups, info-groupsIs used to list the aggregate of the yum groups that thepackages are in, examples are in \"yum grouplist\". Notethat in yum groups a package can be in more than one groupat a time.It is worth noting that some of the above data can be \"unknown\",to yum, at which point a separate aggregation called \"-- Unknown--\" is listed.",
        "name": "yum list data plugin",
        "section": 1
    },
    {
        "command": "yum-ovl",
        "description": "Opening a file on OverlayFS in read-only mode causes the filefromlower layer to be opened, then later on, if the same file isopenedin write mode, a copy-up into the upperlayertakesplace,resulting into a new file being opened.Since yum(8) needs to open the RPMdb first read-only, and thenalso with write access, we need to copy-up the files beforehandtomake sure that the access is consistent.",
        "name": "yum-ovl - Performs an initial copy-up of yum(8) package database.",
        "section": 1
    },
    {
        "command": "yum-torrent",
        "description": "yum-torrent extends the behaviour of yum by downloading thenseeding packages via BitTorrent.Once installed and configured,the plugin works transparently.",
        "name": "yum-torrent - download packages via http and peer-to-peer",
        "section": 1
    },
    {
        "command": "yum-utils",
        "description": "yum-utils is a collection of tools and programs for managing yumrepositories, installing debug packages, source packages,extended information from repositories and administration.",
        "name": "yum-utils - tools for manipulating repositories and extendedpackage management",
        "section": 1
    },
    {
        "command": "yum-verify",
        "description": "This plugin extends yum with some commands that give verificationinformation on the installed system, much like rpm -V. You canchange how the verification is done and which files it appliesto. In case any mismatches are found, the exit status is set to1.added yum commands are:* verify* verify-rpm* verify-allall of which take the same arguments as the list yum command. Youcan only verify packages that are installed on the system.verify Is the generic verification command, and is intended togive the most useful output. It removes all false matchesdue to multilib and ignores changes to configuration filesby default.verify-rpmDoes the same checks as rpm -V.verify-allIs used to list all the differences, including some thatrpm itself will ignore.",
        "name": "yum verify plugin",
        "section": 1
    },
    {
        "command": "yum-versionlock",
        "description": "yum-versionlock(1) is a Yum plugin that takes a set ofname/versions for packages and excludes all other versions ofthose packages (including optionally following obsoletes).Thisallows you to protect packages from being updated by newerversions.The plugin provides a command \"versionlock\" which allows you toview and edit the list of locked packages easily.yum versionlock add <package-wildcard>...Add a versionlock for all of the packages in the rpmdbmatching the given wildcards.yum versionlock exclude <package-wildcard>...Opposite; disallow currently available versions of thepackages matching the given wildcards.yum versionlock listList the current versionlock entries.yum versionlock statusList any available updates that are currently blocked byversionlock.That is, for each entry in the lock list,print the newest package available in the repos unless itis the particular locked/excluded version.yum versionlock delete <entry-wildcard>...Remove any matching versionlock entries.yum versionlock clearRemove all versionlock entries.",
        "name": "yum-versionlock - Version lock rpm packages",
        "section": 1
    },
    {
        "command": "yumdownloader",
        "description": "yumdownloader is a program for downloading RPMs from Yumrepositories.",
        "name": "yumdownloader - download RPM packages from Yum repositories",
        "section": 1
    },
    {
        "command": "zenmap",
        "description": "Zenmap is a multi-platform graphical Nmap frontend and resultsviewer. Zenmap aims to make Nmap easy for beginners to use whilegiving experienced Nmap users advanced features. Frequently usedscans can be saved as profiles to make them easy to runrepeatedly. A command creator allows interactive creation of Nmapcommand lines. Scan results can be saved and viewed later. Savedscan results can be compared with one another to see how theydiffer. The results of recent scans are stored in a searchabledatabase.This man page only describes the few Zenmap command-line optionsand some critical notes. A much more detailed Zenmap User's Guideis available at https://nmap.org/book/zenmap.html . Otherdocumentation and information is available from the Zenmap webpage at https://nmap.org/zenmap/ .",
        "name": "zenmap - Graphical Nmap frontend and results viewer",
        "section": 1
    },
    {
        "command": "zsoelim",
        "description": "zsoelim parses file arguments, or if none are specified, itsstandard input for lines of the form:.so <filename>These requests are replaced by the contents of the filenamespecified.If the request cannot be met, zsoelim looks forfilename.ext where .ext can be one of .gz, .Z or .z.Otherextension types may be supported depending upon compile timeoptions.If the request can be met by a compressed file, thisfile is decompressed using an appropriate decompressor and itsoutput is used to satisfy the request.Traditionally, soelim programs were used to allow roffpreprocessors to be able to preprocess the files referred to bythe requests.This particular version was written to circumventproblems created by support for compressed manual pages.",
        "name": "zsoelim - satisfy .so requests in roff input",
        "section": 1
    },
    {
        "command": "30-systemd-environment-d-generator",
        "description": "systemd-environment-d-generator is asystemd.environment-generator(7) that reads environmentconfiguration specified by environment.d(5) configuration filesand passes it to the systemd(1) user manager instance.",
        "name": "systemd-environment-d-generator, 30-systemd-environment-d-generator - Load variables specified by environment.d",
        "section": 8
    },
    {
        "command": "BPF",
        "description": "Extended Berkeley Packet Filter ( eBPF ) and classic BerkeleyPacket Filter (originally known as BPF, for better distinctionreferred to as cBPF here) are both available as a fullyprogrammable and highly efficient classifier and actions. Theyboth offer a minimal instruction set for implementing smallprograms which can safely be loaded into the kernel and thusexecuted in a tiny virtual machine from kernel space. An in-kernel verifier guarantees that a specified program alwaysterminates and neither crashes nor leaks data from the kernel.In Linux, it's generally considered that eBPF is the successor ofcBPF.The kernel internally transforms cBPF expressions intoeBPF expressions and executes the latter. Execution of them canbe performed in an interpreter or at setup time, they can bejust-in-time compiled (JIT'ed) to run as native machine code.Currently, the eBPF JIT compiler is available for the followingarchitectures:*x86_64 (since Linux 3.18)*arm64 (since Linux 3.18)*s390 (since Linux 4.1)*ppc64 (since Linux 4.8)*sparc64 (since Linux 4.12)*mips64 (since Linux 4.13)*arm32 (since Linux 4.14)*x86_32 (since Linux 4.18)Whereas the following architectures have cBPF, but did not (yet)switch to eBPF JIT support:*ppc32*sparc32*mips32eBPF's instruction set has similar underlying principles as thecBPF instruction set, it however is modelled closer to theunderlying architecture to better mimic native instruction setswith the aim to achieve a better run-time performance. It isdesigned to be JIT'ed with a one to one mapping, which can alsoopen up the possibility for compilers to generate optimized eBPFcode through an eBPF backend that performs almost as fast asnatively compiled code. Given that LLVM provides such an eBPFbackend, eBPF programs can therefore easily be programmed in asubset of the C language. Other than that, eBPF infrastructurealso comes with a construct called \"maps\". eBPF maps arekey/value stores that are shared between multiple eBPF programs,but also between eBPF programs and user space applications.For the traffic control subsystem, classifier and actions thatcan be attached to ingress and egress qdiscs can be written ineBPF or cBPF. The advantage over other classifier and actions isthat eBPF/cBPF provides the generic framework, while users canimplement their highly specialized use cases efficiently. Thismeans that the classifier or action written that way will notsuffer from feature bloat, and can therefore execute its taskhighly efficient. It allows for non-linear classification andeven merging the action part into the classification. Combinedwith efficient eBPF map data structures, user space can push newpolicies like classids into the kernel without reloading aclassifier, or it can gather statistics that are pushed into onemap and use another one for dynamically load balancing trafficbased on the determined load, just to provide a few examples.",
        "name": "BPF - BPF programmable classifier and actions for ingress/egressqueueing disciplines",
        "section": 8
    },
    {
        "command": "CAKE",
        "description": "CAKE (Common Applications Kept Enhanced) is a shaping-capablequeue discipline which uses both AQM and FQ.It combines COBALT,which is an AQM algorithm combining Codel and BLUE, a shaperwhich operates in deficit mode, and a variant of DRR++ for flowisolation.8-way set-associative hashing is used to virtuallyeliminate hash collisions.Priority queuing is available througha simplified diffserv implementation.Overhead compensation forvarious encapsulation schemes is tightly integrated.All settings are optional; the default settings are chosen to besensible in most common deployments.Most people will only needto set the bandwidth parameter to get useful results, but readingthe Overhead Compensation and Round Trip Time sections isstrongly encouraged.",
        "name": "CAKE - Common Applications Kept Enhanced (CAKE)",
        "section": 8
    },
    {
        "command": "CBQ",
        "description": "Class Based Queueing is a classful qdisc that implements a richlinksharing hierarchy of classes. It contains shaping elements aswell as prioritizing capabilities. Shaping is performed usinglink idle time calculations based on the timing of dequeue eventsand underlying link bandwidth.",
        "name": "CBQ - Class Based Queueing",
        "section": 8
    },
    {
        "command": "CBS",
        "description": "The CBS (Credit Based Shaper) qdisc implements the shapingalgorithm defined by the IEEE 802.1Q-2014 Section 8.6.8.2, whichapplies a well defined rate limiting method to the traffic.This queueing discipline is intended to be used by TSN (TimeSensitive Networking) applications, the CBS parameters arederived directly by what is described by the Annex L of the IEEE802.1Q-2014 Specification. The algorithm and how it affects thelatency are detailed there.CBS is meant to be installed under another qdisc that maps packetflows to traffic classes, one example is mqprio(8).",
        "name": "CBS - Credit Based Shaper (CBS) Qdisc",
        "section": 8
    },
    {
        "command": "CoDel",
        "description": "CoDel (pronounced \"coddle\") is an adaptive \"no-knobs\" activequeue management algorithm (AQM) scheme that was developed toaddress the shortcomings of RED and its variants. It wasdeveloped with the following goals in mind:o It should be parameterless.o It should keep delays low while permitting bursts of traffic.o It should control delay.o It should adapt dynamically to changing link rates with noimpact on utilization.o It should be simple and efficient and should scale from simpleto complex routers.",
        "name": "CoDel - Controlled-Delay Active Queue Management algorithm",
        "section": 8
    },
    {
        "command": "ETF",
        "description": "The ETF (Earliest TxTime First) qdisc allows applications tocontrol the instant when a packet should be dequeued from thetraffic control layer into the netdevice. If offload isconfigured and supported by the network interface card, the itwill also control when packets leave the network controller.ETF achieves that by buffering packets until a configurable timebefore their transmission time (i.e. txtime, or deadline), whichcan be configured through the delta option.The qdisc uses a rb-tree internally so packets are always'ordered' by their txtime and will be dequeued following the(next) earliest txtime first.It relies on the SO_TXTIME socket option and the SCM_TXTIME CMSGin each packet field to configure the behavior of time dependentsockets: the clockid to be used as a reference, if the expectedmode of txtime for that socket is deadline or strict mode, and ifpacket drops should be reported on the socket's error queue. Seesocket(7) for more information.The etf qdisc will drop any packets with a txtime in the past, orif a packet expires while waiting for being dequeued.This queueing discipline is intended to be used by TSN (TimeSensitive Networking) applications, and it exposes a trafficshaping functionality that is commonly documented as \"LaunchTime\" or \"Time-Based Scheduling\" by vendors and the documentationof network interface controllers.ETF is meant to be installed under another qdisc that maps packetflows to traffic classes, one example is mqprio(8).",
        "name": "ETF - Earliest TxTime First (ETF) Qdisc",
        "section": 8
    },
    {
        "command": "ETS",
        "description": "The Enhanced Transmission Selection scheduler is a classfulqueuing discipline that merges functionality of PRIO and DRRqdiscs in one scheduler. ETS makes it easy to configure a set ofstrict and bandwidth-sharing bands to implement the transmissionselection described in 802.1Qaz.On creation with 'tc qdisc add', a fixed number of bands iscreated. Each band is a class, although it is not possible todirectly add and remove bands with 'tc class' commands. Thenumber of bands to be created must instead be specified on thecommand line as the qdisc is added.The minor number of classid to use when referring to a band isthe band number increased by one. Thus band 0 will have classidof major:1, band 1 that of major:2, etc.ETS bands are of two types: some number may be in strict mode,the remaining ones are in bandwidth-sharing mode.",
        "name": "ETS - Enhanced Transmission Selection scheduler",
        "section": 8
    },
    {
        "command": "FQ",
        "description": "FQ (Fair Queue) is a classless packet scheduler meant to bemostly used for locally generated traffic.It is designed toachieve per flow pacing.FQ does flow separation, and is able torespect pacing requirements set by TCP stack.All packetsbelonging to a socket are considered as a 'flow'.For non localpackets (router workload), packet hash is used as fallback.An application can specify a maximum pacing rate using theSO_MAX_PACING_RATE setsockopt call.This packet scheduler addsdelay between packets to respect rate limitation set on eachsocket. Note that after linux-4.20, linux adopted EDT (EarliestDeparture Time) and TCP directly sets the appropriate DepartureTime for each skb.Dequeueing happens in a round-robin fashion.A special FIFOqueue is reserved for high priority packets ( TC_PRIO_CONTROLpriority), such packets are always dequeued first.FQ is non-work-conserving.TCP pacing is good for flows having idle times, as the congestionwindow permits TCP stack to queue a possibly large number ofpackets.This removes the 'slow start after idle' choice, badlyhitting large BDP flows and applications delivering chunks ofdata such as video streams.",
        "name": "FQ - Fair Queue traffic policing",
        "section": 8
    },
    {
        "command": "FQ-PIE",
        "description": "FQ-PIE (Flow Queuing with Proportional Integral controllerEnhanced) is a queuing discipline that combines Flow Queuing withthe PIE AQM scheme. FQ-PIE uses a Jenkins hash function toclassify incoming packets into different flows and is used toprovide a fair share of the bandwidth to all the flows using theqdisc. Each such flow is managed by the PIE algorithm.",
        "name": "FQ-PIE - Flow Queue Proportional Integral controller Enhanced",
        "section": 8
    },
    {
        "command": "HFSC",
        "description": null,
        "name": "HFSC - Hierarchical Fair Service Curve's control under linux",
        "section": 8
    },
    {
        "command": "HTB",
        "description": "HTB is meant as a more understandable and intuitive replacementfor the CBQ qdisc in Linux. Both CBQ and HTB help you to controlthe use of the outbound bandwidth on a given link. Both allow youto use one physical link to simulate several slower links and tosend different kinds of traffic on different simulated links. Inboth cases, you have to specify how to divide the physical linkinto simulated links and how to decide which simulated link touse for a given packet to be sent.Unlike CBQ, HTB shapes traffic based on the Token Bucket Filteralgorithm which does not depend on interface characteristics andso does not need to know the underlying bandwidth of the outgoinginterface.",
        "name": "HTB - Hierarchy Token Bucket",
        "section": 8
    },
    {
        "command": "IFE",
        "description": "The ife action allows for a sending side to encapsulate arbitrarymetadata, which is then decapsulated by the receiving end. Thesender runs in encoding mode and the receiver in decode mode.Both sender and receiver must specify the same ethertype. In thefuture, a registered ethertype may be available as a default.",
        "name": "IFE - encapsulate/decapsulate metadata",
        "section": 8
    },
    {
        "command": "MQPRIO",
        "description": "The MQPRIO qdisc is a simple queuing discipline that allowsmapping traffic flows to hardware queue ranges using prioritiesand a configurable priority to traffic class mapping. A trafficclass in this context is a set of contiguous qdisc classes whichmap 1:1 to a set of hardware exposed queues.By default the qdisc allocates a pfifo qdisc (packet limitedfirst in, first out queue) per TX queue exposed by the lowerlayer device. Other queuing disciplines may be addedsubsequently. Packets are enqueued using the map parameter andhashed across the indicated queues in the offset and count.Bydefault these parameters are configured by the hardware driver tomatch the hardware QOS structures.Channel mode supports full offload of the mqprio options, thetraffic classes, the queue configurations and QOS attributes tothe hardware. Enabled hardware can provide hardware QOS with theability to steer traffic flows to designated traffic classesprovided by this qdisc. Hardware based QOS is configured usingthe shaper parameter.bw_rlimit with minimum and maximumbandwidth rates can be used for setting transmission rates oneach traffic class. Also further qdiscs may be added to theclasses of MQPRIO to create more complex configurations.",
        "name": "MQPRIO - Multiqueue Priority Qdisc (Offloaded Hardware QOS)",
        "section": 8
    },
    {
        "command": "PAM",
        "description": "This manual is intended to offer a quick introduction toLinux-PAM. For more information the reader is directed to theLinux-PAM system administrators' guide.Linux-PAM is a system of libraries that handle the authenticationtasks of applications (services) on the system. The libraryprovides a stable general interface (Application ProgrammingInterface - API) that privilege granting programs (such aslogin(1) and su(1)) defer to to perform standard authenticationtasks.The principal feature of the PAM approach is that the nature ofthe authentication is dynamically configurable. In other words,the system administrator is free to choose how individualservice-providing applications will authenticate users. Thisdynamic configuration is set by the contents of the singleLinux-PAM configuration file /etc/pam.conf. Alternatively, theconfiguration can be set by individual configuration fileslocated in the /etc/pam.d/ directory. The presence of thisdirectory will cause Linux-PAM to ignore /etc/pam.conf.Vendor-supplied PAM configuration files might be installed in thesystem directory /usr/lib/pam.d/ or a configurable vendorspecific directory instead of the machine configuration directory/etc/pam.d/. If no machine configuration file is found, thevendor-supplied file is used. All files in /etc/pam.d/ overridefiles with the same name in other directories.From the point of view of the system administrator, for whom thismanual is provided, it is not of primary importance to understandthe internal behavior of the Linux-PAM library. The importantpoint to recognize is that the configuration file(s) define theconnection between applications (services) and the pluggableauthentication modules (PAMs) that perform the actualauthentication tasks.Linux-PAM separates the tasks of authentication into fourindependent management groups: account management; authenticationmanagement; password management; and session management. (Wehighlight the abbreviations used for these groups in theconfiguration file.)Simply put, these groups take care of different aspects of atypical user's request for a restricted service:account - provide account verification types of service: has theuser's password expired?; is this user permitted access to therequested service?authentication - authenticate a user and set up user credentials.Typically this is via some challenge-response request that theuser must satisfy: if you are who you claim to be please enteryour password. Not all authentications are of this type, thereexist hardware based authentication schemes (such as the use ofsmart-cards and biometric devices), with suitable modules, thesemay be substituted seamlessly for more standard approaches toauthentication - such is the flexibility of Linux-PAM.password - this group's responsibility is the task of updatingauthentication mechanisms. Typically, such services are stronglycoupled to those of the auth group. Some authenticationmechanisms lend themselves well to being updated with such afunction. Standard UN*X password-based access is the obviousexample: please enter a replacement password.session - this group of tasks cover things that should be doneprior to a service being given and after it is withdrawn. Suchtasks include the maintenance of audit trails and the mounting ofthe user's home directory. The session management group isimportant as it provides both an opening and closing hook formodules to affect the services available to a user.",
        "name": "PAM, pam - Pluggable Authentication Modules for Linux",
        "section": 8
    },
    {
        "command": "PIE",
        "description": "Proportional Integral controller-Enhanced (PIE) is a controltheoretic active queue management scheme. It is based on theproportional integral controller but aims to control delay. Themain design goals areo Low latency controlo High link utilizationo Simple implementationo Guaranteed stability and fast responsiveness",
        "name": "PIE - Proportional Integral controller-Enhanced AQM algorithm",
        "section": 8
    },
    {
        "command": "PRIO",
        "description": "The PRIO qdisc is a simple classful queueing discipline thatcontains an arbitrary number of classes of differing priority.The classes are dequeued in numerical descending order ofpriority. PRIO is a scheduler and never delays packets - it is awork-conserving qdisc, though the qdiscs contained in the classesmay not be.Very useful for lowering latency when there is no need forslowing down traffic.",
        "name": "PRIO - Priority qdisc",
        "section": 8
    },
    {
        "command": "SELinux",
        "description": "NSA Security-Enhanced Linux (SELinux) is an implementation of aflexible mandatory access control architecture in the Linuxoperating system.The SELinux architecture provides generalsupport for the enforcement of many kinds of mandatory accesscontrol policies, including those based on the concepts of TypeEnforcement\u00ae, Role- Based Access Control, and Multi-LevelSecurity.Background information and technical documentationabout SELinux can be found at https://github.com/SELinuxProject.The /etc/selinux/config configuration file controls whetherSELinux is enabled or disabled, and if enabled, whether SELinuxoperates in permissive mode or enforcing mode.The SELINUXvariable may be set to any one of disabled, permissive, orenforcing to select one of these options.The disabled disablesmost of the SELinux kernel and application code, leaving thesystem running without any SELinux protection.The permissiveoption enables the SELinux code, but causes it to operate in amode where accesses that would be denied by policy are permittedbut audited.The enforcing option enables the SELinux code andcauses it to enforce access denials as well as auditing them.permissive mode may yield a different set of denials thanenforcing mode, both because enforcing mode will prevent anoperation from proceeding past the first denial and because someapplication code will fall back to a less privileged mode ofoperation if denied access.NOTE: Disabling SELinux by setting SELINUX=disabled in/etc/selinux/config is deprecated and depending on kernel versionand configuration it might not lead to SELinux being completelydisabled.Specifically, the SELinux hooks will still be executedinternally, but the SELinux policy will not be loaded and nooperation will be denied.In such state, the system will act asif SELinux was disabled, although some operations might behaveslightly differently.To properly disable SELinux, it isrecommended to use the selinux=0 kernel boot option instead.Inthat case SELinux will be disabled regardless of what is set inthe /etc/selinux/config file.The /etc/selinux/config configuration file also controls whatpolicy is active on the system.SELinux allows for multiplepolicies to be installed on the system, but only one policy maybe active at any given time.At present, multiple kinds ofSELinux policy exist: targeted, mls for example.The targetedpolicy is designed as a policy where most user processes operatewithout restrictions, and only specific services are placed intodistinct security domains that are confined by the policy.Forexample, the user would run in a completely unconfined domainwhile the named daemon or apache daemon would run in a specificdomain tailored to its operation.The MLS (Multi-Level Security)policy is designed as a policy where all processes arepartitioned into fine-grained security domains and confined bypolicy.MLS also supports the Bell And LaPadula model, whereprocesses are not only confined by the type but also the level ofthe data.You can define which policy you will run by setting theSELINUXTYPE environment variable within /etc/selinux/config.Youmust reboot and possibly relabel if you change the policy type tohave it take effect on the system.The corresponding policyconfiguration for each such policy must be installed in the/etc/selinux/{SELINUXTYPE}/ directories.A given SELinux policy can be customized further based on a setof compile-time tunable options and a set of runtime policybooleans.system-config-selinux allows customization of thesebooleans and tunables.Many domains that are protected by SELinux also include SELinuxman pages explaining how to customize their policy.",
        "name": "SELinux - NSA Security-Enhanced Linux (SELinux)",
        "section": 8
    },
    {
        "command": "TAPRIO",
        "description": "The TAPRIO qdisc implements a simplified version of thescheduling state machine defined by IEEE 802.1Q-2018 Section8.6.9, which allows configuration of a sequence of gate states,where each gate state allows outgoing traffic for a subset(potentially empty) of traffic classes.How traffic is mapped to different hardware queues is similar tomqprio(8) and so the map and queues parameters have the samemeaning.The other parameters specify the schedule, and at what point intime it should start (it can behave as the schedule started inthe past).",
        "name": "TAPRIO - Time Aware Priority Shaper",
        "section": 8
    },
    {
        "command": "accessdb",
        "description": "accessdb will output the data contained within a man-db databasein a human readable form.By default, it will dump the data from/var/cache/man/index.<db-type>, where <db-type> is dependent onthe database library in use.Supplying an argument to accessdb will override this default.",
        "name": "accessdb - dumps the content of a man-db database in a humanreadable format",
        "section": 8
    },
    {
        "command": "accton",
        "description": "accton filename turns on process accounting.",
        "name": "accton -turns process accounting on or off",
        "section": 8
    },
    {
        "command": "actions",
        "description": "The actions object in tc allows a user to define actionsindependently of a classifier (filter). These actions can then beassigned to one or more filters, with any packets matching theclassifier's criteria having that action performed on them.Each action type (mirred, police, etc.) will have its own tableto store all created actions.",
        "name": "actions - independently defined actions in tc",
        "section": 8
    },
    {
        "command": "addpart",
        "description": "addpart tells the Linux kernel about the existence of thespecified partition. The command is a simple wrapper around the\"add partition\" ioctl.This command doesn\u2019t manipulate partitions on a block device.",
        "name": "addpart - tell the kernel about the existence of a partition",
        "section": 8
    },
    {
        "command": "agetty",
        "description": "agetty opens a tty port, prompts for a login name and invokes the/bin/login command. It is normally invoked by init(8).agetty has several non-standard features that are useful forhardwired and for dial-in lines:\u2022Adapts the tty settings to parity bits and to erase, kill,end-of-line and uppercase characters when it reads a loginname. The program can handle 7-bit characters with even, odd,none or space parity, and 8-bit characters with no parity.The following special characters are recognized: Control-U(kill); DEL and backspace (erase); carriage return and linefeed (end of line). See also the --erase-chars and--kill-chars options.\u2022Optionally deduces the baud rate from the CONNECT messagesproduced by Hayes(tm)-compatible modems.\u2022Optionally does not hang up when it is given an alreadyopened line (useful for call-back applications).\u2022Optionally does not display the contents of the /etc/issuefile.\u2022Optionally displays an alternative issue files or directoriesinstead of /etc/issue or /etc/issue.d.\u2022Optionally does not ask for a login name.\u2022Optionally invokes a non-standard login program instead of/bin/login.\u2022Optionally turns on hardware flow control.\u2022Optionally forces the line to be local with no need forcarrier detect.This program does not use the /etc/gettydefs (System V) or/etc/gettytab (SunOS 4) files.",
        "name": "agetty - alternative Linux getty",
        "section": 8
    },
    {
        "command": "anacron",
        "description": "Anacron is used to execute commands periodically, with afrequency specified in days.Unlike cron(8), it does not assumethat the machine is running continuously.Hence, it can be usedon machines that are not running 24 hours a day to controlregular jobs as daily, weekly, and monthly jobs.Anacron reads a list of jobs from the /etc/anacrontabconfiguration file (see anacrontab(5)).This file contains thelist of jobs that Anacron controls.Each job entry specifies aperiod in days, a delay in minutes, a unique job identifier, anda shell command.For each job, Anacron checks whether this job has been executedin the last n days, where n is the time period specified for thatjob.If a job has not been executed in n days or more, Anacronruns the job's shell command, after waiting for the number ofminutes specified as the delay parameter.After the command exits, Anacron records the date (excludes thehour) in a special timestamp file for that job, so it knows whento execute that job again.When there are no more jobs to be run, Anacron exits.Anacron only considers jobs whose identifier, as specified inanacrontab(5), matches any of the job command-line arguments.The job command-line arguments can be represented by shellwildcard patterns (be sure to protect them from your shell withadequate quoting).Specifying no job command-line arguments isequivalent to specifying \"*\"(that is, all jobs are consideredby Anacron).Unless Anacron is run with the -d option (specified below), itforks to the background when it starts, and any parent processesexit immediately.Unless Anacron is run with the -s or -n options, it starts jobsimmediately when their delay is over.The execution of differentjobs is completely independent.If an executed job generates any output to standard output or tostandard error, the output is mailed to the user under whomAnacron is running (usually root), or to the address specified inthe MAILTO environment variable in the /etc/anacrontab file, ifsuch exists.If the LOGNAME environment variable is set, it isused in the From: field of the mail.Any informative messages generated by Anacron are sent tosyslogd(8) or rsyslogd(8) under with facility set to cron andpriority set to notice.Any error messages are sent with thepriority error.\"Active\" jobs (i.e., jobs that Anacron already decided to run andare now waiting for their delay to pass, and jobs that arecurrently being executed by Anacron), are \"locked\", so that othercopies of Anacron cannot run them at the same time.",
        "name": "anacron - runs commands periodically",
        "section": 8
    },
    {
        "command": "and",
        "description": "ovn-northd is a centralized daemon responsible for translatingthe high-level OVN configuration into logical configurationconsumable by daemons such as ovn-controller. It translates thelogical network configuration in terms of conventional networkconcepts, taken from the OVN Northbound Database (see ovn-nb(5)),into logical datapath flows in the OVN Southbound Database (seeovn-sb(5)) below it.ovn-northd is implemented in C. ovn-northd-ddlog is a compatibleimplementation written in DDlog, a language for incrementaldatabase processing. This documentation applies to bothimplementations, with differences indicated where relevant.",
        "name": "ovn-northd and ovn-northd-ddlog - Open Virtual Network centralcontrol daemon",
        "section": 8
    },
    {
        "command": "arp",
        "description": "Arp manipulates or displays the kernel's IPv4 network neighbourcache. It can add entries to the table, delete one or display thecurrent content.ARP stands for Address Resolution Protocol, which is used to findthe media access control address of a network neighbour for agiven IPv4 Address.",
        "name": "arp - manipulate the system ARP cache",
        "section": 8
    },
    {
        "command": "arpd",
        "description": "The arpd daemon collects gratuitous ARP information, saving it onlocal disk and feeding it to the kernel on demand to avoidredundant broadcasting due to limited size of the kernel ARPcache.",
        "name": "arpd - userspace arp daemon.",
        "section": 8
    },
    {
        "command": "arping",
        "description": "Ping destination on device interface by ARP packets, using sourceaddress source.arping supports IPv4 addresses only. For IPv6, see ndisc6(8).",
        "name": "arping - send ARP REQUEST to a neighbour host",
        "section": 8
    },
    {
        "command": "arptables",
        "description": "arptables is a user space tool, it is used to set up and maintainthe tables of ARP rules in the Linux kernel. These rules inspectthe ARP frames which they see.arptables is analogous to theiptables user space tool, but arptables is less complicated.CHAINSThe kernel table is used to divide functionality into differentsets of rules. Each set of rules is called a chain.Each chainis an ordered list of rules that can match ARP frames. If a rulematches an ARP frame, then a processing specification tells whatto do with that matching frame. The processing specification iscalled a 'target'. However, if the frame does not match thecurrent rule in the chain, then the next rule in the chain isexamined and so forth.The user can create new (user-defined)chains which can be used as the 'target' of a rule.TARGETSA firewall rule specifies criteria for an ARP frame and a frameprocessing specification called a target.When a frame matches arule, then the next action performed by the kernel is specifiedby the target.The target can be one of these values: ACCEPT,DROP, CONTINUE, RETURN, an 'extension' (see below) or a user-defined chain.ACCEPT means to let the frame through.DROP means the frame hasto be dropped.CONTINUE means the next rule has to be checked.This can be handy to know how many frames pass a certain point inthe chain or to log those frames.RETURN means stop traversingthis chain and resume at the next rule in the previous (calling)chain.For the extension targets please see the TARGETEXTENSIONS section of this man page.TABLESThere is only one ARP table in the Linux kernel.The table isfilter.You can drop the '-t filter' argument to the arptablescommand.The -t argument must be the first argument on thearptables command line, if used.-t, --tablefilter, is the only table and contains two built-inchains: INPUT (for frames destined for the host) andOUTPUT (for locally-generated frames).",
        "name": "arptables - ARP table administration (nft-based)",
        "section": 8
    },
    {
        "command": "arptables-nft",
        "description": "arptables is a user space tool, it is used to set up and maintainthe tables of ARP rules in the Linux kernel. These rules inspectthe ARP frames which they see.arptables is analogous to theiptables user space tool, but arptables is less complicated.CHAINSThe kernel table is used to divide functionality into differentsets of rules. Each set of rules is called a chain.Each chainis an ordered list of rules that can match ARP frames. If a rulematches an ARP frame, then a processing specification tells whatto do with that matching frame. The processing specification iscalled a 'target'. However, if the frame does not match thecurrent rule in the chain, then the next rule in the chain isexamined and so forth.The user can create new (user-defined)chains which can be used as the 'target' of a rule.TARGETSA firewall rule specifies criteria for an ARP frame and a frameprocessing specification called a target.When a frame matches arule, then the next action performed by the kernel is specifiedby the target.The target can be one of these values: ACCEPT,DROP, CONTINUE, RETURN, an 'extension' (see below) or a user-defined chain.ACCEPT means to let the frame through.DROP means the frame hasto be dropped.CONTINUE means the next rule has to be checked.This can be handy to know how many frames pass a certain point inthe chain or to log those frames.RETURN means stop traversingthis chain and resume at the next rule in the previous (calling)chain.For the extension targets please see the TARGETEXTENSIONS section of this man page.TABLESThere is only one ARP table in the Linux kernel.The table isfilter.You can drop the '-t filter' argument to the arptablescommand.The -t argument must be the first argument on thearptables command line, if used.-t, --tablefilter, is the only table and contains two built-inchains: INPUT (for frames destined for the host) andOUTPUT (for locally-generated frames).",
        "name": "arptables - ARP table administration (nft-based)",
        "section": 8
    },
    {
        "command": "arptables-nft-restore",
        "description": "arptables-restore is used to restore ARP Tables from dataspecified on STDIN or via a file as first argument.Use I/Oredirection provided by your shell to read from a filearptables-restoreflushes (deletes) all previous contents of the respectiveARP Table.",
        "name": "arptables-restore - Restore ARP Tables (nft-based)",
        "section": 8
    },
    {
        "command": "arptables-nft-save",
        "description": "arptables-save is used to dump the contents of an ARP Table ineasily parseable format to STDOUT. Use I/O-redirection providedby your shell to write to a file.-M, --modprobe modprobe_programSpecify the path to the modprobe program. By default,arptables-save will inspect /proc/sys/kernel/modprobe todetermine the executable's path.-c, --countersInclude the current values of all packet and byte countersin the output.-V, --versionPrint version information and exit.",
        "name": "arptables-save - dump arptables rules to stdout (nft-based)",
        "section": 8
    },
    {
        "command": "arptables-restore",
        "description": "arptables-restore is used to restore ARP Tables from dataspecified on STDIN or via a file as first argument.Use I/Oredirection provided by your shell to read from a filearptables-restoreflushes (deletes) all previous contents of the respectiveARP Table.",
        "name": "arptables-restore - Restore ARP Tables (nft-based)",
        "section": 8
    },
    {
        "command": "arptables-save",
        "description": "arptables-save is used to dump the contents of an ARP Table ineasily parseable format to STDOUT. Use I/O-redirection providedby your shell to write to a file.-M, --modprobe modprobe_programSpecify the path to the modprobe program. By default,arptables-save will inspect /proc/sys/kernel/modprobe todetermine the executable's path.-c, --countersInclude the current values of all packet and byte countersin the output.-V, --versionPrint version information and exit.",
        "name": "arptables-save - dump arptables rules to stdout (nft-based)",
        "section": 8
    },
    {
        "command": "astraceroute",
        "description": "astraceroute is a small utility to retrieve path information in atraceroute like way, but with additional geographical locationinformation. It tracks the route of a packet from the local hostto the remote host by successively increasing the IP's TTL field,starting from 1, in the hope that each intermediate node willsend an ICMP TIME_EXCEEDED notification back to the local hostwhen the TTL value is decremented to 0.astraceroute supports IPv4 and IPv6 queries and will displaycountry and city information, if available, the AS number the hopbelongs to, and its ISP name. astraceroute also displays timinginformation and reverse DNS data.Due to astraceroute's configurability, it is also possible togather some more useful information about the hop regarding whatit does and does not allow to pass through. This is done by usingclear text strings for probing DPIs or ``great firewalls'' todetermine if they will filter out blocked critical keywords. Thistool might be a good start for further in-depth analysis of suchsystems.",
        "name": "astraceroute - autonomous system trace route utility",
        "section": 8
    },
    {
        "command": "audispd-zos-remote",
        "description": "audispd-zos-remote is a remote-auditing plugin for the Auditsubsystem. It should be started by the auditd(8) daemon and willforward all incoming audit events, as they happen, to aconfigured z/OS SMF (Service Management Facility) database,through an IBM Tivoli Directory Server (ITDS) set for RemoteAudit service.See SMF MAPPING section below for moreinformation about the resulting SMF record format.auditd(8) must be configured to start the plugin. This is done bya configuration file usually located at/etc/audit/plugins.d/audispd-zos-remote.conf, but multipleinstances can be spawned by having multiple configuration filesin /etc/audit/plugins.d for the same plugin executable (seeauditd(8)).Each instance needs a configuration file, located by default at/etc/audit/zos-remote.conf.Check zos-remote.conf(5) for detailsabout the plugin configuration.",
        "name": "audispd-zos-remote - z/OS Remote-services Audit dispatcher plugin",
        "section": 8
    },
    {
        "command": "auditctl",
        "description": "The auditctl program is used to configure kernel options relatedto auditing, to see status of the configuration, and to loaddiscretionary audit rules.",
        "name": "auditctl - a utility to assist controlling the kernel's auditsystem",
        "section": 8
    },
    {
        "command": "auditd",
        "description": "auditd is the userspace component to the Linux Auditing System.It's responsible for writing audit records to the disk. Viewingthe logs is done with the ausearch or aureport utilities.Configuring the audit system or loading rules is done with theauditctl utility. During startup, the rules in/etc/audit/audit.rules are read by auditctl and loaded into thekernel. Alternately, there is also an augenrules program thatreads rules located in /etc/audit/rules.d/ and compiles them intoan audit.rules file. The audit daemon itself has someconfiguration options that the admin may wish to customize. Theyare found in the auditd.conf file.",
        "name": "auditd - The Linux Audit daemon",
        "section": 8
    },
    {
        "command": "augenrules",
        "description": "augenrules is a script that merges all component audit rulesfiles, found in the audit rules directory, /etc/audit/rules.d,placing the merged file in /etc/audit/audit.rules. Componentaudit rule files, must end in .rules in order to be processed.All other files in /etc/audit/rules.d are ignored.The files are concatenated in order, based on their natural sort(see -v option of ls(1)) and stripped of empty and comment (#)lines.The last processed -D directive without an option, if present, isalways emitted as the first line in the resultant file. Thosewith an option are replicated in place.The last processed -bdirective, if present, is always emitted as the second line inthe resultant file.The last processed -f directive, if present,is always emitted as the third line in the resultant file.Thelast processed -e directive, if present, is always emitted as thelast line in the resultant file.The generated file is only copied to /etc/audit/audit.rules, ifit differs.",
        "name": "augenrules - a script that merges component audit rule files",
        "section": 8
    },
    {
        "command": "aureport",
        "description": "aureport is a tool that produces summary reports of the auditsystem logs. The aureport utility can also take input from stdinas long as the input is the raw log data. The reports have acolumn label at the top to help with interpretation of thevarious fields. Except for the main summary report, all reportshave the audit event number. You can subsequently lookup the fullevent with ausearch -a event number. You may need to specifystart & stop times if you get multiple hits. The reports producedby aureport can be used as building blocks for more complicatedanalysis.",
        "name": "aureport - a tool that produces summary reports of audit daemonlogs",
        "section": 8
    },
    {
        "command": "ausearch",
        "description": "ausearch is a tool that can query the audit daemon logs based forevents based on different search criteria. The ausearch utilitycan also take input from stdin as long as the input is the rawlog data. Each commandline option given forms an \"and\" statement.For example, searching with -m and -ui means return events thathave both the requested type and match the user id given. Anexception is the -mand -n options; multiple record types andnodes are allowed in a search which will return any matching nodeand record.It should also be noted that each syscall excursion from userspace into the kernel and back into user space has one event IDthat is unique. Any auditable event that is triggered during thistrip share this ID so that they may be correlated.Different parts of the kernel may add supplemental records. Forexample, an audit event on the syscall \"open\" will also cause thekernel to emit a PATH record with the file name. The ausearchutility will present all records that make up one event together.This could mean that even though you search for a specific kindof record, the resulting events may contain SYSCALL records.Also be aware that not all record types have the requestedinformation. For example, a PATH record does not have a hostnameor a loginuid.",
        "name": "ausearch - a tool to query audit daemon logs",
        "section": 8
    },
    {
        "command": "autofs",
        "description": "autofs controls the operation of the automount(8) daemon(s)running on the Linux system. Usually autofs is invoked at systemboot time with the start parameter and at shutdown time with thestop parameter. Service control actions can also be manuallyinvoked by the system administrator to shut down, restart, reloador obtain service status.",
        "name": "autofs - Service control for the automounter",
        "section": 8
    },
    {
        "command": "automount",
        "description": "The automount program is used to manage mount points for autofs,the inlined Linux automounter.automount works by reading theauto.master(5) map and sets up mount points for each entry in themaster map allowing them to be automatically mounted whenaccessed. The file systems are then automatically umounted aftera period of inactivity.",
        "name": "automount - manage autofs mount points",
        "section": 8
    },
    {
        "command": "autrace",
        "description": "autrace is a program that will add the audit rules to trace aprocess similar to strace. It will then execute the programpassing arguments to it. The resulting audit information will bein the audit logs if the audit daemon is running or syslog. Thiscommand deletes all audit rules prior to executing the targetprogram and after executing it. As a safety precaution, it willnot run unless all rules are deleted with auditctl prior to use.",
        "name": "autrace - a program similar to strace",
        "section": 8
    },
    {
        "command": "avcstat",
        "description": "Display SELinux AVC statistics.If the interval parameter isspecified, the program will loop, displaying updated statisticsevery interval seconds.Relative values are displayed bydefault.",
        "name": "avcstat - Display SELinux AVC statistics",
        "section": 8
    },
    {
        "command": "badblocks",
        "description": "badblocks is used to search for bad blocks on a device (usually adisk partition).device is the special file corresponding to thedevice (e.g /dev/hdc1).last_block is the last block to bechecked; if it is not specified, the last block on the device isused as a default.first_block is an optional parameterspecifying the starting block number for the test, which allowsthe testing to start in the middle of the disk.If it is notspecified the first block on the disk is used as a default.Important note: If the output of badblocks is going to be fed tothe e2fsck or mke2fs programs, it is important that the blocksize is properly specified, since the block numbers which aregenerated are very dependent on the block size in use by the filesystem.For this reason, it is strongly recommended that usersnot run badblocks directly, but rather use the -c option of thee2fsck and mke2fs programs.",
        "name": "badblocks - search a device for bad blocks",
        "section": 8
    },
    {
        "command": "basic",
        "description": "The basic filter allows one to classify packets using theextended match infrastructure.",
        "name": "basic - basic traffic control filter",
        "section": 8
    },
    {
        "command": "bfifo",
        "description": "The pfifo and bfifo qdiscs are unadorned First In, First Outqueues. They are the simplest queues possible and therefore haveno overhead.pfifo constrains the queue size as measured inpackets.bfifo does so as measured in bytes.Like all non-default qdiscs, they maintain statistics. This mightbe a reason to prefer pfifo or bfifo over the default.",
        "name": "pfifo - Packet limited First In, First Out queuebfifo - Byte limited First In, First Out queue",
        "section": 8
    },
    {
        "command": "blkdeactivate",
        "description": "The blkdeactivate utility deactivates block devices. For mountedblock devices, it attempts to unmount it automatically beforetrying to deactivate. The utility currently supports device-mapper devices (DM), including LVM volumes and software RAID MDdevices. LVM volumes are handled directly using the lvm(8)command, the rest of device-mapper based devices are handledusing the dmsetup(8) command.MD devices are handled using themdadm(8) command.",
        "name": "blkdeactivate \u2014 utility to deactivate block devices",
        "section": 8
    },
    {
        "command": "blkdiscard",
        "description": "blkdiscard is used to discard device sectors. This is useful forsolid-state drivers (SSDs) and thinly-provisioned storage. Unlikefstrim(8), this command is used directly on the block device.By default, blkdiscard will discard all blocks on the device.Options may be used to modify this behavior based on range orsize, as explained below.The device argument is the pathname of the block device.WARNING: All data in the discarded region on the device will belost!",
        "name": "blkdiscard - discard sectors on a device",
        "section": 8
    },
    {
        "command": "blkid",
        "description": "The blkid program is the command-line interface to working withthe libblkid(3) library. It can determine the type of content(e.g., filesystem or swap) that a block device holds, and alsothe attributes (tokens, NAME=value pairs) from the contentmetadata (e.g., LABEL or UUID fields).It is recommended to use lsblk(8) command to get informationabout block devices, or lsblk --fs to get an overview offilesystems, or findmnt(8) to search in already mountedfilesystems.lsblk(8) provides more information, better control onoutput formatting, easy to use in scripts and it doesnot require root permissions to get actual information.blkid reads information directly from devices and fornon-root users it returns cached unverified information.blkid is mostly designed for system services and to testlibblkid(3) functionality.When device is specified, tokens from only this device aredisplayed. It is possible to specify multiple device arguments onthe command line. If none is given, all partitions orunpartitioned devices which appear in /proc/partitions are shown,if they are recognized.blkid has two main forms of operation: either searching for adevice with a specific NAME=value pair, or displaying NAME=valuepairs for one or more specified devices.For security reasons blkid silently ignores all devices where theprobing result is ambivalent (multiple colliding filesystems aredetected). The low-level probing mode (-p) provides moreinformation and extra exit status in this case. It\u2019s recommendedto use wipefs(8) to get a detailed overview and to erase obsoletestuff (magic strings) from the device.",
        "name": "blkid - locate/print block device attributes",
        "section": 8
    },
    {
        "command": "blkiomon",
        "description": "blkiomon is a block device I/O monitor. It periodically generatesper-device request size and request latency statistics fromblktrace data. It provides histograms as well as data that can beused to calculate min, max, average and variance. For thispurpose, it consumes D and C traces read from stdin.Note, thatthis doesn't work for logical volumes, as high-level driversdon't see the completion of the events (C).There are options for binary output and human-readable output tofiles and stdout. Output to a message queue is supported as well.There is no need to use blkparse with blkiomon. blkiomon iscapable of consuming binary output written to stdout by blktrace.",
        "name": "blkiomon - monitor block device I/O based o blktrace data",
        "section": 8
    },
    {
        "command": "blkmapd",
        "description": "The blkmapd daemon performs device discovery and mapping for theparallel NFS (pNFS) block layout client [RFC5663].The pNFS block layout protocol builds a complex storage hierarchyfrom a set of simple volumes.These simple volumes are addressedby content, using a signature on the volume to uniquely name eachone.The daemon locates a volume by examining each block devicein the system for the given signature.The topology typically consists of a hierarchy of volumes builtby striping, slicing, and concatenating the simple volumes.Theblkmapd daemon uses the device-mapper driver to construct logicaldevices that reflect the server topology, and passes thesedevices to the kernel for use by the pNFS block layout client.",
        "name": "blkmapd - pNFS block layout mapping daemon",
        "section": 8
    },
    {
        "command": "blkpr",
        "description": "blkpr is used to run persistent reservations command on devicethat supports Persistent Reservations feature.The device argument is the pathname of the block device.",
        "name": "blkpr - run persistent reservations command on a device",
        "section": 8
    },
    {
        "command": "blktrace",
        "description": "blktrace is a block layer IO tracing mechanism which providesdetailed information about request queue operations up to userspace. There are three major components: a kernel component, autility to record the i/o trace information for the kernel touser space, and utilities to analyse and view the traceinformation.This man page describes blktrace, which records thei/o event trace information for a specific block device to afile.The blktrace utility extracts event traces from the kernel (viathe relaying through the debug file system). Some backgrounddetails concerning the run-time behaviour of blktrace will helpto understand some of the more arcane command line options:- blktrace receives data from the kernel in buffers passed upthrough the debug file system (relay). Each device being tracedhas a file created in the mounted directory for the debugfs,which defaults to /sys/kernel/debug -- this can be overriddenwith the -r command line argument.- blktrace defaults to collecting all events that can be traced.To limit the events being captured, you can specify one or morefilter masks via the -a option.Alternatively, one may specify the entire mask utilising ahexadecimal value that is version-specific. (Requiresunderstanding of the internal representation of the filtermask.)- As noted above, the events are passed up via a series ofbuffers stored into debugfs files. The size and number ofbuffers can be specified via the -b and -n argumentsrespectively.- blktrace stores the extracted data into files stored in thelocal directory. The format of the file names is (by default)device.blktrace.cpu, where device is the base device name (e.g,if we are tracing /dev/sda, the base device name would be sda);and cpu identifies a CPU for the event stream.The device portion of the event file name can be changed viathe -o option.- blktrace may also be run concurrently with blkparse to producelive output -- to do this specify -o - for blktrace.- The default behaviour for blktrace is to run forever untilexplicitly killed by the user (via a control-C, or sendingSIGINT signal to the process via invocation the kill (1)utility). Also you can specify a run-time duration for blktracevia the -w option -- then blktrace will run for the specifiednumber of seconds, and then halt.",
        "name": "blktrace - generate traces of the i/o traffic on block devices",
        "section": 8
    },
    {
        "command": "blkzone",
        "description": "blkzone is used to run zone command on device that support theZoned Block Commands (ZBC) or Zoned-device ATA Commands (ZAC).The zones to operate on can be specified using the offset, countand length options.The device argument is the pathname of the block device.",
        "name": "blkzone - run zone command on a device",
        "section": 8
    },
    {
        "command": "blockdev",
        "description": "The utility blockdev allows one to call block device ioctls fromthe command line.",
        "name": "blockdev - call block device ioctls from the command line",
        "section": 8
    },
    {
        "command": "booleans",
        "description": "This manual page describes SELinux policy booleans.The SELinuxpolicy can include conditional rules that are enabled or disabledbased on the current values of a set of policy booleans.Thesepolicy booleans allow runtime modification of the security policywithout having to load a new policy.For example, the boolean httpd_enable_cgi allows the httpd daemonto run cgi scripts if it is enabled.If the administrator doesnot want to allow execution of cgi scripts, he can simply disablethis boolean value.The policy defines a default value for each boolean, typicallyfalse.These default values can be overridden via local settingscreated via the setsebool(8) utility, using -P to make thesetting persistent across reboots.Thesystem-config-securitylevel tool provides a graphical interfacefor altering the settings.The load_policy(8) program willpreserve current boolean settings upon a policy reload bydefault, or can optionally reset booleans to the boot-timedefaults via the -b option.Boolean values can be listed by using the getsebool(8) utilityand passing it the -a option.Boolean values can also be changed at runtime via thesetsebool(8) utility or the togglesebool(8) utility.By default,these utilities only change the current boolean value and do notaffect the persistent settings, unless the -P option is used tosetsebool.",
        "name": "booleans - Policy booleans enable runtime customization ofSELinux policy",
        "section": 8
    },
    {
        "command": "bpfc",
        "description": "bpfc is a small Berkeley Packet Filter assembler and compilerwhich is able to translate BPF assembler-like mnemonics into anumerical or C-like format, that can be read by tools such asnetsniff-ng, iptables (xt_bpf) and many others. BPF is the oneand only upstream filtering construct that is used in combinationwith packet(7) sockets, but also seccomp-BPF for system callsandboxing.The Linux kernel and also BSD kernels implement \"virtual machine\"like constructs and JIT compilers that mimic a small register-based machine in BPF architecture and execute filter code thatis, for example, composed by bpfc on a data buffer that is givenby network packets. The purpose of this is to shift computationin time, so that the kernel can drop or truncate incoming packetsas early as possible without having to push them to user spacefor further analysis first. Meanwhile, BPF constructs also findapplication in other areas such as in the communication betweenuser and kernel space like system call sand-boxing.At the time of writing this man page, the only other availableBPF compiler is part of the pcap(3) library and accessiblethrough a high-level filter language that might be familiar tomany people as tcpdump-like filters.However, it is quite often useful to bypass that compiler andwrite optimized code that cannot be produced by the pcap(3)compiler, or is wrongly optimized, or is defective on purpose inorder to debug test kernel code. Also, a reason to use bpfc couldbe to try out some new BPF extensions that are not supported byother compilers. Furthermore, bpfc can be useful to verify JITcompiler behavior or to find possible bugs that need to be fixed.bpfc is implemented with the help of flex(1) and bison(1),tokenizes the source file in the first stage and parses itscontent into an AST.In two code generation stages it emitstarget opcodes. bpfc furthermore supports Linux kernel BPFextensions. More about that can be found in the syntax section.The Linux kernel BPF JIT compiler is automatically turned on ifdetected by netsniff-ng. However, it can also be manually turnedon through the command ''echo \"1\" >/proc/sys/net/core/bpf_jit_enable'' (normal working mode) or''echo \"2\" > /proc/sys/net/core/bpf_jit_enable'' (debug modewhere emitted opcodes of the image are printed to the kernellog). An architecture agnostic BPF JIT image disassembler can befound in the kernel source tree under''tools/net/bpf_jit_disasm.c'' or within the netsniff-ng Gitrepository.",
        "name": "bpfc - a Berkeley Packet Filter assembler and compiler",
        "section": 8
    },
    {
        "command": "bridge",
        "description": null,
        "name": "bridge - show / manipulate bridge addresses and devices",
        "section": 8
    },
    {
        "command": "btrace",
        "description": "The btrace script provides a quick and easy way to do livetracing of block devices.It calls blktrace on the specifieddevices and pipes the output through blkparse for formatting.See blktrace (8) for more in-depth information about how blktraceworks.",
        "name": "btrace - perform live tracing for block devices",
        "section": 8
    },
    {
        "command": "btrecord",
        "description": "The btrecord and btreplay tools provide the ability to record andreplay IOs captured by the blktrace utility. Attempts are made tomaintain ordering, CPU mappings and time-separation of IOs.The blktrace utility provides the ability to collect detailedtraces from the kernel for each IO processed by the block IOlayer. The traces provide a complete timeline for each IOprocessed, including detailed information concerning when an IOwas first received by the block IO layer \u2014 indicating the device,CPU number, time stamp, IO direction, sector number and IO size(number of sectors). Using this information, one is able toreplay the IO again on the same machine or another set upentirely.The basic operating work-flow to replay IOs would be somethinglike:-Run blktrace to collect traces. Here you specify thedevice or devices that you wish to trace and later replay IOsupon. Note:the only traces you are interested in are QUEUE requests \u2014thus, to save system resources (including storage fortraces), one couldspecify the -a queue command line option to blktrace.-While blktrace is running, you run the workload that youare interested in.-When the work load has completed, you stop the blktraceutility (thus saving all traces over the complete workload).-You extract the pertinent IO information from the tracessaved byblktrace using the btrecord utility. This will parseeach trace file created by blktrace, and crafty IOdescriptionsto be used in the next phase of the workload processing.-Once btrecord has successfully created a series of datafiles to be processed, you can run the btreplay utility whichattempts to generate the same IOs seen during the sampleworkload phase.",
        "name": "btrecord - recreate IO loads recorded by blktrace",
        "section": 8
    },
    {
        "command": "btreplay",
        "description": "The btrecord and btreplay tools provide the ability to record andreplay IOs captured by the blktrace utility. Attempts are made tomaintain ordering, CPU mappings and time-separation of IOs.The blktrace utility provides the ability to collect detailedtraces from the kernel for each IO processed by the block IOlayer. The traces provide a complete timeline for each IOprocessed, including detailed information concerning when an IOwas first received by the block IO layer \u2014 indicating the device,CPU number, time stamp, IO direction, sector number and IO size(number of sectors). Using this information, one is able toreplay the IO again on the same machine or another set upentirely.The basic operating work-flow to replay IOs would be somethinglike:-Run blktrace to collect traces. Here you specify thedevice or devices that you wish to trace and later replay IOsupon. Note:the only traces you are interested in are QUEUE requests \u2014thus, to save system resources (including storage fortraces), one couldspecify the -a queue command line option to blktrace.-While blktrace is running, you run the workload that youare interested in.-When the work load has completed, you stop the blktraceutility (thus saving all traces over the complete workload).-You extract the pertinent IO information from the tracessaved byblktrace using the btrecord utility. This will parseeach trace file created by blktrace, and crafty IOdescriptionsto be used in the next phase of the workload processing.-Once btrecord has successfully created a series of datafiles to be processed, you can run the btreplay utility whichattempts to generate the same IOs seen during the sampleworkload phase.",
        "name": "btreplay - recreate IO loads recorded by blktrace",
        "section": 8
    },
    {
        "command": "btrfs",
        "description": "The btrfs utility is a toolbox for managing btrfs filesystems.There are command groups to work with subvolumes, devices, forwhole filesystem or other specific actions. See section COMMANDS.There are also standalone tools for some tasks like btrfs-convertor btrfstune that were separate historically and/or haven\u2019t beenmerged to the main utility. See section STANDALONE TOOLS for moredetails.For other topics (mount options, etc) please refer to theseparate manual page btrfs(5).",
        "name": "btrfs - a toolbox to manage btrfs filesystems",
        "section": 8
    },
    {
        "command": "btrfs-balance",
        "description": "The primary purpose of the balance feature is to spread blockgroups across all devices so they match constraints defined bythe respective profiles. See mkfs.btrfs(8) section PROFILES formore details. The scope of the balancing process can be furthertuned by use of filters that can select the block groups toprocess. Balance works only on a mounted filesystem. Extentsharing is preserved and reflinks are not broken. Files are notdefragmented nor recompressed, file extents are preserved but thephysical location on devices will change.The balance operation is cancellable by the user. The on-diskstate of the filesystem is always consistent so an unexpectedinterruption (eg. system crash, reboot) does not corrupt thefilesystem. The progress of the balance operation is temporarilystored as an internal state and will be resumed upon mount,unless the mount option skip_balance is specified.Warningrunning balance without filters will take a lot of time as itbasically move data/metadata from the whol filesystem andneeds to update all block pointers.The filters can be used to perform following actions:\u2022convert block group profiles (filter convert)\u2022make block group usage more compact (filter usage)\u2022perform actions only on a given device (filters devid,drange)The filters can be applied to a combination of block group types(data, metadata, system). Note that changing only the system typeneeds the force option. Otherwise system gets automaticallyconverted whenever metadata profile is converted.When metadata redundancy is reduced (eg. from RAID1 to single)the force option is also required and it is noted in system log.Notethe balance operation needs enough work space, ie. space thatis completely unused in the filesystem, otherwise this maylead to ENOSPC reports. See the section ENOSPC for moredetails.",
        "name": "btrfs-balance - balance block groups on a btrfs filesystem",
        "section": 8
    },
    {
        "command": "btrfs-check",
        "description": "The filesystem checker is used to verify structural integrity ofa filesystem and attempt to repair it if requested. It isrecommended to unmount the filesystem prior to running the check,but it is possible to start checking a mounted filesystem (see--force).By default, btrfs check will not modify the device but you canreaffirm that by the option --readonly.btrfsck is an alias of btrfs check command and is now deprecated.WarningDo not use --repair unless you are advised to do so by adeveloper or an experienced user, and then only after havingaccepted that no fsck successfully repair all types offilesystem corruption. Eg. some other software or hardwarebugs can fatally damage a volume.The structural integrity check verifies if internal filesystemobjects or data structures satisfy the constraints, point to theright objects or are correctly connected together.There are several cross checks that can detect wrong referencecounts of shared extents, backreferences, missing extents ofinodes, directory and inode connectivity etc.The amount of memory required can be high, depending on the sizeof the filesystem, similarly the run time. Check the modes thatcan also affect that.",
        "name": "btrfs-check - check or repair a btrfs filesystem",
        "section": 8
    },
    {
        "command": "btrfs-convert",
        "description": "btrfs-convert is used to convert existing source filesystem imageto a btrfs filesystem in-place. The original filesystem image isaccessible in subvolume named like ext2_saved as file image.Supported filesystems:\u2022ext2, ext3, ext4 \u2014 original feature, always built in\u2022reiserfs \u2014 since version 4.13, optionally built, requireslibreiserfscore 3.6.27\u2022ntfs \u2014 external toolhttps://github.com/maharmstone/ntfs2btrfs The list of supported source filesystem by a given binary islisted at the end of help (option --help).WarningIf you are going to perform rollback to the originalfilesystem, you should not execute btrfs balance command onthe converted filesystem. This will change the extent layoutand make btrfs-convert unable to rollback.The conversion utilizes free space of the original filesystem.The exact estimate of the required space cannot be foretold. Thefinal btrfs metadata might occupy several gigabytes on ahundreds-gigabyte filesystem.If the ability to rollback is no longer important, the it isrecommended to perform a few more steps to transition the btrfsfilesystem to a more compact layout. This is because theconversion inherits the original data blocks' fragmentation, andalso because the metadata blocks are bound to the original freespace layout.Due to different constraints, it is only possible to convertfilesystems that have a supported data block size (ie. the samethat would be valid for mkfs.btrfs). This is typically the systempage size (4KiB on x86_64 machines).BEFORE YOU STARTThe source filesystem must be clean, eg. no journal to replay orno repairs needed. The respective fsck utility must be run on thesource filesystem prior to conversion. Please refer to the manualpages in case you encounter problems.For ext2/3/4:# e2fsck -fvy /dev/sdxFor reiserfs:# reiserfsck -fy /dev/sdxSkipping that step could lead to incorrect results on the targetfilesystem, but it may work.REMOVE THE ORIGINAL FILESYSTEM METADATABy removing the subvolume named like ext2_saved orreiserfs_saved, all metadata of the original filesystem will beremoved:# btrfs subvolume delete /mnt/ext2_savedAt this point it is not possible to do a rollback. The filesystemis usable but may be impacted by the fragmentation inherited fromthe original filesystem.MAKE FILE DATA MORE CONTIGUOUSAn optional but recommended step is to run defragmentation on theentire filesystem. This will attempt to make file extents morecontiguous.# btrfs filesystem defrag -v -r -f -t 32M /mnt/btrfsVerbose recursive defragmentation (-v, -r), flush data per-file(-f) with target extent size 32MiB (-t).ATTEMPT TO MAKE BTRFS METADATA MORE COMPACTOptional but recommended step.The metadata block groups after conversion may be smaller thanthe default size (256MiB or 1GiB). Running a balance will attemptto merge the block groups. This depends on the free space layout(and fragmentation) and may fail due to lack of enough workspace. This is a soft error leaving the filesystem usable but theblock group layout may remain unchanged.Note that balance operation takes a lot of time, please see alsobtrfs-balance(8).# btrfs balance start -m /mnt/btrfs",
        "name": "btrfs-convert - convert from ext2/3/4 or reiserfs filesystem tobtrfs in-place",
        "section": 8
    },
    {
        "command": "btrfs-device",
        "description": "The btrfs device command group is used to manage devices of thebtrfs filesystems.",
        "name": "btrfs-device - manage devices of btrfs filesystems",
        "section": 8
    },
    {
        "command": "btrfs-filesystem",
        "description": "btrfs filesystem is used to perform several whole filesystemlevel tasks, including all the regular filesystem operations likeresizing, space stats, label setting/getting, anddefragmentation. There are other whole filesystem tasks likescrub or balance that are grouped in separate commands.",
        "name": "btrfs-filesystem - command group that primarily does work on thewhole filesystems",
        "section": 8
    },
    {
        "command": "btrfs-find-root",
        "description": "btrfs-find-root is used to find the satisfied root, you canfilter by root tree\u2019s objectid, generation, level.",
        "name": "btrfs-find-root - filter to find btrfs root",
        "section": 8
    },
    {
        "command": "btrfs-image",
        "description": "btrfs-image is used to create an image of a btrfs filesystem. Alldata will be zeroed, but metadata and the like is preserved.Mainly used for debugging purposes.In the dump mode, source is the btrfs device/file and target isthe output file (use - for stdout).In the restore mode (option -r), source is the dumped image andtarget is the btrfs device/file.",
        "name": "btrfs-image - create/restore an image of the filesystem",
        "section": 8
    },
    {
        "command": "btrfs-inspect-internal",
        "description": "This command group provides an interface to query internalinformation. The functionality ranges from a simple UI to anioctl or a more complex query that assembles the result fromseveral internal structures. The latter usually requires calls toprivileged ioctls.",
        "name": "btrfs-inspect-internal - query various internal information",
        "section": 8
    },
    {
        "command": "btrfs-map-logical",
        "description": "btrfs-map-logical can be used to find out what the physicaloffsets are on the mirrors, the result is dumped to stdout bydefault.Mainly used for debug purpose.",
        "name": "btrfs-map-logical - map btrfs logical extent to physical extent",
        "section": 8
    },
    {
        "command": "btrfs-property",
        "description": "btrfs property is used to get/set/list property for givenfilesystem object. The object can be an inode (file ordirectory), subvolume or the whole filesystem. See thedescription of get subcommand for more information about bothbtrfs object and property.btrfs property provides an unified and user-friendly method totune different btrfs properties instead of using the traditionalmethod like chattr(1) or lsattr(1).",
        "name": "btrfs-property - get/set/list properties for given filesystemobject",
        "section": 8
    },
    {
        "command": "btrfs-qgroup",
        "description": "btrfs qgroup is used to control quota group (qgroup) of a btrfsfilesystem.NoteTo use qgroup you need to enable quota first using btrfsquota enable command.WarningQgroup is not stable yet and will impact performance incurrent mainline kernel (v4.14).",
        "name": "btrfs-qgroup - control the quota group of a btrfs filesystem",
        "section": 8
    },
    {
        "command": "btrfs-quota",
        "description": "The commands under btrfs quota are used to affect the globalstatus of quotas of a btrfs filesystem. The quota groups(qgroups) are managed by the subcommand btrfs qgroup(8).NoteQgroups are different than the traditional user quotas anddesigned to track shared and exclusive data per-subvolume.Please refer to the section HIERARCHICAL QUOTA GROUP CONCEPTSfor a detailed description.PERFORMANCE IMPLICATIONSWhen quotas are activated, they affect all extent processing,which takes a performance hit. Activation of qgroups is notrecommended unless the user intends to actually use them.STABILITY STATUSThe qgroup implementation has turned out to be quite difficult asit affects the core of the filesystem operation. Qgroup usershave hit various corner cases over time, such as incorrectaccounting or system instability. The situation is graduallyimproving and issues found and fixed.",
        "name": "btrfs-quota - control the global quota status of a btrfsfilesystem",
        "section": 8
    },
    {
        "command": "btrfs-receive",
        "description": "Receive a stream of changes and replicate one or more subvolumesthat were previously generated by btrfs send. The receivedsubvolumes are stored to path, unless --dump option is given.If --dump option is specified, btrfs receive will only do thevalidation of the stream, and print the stream metadata, oneoperation per line.btrfs receive will fail in the following cases:1. receiving subvolume already exists2. previously received subvolume has been changed after it wasreceived3. default subvolume has changed or you didn\u2019t mount thefilesystem at the toplevel subvolumeA subvolume is made read-only after the receiving processfinishes successfully (see BUGS below).Options-f <FILE>read the stream from <FILE> instead of stdin,-C|--chrootconfine the process to path using chroot(1)-eterminate after receiving an end cmd marker in the stream.Without this option the receiver side terminates only in caseof an error on end of file.-E|--max-errors <NERR>terminate as soon as NERR errors occur while streamprocessing commands from the streamDefault value is 1. A value of 0 means no limit.-m <ROOTMOUNT>the root mount point of the destination filesystemBy default the mountpoint is searched in /proc/self/mounts.If /proc is not accessible, eg. in a chroot environment, usethis option to tell us where this filesystem is mounted.--dumpdump the stream metadata, one line per operationDoes not require the path parameter. The filesystem remainsunchanged.-q|--quiet(deprecated) alias for global -q option-v(deprecated) alias for global -v optionGlobal options-v|--verboseincrease verbosity about performed actions, print detailsabout each operation-q|--quietsuppress all messages except errors",
        "name": "btrfs-receive - receive subvolumes from send stream",
        "section": 8
    },
    {
        "command": "btrfs-replace",
        "description": "btrfs replace is used to replace btrfs managed devices with otherdevice.",
        "name": "btrfs-replace - replace devices managed by btrfs with otherdevice.",
        "section": 8
    },
    {
        "command": "btrfs-rescue",
        "description": "btrfs rescue is used to try to recover a damaged btrfsfilesystem.",
        "name": "btrfs-rescue - Recover a damaged btrfs filesystem",
        "section": 8
    },
    {
        "command": "btrfs-restore",
        "description": "btrfs restore is used to try to salvage files from a damagedfilesystem and restore them into <path> or just list thesubvolume tree roots. The filesystem image is not modified.If the filesystem is damaged and cannot be repaired by the othertools (btrfs-check(8) or btrfs-rescue(8)), btrfs restore could beused to retrieve file data, as far as the metadata are readable.The checks done by restore are less strict and the process isusually able to get far enough to retrieve data from the wholefilesystem. This comes at a cost that some data might beincomplete or from older versions if they\u2019re available.There are several options to attempt restoration of various filemetadata type. You can try a dry run first to see how well theprocess goes and use further options to extend the set ofrestored metadata.For images with damaged tree structures, there are severaloptions to point the process to some spare copy.NoteIt is recommended to read the following btrfs wiki page ifyour data is not salvaged with default option:https://btrfs.wiki.kernel.org/index.php/Restore",
        "name": "btrfs-restore - try to restore files from a damaged btrfsfilesystem image",
        "section": 8
    },
    {
        "command": "btrfs-scrub",
        "description": "btrfs scrub is used to scrub a mounted btrfs filesystem, whichwill read all data and metadata blocks from all devices andverify checksums. Automatically repair corrupted blocks ifthere\u2019s a correct copy available.NoteScrub is not a filesystem checker (fsck) and does not verifynor repair structural damage in the filesystem. It reallyonly checks checksums of data and tree blocks, it doesn\u2019tensure the content of tree blocks is valid and consistent.There\u2019s some validation performed when metadata blocks areread from disk but it\u2019s not extensive and cannot substitutefull btrfs check run.The user is supposed to run it manually or via a periodic systemservice. The recommended period is a month but could be less. Theestimated device bandwidth utilization is about 80% on an idlefilesystem. The IO priority class is by default idle sobackground scrub should not significantly interfere with normalfilesystem operation. The IO scheduler set for the device(s)might not support the priority classes though.The scrubbing status is recorded in /var/lib/btrfs/ in textualfiles named scrub.status.UUID for a filesystem identified by thegiven UUID. (Progress state is communicated through a named pipein file scrub.progress.UUID in the same directory.) The statusfile is updated every 5 seconds. A resumed scrub will continuefrom the last saved position.Scrub can be started only on a mounted filesystem, though it\u2019spossible to scrub only a selected device. See scrub start formore.",
        "name": "btrfs-scrub - scrub btrfs filesystem, verify block checksums",
        "section": 8
    },
    {
        "command": "btrfs-select-super",
        "description": "Destructively overwrite all copies of the superblock with aspecified copy. This helps in certain cases, for example whenwrite barriers were disabled during a power failure and not allsuperblocks were written, or if the primary superblock isdamaged, eg. accidentally overwritten.The filesystem specified by device must not be mounted.NotePrior to overwriting the primary superblock, please make surethat the backup copies are valid!To dump a superblock use the btrfs inspect-internal dump-supercommand.Then run the check (in the non-repair mode) using the commandbtrfs check -s where -s specifies the superblock copy to use.Superblock copies exist in the following offsets on the device:\u2022primary: 64KiB (65536)\u20221st copy: 64MiB (67108864)\u20222nd copy: 256GiB (274877906944)A superblock size is 4KiB (4096).",
        "name": "btrfs-select-super - overwrite primary superblock with a backupcopy",
        "section": 8
    },
    {
        "command": "btrfs-send",
        "description": "This command will generate a stream of instructions that describechanges between two subvolume snapshots. The stream can beconsumed by the btrfs receive command to replicate the sentsnapshot on a different filesystem. The command operates in twomodes: full and incremental.All snapshots involved in one send command must be read-only, andthis status cannot be changed as long as there\u2019s a running sendoperation that uses the snapshot.In the full mode, the entire snapshot data and metadata will endup in the stream.In the incremental mode (options -p and -c), previously sentsnapshots that are available on both the sending and receivingside can be used to reduce the amount of information that has tobe sent to reconstruct the sent snapshot on a differentfilesystem.The -p <parent> option can be omitted when -c <clone-src> optionsare given, in which case btrfs send will determine a suitableparent from among the clone sources.You must not specify clone sources unless you guarantee thatthese snapshots are exactly in the same state on both sides\u2014bothfor the sender and the receiver. For implications of changedread-write status of a received snapshot please see sectionSUBVOLUME FLAGS in btrfs-subvolume(8).Options-eif sending multiple subvolumes at once, use the new formatand omit the end cmd marker in the stream separating thesubvolumes-p <parent>send an incremental stream from parent to subvol-c <clone-src>use this snapshot as a clone source for an incremental send(multiple allowed)-f <outfile>output is normally written to standard output so it can be,for example, piped to btrfs receive. Use this option to writeit to a file instead.--no-datasend in NO_FILE_DATA modeThe output stream does not contain any file data and thuscannot be used to transfer changes. This mode is faster andis useful to show the differences in metadata. -q|--quiet::::(deprecated) alias for global -q option -v|--verbose::(deprecated) alias for global -v optionGlobal options-q|--quietsuppress all messages except errors-v|--verboseincrease output verbosity, print generated commands in areadable form",
        "name": "btrfs-send - generate a stream of changes between two subvolumesnapshots",
        "section": 8
    },
    {
        "command": "btrfs-subvolume",
        "description": "btrfs subvolume is used to create/delete/list/show btrfssubvolumes and snapshots.",
        "name": "btrfs-subvolume - manage btrfs subvolumes",
        "section": 8
    },
    {
        "command": "btrfsck",
        "description": "The filesystem checker is used to verify structural integrity ofa filesystem and attempt to repair it if requested. It isrecommended to unmount the filesystem prior to running the check,but it is possible to start checking a mounted filesystem (see--force).By default, btrfs check will not modify the device but you canreaffirm that by the option --readonly.btrfsck is an alias of btrfs check command and is now deprecated.WarningDo not use --repair unless you are advised to do so by adeveloper or an experienced user, and then only after havingaccepted that no fsck successfully repair all types offilesystem corruption. Eg. some other software or hardwarebugs can fatally damage a volume.The structural integrity check verifies if internal filesystemobjects or data structures satisfy the constraints, point to theright objects or are correctly connected together.There are several cross checks that can detect wrong referencecounts of shared extents, backreferences, missing extents ofinodes, directory and inode connectivity etc.The amount of memory required can be high, depending on the sizeof the filesystem, similarly the run time. Check the modes thatcan also affect that.",
        "name": "btrfs-check - check or repair a btrfs filesystem",
        "section": 8
    },
    {
        "command": "btrfstune",
        "description": "btrfstune can be used to enable, disable, or set variousfilesystem parameters. The filesystem must be unmounted.The common usecase is to enable features that were not enabled atmkfs time. Please make sure that you have kernel support for thefeatures. You can find a complete list of features and kernelversion of their introduction athttps://btrfs.wiki.kernel.org/index.php/Changelog#By_feature .Also, the manual page mkfs.btrfs(8) contains more details aboutthe features.Some of the features could be also enabled on a mountedfilesystem by other means. Please refer to the FILESYSTEMFEATURES in btrfs(5).",
        "name": "btrfstune - tune various filesystem parameters",
        "section": 8
    },
    {
        "command": "captest",
        "description": "captest is a program that demonstrates and prints out the currentprocess capabilities. Each option prints the same report. It willoutput current capabilities. then it will try to access/etc/shadow directly to show if that can be done. Then it createsa child process that attempts to read /etc/shadow and outputs theresults of that. Then it outputs the capabilities that a childprocess would have.You can also apply file system capabilities to this program tostudy how they work. For example, filecap /usr/bin/captest chown.Then run captest as a normal user. Another interesting test is tomake captest suid root so that you can see what the interactionis between root's credentials and capabilities. For example,chmod 4755 /usr/bin/captest. When run as a normal user, theprogram will see if privilege escalation is possible. But do notleave this app setuid root after you are don testing so that anattacker cannot take advantage of it.",
        "name": "captest - a program to demonstrate capabilities",
        "section": 8
    },
    {
        "command": "captree",
        "description": "captree displays the capabilities on the mentioned processesindicated by pid or glob-name value(s) given on the command line.If no pid etc values are supplied, pid=1 is implied. A pid valueof 0 displays all the processes known to the kernel.The POSIX.1e capabilities are displayed in double quotes in thecap_from_text(3) format. The IAB tuple of capabilities isdisplayed between square brackets in the text format described incap_iab(3).Note, the IAB tuple text is omitted if it containsempty A and B components. This is because the regular POSIX.1etext contains information about the Inheritable flag already.This behavior can be overridden with the --verbose command lineargument.Optional arguments (which must precede the list of pid|glob-namevalues):--help Displays usage information and exits. Note, modern Goruntimes exit with status 0 in this case, but olderruntimes exit with status 2.--verboseDisplays capability sets and IAB tuples even when they areempty, or redundant.--depth=nDisplays the process tree to a depth of n.Note, thedefault value for this parameter is 0, which impliesinfinite depth.--colo[u]r=falseColo[u]rs the targeted PIDs, if stdout is a TTY, in red.This option defaults to true when running via a TTY. The--color=false argument will suppress this color. Pipingthe output into some other program will also suppress theuse of colo[u]r.",
        "name": "captree - display tree of process capabilities",
        "section": 8
    },
    {
        "command": "catman",
        "description": "catman is used to create an up to date set of pre-formattedmanual pages known as cat pages.Cat pages are generally muchfaster to display than the original manual pages, but requireextra storage space.The decision to support cat pages is thatof the local administrator, who must provide suitable directoriesto contain them.The options available to catman are the manual page hierarchiesand sections to pre-format.The default hierarchies are thosespecified as system hierarchies in the man-db configuration file,and the default sections are either the colon-delimited contentsof the environment variable $MANSECT or the standard set compiledinto man if $MANSECT is undefined.Supplying catman with a setof whitespace-delimited section names will override both of theabove.catman makes use of the index database cache associated with eachhierarchy to determine which files need to be formatted.",
        "name": "catman - create or update the pre-formatted manual pages",
        "section": 8
    },
    {
        "command": "cfdisk",
        "description": "cfdisk is a curses-based program for partitioning any blockdevice. The default device is /dev/sda.Note that cfdisk provides basic partitioning functionality with auser-friendly interface. If you need advanced features, usefdisk(8) instead.All disk label changes will remain in memory only, and the diskwill be unmodified until you decide to write your changes. Becareful before using the write command.Since version 2.25 cfdisk supports MBR (DOS), GPT, SUN and SGIdisk labels, but no longer provides any functionality for CHS(Cylinder-Head-Sector) addressing. CHS has never been importantfor Linux, and this addressing concept does not make any sensefor new devices.Since version 2.25 cfdisk also does not provide a 'print' commandany more. This functionality is provided by the utilitiespartx(8) and lsblk(8) in a very comfortable and rich way.If you want to remove an old partition table from a device, usewipefs(8).",
        "name": "cfdisk - display or manipulate a disk partition table",
        "section": 8
    },
    {
        "command": "cgroup",
        "description": "This filter serves as a hint to tc that the assigned class ID ofthe net_cls control group the process the packet originates frombelongs to should be used for classification. Obviously, it isuseful for locally generated packets only.",
        "name": "cgroup - control group based traffic control filter",
        "section": 8
    },
    {
        "command": "chat",
        "description": "The chat program defines a conversational exchange between thecomputer and the modem. Its primary purpose is to establish theconnection between the Point-to-Point Protocol Daemon (pppd) andthe remote's pppd process.",
        "name": "chat - Automated conversational script with a modem",
        "section": 8
    },
    {
        "command": "chcat",
        "description": "Use +/- to add/remove categories from a file or user (only asingle category can be specified at a time). Or specify thedesired list/range of categories to be applied (replacing theexisting categories).Note: When removing a category you must specify '--' on thecommand line before using the -Category syntax. This tells thecommand that you have finished entering options and are nowspecifying a category name instead.-ddelete all categories from given FILE/USER.-Llist available categories.-lTells chcat to operate on users instead of files.",
        "name": "chcat - change SELinux security categories of files/users",
        "section": 8
    },
    {
        "command": "chcpu",
        "description": "chcpu can modify the state of CPUs. It can enable or disableCPUs, scan for new CPUs, change the CPU dispatching mode of theunderlying hypervisor, and request CPUs from the hypervisor(configure) or return CPUs to the hypervisor (deconfigure).Some options have a cpu-list argument. Use this argument tospecify a comma-separated list of CPUs. The list can containindividual CPU addresses or ranges of addresses. For example,0,5,7,9-11 makes the command applicable to the CPUs with theaddresses 0, 5, 7, 9, 10, and 11.",
        "name": "chcpu - configure CPUs",
        "section": 8
    },
    {
        "command": "checkmodule",
        "description": "This manual page describes the checkmodule command.checkmodule is a program that checks and compiles a SELinuxsecurity policy module into a binary representation.It cangenerate either a base policy module (default) or a non-basepolicy module (-m option); typically, you would build a non-basepolicy module to add to an existing module store that already hasa base module provided by the base policy.Usesemodule_package(8) to combine this module with its optional filecontexts to create a policy package, and then use semodule(8) toinstall the module package into the module store and load theresulting policy.",
        "name": "checkmodule - SELinux policy module compiler",
        "section": 8
    },
    {
        "command": "checkpolicy",
        "description": "This manual page describes the checkpolicy command.checkpolicy is a program that checks and compiles a SELinuxsecurity policy configuration into a binary representation thatcan be loaded into the kernel.If no input file name isspecified, checkpolicy will attempt to read from policy.conf orpolicy, depending on whether the -b flag is specified.",
        "name": "checkpolicy - SELinux policy compiler",
        "section": 8
    },
    {
        "command": "chgpasswd",
        "description": "The chgpasswd command reads a list of group name and passwordpairs from standard input and uses this information to update aset of existing groups. Each line is of the format:group_name:passwordBy default the supplied password must be in clear-text, and isencrypted by chgpasswd.The default encryption algorithm can be defined for the systemwith the ENCRYPT_METHOD variable of /etc/login.defs, and can beoverwritten with the -e, -m, or -c options.This command is intended to be used in a large system environmentwhere many accounts are created at a single time.",
        "name": "chgpasswd - update group passwords in batch mode",
        "section": 8
    },
    {
        "command": "chkcon",
        "description": "This utility validates (the string representation of) a securitycontext specified by the argument context against configurationdata read in from a policy database binary representation filespecified by the argument policy_file.",
        "name": "chkcon -determine if a security context is valid for a givenbinary policy",
        "section": 8
    },
    {
        "command": "chmem",
        "description": "The chmem command sets a particular size or range of memoryonline or offline.\u2022Specify SIZE as <size>[m|M|g|G]. With m or M, <size>specifies the memory size in MiB (1024 x 1024 bytes). With gor G, <size> specifies the memory size in GiB (1024 x 1024 x1024 bytes). The default unit is MiB.\u2022Specify RANGE in the form 0x<start>-0x<end> as shown in theoutput of the lsmem(1) command. <start> is the hexadecimaladdress of the first byte and <end> is the hexadecimaladdress of the last byte in the memory range.\u2022Specify BLOCKRANGE in the form <first>-<last> or <block> asshown in the output of the lsmem(1) command. <first> is thenumber of the first memory block and <last> is the number ofthe last memory block in the memory range. Alternatively asingle block can be specified. BLOCKRANGE requires the--blocks option.\u2022Specify ZONE as the name of a memory zone, as shown in theoutput of the lsmem -o +ZONES command. The output shows oneor more valid memory zones for each memory range. If multiplezones are shown, then the memory range currently belongs tothe first zone. By default, chmem will set memory online tothe zone Movable, if this is among the valid zones. Thisdefault can be changed by specifying the --zone option withanother valid zone. For memory ballooning, it is recommendedto select the zone Movable for memory online and offline, ifpossible. Memory in this zone is much more likely to be ableto be offlined again, but it cannot be used for arbitrarykernel allocations, only for migratable pages (e.g.,anonymous and page cache pages). Use the --help option to seeall available zones.SIZE and RANGE must be aligned to the Linux memory block size, asshown in the output of the lsmem(1) command.Setting memory online can fail for various reasons. Onvirtualized systems it can fail if the hypervisor does not haveenough memory left, for example because memory was overcommitted.Setting memory offline can fail if Linux cannot free the memory.If only part of the requested memory can be set online oroffline, a message tells you how much memory was set online oroffline instead of the requested amount.When setting memory online chmem starts with the lowest memoryblock numbers. When setting memory offline chmem starts with thehighest memory block numbers.",
        "name": "chmem - configure memory",
        "section": 8
    },
    {
        "command": "choke",
        "description": "CHOKe (CHOose and Keep for responsive flows, CHOose and Kill forunresponsive flows) is a classless qdisc designed to bothidentify and penalize flows that monopolize the queue. CHOKe is avariation of RED, and the configuration is similar to RED.",
        "name": "choke - choose and keep scheduler",
        "section": 8
    },
    {
        "command": "chpasswd",
        "description": "The chpasswd command reads a list of user name and password pairsfrom standard input and uses this information to update a groupof existing users. Each line is of the format:user_name:passwordBy default the passwords must be supplied in clear-text, and areencrypted by chpasswd. Also the password age will be updated, ifpresent.By default, passwords are encrypted by PAM, but (even if notrecommended) you can select a different encryption method withthe -e, -m, or -c options.Except when PAM is used to encrypt the passwords, chpasswd firstupdates all the passwords in memory, and then commits all thechanges to disk if no errors occurred for any user.When PAM is used to encrypt the passwords (and update thepasswords in the system database) then if a password cannot beupdated chpasswd continues updating the passwords of the nextusers, and will return an error code on exit.This command is intended to be used in a large system environmentwhere many accounts are created at a single time.",
        "name": "chpasswd - update passwords in batch mode",
        "section": 8
    },
    {
        "command": "clockdiff",
        "description": "clockdiff Measures clock difference between us and destinationwith 1 msec resolution using ICMP TIMESTAMP [2] packets or,optionally, IP TIMESTAMP option [3] added to ICMP ECHO. [1]",
        "name": "clockdiff - measure clock difference between hosts",
        "section": 8
    },
    {
        "command": "cmirrord",
        "description": "cmirrord is the daemon that tracks mirror log information in acluster.It is specific to device-mapper based mirrors (and byextension, LVM cluster mirrors).Cluster mirrors are notpossible without this daemon running.This daemon relies on the cluster infrastructure provided by thecorosync, which must be set up and running in order for cmirrordto function.Output is logged via syslog(3). The SIGUSR1 signal(7) can beissued to cmirrord to gather current status information fordebugging purposes.Once started, cmirrord will run until it is shutdown via SIGINTsignal. If there are still active cluster mirrors, however, thesignal will be ignored. Active cluster mirrors should be shutdownbefore stopping the cluster mirror log daemon.",
        "name": "cmirrord \u2014 cluster mirror log daemon",
        "section": 8
    },
    {
        "command": "connmark",
        "description": "The connmark action is used to restore the connection's markvalue into the packet's fwmark.",
        "name": "connmark - netfilter connmark retriever action",
        "section": 8
    },
    {
        "command": "convertquota",
        "description": "convertquota converts old quota files quota.user and quota.groupto files aquota.user and aquota.group in new format currentlyused by 2.4.0-ac? and newer or by SuSE or Red Hat Linux 2.4kernels on filesystem.New file format allows using quotas for 32-bit uids / gids,setting quotas for root, accounting used space in bytes (and soallowing use of quotas in ReiserFS) and it is also architectureindependent. This format introduces Radix Tree (a simple form oftree structure) to quota file.",
        "name": "convertquota - convert quota from old file format to new one",
        "section": 8
    },
    {
        "command": "crash",
        "description": "Crash is a tool for interactively analyzing the state of theLinux system while it is running, or after a kernel crash hasoccurred and a core dump has been created by the netdump,diskdump, LKCD, kdump, xendump kvmdump or VMware facilities.Itis loosely based on the SVR4 UNIX crash command, but has beensignificantly enhanced by completely merging it with the gdb(1)debugger. The marriage of the two effectively combines thekernel-specific nature of the traditional UNIX crash utility withthe source code level debugging capabilities of gdb(1).In the dumpfile form, both a NAMELIST and a MEMORY-IMAGE argumentmust be entered.In the live system form, the NAMELIST argumentmust be entered if the kernel's vmlinux file is not located in aknown location, such as the /usr/lib/debug/lib/modules/<kernel-version> directory.The crash utility has also been extended to support the analysisof dumpfiles generated by a crash of the Xen hypervisor.In thatcase, the NAMELIST argument must be that of the xen-syms binary.Live system analysis is not supported for the Xen hypervisor.The crash utility command set consists of common kernel coreanalysis tools such as kernel stack back traces of all processes,source code disassembly, formatted kernel structure and variabledisplays, virtual memory data, dumps of linked-lists, etc., alongwith several commands that delve deeper into specific kernelsubsystems.Appropriate gdb commands may also be entered, whichin turn are passed on to the gdb module for execution.Ifdesired, commands may be placed in either a $HOME/.crashrc fileand/or in a .crashrc file in the current directory.Duringinitialization, the commands in $HOME/.crashrc are executedfirst, followed by those in the ./.crashrc file.The crash utility is designed to be independent of Linux versiondependencies. When new kernel source code impacts the correctfunctionality of crash and its command set, the utility will beupdated to recognize new kernel code changes, while maintainingbackwards compatibility with earlier releases.",
        "name": "crash - Analyze Linux crash dump data or a live system",
        "section": 8
    },
    {
        "command": "cron",
        "description": "Cron is started from /etc/rc.d/init.d or /etc/init.d whenclassical sysvinit scripts are used. In case systemd is enabled,then unit file is installed into/lib/systemd/system/crond.service and daemon is started bysystemctl start crond.service command. It returns immediately,thus, there is no need to need to start it with the '&'parameter.Cron searches /var/spool/cron for crontab files which are namedafter accounts in /etc/passwd; The found crontabs are loaded intothe memory.Cron also searches for /etc/anacrontab and any filesin the /etc/cron.d directory, which have a different format (seecrontab(5)).Cron examines all stored crontabs and checks eachjob to see if it needs to be run in the current minute.Whenexecuting commands, any output is mailed to the owner of thecrontab (or to the user specified in the MAILTO environmentvariable in the crontab, if such exists).Any job output canalso be sent to syslog by using the -s option.There are two ways how changes in crontables are checked.Thefirst method is checking the modtime of a file.The secondmethod is using the inotify support.Using of inotify is loggedin the /var/log/cron log after the daemon is started.Theinotify support checks for changes in all crontables and accessesthe hard disk only when a change is detected.When using the modtime option, Cron checks its crontables'modtimes every minute to check for any changes and reloads thecrontables which have changed.There is no need to restart Cronafter some of the crontables were modified.The modtime optionis also used when inotify can not be initialized.Cron checks these files and directories:/etc/crontabsystem crontab.Nowadays the file is empty by default.Originally it was usually used to run daily, weekly,monthly jobs.By default these jobs are now run throughanacron which reads /etc/anacrontab configuration file.See anacrontab(5) for more details./etc/cron.d/directory that contains system cronjobs stored fordifferent users./var/spool/crondirectory that contains user crontables created by thecrontab command.Note that the crontab(1) command updates the modtime of the spooldirectory whenever it changes a crontab.Daylight Saving Time and other time changesLocal time changes of less than three hours, such as those causedby the Daylight Saving Time changes, are handled in a specialway.This only applies to jobs that run at a specific time andjobs that run with a granularity greater than one hour.Jobsthat run more frequently are scheduled normally.If time was adjusted one hour forward, those jobs that would haverun in the interval that has been skipped will be runimmediately.Conversely, if time was adjusted backward, runningthe same job twice is avoided.Time changes of more than 3 hours are considered to becorrections to the clock or the timezone, and the new time isused immediately.It is possible to use different time zones for crontables.Seecrontab(5) for more information.PAM Access ControlCron supports access control with PAM if the system has PAMinstalled.For more information, see pam(8).A PAMconfiguration file for crond is installed in /etc/pam.d/crond.The daemon loads the PAM environment from the pam_env module.This can be overridden by defining specific settings in theappropriate crontab file.",
        "name": "crond - daemon to execute scheduled commands",
        "section": 8
    },
    {
        "command": "crond",
        "description": "Cron is started from /etc/rc.d/init.d or /etc/init.d whenclassical sysvinit scripts are used. In case systemd is enabled,then unit file is installed into/lib/systemd/system/crond.service and daemon is started bysystemctl start crond.service command. It returns immediately,thus, there is no need to need to start it with the '&'parameter.Cron searches /var/spool/cron for crontab files which are namedafter accounts in /etc/passwd; The found crontabs are loaded intothe memory.Cron also searches for /etc/anacrontab and any filesin the /etc/cron.d directory, which have a different format (seecrontab(5)).Cron examines all stored crontabs and checks eachjob to see if it needs to be run in the current minute.Whenexecuting commands, any output is mailed to the owner of thecrontab (or to the user specified in the MAILTO environmentvariable in the crontab, if such exists).Any job output canalso be sent to syslog by using the -s option.There are two ways how changes in crontables are checked.Thefirst method is checking the modtime of a file.The secondmethod is using the inotify support.Using of inotify is loggedin the /var/log/cron log after the daemon is started.Theinotify support checks for changes in all crontables and accessesthe hard disk only when a change is detected.When using the modtime option, Cron checks its crontables'modtimes every minute to check for any changes and reloads thecrontables which have changed.There is no need to restart Cronafter some of the crontables were modified.The modtime optionis also used when inotify can not be initialized.Cron checks these files and directories:/etc/crontabsystem crontab.Nowadays the file is empty by default.Originally it was usually used to run daily, weekly,monthly jobs.By default these jobs are now run throughanacron which reads /etc/anacrontab configuration file.See anacrontab(5) for more details./etc/cron.d/directory that contains system cronjobs stored fordifferent users./var/spool/crondirectory that contains user crontables created by thecrontab command.Note that the crontab(1) command updates the modtime of the spooldirectory whenever it changes a crontab.Daylight Saving Time and other time changesLocal time changes of less than three hours, such as those causedby the Daylight Saving Time changes, are handled in a specialway.This only applies to jobs that run at a specific time andjobs that run with a granularity greater than one hour.Jobsthat run more frequently are scheduled normally.If time was adjusted one hour forward, those jobs that would haverun in the interval that has been skipped will be runimmediately.Conversely, if time was adjusted backward, runningthe same job twice is avoided.Time changes of more than 3 hours are considered to becorrections to the clock or the timezone, and the new time isused immediately.It is possible to use different time zones for crontables.Seecrontab(5) for more information.PAM Access ControlCron supports access control with PAM if the system has PAMinstalled.For more information, see pam(8).A PAMconfiguration file for crond is installed in /etc/pam.d/crond.The daemon loads the PAM environment from the pam_env module.This can be overridden by defining specific settings in theappropriate crontab file.",
        "name": "crond - daemon to execute scheduled commands",
        "section": 8
    },
    {
        "command": "cryptsetup",
        "description": "cryptsetup is used to conveniently setup dm-crypt manageddevice-mapper mappings. These include plain dm-crypt volumes andLUKS volumes. The difference is that LUKS uses a metadata headerand can hence offer more features than plain dm-crypt. On theother hand, the header is visible and vulnerable to damage.In addition, cryptsetup provides limited support for the use ofloop-AES volumes, TrueCrypt, VeraCrypt, BitLocker and FileVault2compatible volumes.For more information about specific cryptsetup action seecryptsetup-<action>(8), where <action> is the name of thecryptsetup action.",
        "name": "cryptsetup - manage plain dm-crypt, LUKS, and other encryptedvolumes",
        "section": 8
    },
    {
        "command": "cryptsetup-benchmark",
        "description": "Benchmarks ciphers and KDF (key derivation function). Withoutparameters, it tries to measure few common configurations.To benchmark other ciphers or modes, you need to specify --cipherand --key-size options.To benchmark PBKDF you need to specify --pbkdf or --hash withoptional cost parameters --iter-time, --pbkdf-memory or--pbkdf-parallel.NOTE: This benchmark uses memory only and is only informative.You cannot directly predict real storage encryption speed fromit.For testing block ciphers, this benchmark requires kerneluserspace crypto API to be available (introduced in Linux kernel2.6.38). If you are configuring kernel yourself, enable\"User-space interface for symmetric key cipher algorithms\" in\"Cryptographic API\" section (CRYPTO_USER_API_SKCIPHER .configoption).<options> can be [--cipher, --key-size, --hash, --pbkdf,--iter-time, --pbkdf-memory, --pbkdf-parallel].",
        "name": "cryptsetup-benchmark - benchmarks ciphers and KDF",
        "section": 8
    },
    {
        "command": "cryptsetup-bitlkDump",
        "description": "Dump the header information of a BITLK (BitLocker compatible)device.If the --dump-volume-key option is used, the BITLK device volumekey is dumped instead of header information. You have to providepassword or keyfile to dump volume key.Beware that the volume key can be used to decrypt the data storedin the container without a passphrase. This means that if thevolume key is compromised, the whole device has to be erased toprevent further access. Use this option carefully.<options> can be [--dump-volume-key, --volume-key-file,--key-file, --keyfile-offset, --keyfile-size, --timeout].",
        "name": "cryptsetup-bitlkDump - dump the header information of a BITLK(BitLocker compatible) device",
        "section": 8
    },
    {
        "command": "cryptsetup-bitlkOpen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-bitlkdump",
        "description": "Dump the header information of a BITLK (BitLocker compatible)device.If the --dump-volume-key option is used, the BITLK device volumekey is dumped instead of header information. You have to providepassword or keyfile to dump volume key.Beware that the volume key can be used to decrypt the data storedin the container without a passphrase. This means that if thevolume key is compromised, the whole device has to be erased toprevent further access. Use this option carefully.<options> can be [--dump-volume-key, --volume-key-file,--key-file, --keyfile-offset, --keyfile-size, --timeout].",
        "name": "cryptsetup-bitlkDump - dump the header information of a BITLK(BitLocker compatible) device",
        "section": 8
    },
    {
        "command": "cryptsetup-bitlkopen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-close",
        "description": "Removes the existing mapping <name> and wipes the key from kernelmemory.For backward compatibility, there are close command aliases:remove, plainClose, luksClose, loopaesClose, tcryptClose,bitlkClose (all behave exactly the same, device type isdetermined automatically from the active device).<options> can be [--deferred, --cancel-deferred, --header,--disable-locks].",
        "name": "cryptsetup-close - removes the existing mapping <name> (and theassociated key)",
        "section": 8
    },
    {
        "command": "cryptsetup-config",
        "description": "Set permanent configuration options (store to LUKS header). Theconfig command is supported only for LUKS2.The permanent options can be --priority to set priority (normal,prefer, ignore) for keyslot (specified by --key-slot) or --labeland --subsystem.<options> can be [--priority, --label, --subsystem, --key-slot,--header, --disable-locks].",
        "name": "cryptsetup-config - set permanent configuration options (store toLUKS header)",
        "section": 8
    },
    {
        "command": "cryptsetup-convert",
        "description": "Converts the device between LUKS1 and LUKS2 format (if possible).The conversion will not be performed if there is an additionalLUKS2 feature or LUKS1 has unsupported header size.Conversion (both directions) must be performed on inactivedevice. There must not be active dm-crypt mapping established forLUKS header requested for conversion.The --type option is mandatory with the following acceptedvalues: luks1 or luks2.WARNING: The convert action can destroy the LUKS header in thecase of a crash during conversion or if a media error occurs.Always create a header backup before performing this operation!<options> can be [--header, --type, --disable-locks].",
        "name": "cryptsetup-convert - converts the device between LUKS1 and LUKS2format",
        "section": 8
    },
    {
        "command": "cryptsetup-create",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-erase",
        "description": "Erase all keyslots and make the LUKS container permanentlyinaccessible. You do not need to provide any password for thisoperation.WARNING: This operation is irreversible.<options> can be [--header, --disable-locks].",
        "name": "cryptsetup-erase, cryptsetup-luksErase - erase all keyslots",
        "section": 8
    },
    {
        "command": "cryptsetup-fvault2Dump",
        "description": "Dump the header information of a FVAULT2 (FileVault2 compatible)device.If the --dump-volume-key option is used, the FVAULT2 devicevolume key is dumped instead of header information. You have toprovide password or keyfile to dump volume key.Beware that the volume key can be used to decrypt the data storedin the container without a passphrase. This means that if thevolume key is compromised, the whole device has to be erased toprevent further access. Use this option carefully.<options> can be [--dump-volume-key, --volume-key-file,--key-file, --keyfile-offset, --keyfile-size, --timeout].",
        "name": "cryptsetup-fvault2Dump - dump the header information of a FVAULT2(FileVault2 compatible) device",
        "section": 8
    },
    {
        "command": "cryptsetup-fvault2Open",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-fvault2dump",
        "description": "Dump the header information of a FVAULT2 (FileVault2 compatible)device.If the --dump-volume-key option is used, the FVAULT2 devicevolume key is dumped instead of header information. You have toprovide password or keyfile to dump volume key.Beware that the volume key can be used to decrypt the data storedin the container without a passphrase. This means that if thevolume key is compromised, the whole device has to be erased toprevent further access. Use this option carefully.<options> can be [--dump-volume-key, --volume-key-file,--key-file, --keyfile-offset, --keyfile-size, --timeout].",
        "name": "cryptsetup-fvault2Dump - dump the header information of a FVAULT2(FileVault2 compatible) device",
        "section": 8
    },
    {
        "command": "cryptsetup-fvault2open",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-isLuks",
        "description": "Returns true, if <device> is a LUKS device, false otherwise.Use option -v to get human-readable feedback. 'Commandsuccessful.' means the device is a LUKS device.By specifying --type you may query for specific LUKS version.<options> can be [--header, --type, --disable-locks].",
        "name": "cryptsetup-isLuks - check if a device is a LUKS device",
        "section": 8
    },
    {
        "command": "cryptsetup-isluks",
        "description": "Returns true, if <device> is a LUKS device, false otherwise.Use option -v to get human-readable feedback. 'Commandsuccessful.' means the device is a LUKS device.By specifying --type you may query for specific LUKS version.<options> can be [--header, --type, --disable-locks].",
        "name": "cryptsetup-isLuks - check if a device is a LUKS device",
        "section": 8
    },
    {
        "command": "cryptsetup-loopaesOpen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-loopaesopen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-luksAddKey",
        "description": "Adds a keyslot protected by a new passphrase. An existingpassphrase must be supplied interactively, via --key-file orLUKS2 token (plugin). Alternatively to existing passphrase usermay pass directly volume key (via --volume-key-file). The newpassphrase to be added can be specified interactively, read fromthe file given as the positional argument (also via --new-keyfileparameter) or via LUKS2 token.NOTE: with --unbound option the action creates new unbound LUKS2keyslot. The keyslot cannot be used for device activation. If youdon\u2019t pass new key via --volume-key-file option, new random keyis generated. Existing passphrase for any active keyslot is notrequired.NOTE: some parameters are effective only if used with LUKS2format that supports per-keyslot parameters. For LUKS1, PBKDFtype and hash algorithm is always the same for all keyslots.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--new-keyfile, --new-keyfile-offset, --new-keyfile-size,--key-slot, --new-key-slot, --volume-key-file, --force-password,--hash, --header, --disable-locks, --iter-time, --pbkdf,--pbkdf-force-iterations, --pbkdf-memory, --pbkdf-parallel,--unbound, --type, --keyslot-cipher, --keyslot-key-size,--key-size, --timeout, --token-id, --token-type, --token-only,--new-token-id, --verify-passphrase].",
        "name": "cryptsetup-luksAddKey - add a new passphrase",
        "section": 8
    },
    {
        "command": "cryptsetup-luksChangeKey",
        "description": "Changes an existing passphrase. The passphrase to be changed mustbe supplied interactively or via --key-file. The new passphrasecan be supplied interactively or in a file given as thepositional argument.If a key-slot is specified (via --key-slot), the passphrase forthat key-slot must be given and the new passphrase will overwritethe specified key-slot. If no key-slot is specified and there isstill a free key-slot, then the new passphrase will be put into afree key-slot before the key-slot containing the old passphraseis purged. If there is no free key-slot, then the key-slot withthe old passphrase is overwritten directly.WARNING: If a key-slot is overwritten, a media failure duringthis operation can cause the overwrite to fail after the oldpassphrase has been wiped and make the LUKS containerinaccessible.NOTE: some parameters are effective only if used with LUKS2format that supports per-keyslot parameters. For LUKS1, PBKDFtype and hash algorithm is always the same for all keyslots.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--new-keyfile-offset, --iter-time, --pbkdf,--pbkdf-force-iterations, --pbkdf-memory, --pbkdf-parallel,--new-keyfile-size, --key-slot, --force-password, --hash,--header, --disable-locks, --type, --keyslot-cipher,--keyslot-key-size, --timeout, --verify-passphrase].",
        "name": "cryptsetup-luksChangeKey - change an existing passphrase",
        "section": 8
    },
    {
        "command": "cryptsetup-luksConvertKey",
        "description": "Converts an existing LUKS2 keyslot to new PBKDF parameters. Thepassphrase for keyslot to be converted must be suppliedinteractively or via --key-file. If no --pbkdf parameters arespecified LUKS2 default PBKDF values will apply.If a keyslot is specified (via --key-slot), the passphrase forthat keyslot must be given. If no keyslot is specified and thereis still a free keyslot, then the new parameters will be put intoa free keyslot before the keyslot containing the old parametersis purged. If there is no free keyslot, then the keyslot with theold parameters is overwritten directly.WARNING: If a keyslot is overwritten, a media failure during thisoperation can cause the overwrite to fail after the oldparameters have been wiped and make the LUKS containerinaccessible.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-slot, --hash, --header, --disable-locks, --iter-time,--pbkdf, --pbkdf-force-iterations, --pbkdf-memory,--pbkdf-parallel, --keyslot-cipher, --keyslot-key-size,--timeout, --verify-passphrase].",
        "name": "cryptsetup-luksConvertKey - converts an existing LUKS2 keyslot tonew PBKDF parameters",
        "section": 8
    },
    {
        "command": "cryptsetup-luksDump",
        "description": "Dump the header information of a LUKS device.If the --dump-volume-key option is used, the LUKS device volumekey is dumped instead of the keyslot info. Together with the--volume-key-file option, volume key is dumped to a file insteadof standard output. Beware that the volume key cannot be changedwithout reencryption and can be used to decrypt the data storedin the LUKS container without a passphrase and even without theLUKS header. This means that if the volume key is compromised,the whole device has to be erased or reencrypted to preventfurther access. Use this option carefully.To dump the volume key, a passphrase has to be supplied, eitherinteractively or via --key-file.To dump unbound key (LUKS2 format only), --unbound parameter,specific --key-slot id and proper passphrase has to be supplied,either interactively or via --key-file. Optional--volume-key-file parameter enables unbound keyslot dump to afile.To dump LUKS2 JSON metadata (without basic header informationlike UUID) use --dump-json-metadata option.<options> can be [--dump-volume-key, --dump-json-metadata,--key-file, --keyfile-offset, --keyfile-size, --header,--disable-locks, --volume-key-file, --type, --unbound,--key-slot, --timeout].WARNING: If --dump-volume-key is used with --key-file and theargument to --key-file is '-', no validation question will beasked and no warning given.",
        "name": "cryptsetup-luksDump - dump the header information of a LUKSdevice",
        "section": 8
    },
    {
        "command": "cryptsetup-luksErase",
        "description": "Erase all keyslots and make the LUKS container permanentlyinaccessible. You do not need to provide any password for thisoperation.WARNING: This operation is irreversible.<options> can be [--header, --disable-locks].",
        "name": "cryptsetup-erase, cryptsetup-luksErase - erase all keyslots",
        "section": 8
    },
    {
        "command": "cryptsetup-luksFormat",
        "description": "Initializes a LUKS partition and sets the initial passphrase (forkey-slot 0), either via prompting or via <key file>. Note that ifthe second argument is present, then the passphrase is taken fromthe file given there, without the need to use the --key-fileoption. Also note that for both forms of reading the passphrasefrom a file you can give '-' as file name, which results in thepassphrase being read from stdin and the safety-question beingskipped.You cannot call luksFormat on a device or filesystem that ismapped or in use, e.g., mounted filesystem, used in LVM, activeRAID member, etc. The device or filesystem has to be un-mountedin order to call luksFormat.To use specific version of LUKS format, use --type luks1 or typeluks2.<options> can be [--hash, --cipher, --verify-passphrase,--key-size, --key-slot, --key-file (takes precedence overoptional second argument), --keyfile-offset, --keyfile-size,--use-random, --use-urandom, --uuid, --volume-key-file,--iter-time, --header, --pbkdf-force-iterations,--force-password, --disable-locks, --timeout, --type, --offset,--align-payload (deprecated)].For LUKS2, additional <options> can be [--integrity,--integrity-no-wipe, --sector-size, --label, --subsystem,--pbkdf, --pbkdf-memory, --pbkdf-parallel, --disable-locks,--disable-keyring, --luks2-metadata-size, --luks2-keyslots-size,--keyslot-cipher, --keyslot-key-size,--integrity-legacy-padding].WARNING: Doing a luksFormat on an existing LUKS container willmake all data in the old container permanently irretrievableunless you have a header backup.",
        "name": "cryptsetup-luksFormat - initialize a LUKS partition and set theinitial passphrase",
        "section": 8
    },
    {
        "command": "cryptsetup-luksHeaderBackup",
        "description": "Stores a binary backup of the LUKS header and keyslot area.NOTE: Using '-' as filename writes the header backup to a filenamed '-'.<options> can be [--header, --header-backup-file,--disable-locks].WARNING: This backup file and a passphrase valid at the time ofbackup allows decryption of the LUKS data area, even if thepassphrase was later changed or removed from the LUKS device.Also note that with a header backup you lose the ability tosecurely wipe the LUKS device by just overwriting the header andkey-slots. You either need to securely erase all header backupsin addition or overwrite the encrypted data area as well. Thesecond option is less secure, as some sectors can survive, e.g.,due to defect management.",
        "name": "cryptsetup-luksHeaderBackup - store a binary backup of the LUKSheader and keyslot area",
        "section": 8
    },
    {
        "command": "cryptsetup-luksHeaderRestore",
        "description": "Restores a binary backup of the LUKS header and keyslot area fromthe specified file.NOTE: Using '-' as filename reads the header backup from a filenamed '-'.<options> can be [--header, --header-backup-file,--disable-locks].WARNING: Header and keyslots will be replaced, only thepassphrases from the backup will work afterward.This command requires that the volume key size and data offset ofthe LUKS header already on the device and of the header backupmatch. Alternatively, if there is no LUKS header on the device,the backup will also be written to it.",
        "name": "cryptsetup-luksHeaderRestore - restore a binary backup of theLUKS header and keyslot area",
        "section": 8
    },
    {
        "command": "cryptsetup-luksKillSlot",
        "description": "Wipe the key-slot number <key slot> from the LUKS device. Exceptrunning in batch-mode (-q) a remaining passphrase must besupplied, either interactively or via --key-file. This commandcan remove the last remaining key-slot, but requires aninteractive confirmation when doing so. Removing the lastpassphrase makes a LUKS container permanently inaccessible.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--header, --disable-locks, --type, --verify-passphrase,--timeout].WARNING: If you read the passphrase from stdin (without furtherargument or with '-' as an argument to --key-file), batch-mode(-q) will be implicitly switched on and no warning will be givenwhen you remove the last remaining passphrase from a LUKScontainer. Removing the last passphrase makes the LUKS containerpermanently inaccessible.NOTE: If there is no passphrase provided (on stdin or through--key-file argument) and batch-mode (-q) is active, the key-slotis removed without any other warning.",
        "name": "cryptsetup-luksKillSlot - wipe a key-slot from the LUKS device",
        "section": 8
    },
    {
        "command": "cryptsetup-luksOpen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-luksRemoveKey",
        "description": "Removes the supplied passphrase from the LUKS device. Thepassphrase to be removed can be specified interactively, as thepositional argument or via --key-file.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--header, --disable-locks, --type, --timeout,--verify-passphrase].WARNING: If you read the passphrase from stdin (without furtherargument or with '-' as an argument to --key-file), batch-mode(-q) will be implicitly switched on and no warning will be givenwhen you remove the last remaining passphrase from a LUKScontainer. Removing the last passphrase makes the LUKS containerpermanently inaccessible.",
        "name": "cryptsetup-luksRemoveKey - remove the supplied passphrase fromthe LUKS device",
        "section": 8
    },
    {
        "command": "cryptsetup-luksResume",
        "description": "Resumes a suspended device and reinstates the encryption key.Prompts interactively for a passphrase if no token is usable(LUKS2 only) or --key-file is not given.<options> can be [--key-file, --keyfile-size, --keyfile-offset,--key-slot, --header, --disable-keyring, --disable-locks,--token-id, --token-only, --token-type,--disable-external-tokens, --type, --tries, --timeout,--verify-passphrase].",
        "name": "cryptsetup-luksResume - resume a suspended device and reinstatethe key",
        "section": 8
    },
    {
        "command": "cryptsetup-luksSuspend",
        "description": "Suspends an active device (all IO operations will block andaccesses to the device will wait indefinitely) and wipes theencryption key from kernel memory. Needs kernel 2.6.19 or later.After this operation, you have to use luksResume to reinstate theencryption key and unblock the device or close to remove themapped device.<options> can be [--header, --disable-locks].WARNING: Never suspend the device on which the cryptsetup binaryresides.",
        "name": "cryptsetup-luksSuspend - suspends an active device and wipes thekey",
        "section": 8
    },
    {
        "command": "cryptsetup-luksUUID",
        "description": "Print the UUID of a LUKS device.Set new UUID if --uuid option is specified.<options> can be [--header, --uuid, --type, --disable-locks].",
        "name": "cryptsetup-luksUUID - print or set the UUID of a LUKS device",
        "section": 8
    },
    {
        "command": "cryptsetup-luksaddkey",
        "description": "Adds a keyslot protected by a new passphrase. An existingpassphrase must be supplied interactively, via --key-file orLUKS2 token (plugin). Alternatively to existing passphrase usermay pass directly volume key (via --volume-key-file). The newpassphrase to be added can be specified interactively, read fromthe file given as the positional argument (also via --new-keyfileparameter) or via LUKS2 token.NOTE: with --unbound option the action creates new unbound LUKS2keyslot. The keyslot cannot be used for device activation. If youdon\u2019t pass new key via --volume-key-file option, new random keyis generated. Existing passphrase for any active keyslot is notrequired.NOTE: some parameters are effective only if used with LUKS2format that supports per-keyslot parameters. For LUKS1, PBKDFtype and hash algorithm is always the same for all keyslots.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--new-keyfile, --new-keyfile-offset, --new-keyfile-size,--key-slot, --new-key-slot, --volume-key-file, --force-password,--hash, --header, --disable-locks, --iter-time, --pbkdf,--pbkdf-force-iterations, --pbkdf-memory, --pbkdf-parallel,--unbound, --type, --keyslot-cipher, --keyslot-key-size,--key-size, --timeout, --token-id, --token-type, --token-only,--new-token-id, --verify-passphrase].",
        "name": "cryptsetup-luksAddKey - add a new passphrase",
        "section": 8
    },
    {
        "command": "cryptsetup-lukschangekey",
        "description": "Changes an existing passphrase. The passphrase to be changed mustbe supplied interactively or via --key-file. The new passphrasecan be supplied interactively or in a file given as thepositional argument.If a key-slot is specified (via --key-slot), the passphrase forthat key-slot must be given and the new passphrase will overwritethe specified key-slot. If no key-slot is specified and there isstill a free key-slot, then the new passphrase will be put into afree key-slot before the key-slot containing the old passphraseis purged. If there is no free key-slot, then the key-slot withthe old passphrase is overwritten directly.WARNING: If a key-slot is overwritten, a media failure duringthis operation can cause the overwrite to fail after the oldpassphrase has been wiped and make the LUKS containerinaccessible.NOTE: some parameters are effective only if used with LUKS2format that supports per-keyslot parameters. For LUKS1, PBKDFtype and hash algorithm is always the same for all keyslots.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--new-keyfile-offset, --iter-time, --pbkdf,--pbkdf-force-iterations, --pbkdf-memory, --pbkdf-parallel,--new-keyfile-size, --key-slot, --force-password, --hash,--header, --disable-locks, --type, --keyslot-cipher,--keyslot-key-size, --timeout, --verify-passphrase].",
        "name": "cryptsetup-luksChangeKey - change an existing passphrase",
        "section": 8
    },
    {
        "command": "cryptsetup-luksconvertkey",
        "description": "Converts an existing LUKS2 keyslot to new PBKDF parameters. Thepassphrase for keyslot to be converted must be suppliedinteractively or via --key-file. If no --pbkdf parameters arespecified LUKS2 default PBKDF values will apply.If a keyslot is specified (via --key-slot), the passphrase forthat keyslot must be given. If no keyslot is specified and thereis still a free keyslot, then the new parameters will be put intoa free keyslot before the keyslot containing the old parametersis purged. If there is no free keyslot, then the keyslot with theold parameters is overwritten directly.WARNING: If a keyslot is overwritten, a media failure during thisoperation can cause the overwrite to fail after the oldparameters have been wiped and make the LUKS containerinaccessible.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-slot, --hash, --header, --disable-locks, --iter-time,--pbkdf, --pbkdf-force-iterations, --pbkdf-memory,--pbkdf-parallel, --keyslot-cipher, --keyslot-key-size,--timeout, --verify-passphrase].",
        "name": "cryptsetup-luksConvertKey - converts an existing LUKS2 keyslot tonew PBKDF parameters",
        "section": 8
    },
    {
        "command": "cryptsetup-luksdump",
        "description": "Dump the header information of a LUKS device.If the --dump-volume-key option is used, the LUKS device volumekey is dumped instead of the keyslot info. Together with the--volume-key-file option, volume key is dumped to a file insteadof standard output. Beware that the volume key cannot be changedwithout reencryption and can be used to decrypt the data storedin the LUKS container without a passphrase and even without theLUKS header. This means that if the volume key is compromised,the whole device has to be erased or reencrypted to preventfurther access. Use this option carefully.To dump the volume key, a passphrase has to be supplied, eitherinteractively or via --key-file.To dump unbound key (LUKS2 format only), --unbound parameter,specific --key-slot id and proper passphrase has to be supplied,either interactively or via --key-file. Optional--volume-key-file parameter enables unbound keyslot dump to afile.To dump LUKS2 JSON metadata (without basic header informationlike UUID) use --dump-json-metadata option.<options> can be [--dump-volume-key, --dump-json-metadata,--key-file, --keyfile-offset, --keyfile-size, --header,--disable-locks, --volume-key-file, --type, --unbound,--key-slot, --timeout].WARNING: If --dump-volume-key is used with --key-file and theargument to --key-file is '-', no validation question will beasked and no warning given.",
        "name": "cryptsetup-luksDump - dump the header information of a LUKSdevice",
        "section": 8
    },
    {
        "command": "cryptsetup-lukserase",
        "description": "Erase all keyslots and make the LUKS container permanentlyinaccessible. You do not need to provide any password for thisoperation.WARNING: This operation is irreversible.<options> can be [--header, --disable-locks].",
        "name": "cryptsetup-erase, cryptsetup-luksErase - erase all keyslots",
        "section": 8
    },
    {
        "command": "cryptsetup-luksformat",
        "description": "Initializes a LUKS partition and sets the initial passphrase (forkey-slot 0), either via prompting or via <key file>. Note that ifthe second argument is present, then the passphrase is taken fromthe file given there, without the need to use the --key-fileoption. Also note that for both forms of reading the passphrasefrom a file you can give '-' as file name, which results in thepassphrase being read from stdin and the safety-question beingskipped.You cannot call luksFormat on a device or filesystem that ismapped or in use, e.g., mounted filesystem, used in LVM, activeRAID member, etc. The device or filesystem has to be un-mountedin order to call luksFormat.To use specific version of LUKS format, use --type luks1 or typeluks2.<options> can be [--hash, --cipher, --verify-passphrase,--key-size, --key-slot, --key-file (takes precedence overoptional second argument), --keyfile-offset, --keyfile-size,--use-random, --use-urandom, --uuid, --volume-key-file,--iter-time, --header, --pbkdf-force-iterations,--force-password, --disable-locks, --timeout, --type, --offset,--align-payload (deprecated)].For LUKS2, additional <options> can be [--integrity,--integrity-no-wipe, --sector-size, --label, --subsystem,--pbkdf, --pbkdf-memory, --pbkdf-parallel, --disable-locks,--disable-keyring, --luks2-metadata-size, --luks2-keyslots-size,--keyslot-cipher, --keyslot-key-size,--integrity-legacy-padding].WARNING: Doing a luksFormat on an existing LUKS container willmake all data in the old container permanently irretrievableunless you have a header backup.",
        "name": "cryptsetup-luksFormat - initialize a LUKS partition and set theinitial passphrase",
        "section": 8
    },
    {
        "command": "cryptsetup-luksheaderbackup",
        "description": "Stores a binary backup of the LUKS header and keyslot area.NOTE: Using '-' as filename writes the header backup to a filenamed '-'.<options> can be [--header, --header-backup-file,--disable-locks].WARNING: This backup file and a passphrase valid at the time ofbackup allows decryption of the LUKS data area, even if thepassphrase was later changed or removed from the LUKS device.Also note that with a header backup you lose the ability tosecurely wipe the LUKS device by just overwriting the header andkey-slots. You either need to securely erase all header backupsin addition or overwrite the encrypted data area as well. Thesecond option is less secure, as some sectors can survive, e.g.,due to defect management.",
        "name": "cryptsetup-luksHeaderBackup - store a binary backup of the LUKSheader and keyslot area",
        "section": 8
    },
    {
        "command": "cryptsetup-luksheaderrestore",
        "description": "Restores a binary backup of the LUKS header and keyslot area fromthe specified file.NOTE: Using '-' as filename reads the header backup from a filenamed '-'.<options> can be [--header, --header-backup-file,--disable-locks].WARNING: Header and keyslots will be replaced, only thepassphrases from the backup will work afterward.This command requires that the volume key size and data offset ofthe LUKS header already on the device and of the header backupmatch. Alternatively, if there is no LUKS header on the device,the backup will also be written to it.",
        "name": "cryptsetup-luksHeaderRestore - restore a binary backup of theLUKS header and keyslot area",
        "section": 8
    },
    {
        "command": "cryptsetup-lukskillslot",
        "description": "Wipe the key-slot number <key slot> from the LUKS device. Exceptrunning in batch-mode (-q) a remaining passphrase must besupplied, either interactively or via --key-file. This commandcan remove the last remaining key-slot, but requires aninteractive confirmation when doing so. Removing the lastpassphrase makes a LUKS container permanently inaccessible.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--header, --disable-locks, --type, --verify-passphrase,--timeout].WARNING: If you read the passphrase from stdin (without furtherargument or with '-' as an argument to --key-file), batch-mode(-q) will be implicitly switched on and no warning will be givenwhen you remove the last remaining passphrase from a LUKScontainer. Removing the last passphrase makes the LUKS containerpermanently inaccessible.NOTE: If there is no passphrase provided (on stdin or through--key-file argument) and batch-mode (-q) is active, the key-slotis removed without any other warning.",
        "name": "cryptsetup-luksKillSlot - wipe a key-slot from the LUKS device",
        "section": 8
    },
    {
        "command": "cryptsetup-luksopen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-luksremovekey",
        "description": "Removes the supplied passphrase from the LUKS device. Thepassphrase to be removed can be specified interactively, as thepositional argument or via --key-file.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--header, --disable-locks, --type, --timeout,--verify-passphrase].WARNING: If you read the passphrase from stdin (without furtherargument or with '-' as an argument to --key-file), batch-mode(-q) will be implicitly switched on and no warning will be givenwhen you remove the last remaining passphrase from a LUKScontainer. Removing the last passphrase makes the LUKS containerpermanently inaccessible.",
        "name": "cryptsetup-luksRemoveKey - remove the supplied passphrase fromthe LUKS device",
        "section": 8
    },
    {
        "command": "cryptsetup-luksresume",
        "description": "Resumes a suspended device and reinstates the encryption key.Prompts interactively for a passphrase if no token is usable(LUKS2 only) or --key-file is not given.<options> can be [--key-file, --keyfile-size, --keyfile-offset,--key-slot, --header, --disable-keyring, --disable-locks,--token-id, --token-only, --token-type,--disable-external-tokens, --type, --tries, --timeout,--verify-passphrase].",
        "name": "cryptsetup-luksResume - resume a suspended device and reinstatethe key",
        "section": 8
    },
    {
        "command": "cryptsetup-lukssuspend",
        "description": "Suspends an active device (all IO operations will block andaccesses to the device will wait indefinitely) and wipes theencryption key from kernel memory. Needs kernel 2.6.19 or later.After this operation, you have to use luksResume to reinstate theencryption key and unblock the device or close to remove themapped device.<options> can be [--header, --disable-locks].WARNING: Never suspend the device on which the cryptsetup binaryresides.",
        "name": "cryptsetup-luksSuspend - suspends an active device and wipes thekey",
        "section": 8
    },
    {
        "command": "cryptsetup-luksuuid",
        "description": "Print the UUID of a LUKS device.Set new UUID if --uuid option is specified.<options> can be [--header, --uuid, --type, --disable-locks].",
        "name": "cryptsetup-luksUUID - print or set the UUID of a LUKS device",
        "section": 8
    },
    {
        "command": "cryptsetup-open",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-plainOpen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-plainopen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-reencrypt",
        "description": "Run LUKS device reencryption.There are 3 basic modes of operation:\u2022device reencryption (reencrypt)\u2022device encryption (reencrypt --encrypt/--new/-N)\u2022device decryption (reencrypt --decrypt)<device> or --active-name <name> (LUKS2 only) is mandatoryparameter.Cryptsetup reencrypt action can be used to change reencryptionparameters which otherwise require full on-disk data change(re-encryption). The reencrypt action reencrypts data on LUKSdevice in-place.You can regenerate volume key (the real key used in on-diskencryption unclocked by passphrase), cipher, cipher mode orencryption sector size (LUKS2 only).Reencryption process may be safely interrupted by a user viaSIGINT signal (ctrl+c). Same applies to SIGTERM signal (i.e.issued by systemd during system shutdown).For in-place encryption mode, the reencrypt action additionallytakes all options available for luksFormat action for respectiveLUKS version (see cryptsetup-luksFormat man page for moredetails). See cryptsetup-luksFormat(8).NOTE that for encrypt and decrypt mode, the whole device must betreated as unencrypted \u2014 there are no quarantees ofconfidentiality as part of the device contains plaintext.ALWAYS BE SURE YOU HAVE RELIABLE BACKUP BEFORE USING THIS ACTIONON LUKS DEVICE.<options> can be [--batch-mode, --block-size, --cipher, --debug,--debug-json, --decrypt, --device-size, --disable-locks,--encrypt, --force-offline-reencrypt, --hash, --header,--hotzone-size, --iter-time, --init-only, --keep-key, --key-file,--key-size, --key-slot, --keyfile-offset, --keyfile-size,--tries, --timeout, --pbkdf, --pbkdf-force-iterations,--pbkdf-memory, --pbkdf-parallel, --progress-frequency,--progress-json, --reduce-device-size, --resilience,--resilience-hash, --resume-only, --sector-size, --use-directio,--use-random, --use-urandom, --use-fsync, --uuid, --verbose,--volume-key-file, --write-log].",
        "name": "cryptsetup-reencrypt - reencrypt LUKS encrypted volumes in-place",
        "section": 8
    },
    {
        "command": "cryptsetup-refresh",
        "description": "Refreshes parameters of active mapping <name>.Updates parameters of active device <name> without the need todeactivate the device (and umount filesystem). Currently, itsupports parameters refresh on following devices: LUKS1, LUKS2(including authenticated encryption), plain crypt and loop-AES.Mandatory parameters are identical to those of an open action forthe respective device type.You may change following parameters on all devices--perf-same_cpu_crypt, --perf-submit_from_crypt_cpus,--perf-no_read_workqueue, --perf-no_write_workqueue and--allow-discards.Refreshing the device without any optional parameter will refreshthe device with default setting (respective to device type).LUKS2 only:The --integrity-no-journal parameter affects only LUKS2 deviceswith the underlying dm-integrity device.Adding option --persistent stores any combination of deviceparameters above in LUKS2 metadata (only after successful refreshoperation).The --disable-keyring parameter refreshes a device with volumekey passed in dm-crypt driver.<options> can be [--allow-discards, --perf-same_cpu_crypt,--perf-submit_from_crypt_cpus, --perf-no_read_workqueue,--perf-no_write_workqueue, --header, --disable-keyring,--disable-locks, --persistent, --integrity-no-journal].",
        "name": "cryptsetup-refresh - refresh parameters of an active mapping",
        "section": 8
    },
    {
        "command": "cryptsetup-repair",
        "description": "Tries to repair the device metadata if possible. Currentlysupported only for LUKS device type.This command is useful to fix some known benign LUKS metadataheader corruptions. Only basic corruptions of unused keyslot arefixable. This command will only change the LUKS header, not anykey-slot data. You may enforce LUKS version by adding --typeoption.It also repairs (upgrades) LUKS2 reencryption metadata by addinga metadata digest that protects it against malicious changes.If LUKS2 reencryption was interrupted in the middle of writingreencryption segment the repair command can be used to performreencryption recovery so that reencryption can continue later.Repairing reencryption requires verification of reencryptionkeyslot so passphrase or keyfile is needed.<options> can be [--timeout, --verify-passphrase,--disable-locks, --type, --header, --key-file, --keyfile-size,--keyfile-offset, --key-slot].WARNING: Always create a binary backup of the original headerbefore calling this command.",
        "name": "cryptsetup-repair - repair the device metadata",
        "section": 8
    },
    {
        "command": "cryptsetup-resize",
        "description": "Resizes an active mapping <name>.If --size (in 512-bytes sectors) or --device-size are notspecified, the size is computed from the underlying device. ForLUKS it is the size of the underlying device without the areareserved for LUKS header (see data payload offset in luksDumpcommand). For plain crypt device, the whole device size is used.Note that this does not change the raw device geometry, it justchanges how many sectors of the raw device are represented in themapped device.If cryptsetup detected volume key for active device loaded inkernel keyring service, resize action would first try to retrievethe key using a token. Only if it failed, it\u2019d ask for apassphrase to unlock a keyslot (LUKS) or to derive a volume keyagain (plain mode). The kernel keyring is used by default forLUKS2 devices.<options> can be [--size, --device-size, --token-id,--token-only, --token-type, --key-slot, --key-file,--keyfile-size, --keyfile-offset, --timeout,--disable-external-tokens, --disable-locks, --disable-keyring,--verify-passphrase, --timeout].",
        "name": "cryptsetup-resize - resize an active mapping",
        "section": 8
    },
    {
        "command": "cryptsetup-ssh",
        "description": "Experimental cryptsetup plugin for unlocking LUKS2 devices withtoken connected to an SSH server.This plugin currently allows only adding a token to an existingkey slot. See cryptsetup(8) for instructions on how to remove,import or export the token.Add operationadd <options> <device>Adds the SSH token to <device>.The specified SSH server must contain a key file on the specifiedpath with a passphrase for an existing key slot on the device.Provided credentials will be used by cryptsetup to get thepassword when opening the device using the token.Options --ssh-server, --ssh-user, --ssh-keypath and --ssh-pathare required for this operation.",
        "name": "cryptsetup-ssh - manage LUKS2 SSH token",
        "section": 8
    },
    {
        "command": "cryptsetup-status",
        "description": "Reports the status for the mapping <name>.<options> can be [--header, --disable-locks].",
        "name": "cryptsetup-status - report the status for a mapping",
        "section": 8
    },
    {
        "command": "cryptsetup-tcryptDump",
        "description": "Dump the header information of a TCRYPT (TrueCrypt or VeraCryptcompatible) device.If the --dump-volume-key option is used, the TCRYPT device volumekey is dumped instead of TCRYPT header info. Beware that thevolume key (or concatenated volume keys if cipher chain is used)can be used to decrypt the data stored in the TCRYPT containerwithout a passphrase. This means that if the volume key iscompromised, the whole device has to be erased to prevent furtheraccess. Use this option carefully.<options> can be [--dump-volume-key, --key-file, --tcrypt-hidden,--tcrypt-system, --tcrypt-backup, --veracrypt (ignored),--disable-veracrypt, --veracrypt-pim, --veracrypt-query-pim,--cipher, --hash, --header, --verify-passphrase, --timeout].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated.",
        "name": "cryptsetup-tcryptDump - dump the header information of a TCRYPT(TrueCrypt or VeraCrypt compatible) device",
        "section": 8
    },
    {
        "command": "cryptsetup-tcryptOpen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-tcryptdump",
        "description": "Dump the header information of a TCRYPT (TrueCrypt or VeraCryptcompatible) device.If the --dump-volume-key option is used, the TCRYPT device volumekey is dumped instead of TCRYPT header info. Beware that thevolume key (or concatenated volume keys if cipher chain is used)can be used to decrypt the data stored in the TCRYPT containerwithout a passphrase. This means that if the volume key iscompromised, the whole device has to be erased to prevent furtheraccess. Use this option carefully.<options> can be [--dump-volume-key, --key-file, --tcrypt-hidden,--tcrypt-system, --tcrypt-backup, --veracrypt (ignored),--disable-veracrypt, --veracrypt-pim, --veracrypt-query-pim,--cipher, --hash, --header, --verify-passphrase, --timeout].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated.",
        "name": "cryptsetup-tcryptDump - dump the header information of a TCRYPT(TrueCrypt or VeraCrypt compatible) device",
        "section": 8
    },
    {
        "command": "cryptsetup-tcryptopen",
        "description": "Opens (creates a mapping with) <name> backed by device <device>.Device type can be plain, luks (default), luks1, luks2, loopaesor tcrypt.For backward compatibility there are open command aliases:create (argument-order <name> <device>): open --type plainplainOpen: open --type plainluksOpen: open --type luksloopaesOpen: open --type loopaestcryptOpen: open --type tcryptbitlkOpen: open --type bitlk<options> are type specific and are described below forindividual device types. For create, the order of the <name> and<device> options is inverted for historical reasons, all otheraliases use the standard <device> <name> order.PLAINopen --type plain <device> <name>plainOpen <device> <name> (old syntax)create <name> <device> (OBSOLETE syntax)Opens (creates a mapping with) <name> backed by device <device>.<options> can be [--hash, --cipher, --verify-passphrase,--sector-size, --key-file, --keyfile-size, --keyfile-offset,--key-size, --offset, --skip, --device-size, --size, --readonly,--shared, --allow-discards, --refresh, --timeout,--verify-passphrase, --iv-large-sectors].Example: 'cryptsetup open --type plain /dev/sda10 e1' maps theraw encrypted device /dev/sda10 to the mapped (decrypted) device/dev/mapper/e1, which can then be mounted, fsck-ed or have afilesystem created on it.LUKSopen <device> <name>open --type <luks1|luks2> <device> <name> (explicit versionrequest)luksOpen <device> <name> (old syntax)Opens the LUKS device <device> and sets up a mapping <name> aftersuccessful verification of the supplied passphrase.First, the passphrase is searched in LUKS tokens. If it\u2019s notfound in any token and also the passphrase is not supplied via--key-file, the command prompts for it interactively.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--readonly, --test-passphrase, --allow-discards, --header,--key-slot, --volume-key-file, --token-id, --token-only,--token-type, --disable-external-tokens, --disable-keyring,--disable-locks, --type, --refresh,--serialize-memory-hard-pbkdf, --unbound, --tries, --timeout,--verify-passphrase, --persistent].loopAESopen --type loopaes <device> <name> --key-file <keyfile>loopaesOpen <device> <name> --key-file <keyfile> (old syntax)Opens the loop-AES <device> and sets up a mapping <name>.If the key file is encrypted with GnuPG, then you have to use--key-file=- and decrypt it before use, e.g., like this:gpg --decrypt <keyfile> | cryptsetup loopaesOpen --key-file=-<device> <name>WARNING: The loop-AES extension cannot use the direct input ofthe key file on the real terminal because the keys are separatedby end-of-line and only part of the multi-key file would be read.If you need it in script, just use the pipe redirection:echo $keyfile | cryptsetup loopaesOpen --key-file=- <device><name>Use --keyfile-size to specify the proper key length if needed.Use --offset to specify device offset. Note that the units needto be specified in number of 512 byte sectors.Use --skip to specify the IV offset. If the original device usedan offset and but did not use it in IV sector calculations, youhave to explicitly use --skip 0 in addition to the offsetparameter.Use --hash to override the default hash function for passphrasehashing (otherwise it is detected according to key size).<options> can be [--cipher, --key-file, --keyfile-size,--keyfile-offset, --key-size, --offset, --skip, --hash,--readonly, --allow-discards, --refresh].TrueCrypt and VeraCryptopen --type tcrypt <device> <name>tcryptOpen <device> <name> (old syntax)Opens the TCRYPT (TrueCrypt and VeraCrypt compatible) <device>and sets up a mapping <name>.<options> can be [--key-file, --tcrypt-hidden, --tcrypt-system,--tcrypt-backup, --readonly, --test-passphrase, --allow-discards,--veracrypt (ignored), --disable-veracrypt, --veracrypt-pim,--veracrypt-query-pim, --header, --cipher, --hash, --tries,--timeout, --verify-passphrase].The keyfile parameter allows a combination of file content withthe passphrase and can be repeated. Note that using keyfiles iscompatible with TCRYPT and is different from LUKS keyfile logic.If --cipher or --hash options are used, only cipher chains orPBKDF2 variants with the specified hash algorithms are checked.This could speed up unlocking the device (but also it revealssome information about the container).If you use --header in combination with hidden or system options,the header file must contain specific headers on the samepositions as the original encrypted container.WARNING: Option --allow-discards cannot be combined with option--tcrypt-hidden. For normal mapping, it can cause the destructionof hidden volume (hidden volume appears as unused space for outervolume so this space can be discarded).BitLockeropen --type bitlk <device> <name>bitlkOpen <device> <name> (old syntax)Opens the BITLK (a BitLocker compatible) <device> and sets up amapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].FileVault2open --type fvault2 <device> <name>fvault2Open <device> <name> (old syntax)Opens the FVAULT2 (a FileVault2 compatible) <device> and sets upa mapping <name>.<options> can be [--key-file, --keyfile-offset, --keyfile-size,--key-size, --readonly, --test-passphrase, --allow-discards--volume-key-file, --tries, --timeout, --verify-passphrase].",
        "name": "cryptsetup-open, cryptsetup-create, cryptsetup-plainOpen,cryptsetup-luksOpen, cryptsetup-loopaesOpen, cryptsetup-tcryptOpen, cryptsetup-bitlkOpen, cryptsetup-fvault2Open - openan encrypted device and create a mapping with a specified name",
        "section": 8
    },
    {
        "command": "cryptsetup-token",
        "description": "Action add creates a new keyring token to enable auto-activationof the device. For the auto-activation, the passphrase must bestored in keyring with the specified description. Usually, thepassphrase should be stored in user or user-session keyring. Thetoken command is supported only for LUKS2.For adding new keyring token, option --key-description ismandatory. Also, new token is assigned to key slot specified with--key-slot option or to all active key slots in the case--key-slot option is omitted.To remove existing token, specify the token ID which should beremoved with --token-id option.WARNING: The action token remove removes any token type, not justkeyring type from token slot specified by --token-id option.Action import can store arbitrary valid token json in LUKS2header. It may be passed via standard input or via file passed in--json-file option. If you specify --key-slot then successfullyimported token is also assigned to the key slot.Action export writes requested token JSON to a file passed with--json-file or to standard output.Action unassign removes token binding to specified keyslot. Bothtoken and keyslot must be specified by --token-id and --key-slotparameters.If --token-id is used with action add or action import and atoken with that ID already exists, option --token-replace can beused to replace the existing token.<options> can be [--header, --token-id, --key-slot,--key-description, --disable-external-tokens, --disable-locks,--disable-keyring, --json-file, --token-replace, --unbound].",
        "name": "cryptsetup-token - manage LUKS2 tokens",
        "section": 8
    },
    {
        "command": "csum",
        "description": "The csum action triggers checksum recalculation of specifiedpacket headers. It is commonly used to fix incorrect checksumsafter the pedit action has modified the packet content.",
        "name": "csum - checksum update action",
        "section": 8
    },
    {
        "command": "csysdig",
        "description": null,
        "name": null,
        "section": 8
    },
    {
        "command": "ct",
        "description": "The ct action is a tc action for sending packets and interactingwith the netfilter conntrack module.It can (as shown in the synopsis, in order):Send the packet to conntrack, and commit the connection, whileconfiguring a 32bit mark, 128bit label, and src/dst nat.Send the packet to conntrack, which will mark the packet with theconnection's state and configured metadata (mark/label), andexecute previous configured nat.Clear the packet's of previous connection tracking state.",
        "name": "ct - tc connection tracking action",
        "section": 8
    },
    {
        "command": "ctinfo",
        "description": "CTINFO (Conntrack Information) is a tc action for retrieving datafrom conntrack marks into various fields.At present it has twoindependent processing modes which may be viewed as sub-functions.DSCP mode copies a DSCP stored in conntrack's connmark into theIPv4/v6 diffserv field.The copying may conditionally occurbased on a flag also stored in the connmark.DSCP mode wasdesigned to assist in restoring packet classifications oningress, classifications which may then be used by qdiscs such asCAKE.It may be used in any circumstance where ingressclassification needs to be maintained across links that otherwisebleach or remap according to their own policies.CPMARK (copymark) mode copies the conntrack connmark into thepacket's mark field.Without additional parameters it isfunctionally completely equivalent to the existing connmarkaction.An optional mask may be specified to mask which bits ofthe connmark are restored.This may be useful when DSCP andCPMARK modes are combined.Simple statistics (tc -s) on DSCP restores and CPMARK copies aremaintained where values for set indicate a count of packetsaltered for that mode.DSCP includes an error count where thedestination packet's diffserv field was unwriteable.",
        "name": "ctinfo - tc connmark processing action",
        "section": 8
    },
    {
        "command": "ctrlaltdel",
        "description": "Based on examination of the linux/kernel/reboot.c code, it isclear that there are two supported functions that the<Ctrl-Alt-Del> sequence can perform.hardImmediately reboot the computer without calling sync(2) andwithout any other preparation. This is the default.softMake the kernel send the SIGINT (interrupt) signal to theinit process (this is always the process with PID 1). If thisoption is used, the init(8) program must support thisfeature. Since there are now several init(8) programs in theLinux community, please consult the documentation for theversion that you are currently using.When the command is run without any argument, it will display thecurrent setting.The function of ctrlaltdel is usually set in the /etc/rc.localfile.",
        "name": "ctrlaltdel - set the function of the Ctrl-Alt-Del combination",
        "section": 8
    },
    {
        "command": "ctstat",
        "description": "This manual page documents briefly the lnstat command.lnstat is a generalized and more feature-complete replacement forthe old rtstat program. It is commonly used to periodically printa selection of statistical values exported by the kernel.Inaddition to routing cache statistics, it supports any kind ofstatistics the linux kernel exports via a file in/proc/net/stat/.Each file in /proc/net/stat/ contains a header line listing thecolumn names.These names are used by lnstat as keys forselecting which statistics to print. For every CPU present in thesystem, a line follows which lists the actual values for eachcolumn of the file. lnstat sums these values up (which in factare counters) before printing them. After each interval, only thedifference to the last value is printed.Files and columns may be selected by using the -f and -kparameters. By default, all columns of all files are printed.",
        "name": "lnstat - unified linux network statistics",
        "section": 8
    },
    {
        "command": "cups-lpd",
        "description": "cups-lpd is the CUPS Line Printer Daemon (\"LPD\") mini-server thatsupports legacy client systems that use the LPD protocol.cups-lpd does not act as a standalone network daemon but insteadoperates using any of the Internet \"super-servers\" such asinetd(8), launchd(8), and systemd(8).",
        "name": "cups-lpd - receive print jobs and report printer status to lpdclients (deprecated)",
        "section": 8
    },
    {
        "command": "cups-snmp",
        "description": "The DEPRECATED CUPS SNMP backend provides legacy discovery andidentification of network printers using SNMPv1.When used fordiscovery through the scheduler, the backend will list allprinters that respond to a broadcast SNMPv1 query with the\"public\" community name.Additional queries are then sent toprinters that respond in order to determine the correct deviceURI, make and model, and other information needed for printing.In the first form, the SNMP backend is run directly by the userto look up the device URI and other information when you have anIP address or hostname.This can be used for programs that needto configure print queues where the user has supplied an addressbut nothing else.In the second form, the SNMP backend is run indirectly using thelpinfo(8) command.The output provides all printers detected viaSNMP on the configured broadcast addresses.Note: no broadcastaddresses are configured by default.",
        "name": "snmp - cups snmp backend (deprecated)",
        "section": 8
    },
    {
        "command": "cupsaccept",
        "description": "The cupsaccept command instructs the printing system to acceptprint jobs to the specified destinations.The cupsreject command instructs the printing system to rejectprint jobs to the specified destinations.The -r option sets thereason for rejecting print jobs. If not specified, the reasondefaults to \"Reason Unknown\".",
        "name": "cupsaccept/cupsreject - accept/reject jobs sent to a destination",
        "section": 8
    },
    {
        "command": "cupsctl",
        "description": "cupsctl updates or queries the cupsd.conf file for a server. Whenno changes are requested, the current configuration values arewritten to the standard output in the format \"name=value\", oneper line.",
        "name": "cupsctl - configure cupsd.conf options",
        "section": 8
    },
    {
        "command": "cupsd",
        "description": "cupsd is the scheduler for CUPS. It implements a printing systembased upon the Internet Printing Protocol, version 2.1, andsupports most of the requirements for IPP Everywhere. If nooptions are specified on the command-line then the defaultconfiguration file /etc/cups/cupsd.conf will be used.",
        "name": "cupsd - cups scheduler",
        "section": 8
    },
    {
        "command": "cupsd-helper",
        "description": "The cupsd-helper programs perform long-running operations onbehalf of the scheduler, cupsd(8).The cups-deviced helperprogram runs each CUPS backend(7) with no arguments in order todiscover the available printers.The cups-driverd helper program lists all available printerdrivers, a subset of \"matching\" printer drivers, or a copy of aspecific driver PPD file.The cups-exec helper program runs backends, filters, and otherprograms. On macOS these programs are run in a secure sandbox.",
        "name": "cupsd-helper - cupsd helper programs (deprecated)",
        "section": 8
    },
    {
        "command": "cupsdisable",
        "description": "cupsenable starts the named printers or classes while cupsdisablestops the named printers or classes.",
        "name": "cupsdisable, cupsenable - stop/start printers and classes",
        "section": 8
    },
    {
        "command": "cupsenable",
        "description": "cupsenable starts the named printers or classes while cupsdisablestops the named printers or classes.",
        "name": "cupsdisable, cupsenable - stop/start printers and classes",
        "section": 8
    },
    {
        "command": "cupsfilter",
        "description": "cupsfilter is a front-end to the CUPS filter subsystem whichallows you to convert a file to a specific format, just as if youhad printed the file through CUPS. By default, cupsfiltergenerates a PDF file. The converted file is sent to the standardoutput.",
        "name": "cupsfilter - convert a file to another format using cups filters(deprecated)",
        "section": 8
    },
    {
        "command": "curvetun",
        "description": "curvetun is a lightweight, high-speed ECDH multiuser IP tunnelfor Linux that is based on epoll(2).curvetun uses the LinuxTUN/TAP interface and supports {IPv4, IPv6} over {IPv4, IPv6}with UDP or TCP as carrier protocols.It has an integrated packet forwarding tree, thus multiple userswith different IPs can be handled via a single tunnel device onthe server side, and flows are scheduled for processing in a CPUefficient way, at least in the case of TCP as the carrierprotocol.For key management, public-key cryptography based on ellipticcurves are used and packets are encrypted end-to-end by thesymmetric stream cipher Salsa20 and authenticated by the MACPoly1305, where keys have previously been computed with the ECDHkey agreement protocol Curve25519.Cryptography is based on Daniel J. Bernstein's networking andcryptography library \u201cNaCl\u201d. By design, curvetun does not provideany particular pattern or default port numbers that givescertainty that the connection from a particular flow is actuallyrunning curvetun.However, if you have a further need to bypass censorship, you cantry using curvetun in combination with Tor's obfsproxy or Telex.Furthermore, curvetun also protects you against replay attacksand DH man-in-the-middle attacks.Additionally, server-sidesyslog event logging can also be disabled to avoid revealingcritical user connection data.1. obfsproxy from the TOR projecthttps://www.torproject.org/projects/obfsproxy.html.en2. Telex, anti-censorship in the network infrastructurehttps://telex.cc/",
        "name": "curvetun - a lightweight curve25519 ip4/6 tunnel",
        "section": 8
    },
    {
        "command": "dcb",
        "description": null,
        "name": "dcb - show / manipulate DCB (Data Center Bridging) settings",
        "section": 8
    },
    {
        "command": "dcb-app",
        "description": "dcb app is used to configure APP table, or application prioritytable in the DCB (Data Center Bridging) subsystem. The APP tableis used to assign priority to traffic based on value in one ofseveral headers: EtherType, L4 destination port, or DSCP. It alsoallows configuration of port-default priority that is chosen ifno other prioritization rule applies.DCB APP entries are 3-tuples of selector, protocol ID, andpriority. Selector is an enumeration that picks one of theprioritization namespaces. Currently it mostly corresponds toconfigurable parameters described below. Protocol ID is a valuein the selector namespace. E.g. for EtherType selector, protocolIDs are the individual EtherTypes, for DSCP they are individualcode points. The priority is the priority that should be assignedto traffic that matches the selector and protocol ID.The APP table is a set of DCB APP entries. The only requirementis that duplicate entries are not added. Notably, it is valid tohave conflicting priority assignment for the same selector andprotocol ID. For example, the set of two APP entries (DSCP, 10,1) and (DSCP, 10, 2), where packets with DSCP of 10 should getpriority of both 1 and 2, form a well-defined APP table. The dcbapp tool allows low-level management of the app table by addingand deleting individual APP 3-tuples through add and delcommands. On the other other hand, the command replace does whatone would typically want in this situation--first adds the newconfiguration, and then removes the obsolete one, so that onlyone prioritization is in effect for a given selector and protocolID.",
        "name": "dcb-app - show / manipulate application priority table of the DCB(Data Center Bridging) subsystem",
        "section": 8
    },
    {
        "command": "dcb-apptrust",
        "description": "dcb apptrust is used to configure per-selector trust and trustorder of the Application Priority Table, see dcb-app(8) fordetails on how to configure app table entries.Selector trust can be used by the software stack, or drivers(most likely the latter), when querying the APP table, todetermine if an APP entry should take effect, or not.Additionaly, the order of the trusted selectors will dictatewhich selector should take precedence, in the case of multipledifferent APP table selectors being present.",
        "name": "dcb-apptrust - show / configure per-selector trust and trustorder of the application priority table of the DCB (Data CenterBridging) subsystem.",
        "section": 8
    },
    {
        "command": "dcb-buffer",
        "description": "dcb buffer is used to configure assignment of traffic to portbuffers based on traffic priority, and sizes of those buffers. Itcan be also used to inspect the current configuration, as well astotal device memory that the port buffers take.",
        "name": "dcb-buffer - show / manipulate port buffer settings of the DCB(Data Center Bridging) subsystem",
        "section": 8
    },
    {
        "command": "dcb-dcbx",
        "description": "Data Center Bridging eXchange (DCBX) is a protocol used by DCBdevices to exchange configuration information with directlyconnected peers. The Linux DCBX object is a 1-byte bitfield offlags that configure whether DCBX is implemented in the device orin the host, and which version of the protocol should be used.dcb dcbx is used to access the per-port Linux DCBX object.There are two principal modes of operation: in host mode, DCBXprotocol is implemented by the host LLDP agent, and the DCBinterfaces are used to propagate the negotiate parameters tocapable devices. In lld-managed mode, the configuration ishandled by the device, and DCB interfaces are used for inspectionof negotiated parameters, and can also be used to set initialparameters.",
        "name": "dcb-dcbx - show / manipulate port DCBX (Data Center BridgingeXchange)",
        "section": 8
    },
    {
        "command": "dcb-ets",
        "description": "dcb ets is used to configure Enhanced Transmission Selectionattributes through Linux DCB (Data Center Bridging) interface.ETS permits configuration of mapping of priorities to trafficclasses, traffic selection algorithm to use per traffic class,bandwidth allocation, etc.Two DCB TLVs are related to the ETS feature: a configuration andrecommendation values. Recommendation values are named with aprefix reco-, while the configuration ones have plain names.",
        "name": "dcb-ets - show / manipulate ETS (Enhanced Transmission Selection)settings of the DCB (Data Center Bridging) subsystem",
        "section": 8
    },
    {
        "command": "dcb-maxrate",
        "description": "dcb maxrate is used to configure and inspect maximum rate atwhich traffic is allowed to egress from a given traffic class.",
        "name": "dcb-maxrate - show / manipulate port maxrate settings of the DCB(Data Center Bridging) subsystem",
        "section": 8
    },
    {
        "command": "dcb-pfc",
        "description": "dcb pfc is used to configure Priority-based Flow Controlattributes through Linux DCB (Data Center Bridging) interface.PFC permits marking flows with a certain priority as lossless,and holds related configuration, as well as PFC counters.",
        "name": "dcb-pfc - show / manipulate PFC (Priority-based Flow Control)settings of the DCB (Data Center Bridging) subsystem",
        "section": 8
    },
    {
        "command": "debugfs",
        "description": "The debugfs program is an interactive file system debugger. Itcan be used to examine and change the state of an ext2, ext3, orext4 file system.device is a block device (e.g., /dev/sdXX) or a file containingthe file system.",
        "name": "debugfs - ext2/ext3/ext4 file system debugger",
        "section": 8
    },
    {
        "command": "delpart",
        "description": "delpart asks the Linux kernel to forget about the specifiedpartition (a number) on the specified device. The command is asimple wrapper around the \"del partition\" ioctl.This command doesn\u2019t manipulate partitions on a block device.",
        "name": "delpart - tell the kernel to forget about a partition",
        "section": 8
    },
    {
        "command": "depmod",
        "description": "Linux kernel modules can provide services (called \"symbols\") forother modules to use (using one of the EXPORT_SYMBOL variants inthe code). If a second module uses this symbol, that secondmodule clearly depends on the first module. These dependenciescan get quite complex.depmod creates a list of module dependencies by reading eachmodule under /lib/modules/version and determining what symbols itexports and what symbols it needs. By default, this list iswritten to modules.dep, and a binary hashed version namedmodules.dep.bin, in the same directory. If filenames are given onthe command line, only those modules are examined (which israrely useful unless all modules are listed).depmod alsocreates a list of symbols provided by modules in the file namedmodules.symbols and its binary hashed version,modules.symbols.bin. Finally, depmod will output a file namedmodules.devname if modules supply special device names (devname)that should be populated in /dev on boot (by a utility such assystemd-tmpfiles).If a version is provided, then that kernel version's moduledirectory is used rather than the current kernel version (asreturned by uname -r).",
        "name": "depmod - Generate modules.dep and map files.",
        "section": 8
    },
    {
        "command": "devlink",
        "description": null,
        "name": "devlink - Devlink tool",
        "section": 8
    },
    {
        "command": "devlink-dev",
        "description": "devlink dev show - display devlink device attributesDEV - specifies the devlink device to show.If this argument isomitted all devices are listed.Format is:BUS_NAME/BUS_ADDRESSdevlink dev eswitch show - display devlink device eswitch attributesdevlink dev eswitch set - sets devlink device eswitch attributesmode { legacy | switchdev }Set eswitch modelegacy - Legacy SRIOVswitchdev - SRIOV switchdev offloadsinline-mode { none | link | network | transport }Some HWs need the VF driver to put part of the packetheaders on the TX descriptor so the e-switch can do propermatching and steering.none - Nonelink - L2 modenetwork - L3 modetransport - L4 modeencap-mode { none | basic }Set eswitch encapsulation supportnone - Disable encapsulation supportbasic - Enable encapsulation supportdevlink dev param set - set new value to devlink device configurationparametername PARAMETERSpecify parameter name to set.value VALUENew value to set.cmode { runtime | driverinit | permanent }Configuration mode in which the new value is set.runtime - Set new value while driver is running. Thisconfiguration mode doesn't require any reset to apply thenew value.driverinit - Set new value which will be applied duringdriver initialization. This configuration mode requiresrestart driver by devlink reload command to apply the newvalue.permanent - New value is written to device's non-volatilememory. This configuration mode requires hard reset toapply the new value.devlink dev param show - display devlink device supportedconfiguration parameters attributesname PARAMETER Specify parameter name to show.If this argumentis omitted all parameters supported by devlink devices arelisted.devlink dev reload - perform hot reload of the driver.DEV - Specifies the devlink device to reload.netns { PID | NAME | ID } - Specifies the network namespace toreload into, either by pid, name or id.action { driver_reinit | fw_activate } - Specifies the reloadaction required.If this argument is omitted driver_reinitaction will be used.Note that even though user asks for aspecific action, the driver implementation might require toperform another action alongside with it. For example, somedriver do not support driver reinitialization being performedwithout fw activation. Therefore, the devlink reload commandreturns the list of actions which were actrually performed.driver_reinit - Driver entities re-initialization, applyingdevlink-param and devlink-resource values.fw_activate - Activates new firmware if such image is stored andpending activation. If no limitation specified this action mayinvolve firmware reset. If no new image pending this action willreload current firmware image.limit no_reset - Specifies limitation on reload action.If thisargument is omitted limit is unspecified and the reload action isnot limited. In such case driver implementation may include resetor downtime as needed to perform the actions.no_reset - No reset allowed, no down time allowed, no link flapand no configuration is lost.devlink dev info - display device information.Display device information provided by the driver. This commandcan be used to query versions of the hardware components ordevice components which can't be updated ( fixed ) as well asdevice firmware which can be updated. For firmware componentsrunning displays the versions of firmware currently loaded intothe device, while stored reports the versions in device's flash.Running and stored versions may differ after flash has beenupdated, but before reboot.DEV - specifies the devlink device to show.If this argument isomitted all devices are listed.devlink dev flash - write device's non-volatile memory.DEV - specifies the devlink device to write to.file PATH - Path to the file which will be written into device'sflash. The path needs to be relative to one of the directoriessearched by the kernel firmware loader, such as /lib/firmware.component NAME - If device stores multiple firmware images innon-volatile memory, this parameter may be used to indicate whichfirmware image should be written.The value of NAME should matchthe component names from devlink dev info and may be driver-dependent.devlink dev selftests show - shows supported selftests on devlinkdevice.DEV - specifies the devlink device.If this argument is omittedall selftests for devlink devices are listed.devlink dev selftests run - runs selftests on devlink device.DEV - specifies the devlink device to execute selftests.id ID...- The value of ID(s) should match the selftests shownin devlink dev selftests show to execute selftests on the devlinkdevice.If this argument is omitted all selftests supported bydevlink devices are executed.",
        "name": "devlink-dev - devlink device configuration",
        "section": 8
    },
    {
        "command": "devlink-dpipe",
        "description": "devlink dpipe table show - display devlink dpipe table attributesname TABLE_NAMESpecifies the table to operate on.devlink dpipe table set - set devlink dpipe table attributesname TABLE_NAMESpecifies the table to operate on.devlink dpipe table dump - dump devlink dpipe table entriesname TABLE_NAMESpecifies the table to operate on.devlink dpipe header show - display devlink dpipe header attributesname TABLE_NAMESpecifies the table to operate on.",
        "name": "devlink-dpipe - devlink dataplane pipeline visualization",
        "section": 8
    },
    {
        "command": "devlink-health",
        "description": "devlink health show - Show status and configuration on all supportedreporters.Displays info about reporters registered on devlink devices andports.DEV - specifies the devlink device.DEV/PORT_INDEX - specifies the devlink port.REPORTER - specifies the reporter's name registered on specifieddevlink device or port.devlink health recover - Initiate a recovery operation on a reporter.This action performs a recovery and increases the recoveriescounter on success.DEV - specifies the devlink device.DEV/PORT_INDEX - specifies the devlink port.REPORTER - specifies the reporter's name registered on specifieddevlink device or port.devlink health diagnose - Retrieve diagnostics data on a reporter.DEV - specifies the devlink device.DEV/PORT_INDEX - specifies the devlink port.REPORTER - specifies the reporter's name registered on specifieddevlink device or port.devlink health test - Trigger a test event on a reporter.DEV - specifies the devlink device.REPORTER - specifies the reporter's name registered on thedevlink device.devlink health dump show - Display the last saved dump.devlink health saves a single dump per reporter. If an dump isnot already stored by the Devlink, this command will generate anewdump. The dump can be generated either automatically when areporter reports on an error or manually at the user's request.DEV - specifies the devlink device.DEV/PORT_INDEX - specifies the devlink port.REPORTER - specifies the reporter's name registered on specifieddevlink device or port.devlink health dump clear - Delete the saved dump.Deleting the saved dump enables a generation of a new dump onthe next \"devlink health dump show\" command.DEV - specifies the devlink device.DEV/PORT_INDEX - specifies the devlink port.REPORTER - specifies the reporter's name registered on specifieddevlink device or port.devlink health set - Configure health reporter.Please note that some params are not supported on a reporterwhich doesn't support a recovery or dump method.DEV - specifies the devlink device.DEV/PORT_INDEX - specifies the devlink port.REPORTER - specifies the reporter's name registered on specifieddevlink device or port.grace_period MSECTime interval between consecutive auto recoveries.auto_recover { true | false }Indicates whether the devlink should execute automaticrecover on error.auto_dump { true | false }Indicates whether the devlink should execute automaticdump on error.",
        "name": "devlink-health - devlink health reporting and recovery",
        "section": 8
    },
    {
        "command": "devlink-lc",
        "description": "devlink lc set - change line card attributesDEVSpecifies the devlink device to operate on.Format is:BUS_NAME/BUS_ADDRESSlc LC_INDEXSpecifies index of a line card slot to set.type { LC_TYPE | notype } Type of line card to provision. Eachdriver provides a list of supported line card types whichis shown in the output of devlink lc show command.devlink lc show - display line card attributesDEVSpecifies the devlink device to operate on. If this and lcarguments are omitted all line cards of all devices arelisted.lc LC_INDEXSpecifies index of a line card slot to show.",
        "name": "devlink-lc - devlink line card configuration",
        "section": 8
    },
    {
        "command": "devlink-monitor",
        "description": "The devlink utility can monitor the state of devlink devices andports continuously. This option has a slightly different format.Namely, the monitor command is the first in the command line andthen the object list.OBJECT-LIST is the list of object types that we want to monitor.It may contain dev, port, health, trap, trap-group, trap-policer.devlink opens Devlink Netlink socket, listens on it and dumpsstate changes.",
        "name": "devlink-monitor - state monitoring",
        "section": 8
    },
    {
        "command": "devlink-port",
        "description": "devlink port set - change devlink port attributesDEV/PORT_INDEX - specifies the devlink port to operate on.Format is:BUS_NAME/BUS_ADDRESS/PORT_INDEXtype { eth | ib | auto }set port typeeth - Ethernetib - Infinibandauto - autoselectdevlink port split - split devlink port into moreDEV/PORT_INDEX - specifies the devlink port to operate on.count COUNTnumber of ports to split to.devlink port unsplit - unsplit previously split devlink portCould be performed on any split port of the same split group.DEV/PORT_INDEX - specifies the devlink port to operate on.devlink port show - display devlink port attributesDEV/PORT_INDEX - specifies the devlink port to show.If thisargument is omitted all ports are listed.devlink port health - devlink health reporting and recoveryIs an alias for devlink-health(8).devlink port add - add a devlink portDEV - specifies the devlink device to operate on. orDEV/PORT_INDEX - specifies the devlink port index to use for therequested new port.This is optional. When omitted, driverallocates unique port index.flavour { pcipf | pcisf }set port flavourpcipf - PCI PF portpcisf - PCI SF portpfnum PFNUMBERSpecifies PCI pfnumber to use on which a SF device tocreatesfnum SFNUMBERSpecifies sfnumber to assign to the device of the SF.This field is optional for those devices which supportsauto assignment of the SF number.controller CNUMSpecifies controller number for which the SF port iscreated.This field is optional. It is used only when SFport is created for the external controller.devlink port function set - Set the port function attribute(s).DEV/PORT_INDEX - specifies the devlink port to operate on.hw_addr ADDRHardware address of the function to set. This is aEthernet MAC address when port type is Ethernet.state { active | inactive }New state of the function to change to.active - Once configuration of the function is done,activate the function.inactive - To inactivate the function and its device(s),set to inactive.roce { enable | disable }Set the RoCE capability of the function.migratable { enable | disable }Set the migratable capability of the function.devlink port del - delete a devlink portDEV/PORT_INDEX - specifies the devlink port to delete.devlink port param set - set new value to devlink port configurationparameterDEV/PORT_INDEX - specifies the devlink port to operate on.name PARAMETERSpecify parameter name to set.value VALUENew value to set.cmode { runtime | driverinit | permanent }Configuration mode in which the new value is set.runtime - Set new value while driver is running. Thisconfiguration mode doesn't require any reset to apply thenew value.driverinit - Set new value which will be applied duringdriver initialization. This configuration mode requiresrestart driver by devlink reload command to apply the newvalue.permanent - New value is written to device's non-volatilememory. This configuration mode requires hard reset toapply the new value.devlink port param show - display devlink port supportedconfiguration parameters attributesDEV/PORT_INDEX - specifies the devlink port to operate on.name PARAMETER Specify parameter name to show.If this argument,as well as port index, are omitted - all parameters supported bydevlink device ports are listed.devlink port function rate - manage devlink rate objectsIs an alias for devlink-rate(8).",
        "name": "devlink-port - devlink port configuration",
        "section": 8
    },
    {
        "command": "devlink-rate",
        "description": "devlink port function rate show - display rate objects.Displays specified rate object or, if not specified, all rateobjects. Rate object can be presented by one of the two types:leafRepresents a single devlink port; created/destroyed bythe driver and bound to the devlink port. As example,some driver may create leaf rate object for every devlinkport associated with VF. Since leaf have 1to1 mapping toit's devlink port, in user space it is referred ascorresponding devlink port DEV/PORT_INDEX;nodeRepresents a group of rate objects; created/deleted bythe user (see command below) and bound to the devlinkdevice rather then to the devlink port. In userspace itis referred as DEV/NODE_NAME, where node name can be any,except decimal number, to avoid collisions with leafs.Command output show rate object identifier, it's type and ratevalues along with parent node name. Rate values printed in SIunits which are more suitable to represent specific value. Toprint values in IEC units -i switch is used. JSON (-j) outputalways print rate values in bytes per second. Zero rate valuesmeans \"unlimited\" rates and omitted in output, as well as parentnode name.devlink port function rate set - set rate object parameters.Allows set rate object's parameters. If any parameter specifiedmultiple times the last occurrence is used.DEV/PORT_INDEX - specifies devlink leaf rate object.DEV/NODE_NAME - specifies devlink node rate object.tx_share VALUE - specifies minimal tx rate value shared among allrate objects. If rate object is a part of some rate group, thenthis value shared with rate objects of this rate group.tx_max VALUE - specifies maximum tx rate value.tx_priority N - allows for usage of strict priority arbiter amongsiblings. This arbitration scheme attempts to schedule nodesbased on their priority as long as the nodes remain within theirbandwidth limit. The higher the priority the higher theprobability that the node will get selected for scheduling.tx_weight N - allows for usage of Weighted Fair Queuingarbitration scheme among siblings.This arbitration scheme canbe used simultaneously with the strict priority.As a node isconfigured with a higher rate it gets more BW relative to it'ssiblings. Values are relative like a percentage points, theybasically tell how much BW should node take relative to it'ssiblings.VALUEThese parameter accept a floating point number, possiblyfollowed by either a unit (both SI and IEC unitssupported).bit or a bare numberBits per secondkbitKilobits per secondmbitMegabits per secondgbitGigabits per secondtbitTerabits per secondbpsBytes per secondkbpsKilobytes per secondmbpsMegabytes per secondgbpsGigabytes per secondtbpsTerabytes per secondTo specify in IEC units, replace the SI prefix (k-, m-,g-, t-) with IEC prefix (ki-, mi-, gi- and ti-)respectively. Input is case-insensitive.NThese parameter accept integer meaning weight or priorityof a node.parent NODE_NAME | noparent - set rate object parent to existingnode with name NODE_NAME or unset parent. Rate limits of theparent node applied to all it's children. Actual behaviour isdetails of driver's implementation. Setting parent to empty (\"\")name due to the kernel logic threated as parent unset.devlink port function rate add - create node rate object withspecified parameters.Creates rate object of type node and sets parameters. Parameterssame as for the \"set\" command.DEV/NODE_NAME - specifies the devlink node rate object to create.devlink port function rate del - delete node rate objectDelete specified devlink node rate object. Node can't be deletedif there is any child, user must explicitly unset the parent.DEV/NODE_NAME - specifies devlink node rate object to delete.devlink port function rate help - display usage informationDisplay devlink rate usage information",
        "name": "devlink-rate - devlink rate management",
        "section": 8
    },
    {
        "command": "devlink-region",
        "description": "devlink region show - Show all supported address regions names,snapshots and sizesDEV/REGION - specifies the devlink device and address-region toquery.devlink region new - Create a snapshot specified by address-regionname and snapshot IDDEV/REGION - specifies the devlink device and address-region tosnapshotsnapshot SNAPSHOT_ID - optionally specifies the snapshot ID toassign. If not specified, devlink will assign a unique ID to thesnapshot.devlink region del - Delete a snapshot specified by address-regionname and snapshot IDDEV/REGION - specifies the devlink device and address-region todelete the snapshot fromsnapshot SNAPSHOT_ID - specifies the snapshot ID to deletedevlink region dump - Dump all the available data from a region orfrom snapshot of a regionDEV/REGION - specifies the device and address-region to dumpfrom.snapshot SNAPSHOT_ID - specifies the snapshot-id of the region todump.devlink region read - Read from a specific region address for a givenlengthDEV/REGION - specifies the device and address-region to readfrom.snapshot SNAPSHOT_ID - specifies the snapshot-id of the region toread.address ADDRESS - specifies the address to read from.length LENGTH - specifies the length of data to read.",
        "name": "devlink-region - devlink address region access",
        "section": 8
    },
    {
        "command": "devlink-resource",
        "description": "devlink resource show - display devlink device's resosourcesDEV - specifies the devlink device to show.Format is:BUS_NAME/BUS_ADDRESSdevlink resource set - sets resource size of specific resourceDEV - specifies the devlink device.path RESOURCE_PATHResource's path.size RESOURCE_SIZEThe new resource's size.",
        "name": "devlink-resource - devlink device resource configuration",
        "section": 8
    },
    {
        "command": "devlink-sb",
        "description": "devlink sb show - display available shared buffers and theirattributesDEV - specifies the devlink device to show shared buffers.Ifthis argument is omitted all shared buffers of all devices arelisted.SB_INDEX - specifies the shared buffer.If this argument isomitted shared buffer with index 0 is selected.Behaviour ofthis argument it the same for every command.devlink sb pool show - display available pools and their attributesDEV - specifies the devlink device to show pools.If thisargument is omitted all pools of all devices are listed.Display available pools listing their type, size, thtype andcell_size. cell_size is the allocation granularity of memorywithin the shared buffer. Drivers may round up, round down orreject size passed to the set command if it is not multiple ofcell_size.devlink sb pool set - set attributes of poolDEV - specifies the devlink device to set pool.size POOL_SIZEsize of the pool in Bytes.thtype { static | dynamic }pool threshold type.static - Threshold values for the pool will be passed inBytes.dynamic - Threshold values (\"to_alpha\") for the pool willbe used to compute alpha parameter according to formula:alpha = 2 ^ (to_alpha - 10)The range of the passed value is between 0 to20. The computed alpha is used to determine themaximum usage of the flow:max_usage = alpha / (1 + alpha) *Free_Bufferdevlink sb port pool show - display port-pool combinations andthreshold for eachDEV/PORT_INDEX - specifies the devlink port.pool POOL_INDEXpool index.devlink sb port pool set - set port-pool thresholdDEV/PORT_INDEX - specifies the devlink port.pool POOL_INDEXpool index.th THRESHOLDthreshold value. Type of the value is either Bytes or\"to_alpha\", depends on thtype set for the pool.devlink sb tc bind show - display port-TC to pool bindings andthreshold for eachDEV/PORT_INDEX - specifies the devlink port.tc TC_INDEXindex of either ingress or egress TC, usually in range 0to 8 (depends on device).type { ingress | egress }TC type.devlink sb tc bind set - set port-TC to pool binding with specifiedthresholdDEV/PORT_INDEX - specifies the devlink port.tc TC_INDEXindex of either ingress or egress TC, usually in range 0to 8 (depends on device).type { ingress | egress }TC type.pool POOL_INDEXindex of pool to bind this to.th THRESHOLDthreshold value. Type of the value is either Bytes or\"to_alpha\", depends on thtype set for the pool.devlink sb occupancy show - display shared buffer occupancy valuesfor device or portThis command is used to browse shared buffer occupancy values.Values are showed for every port-pool combination as well as forall port-TC combinations (with pool this port-TC is bound to).Format of value is:current_value/max_valueNote that before showing values, one has to issue occupancysnapshot command first.DEV - specifies the devlink device to show occupancy values for.DEV/PORT_INDEX - specifies the devlink port to show occupancyvalues for.devlink sb occupancy snapshot - take occupancy snapshot of sharedbuffer for deviceThis command is used to take a snapshot of shared bufferoccupancy values. After that, the values can be showed usingoccupancy show command.DEV - specifies the devlink device to take occupancy snapshot on.devlink sb occupancy clearmax - clear occupancy watermarks of sharedbuffer for deviceThis command is used to reset maximal occupancy values reachedfor whole device. Note that before browsing reset values, one hasto issue occupancy snapshot command.DEV - specifies the devlink device to clear occupancy watermarkson.",
        "name": "devlink-sb - devlink shared buffer configuration",
        "section": 8
    },
    {
        "command": "devlink-trap",
        "description": "devlink trap show - display available packet traps and theirattributesDEV - specifies the devlink device from which to show packettraps.If this argument is omitted all packet traps of alldevices are listed.trap TRAP - specifies the packet trap.Only applicable if adevlink device is also specified.devlink trap set - set attributes of a packet trapDEV - specifies the devlink device the packet trap belongs to.trap TRAP - specifies the packet trap.action { trap | drop | mirror }packet trap action.trap - the sole copy of the packet is sent to the CPU.drop - the packet is dropped by the underlying device anda copy is not sent to the CPU.mirror - the packet is forwarded by the underlying deviceand a copy is sent to the CPU.devlink trap group show - display available packet trap groups andtheir attributesDEV - specifies the devlink device from which to show packet trapgroups.If this argument is omitted all packet trap groups ofall devices are listed.group GROUP - specifies the packet trap group.Only applicableif a devlink device is also specified.devlink trap group set - set attributes of a packet trap groupDEV - specifies the devlink device the packet trap group belongsto.group GROUP - specifies the packet trap group.action { trap | drop | mirror }packet trap action. The action is set for all the packettraps member in the trap group. The actions of non-droptraps cannot be changed and are thus skipped.policer POLICERpacket trap policer. The policer to bind to the packettrap group. A value of \"0\" will unbind the currently boundpolicer.nopolicerUnbind packet trap policer from the packet trap group.devlink trap policer set - set attributes of packet trap policerDEV - specifies the devlink device the packet trap policerbelongs to.policer POLICER - specifies the packet trap policer.rate RATE - packet trap policer rate in packets per second.burst BURST - packet trap policer burst size in packets.",
        "name": "devlink-trap - devlink trap configuration",
        "section": 8
    },
    {
        "command": "dmeventd",
        "description": "dmeventd is the event monitoring daemon for device-mapperdevices.Library plugins can register and carry out actionstriggered when particular events occur.",
        "name": "dmeventd \u2014 Device-mapper event daemon",
        "section": 8
    },
    {
        "command": "dmfilemapd",
        "description": "The dmfilemapd daemon monitors groups of dmstats(8) regions thatcorrespond to the extents of a file, adding and removing regionsto reflect the changing state of the file on-disk.The daemon is normally launched automatically by the dmstatscreate command, but can be run manually, either to create a newdaemon where one did not previously exist, or to change theoptions previously used, by killing the existing daemon andstarting a new one.",
        "name": "dmfilemapd \u2014 device-mapper filemap monitoring daemon",
        "section": 8
    },
    {
        "command": "dmsetup",
        "description": "dmsetup manages logical devices that use the device-mapperdriver.Devices are created by loading a table that specifies atarget for each sector (512 bytes) in the logical device.The first argument to dmsetup is a command.The second argumentis the logical device name or uuid.Invoking the dmsetup tool as devmap_name (which is not normallydistributed and is supported only for historical reasons) isequivalent to dmsetup info -c --noheadings -j major -m minor.",
        "name": "dmsetup \u2014 low level logical volume management",
        "section": 8
    },
    {
        "command": "dmstats",
        "description": "The dmstats program manages IO statistics regions for devicesthat use the device-mapper driver. Statistics regions may becreated, deleted, listed and reported on using the tool.The first argument to dmstats is a command.The second argument is the device name, uuid or major and minornumbers.Further options permit the selection of regions, output formatcontrol, and reporting behaviour.When no device argument is given dmstats will by default operateon all device-mapper devices present. The create and deletecommands require the use of --alldevices when used in this way.",
        "name": "dmstats \u2014 device-mapper statistics management",
        "section": 8
    },
    {
        "command": "dnf",
        "description": "DNF is the next upcoming major version of YUM, a package managerfor RPM-based Linux distributions. It roughly maintains CLIcompatibility with YUM and defines a strict API for extensionsand plugins.Plugins can modify or extend features of DNF or provideadditional CLI commands on top of those mentioned below. If youknow the name of such a command (including commands mentionedbelow), you may find/install the package which provides it usingthe appropriate virtual provide in the form ofdnf-command(<alias>), where <alias> is the name of the command;e.g.``dnf install 'dnf-command(versionlock)'`` installs aversionlock plugin. This approach also applies to specifyingdependencies of packages that require a particular DNF command.Return values:\u2022 0: Operation was successful.\u2022 1: An error occurred, which was handled by dnf.\u2022 3: An unknown unhandled error occurred during operation.\u2022 100: See check-update\u2022 200: There was a problem with acquiring or releasing of locks.Available commands:\u2022 alias\u2022 autoremove\u2022 check\u2022 check-update\u2022 clean\u2022 deplist\u2022 distro-sync\u2022 downgrade\u2022 group\u2022 help\u2022 history\u2022 info\u2022 install\u2022 list\u2022 makecache\u2022 mark\u2022 module\u2022 provides\u2022 reinstall\u2022 remove\u2022 repoinfo\u2022 repolist\u2022 repoquery\u2022 repository-packages\u2022 search\u2022 shell\u2022 swap\u2022 updateinfo\u2022 upgrade\u2022 upgrade-minimalAdditional information:\u2022 Options\u2022 Specifying Packages\u2022 Specifying Provides\u2022 Specifying File Provides\u2022 Specifying Groups\u2022 Specifying Transactions\u2022 Metadata Synchronization\u2022 Configuration Files Replacement Policy\u2022 Files\u2022 See Also",
        "name": "dnf - DNF Command Reference",
        "section": 8
    },
    {
        "command": "dnf-automatic",
        "description": "Alternative CLI to dnf upgrade with specific facilities to makeit suitable to be executed automatically and regularly fromsystemd timers, cron jobs and similar.The operation of the tool is usually controlled by theconfiguration file or the function-specific timer units (seebelow). The command only accepts a single optional argumentpointing to the config file, and some control arguments intendedfor use by the services that back the timer units. If noconfiguration file is passed from the command line,/etc/dnf/automatic.conf is used.The tool synchronizes package metadata as needed and then checksfor updates available for the given system and then either exits,downloads the packages or downloads and applies the packages. Theoutcome of the operation is then reported by a selectedmechanism, for instance via the standard output, email or MOTDmessages.The systemd timer unit dnf-automatic.timer will behave as theconfiguration file specifies (see below) with regard to whetherto download and apply updates. Some other timer units areprovided which override the configuration file with some standardbehaviours:\u2022 dnf-automatic-notifyonly\u2022 dnf-automatic-download\u2022 dnf-automatic-installRegardless of the configuration file settings, the first willonly notify of available updates. The second will download, butnot install them. The third will download and install them.",
        "name": "dnf-automatic - DNF Automatic",
        "section": 8
    },
    {
        "command": "dosfsck",
        "description": "fsck.fat verifies the consistency of MS-DOS filesystems andoptionally tries to repair them.The following filesystem problems can be corrected (in thisorder):\u2022FAT contains invalid cluster numbers.Cluster is changed toEOF.\u2022File's cluster chain contains a loop.The loop is broken.\u2022Bad clusters (read errors).The clusters are marked bad andthey are removed from files owning them.This check isoptional.\u2022Directories with a large number of bad entries (probablycorrupt).The directory can be deleted.\u2022Files . and .. are non-directories.They can be deleted orrenamed.\u2022Directories . and .. in root directory.They are deleted.\u2022Bad filenames.They can be renamed.\u2022Duplicate directory entries.They can be deleted or renamed.\u2022Directories with non-zero size field.Size is set to zero.\u2022Directory . does not point to parent directory.The startpointer is adjusted.\u2022Directory .. does not point to parent of parent directory.The start pointer is adjusted.\u2022. and .. are not the two first entries in a non-rootdirectory.The entries are created, moving occupied slots ifnecessary.\u2022Start cluster number of a file is invalid.The file istruncated.\u2022File contains bad or free clusters.The file is truncated.\u2022File's cluster chain is longer than indicated by the sizefields.The file is truncated.\u2022Two or more files share the same cluster(s).All but one ofthe files are truncated.If the file being truncated is adirectory file that has already been read, the filesystemcheck is restarted after truncation.\u2022File's cluster chain is shorter than indicated by the sizefields.The file is truncated.\u2022Volume label in root directory or label in boot sector isinvalid.Invalid labels are removed.\u2022Volume label in root directory and label in boot sector aredifferent.Volume label from root directory is copied toboot sector.\u2022Clusters are marked as used but are not owned by a file.They are marked as free.Additionally, the following problems are detected, but notrepaired:\u2022Invalid parameters in boot sectorWhen fsck.fat checks a filesystem, it accumulates all changes inmemory and performs them only after all checks are complete.This can be disabled with the -w option.Two different variants of the FAT filesystem are supported.Standard is the FAT12, FAT16 and FAT32 filesystems as defined byMicrosoft and widely used on hard disks and removable media likeUSB sticks and SD cards.The other is the legacy Atari variantused on Atari ST.There are some minor differences in Atari format: Some bootsector fields are interpreted slightly different, and the specialFAT entries for end-of-file and bad cluster can be different.Under MS-DOS 0xfff8 is used for EOF and Atari employs 0xffff bydefault, but both systems recognize all values from 0xfff8\u20130xffffas end-of-file.MS-DOS uses only 0xfff7 for bad clusters, whereon Atari values 0xfff0\u20130xfff7 are for this purpose (but thestandard value is still 0xfff7).",
        "name": "fsck.fat - check and repair MS-DOS FAT filesystems",
        "section": 8
    },
    {
        "command": "dosfslabel",
        "description": "fatlabel will display or change the volume label or volume ID onthe MS-DOS filesystem located on DEVICE.By default it works inlabel mode.It can be switched to volume ID mode with the option-i or --volume-id.If NEW is omitted, then the existing label or volume ID iswritten to the standard output.A label can't be longer than 11bytes and should be in all upper case for best compatibility.Anempty string or a label consisting only of white space is notallowed.A volume ID must be given as a hexadecimal number (noleading \"0x\" or similar) and must fit into 32 bits.",
        "name": "fatlabel - set or get MS-DOS filesystem label or volume ID",
        "section": 8
    },
    {
        "command": "dpkg-fsys-usrunmess",
        "description": "dpkg-fsys-usrunmess is a tool to fix up filesystems that havebeen installed anew with recent installers with unfortunatedefaults or migrated to the broken merged /usr via aliaseddirectories layout, which is not supported by dpkg. See the dpkgFAQ.Note: In Debian its tech-ctte has decreed that on the releaseafter bookworm the non-usrmerged layout is not going to besupported, and thus some of its maintainers might not fix issuesor intentionally remove non-usrmerged support, so you will needdecide and pick your poison.The program will perform the following overall actions:\u2022Check whether the system needs to be switched, otherwise donothing,\u2022Check for dpkg database consistency and otherwise abort.\u2022Generate and install a regression prevention package, ifrequested on the command-line or otherwise on the prompt.\u2022Get the list of files and alternatives that need to berestored.\u2022Look for untracked kernel modules files that need to be movedtoo.\u2022Create a shadow hierarchy under /.usrunmess, by creating thedirectories symlinks or hardlinking or copying the files asneeded.\u2022Prompt for confirmation before proceeding, if requested onthe command-line.\u2022Lock the dpkg database.\u2022Mark all packages as half-configured to force runningmaintainer scripts that might need to recreate files.\u2022Replace the aliased directories with the shadow ones, bycreating a backup of the old symlinked directories andrenaming the shadow ones over.\u2022Relabel MAC information for directories and symlinks ifnecessary.\u2022Remove backup symlinks.\u2022Remove old moved objects, but defer directory removal.\u2022Remove old deferred directories that are not referenced bydpkg-query.\u2022Remove shadow root directory.\u2022Register a policy-rc.d to disallow service actions, whichmeans that services might need to be restarted afterwards,ideally via a reboot.\u2022Reconfigure all packages.\u2022Unregister the policy-rc.d and restore the alternativesstate.Note: When running the program from some shells such as bash(1)or zsh(1), after executing it, you might need to request theshell to forget all remembered executable locations with forexample \"hash -r\".Note: Some directories might linger after the migration in casethey contain untracked files. A list is printed once the scripthas finished for further investigation.Warning: Note that this operation has the potential to render thesystem unusable or broken in case of a sudden crash or reboot,unexpected state of the system, or possible bugs in the script.Be prepared with recovery media and consider doing backupsbeforehand.",
        "name": "dpkg-fsys-usrunmess - undoes the merged-/usr-via-aliased-dirsmess",
        "section": 8
    },
    {
        "command": "dracut",
        "description": "Create an initramfs <image> for the kernel with the version<kernel version>. If <kernel version> is omitted, then theversion of the actual running kernel is used. If <image> isomitted or empty, depending on bootloader specification, thedefault location can be/efi/<machine-id>/<kernel-version>/initrd,/boot/<machine-id>/<kernel-version>/initrd,/boot/efi/<machine-id>/<kernel-version>/initrd,/lib/modules/<kernel-version>/initrd or/boot/initramfs-<kernel-version>.img.dracut creates an initial image used by the kernel for preloadingthe block device modules (such as IDE, SCSI or RAID) which areneeded to access the root filesystem, mounting the rootfilesystem and booting into the real system.At boot time, the kernel unpacks that archive into RAM disk,mounts and uses it as initial root file system. All finding ofthe root device happens in this early userspace.Initramfs images are also called \"initrd\".For a complete list of kernel command line options seedracut.cmdline(7).If you are dropped to an emergency shell, while booting yourinitramfs, the file /run/initramfs/rdsosreport.txt is created,which can be saved to a (to be mounted by hand) partition(usually /boot) or a USB stick. Additional debugging info can beproduced by adding rd.debug to the kernel command line./run/initramfs/rdsosreport.txt contains all logs and the outputof some tools. It should be attached to any report about dracutproblems.",
        "name": "dracut - low-level tool for generating an initramfs/initrd image",
        "section": 8
    },
    {
        "command": "dracut-catimages",
        "description": "dracut-catimages creates an initial ramdisk image byconcatenating several images from the command line and/boot/dracut/*.img",
        "name": "dracut-catimages - creates initial ramdisk image by concatenatingimages",
        "section": 8
    },
    {
        "command": "drr",
        "description": "The Deficit Round Robin Scheduler is a classful queuingdiscipline as a more flexible replacement for Stochastic FairnessQueuing.Unlike SFQ, there are no built-in queues -- you need to addclasses and then set up filters to classify packets accordingly.This can be useful e.g. for using RED qdiscs with differentsettings for particular traffic. There is no default class -- ifa packet cannot be classified, it is dropped.",
        "name": "drr - deficit round robin scheduler",
        "section": 8
    },
    {
        "command": "dump-acct",
        "description": "The dump-acct command transforms the output file from the acctonformat to the human-readable format: one record per line.Eachrecord consists of severald fields which are separated bycharacter \"|\" (the meaning of concreate field depends on theversion of kernel package - with which the accton file wascreated).",
        "name": "dump-acct - print an acct/pacct file in human-readable format",
        "section": 8
    },
    {
        "command": "dump-utmp",
        "description": null,
        "name": "dump-utmp - print a utmp file in human-readable format",
        "section": 8
    },
    {
        "command": "dumpe2fs",
        "description": "dumpe2fs prints the super block and blocks group information forthe file system present on device.Note: When used with a mounted file system, the printedinformation may be old or inconsistent.",
        "name": "dumpe2fs - dump ext2/ext3/ext4 file system information",
        "section": 8
    },
    {
        "command": "e2freefrag",
        "description": "e2freefrag is used to report free space fragmentation on ext2/3/4file systems.filesys is the file system device name (e.g./dev/hdc1, /dev/md0).The e2freefrag program will scan the blockbitmap information to check how many free blocks are present ascontiguous and aligned free space. The percentage of contiguousfree blocks of size and of alignment chunk_kb is reported.Italso displays the minimum/maximum/average free chunk size in thefile system, along with a histogram of all free chunks.Thisinformation can be used to gauge the level of free spacefragmentation in the file system.",
        "name": "e2freefrag - report free space fragmentation information",
        "section": 8
    },
    {
        "command": "e2fsck",
        "description": "e2fsck is used to check the ext2/ext3/ext4 family of filesystems.For ext3 and ext4 file systems that use a journal, ifthe system has been shut down uncleanly without any errors,normally, after replaying the committed transactionsin thejournal, the file system should be marked as clean.Hence, forfile systems that use journaling, e2fsck will normally replay thejournal and exit, unless its superblock indicates that furtherchecking is required.device is a block device (e.g., /dev/sdc1) or file containing thefile system.Note that in general it is not safe to run e2fsck on mounted filesystems.The only exception is if the -n option is specified,and -c, -l, or -L options are not specified.However, even ifit is safe to do so, the results printed by e2fsck are not validif the file system is mounted.If e2fsck asks whether or notyou should check a file system which is mounted, the only correctanswer is ``no''.Only experts who really know what they aredoing should consider answering this question in any other way.If e2fsck is run in interactive mode (meaning that none of -y,-n, or -p are specified), the program will ask the user to fixeach problem found in the file system.A response of 'y' willfix the error; 'n' will leave the error unfixed; and 'a' will fixthe problem and all subsequent problems; pressing Enter willproceed with the default response, which is printed before thequestion mark.Pressing Control-C terminates e2fsck immediately.",
        "name": "e2fsck - check a Linux ext2/ext3/ext4 file system",
        "section": 8
    },
    {
        "command": "e2image",
        "description": "The e2image program will save critical ext2, ext3, or ext4 filesystem metadata located on device to a file specified by image-file.The image file may be examined by dumpe2fs and debugfs, byusing the -i option to those programs.This can assist an expertin recovering catastrophically corrupted file systems.It is a very good idea to create image files for all file systemson a system and save the partition layout (which can be generatedusing the fdisk -l command) at regular intervals --- at boottime, and/or every week or so.The image file should be storedon some file system other than the file system whose data itcontains, to ensure that this data is accessible in the casewhere the file system has been badly damaged.To save disk space, e2image creates the image file as a sparsefile, or in QCOW2 format.Hence, if the sparse image file needsto be copied to another location, it should either be compressedfirst or copied using the --sparse=always option to the GNUversion of cp(1).This does not apply to the QCOW2 image, whichis not sparse.The size of an ext2 image file depends primarily on the size ofthe file systems and how many inodes are in use.For a typical10 Gigabyte file system, with 200,000 inodes in use out of 1.2million inodes, the image file will be approximately 35Megabytes; a 4 Gigabyte file system with 15,000 inodes in use outof 550,000 inodes will result in a 3 Megabyte image file.Imagefiles tend to be quite compressible; an image file taking up 32Megabytes of space on disk will generally compress down to 3 or 4Megabytes.If image-file is -, then the output of e2image will be sent tostandard output, so that the output can be piped to anotherprogram, such as gzip(1).(Note that this is currently onlysupported when creating a raw image file using the -r option,since the process of creating a normal image file, or QCOW2 imagecurrently requires random access to the file, which cannot bedone using a pipe.",
        "name": "e2image - Save critical ext2/ext3/ext4 file system metadata to afile",
        "section": 8
    },
    {
        "command": "e2label",
        "description": "e2label will display or change the volume label on the ext2,ext3, or ext4 file system located on device.If the optional argument volume-label is not present, e2labelwill simply display the current volume label.If the optional argument volume-label is present, then e2labelwill set the volume label to be volume-label.Ext2 volume labelscan be at most 16 characters long; if volume-label is longer than16 characters, e2label will truncate it and print a warningmessage.For other file systems that support online labelmanipulation and are mounted e2label will work as well, but itwill not attempt to truncate the volume-label at all.It is also possible to set the volume label using the -L optionof tune2fs(8).",
        "name": "e2label - Change the label on an ext2/ext3/ext4 file system",
        "section": 8
    },
    {
        "command": "e2mmpstatus",
        "description": "e2mmpstatus is used to check Multiple-Mount Protection (MMP)status of an ext4 file system with the mmp feature enabled.Thespecified file system can be a device name (e.g./dev/hdc1,/dev/sdb2), or an ext4 file system label or UUID, for exampleUUID=8868abf6-88c5-4a83-98b8-bfc24057f7bd or LABEL=root.Bydefault, the e2mmpstatus program checks whether it is safe tomount the file system without taking the risk of mounting it morethan once.MMP (multiple-mount protection) is a feature that adds protectionagainst the file system being modified simultaneously by morethan one node.It is NOT safe to mount a file system when one ofthe following conditions is true:1. e2fsck is running on the file system.2. the file system is in use by another node.3. The MMP block is corrupted or cannot be read for somereason.The e2mmpstatus program might wait for some time to see whetherthe MMP block is being updated by any node during this period.The time taken depends on how frequently the MMP block is beingwritten by the other node.",
        "name": "e2mmpstatus - Check MMP status of an ext4 file system",
        "section": 8
    },
    {
        "command": "e2undo",
        "description": "e2undo will replay the undo log undo_log for an ext2/ext3/ext4file system found on device.This can be used to undo a failedoperation by an e2fsprogs program.",
        "name": "e2undo - Replay an undo log for an ext2/ext3/ext4 file system",
        "section": 8
    },
    {
        "command": "e4crypt",
        "description": "e4crypt performs encryption management for ext4 file systems.",
        "name": "e4crypt - ext4 file system encryption utility",
        "section": 8
    },
    {
        "command": "e4defrag",
        "description": "e4defrag reduces fragmentation of extent based file. The filetargeted by e4defrag is created on ext4 file system made with \"-Oextent\" option (see mke2fs(8)).The targeted file gets morecontiguous blocks and improves the file access speed.target is a regular file, a directory, or a device that ismounted as ext4 file system.If target is a directory, e4defragreduces fragmentation of all files in it. If target is a device,e4defrag gets the mount point of it and reduces fragmentation ofall files in this mount point.",
        "name": "e4defrag - online defragmenter for ext4 file system",
        "section": 8
    },
    {
        "command": "ebtables",
        "description": "ebtables is an application program used to set up and maintainthe tables of rules (inside the Linux kernel) that inspectEthernet frames.It is analogous to the iptables application,but less complicated, due to the fact that the Ethernet protocolis much simpler than the IP protocol.CHAINSThere are three ebtables tables with built-in chains in the Linuxkernel. These tables are used to divide functionality intodifferent sets of rules. Each set of rules is called a chain.Each chain is an ordered list of rules that can match Ethernetframes. If a rule matches an Ethernet frame, then a processingspecification tells what to do with that matching frame. Theprocessing specification is called a 'target'. However, if theframe does not match the current rule in the chain, then the nextrule in the chain is examined and so forth.The user can createnew (user-defined) chains that can be used as the 'target' of arule. User-defined chains are very useful to get betterperformance over the linear traversal of the rules and are alsoessential for structuring the filtering rules into well-organizedand maintainable sets of rules.TARGETSA firewall rule specifies criteria for an Ethernet frame and aframe processing specification called a target.When a framematches a rule, then the next action performed by the kernel isspecified by the target.The target can be one of these values:ACCEPT, DROP, CONTINUE, RETURN, an 'extension' (see below) or ajump to a user-defined chain.ACCEPT means to let the frame through.DROP means the frame hasto be dropped. In the BROUTING chain however, the ACCEPT and DROPtarget have different meanings (see the info provided for the -toption).CONTINUE means the next rule has to be checked. Thiscan be handy, f.e., to know how many frames pass a certain pointin the chain, to log those frames or to apply multiple targets ona frame.RETURN means stop traversing this chain and resume atthe next rule in the previous (calling) chain.For the extensiontargets please refer to the TARGET EXTENSIONS section of this manpage.TABLESAs stated earlier, the table names are filter, nat and broute.Of these tables, the filter table is the default table that thecommand operates on.If you are working with a table other thanfilter, you will need to provide the -t argument.Moreover, the-t argument must be the first argument on the ebtables commandline, if used.-t, --tablefilter is the default table and contains three built-inchains: INPUT (for frames destined for the bridge itself,on the level of the MAC destination address), OUTPUT (forlocally-generated or (b)routed frames) and FORWARD (forframes being forwarded by the bridge).nat is mostly used to change the mac addresses andcontains three built-in chains: PREROUTING (for alteringframes as soon as they come in), OUTPUT (for alteringlocally generated or (b)routed frames before they arebridged) and POSTROUTING (for altering frames as they areabout to go out). A small note on the naming of chainsPREROUTING and POSTROUTING: it would be more accurate tocall them PREFORWARDING and POSTFORWARDING, but for allthose who come from the iptables world to ebtables it iseasier to have the same names. Note that you can changethe name (-E) if you don't like the default.broute is used to make a brouter, it has one built-inchain: BROUTING.The targets DROP and ACCEPT have aspecial meaning in the broute table (these names are usedfor compatibility reasons with ebtables-legacy).DROPactually means the frame has to be routed, while ACCEPTmeans the frame has to be bridged. The BROUTING chain istraversed very early.Normally those frames would bebridged, but you can decide otherwise here.",
        "name": "ebtables - Ethernet bridge frame table administration (nft-based)",
        "section": 8
    },
    {
        "command": "ebtables-nft",
        "description": "ebtables is an application program used to set up and maintainthe tables of rules (inside the Linux kernel) that inspectEthernet frames.It is analogous to the iptables application,but less complicated, due to the fact that the Ethernet protocolis much simpler than the IP protocol.CHAINSThere are three ebtables tables with built-in chains in the Linuxkernel. These tables are used to divide functionality intodifferent sets of rules. Each set of rules is called a chain.Each chain is an ordered list of rules that can match Ethernetframes. If a rule matches an Ethernet frame, then a processingspecification tells what to do with that matching frame. Theprocessing specification is called a 'target'. However, if theframe does not match the current rule in the chain, then the nextrule in the chain is examined and so forth.The user can createnew (user-defined) chains that can be used as the 'target' of arule. User-defined chains are very useful to get betterperformance over the linear traversal of the rules and are alsoessential for structuring the filtering rules into well-organizedand maintainable sets of rules.TARGETSA firewall rule specifies criteria for an Ethernet frame and aframe processing specification called a target.When a framematches a rule, then the next action performed by the kernel isspecified by the target.The target can be one of these values:ACCEPT, DROP, CONTINUE, RETURN, an 'extension' (see below) or ajump to a user-defined chain.ACCEPT means to let the frame through.DROP means the frame hasto be dropped. In the BROUTING chain however, the ACCEPT and DROPtarget have different meanings (see the info provided for the -toption).CONTINUE means the next rule has to be checked. Thiscan be handy, f.e., to know how many frames pass a certain pointin the chain, to log those frames or to apply multiple targets ona frame.RETURN means stop traversing this chain and resume atthe next rule in the previous (calling) chain.For the extensiontargets please refer to the TARGET EXTENSIONS section of this manpage.TABLESAs stated earlier, the table names are filter, nat and broute.Of these tables, the filter table is the default table that thecommand operates on.If you are working with a table other thanfilter, you will need to provide the -t argument.Moreover, the-t argument must be the first argument on the ebtables commandline, if used.-t, --tablefilter is the default table and contains three built-inchains: INPUT (for frames destined for the bridge itself,on the level of the MAC destination address), OUTPUT (forlocally-generated or (b)routed frames) and FORWARD (forframes being forwarded by the bridge).nat is mostly used to change the mac addresses andcontains three built-in chains: PREROUTING (for alteringframes as soon as they come in), OUTPUT (for alteringlocally generated or (b)routed frames before they arebridged) and POSTROUTING (for altering frames as they areabout to go out). A small note on the naming of chainsPREROUTING and POSTROUTING: it would be more accurate tocall them PREFORWARDING and POSTFORWARDING, but for allthose who come from the iptables world to ebtables it iseasier to have the same names. Note that you can changethe name (-E) if you don't like the default.broute is used to make a brouter, it has one built-inchain: BROUTING.The targets DROP and ACCEPT have aspecial meaning in the broute table (these names are usedfor compatibility reasons with ebtables-legacy).DROPactually means the frame has to be routed, while ACCEPTmeans the frame has to be bridged. The BROUTING chain istraversed very early.Normally those frames would bebridged, but you can decide otherwise here.",
        "name": "ebtables - Ethernet bridge frame table administration (nft-based)",
        "section": 8
    },
    {
        "command": "ebtables-translate",
        "description": "There is a set of tools to help the system administratortranslate a given ruleset from iptables(8), ip6tables(8) andebtables(8) to nftables(8).The available commands are:\u2022 iptables-translate\u2022 iptables-restore-translate\u2022 ip6tables-translate\u2022 ip6tables-restore-translate\u2022 ebtables-translate",
        "name": "iptables-translate \u2014 translation tool to migrate from iptables tonftablesip6tables-translate \u2014 translation tool to migrate from ip6tablesto nftablesebtables-translate \u2014 translation tool to migrate from ebtables tonftables",
        "section": 8
    },
    {
        "command": "edquota",
        "description": "edquota is a quota editor.One or more users, groups, orprojects may be specified on the command line. If a number isgiven in the place of user/group/project name it is treated as anUID/GID/Project ID. For each user, group, or project a temporaryfile is created with an ASCII representation of the current diskquotas for that user, group, or project and an editor is theninvoked on the file.The quotas may then be modified, new quotasadded, etc.Setting a quota to zero indicates that no quotashould be imposed.Block usage and limits are reported and interpreted as multiplesof kibibyte (1024 bytes) blocks by default. Symbols K, M, G, andT can be appended to numeric value to express kibibytes,mebibytes, gibibytes, and tebibytes.Inode usage and limits are interpreted literally. Symbols k, m,g, and t can be appended to numeric value to express multiples of10^3, 10^6, 10^9, and 10^12 inodes.Users are permitted to exceed their soft limits for a graceperiod that may be specified per filesystem.Once the graceperiod has expired, the soft limit is enforced as a hard limit.The current usage information in the file is for informationalpurposes; only the hard and soft limits can be changed.Upon leaving the editor, edquota reads the temporary file andmodifies the binary quota files to reflect the changes made.The editor invoked is vi(1) unless either the EDITOR or theVISUAL environment variable specifies otherwise.Only the super-user may edit quotas.",
        "name": "edquota - edit user quotas",
        "section": 8
    },
    {
        "command": "ematch",
        "description": null,
        "name": "ematch - extended matches for use with \"basic\", \"cgroup\"or\"flow\" filters",
        "section": 8
    },
    {
        "command": "ethtool",
        "description": "ethtool is used to query and control network device driver andhardware settings, particularly for wired Ethernet devices.devname is the name of the network device on which ethtool shouldoperate.",
        "name": "ethtool - query or control network driver and hardware settings",
        "section": 8
    },
    {
        "command": "execstack",
        "description": "execstack is a program which sets, clears, or queries executablestack flag of ELF binaries and shared libraries.Linux has inthe past allowed execution of instructions on the stack and thereare lots of binaries and shared libraries assuming thisbehaviour.Furthermore, GCC trampoline code for e.g. nestedfunctions requires executable stack on many architectures.Toavoid breaking binaries and shared libraries which needexecutable stack, ELF binaries and shared libraries now can bemarked as requiring executable stack or not requiring it.Thismarking is done through the p_flags field in the PT_GNU_STACKprogram header entry.If the marking is missing, kernel ordynamic linker need to assume it might need executable stack.The marking is done automatically by recent GCC versions (objectsusing trampolines on the stack are marked as requiring executablestack, all other newly built objects are marked as not requiringit) and linker collects these markings into marking of the wholebinary or shared library.The user can override this at assemblytime (through --execstack or --noexecstack assembler options), atlink time (through -z execstack or -z noexecstack linker options)and using the execstack tool also on an already linker binary orshared library.This tool is especially useful for third partyshared libraries where it is known that they don't needexecutable stack or testing proves it.",
        "name": "execstack - tool to set, clear, or query executable stack flag ofELF binaries and shared libraries",
        "section": 8
    },
    {
        "command": "exportd",
        "description": "The nfsv4.exportd is used to manage NFSv4 exports.The NFSserver (nfsd) maintains a cache of authentication andauthorization information which is used to identify the source ofeach request, and then what access permissions that source has toany local filesystem.When required information is not found inthe cache, the server sends a request to nfsv4.exportd to fill inthe missing information.nfsv4.exportd uses a table ofinformation stored in /var/lib/nfs/etab and maintained byexportfs(8), possibly based on the contents of exports(5), torespond to each request.",
        "name": "nfsv4.exportd - NFSv4 Server Mount Daemon",
        "section": 8
    },
    {
        "command": "exportfs",
        "description": "An NFS server maintains a table of local physical file systemsthat are accessible to NFS clients.Each file system in thistable isreferred to as an exported file system, or export, forshort.The exportfs command maintains the current table of exports forthe NFS server.The master export table is kept in a file named/var/lib/nfs/etab.This file is read by rpc.mountd when a clientsends an NFS MOUNT request.Normally the master export table is initialized with the contentsof /etc/exports and files under /etc/exports.d by invokingexportfs -a.However, a system administrator can choose to addor delete exports without modifying /etc/exports or files under/etc/exports.d by using the exportfs command.exportfs and its partner program rpc.mountd work in one of twomodes: a legacy mode which applies to 2.4 and earlier versions ofthe Linux kernel, and a new mode which applies to 2.6 and laterversions, providing the nfsd virtual filesystem has been mountedat /proc/fs/nfsd or /proc/fs/nfs.On 2.6 kernels, if thisfilesystem is not mounted, the legacy mode is used.In the new mode, exportfs does not give any information to thekernel, but provides it only to rpc.mountd through the/var/lib/nfs/etab file.rpc.mountd then manages kernel requestsfor information about exports, as needed.In the legacy mode, exports which identify a specific host,rather than a subnet or netgroup, are entered directly into thekernel's export table, as well as being written to/var/lib/nfs/etab.Further, exports listed in /var/lib/nfs/rmtabwhich match a non host-specific export request will cause anappropriate export entry for the host given in rmtab to be addedto the kernel's export table.",
        "name": "exportfs - maintain table of exported NFS file systems",
        "section": 8
    },
    {
        "command": "faillock",
        "description": "The pam_faillock.so module maintains a list of failedauthentication attempts per user during a specified interval andlocks the account in case there were more than deny consecutivefailed authentications. It stores the failure records intoper-user files in the tally directory.The faillock command is an application which can be used toexamine and modify the contents of the tally files. It candisplay the recent failed authentication attempts of the usernameor clear the tally files of all or individual usernames.",
        "name": "faillock - Tool for displaying and modifying the authenticationfailure record files",
        "section": 8
    },
    {
        "command": "faillog",
        "description": "faillog displays the contents of the failure log database(/var/log/faillog). It can also set the failure counters andlimits. When faillog is run without arguments, it only displaysthe faillog records of the users who had a login failure.",
        "name": "faillog - display faillog records or set login failure limits",
        "section": 8
    },
    {
        "command": "fatlabel",
        "description": "fatlabel will display or change the volume label or volume ID onthe MS-DOS filesystem located on DEVICE.By default it works inlabel mode.It can be switched to volume ID mode with the option-i or --volume-id.If NEW is omitted, then the existing label or volume ID iswritten to the standard output.A label can't be longer than 11bytes and should be in all upper case for best compatibility.Anempty string or a label consisting only of white space is notallowed.A volume ID must be given as a hexadecimal number (noleading \"0x\" or similar) and must fit into 32 bits.",
        "name": "fatlabel - set or get MS-DOS filesystem label or volume ID",
        "section": 8
    },
    {
        "command": "fdisk",
        "description": "fdisk is a dialog-driven program for creation and manipulation ofpartition tables. It understands GPT, MBR, Sun, SGI and BSDpartition tables.Block devices can be divided into one or more logical diskscalled partitions. This division is recorded in the partitiontable, usually found in sector 0 of the disk. (In the BSD worldone talks about `disk slices' and a `disklabel'.)All partitioning is driven by device I/O limits (the topology) bydefault. fdisk is able to optimize the disk layout for a4K-sector size and use an alignment offset on modern devices forMBR and GPT. It is always a good idea to follow fdisk's defaultsas the default values (e.g., first and last partition sectors)and partition sizes specified by the +/-<size>{M,G,...} notationare always aligned according to the device properties.CHS (Cylinder-Head-Sector) addressing is deprecated and not usedby default. Please, do not follow old articles andrecommendations with fdisk -S <n> -H <n> advices for SSD or4K-sector devices.Note that partx(8) provides a rich interface for scripts to printdisk layouts, fdisk is mostly designed for humans. Backwardcompatibility in the output of fdisk is not guaranteed. The input(the commands) should always be backward compatible.",
        "name": "fdisk - manipulate disk partition table",
        "section": 8
    },
    {
        "command": "filecap",
        "description": "filecap is a program that prints out a report of programs withfile based capabilities. If a file is not in the report or thereis no report at all, no capabilities were found. For expedience,the default is to check only the directories in the PATHenvironmental variable. If the -a command line option is given,then all directories will be checked. If a directory is passed,it will recursively check that directory. If a path to a file isgiven, it will only check that file. If a file is given followedby capabilities, then the capabilities are written to the file.",
        "name": "filecap - a program to see capabilities",
        "section": 8
    },
    {
        "command": "filefrag",
        "description": "filefrag reports on how badly fragmented a particular file mightbe.It makes allowances for indirect blocks for ext2 and ext3file systems, but can be used on files for any file system.The filefrag program initially attempts to get the extentinformation using FIEMAP ioctl which is more efficient andfaster.If FIEMAP is not supported then filefrag will fall backto using FIBMAP.",
        "name": "filefrag - report on file fragmentation",
        "section": 8
    },
    {
        "command": "findfs",
        "description": "findfs will search the block devices in the system looking for afilesystem or partition with specified tag. The currentlysupported tags are:LABEL=<label>Specifies filesystem label.UUID=<uuid>Specifies filesystem UUID.PARTUUID=<uuid>Specifies partition UUID. This partition identifier issupported for example for GUID Partition Table (GPT)partition tables.PARTLABEL=<label>Specifies partition label (name). The partition labels aresupported for example for GUID Partition Table (GPT) or MACpartition tables.If the filesystem or partition is found, the device name will beprinted on stdout.The complete overview about filesystems and partitions you canget for example bylsblk --fspartx --show <disk>blkid-h, --helpDisplay help text and exit.-V, --versionPrint version and exit.",
        "name": "findfs - find a filesystem by label or UUID",
        "section": 8
    },
    {
        "command": "findmnt",
        "description": "findmnt will list all mounted filesystems or search for afilesystem. The findmnt command is able to search in /etc/fstab,/etc/mtab or /proc/self/mountinfo. If device or mountpoint is notgiven, all filesystems are shown.The device may be specified by device name, major:minor numbers,filesystem label or UUID, or partition label or UUID. Note thatfindmnt follows mount(8) behavior where a device name may beinterpreted as a mountpoint (and vice versa) if the --target,--mountpoint or --source options are not specified.The command-line option --target accepts any file or directoryand then findmnt displays the filesystem for the given path.The command prints all mounted filesystems in the tree-likeformat by default. The default output, is subject to change. Sowhenever possible, you should avoid using default output in yourscripts. Always explicitly define expected columns by using--output columns-list in environments where a stable output isrequired.The relationship between block devices and filesystems is notalways one-to-one. The filesystem may use more block devices.This is why findmnt providesSOURCE and SOURCES (pl.) columns.The column SOURCES displays all devices where it is possible tofind the same filesystemUUID (or another tag specified in fstabwhen executed with --fstab and --evaluate).",
        "name": "findmnt - find a filesystem",
        "section": 8
    },
    {
        "command": "fixfiles",
        "description": "This manual page describes the fixfiles script.This script is primarily used to correct the security contextdatabase (extended attributes) on filesystems.It can also be run at any time to relabel when adding support fornew policy, orjust check whether the file contexts are all asyou expect.By default it will relabel all mounted ext2, ext3,ext4, gfs2, xfs, jfs and btrfs file systems as long as they donot have a security context mount option.You can use the -Rflag to use rpmpackages as an alternative.The file/etc/selinux/fixfiles_exclude_dirs can contain a list ofdirectories excluded from relabeling.fixfiles onboot will setup the machine to relabel on the nextreboot.",
        "name": "fixfiles - fix file SELinux security contexts.",
        "section": 8
    },
    {
        "command": "flow",
        "description": "The flow classifier is meant to extend the SFQ hashingcapabilities without hard-coding new hash functions. It alsoallows deterministic mappings of keys to classes.",
        "name": "flow - flow based traffic control filter",
        "section": 8
    },
    {
        "command": "flower",
        "description": "The flower filter matches flows to the set of keys specified andassigns an arbitrarily chosen class ID to packets belonging tothem. Additionally (or alternatively) an action from the genericaction framework may be called.",
        "name": "flower - flow based traffic control filter",
        "section": 8
    },
    {
        "command": "flowtop",
        "description": "flowtop is a top-like connection tracking tool that can run on anend host or small home router. It is able to present TCP,UDP/UDP-lite, SCTP, DCCP, and ICMP(v6) flows that have beencollected by the kernel's netfilter connection trackingframework, thus no packet capturing in user space needs to bedone.flowtop is able to give you a quick overview of currentconnections on your local system, e.g. for debugging purposes orto answer questions like:* If you access website X, what other connections are beingopened inthe background that I'm not aware of?* What connections are active that pass one's router?* I have this proprietary binary Y, to where does it connect?* To which countries am I sending data?* Are there any suspicious background connections on mymachine?* How many active connections does binary Y have?* How long are connections active already?* At which rate am I sending/receiving data?The following information will be presented in flowtop's output:* Application name and PID when run on local machine* Reverse DNS for source and destination* Geo-location information (country, city)* Used protocols (IPv4, IPv6, TCP, UDP, SCTP, ICMP, ...)* Flow port's service name heuristic* Transport protocol state machine information* Byte/packet counters (if they are enabled)* Connection duration (if timestamping is enabled)* Flow send/receive rate (if byte/packet counters areenabled)In order for flowtop to work, netfilter must be active andrunning on your machine, thus kernel-side connection tracking isactive. If netfilter is not running, you can activate it withiptables(8):iptables -A INPUT -p tcp -m state --state ESTABLISHED -jACCEPTiptables -A OUTPUT -p tcp -m state --state NEW,ESTABLISHED -jACCEPTor by loading the following kernel modules:modprobe nf_conntrack_ipv4modprobe nf_conntrack_ipv6To dump byte/packet counters flowtop enables the sysctl(8)parameter net.netfilter.nf_conntrack_acct via:echo 1 > /proc/sys/net/netfilter/nf_conntrack_acctand resets it to the previously set value on exit. These counterswill only be active on connections which were created afteraccounting was enabled. Thus, to have these counters be activeall the time the parameter should be enabled after the system isup. To automatically enable it, sysctl.conf(8) or sysctl.d(8)might be used.To calculate the connection duration flowtop enables thesysctl(8) parameter net.netfilter.nf_conntrack_timestamp via:echo 1 > /proc/sys/net/netfilter/nf_conntrack_timestampand resets it to the previously set value on exit.flowtop's intention is just to get a quick look over your activeconnections.If you want logging support, have a look atnetfilter's conntrack(8) tools instead.",
        "name": "flowtop - top-like netfilter TCP/UDP/SCTP/DCCP/ICMP(v6) flowtracking",
        "section": 8
    },
    {
        "command": "fsadm",
        "description": "fsadm utility checks or resizes the filesystem on a device (canbe also dm-crypt encrypted device).It tries to use the same APIfor ext2, ext3, ext4, ReiserFS and XFS filesystem.",
        "name": "fsadm \u2014 utility to resize or check filesystem on a device",
        "section": 8
    },
    {
        "command": "fsck",
        "description": "fsck is used to check and optionally repair one or more Linuxfilesystems. filesystem can be a device name (e.g., /dev/hdc1,/dev/sdb2), a mount point (e.g., /, /usr, /home), or a filesystemlabel or UUID specifier (e.g.,UUID=8868abf6-88c5-4a83-98b8-bfc24057f7bd or LABEL=root).Normally, the fsck program will try to handle filesystems ondifferent physical disk drives in parallel to reduce the totalamount of time needed to check all of them.If no filesystems are specified on the command line, and the -Aoption is not specified, fsck will default to checkingfilesystems in /etc/fstab serially. This is equivalent to the -Asoptions.The exit status returned by fsck is the sum of the followingconditions:0No errors1Filesystem errors corrected2System should be rebooted4Filesystem errors left uncorrected8Operational error16Usage or syntax error32Checking canceled by user request128Shared-library errorThe exit status returned when multiple filesystems are checked isthe bit-wise OR of the exit statuses for each filesystem that ischecked.In actuality, fsck is simply a front-end for the variousfilesystem checkers (fsck.fstype) available under Linux. Thefilesystem-specific checker is searched for in the PATHenvironment variable. If the PATH is undefined then fallback to/sbin.Please see the filesystem-specific checker manual pages forfurther details.",
        "name": "fsck - check and repair a Linux filesystem",
        "section": 8
    },
    {
        "command": "fsck.btrfs",
        "description": "fsck.btrfs is a type of utility that should exist for anyfilesystem and is called during system setup when thecorresponding /etc/fstab entries contain non-zero value forfs_passno, see fstab(5) for more.Traditional filesystems need to run their respective fsck utilityin case the filesystem was not unmounted cleanly and the logneeds to be replayed before mount. This is not needed for BTRFS.You should set fs_passno to 0.If you wish to check the consistency of a BTRFS filesystem orrepair a damaged filesystem, see btrfs-check(8). By defaultfilesystem consistency is checked, the repair mode is enabled viathe --repair option (use with care!).",
        "name": "fsck.btrfs - do nothing, successfully",
        "section": 8
    },
    {
        "command": "fsck.cramfs",
        "description": "fsck.cramfs is used to check the cramfs file system.",
        "name": "fsck.cramfs - fsck compressed ROM file system",
        "section": 8
    },
    {
        "command": "fsck.fat",
        "description": "fsck.fat verifies the consistency of MS-DOS filesystems andoptionally tries to repair them.The following filesystem problems can be corrected (in thisorder):\u2022FAT contains invalid cluster numbers.Cluster is changed toEOF.\u2022File's cluster chain contains a loop.The loop is broken.\u2022Bad clusters (read errors).The clusters are marked bad andthey are removed from files owning them.This check isoptional.\u2022Directories with a large number of bad entries (probablycorrupt).The directory can be deleted.\u2022Files . and .. are non-directories.They can be deleted orrenamed.\u2022Directories . and .. in root directory.They are deleted.\u2022Bad filenames.They can be renamed.\u2022Duplicate directory entries.They can be deleted or renamed.\u2022Directories with non-zero size field.Size is set to zero.\u2022Directory . does not point to parent directory.The startpointer is adjusted.\u2022Directory .. does not point to parent of parent directory.The start pointer is adjusted.\u2022. and .. are not the two first entries in a non-rootdirectory.The entries are created, moving occupied slots ifnecessary.\u2022Start cluster number of a file is invalid.The file istruncated.\u2022File contains bad or free clusters.The file is truncated.\u2022File's cluster chain is longer than indicated by the sizefields.The file is truncated.\u2022Two or more files share the same cluster(s).All but one ofthe files are truncated.If the file being truncated is adirectory file that has already been read, the filesystemcheck is restarted after truncation.\u2022File's cluster chain is shorter than indicated by the sizefields.The file is truncated.\u2022Volume label in root directory or label in boot sector isinvalid.Invalid labels are removed.\u2022Volume label in root directory and label in boot sector aredifferent.Volume label from root directory is copied toboot sector.\u2022Clusters are marked as used but are not owned by a file.They are marked as free.Additionally, the following problems are detected, but notrepaired:\u2022Invalid parameters in boot sectorWhen fsck.fat checks a filesystem, it accumulates all changes inmemory and performs them only after all checks are complete.This can be disabled with the -w option.Two different variants of the FAT filesystem are supported.Standard is the FAT12, FAT16 and FAT32 filesystems as defined byMicrosoft and widely used on hard disks and removable media likeUSB sticks and SD cards.The other is the legacy Atari variantused on Atari ST.There are some minor differences in Atari format: Some bootsector fields are interpreted slightly different, and the specialFAT entries for end-of-file and bad cluster can be different.Under MS-DOS 0xfff8 is used for EOF and Atari employs 0xffff bydefault, but both systems recognize all values from 0xfff8\u20130xffffas end-of-file.MS-DOS uses only 0xfff7 for bad clusters, whereon Atari values 0xfff0\u20130xfff7 are for this purpose (but thestandard value is still 0xfff7).",
        "name": "fsck.fat - check and repair MS-DOS FAT filesystems",
        "section": 8
    },
    {
        "command": "fsck.minix",
        "description": "fsck.minix performs a consistency check for the Linux MINIXfilesystem.The program assumes the filesystem is quiescent. fsck.minixshould not be used on a mounted device unless you can be surenobody is writing to it. Remember that the kernel can write todevice when it searches for files.The device name will usually have the following form:\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\u2502\u2502\u2502/dev/hda[1-63] \u2502 IDE disk 1\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502/dev/hdb[1-63] \u2502 IDE disk 2\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502/dev/sda[1-15] \u2502 SCSI disk 1 \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502\u2502\u2502\u2502/dev/sdb[1-15] \u2502 SCSI disk 2 \u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518If the filesystem was changed, i.e., repaired, thenfsck.minix will print \"FILE SYSTEM HAS BEEN CHANGED\" andwill sync(2) three times before exiting. There is no needto reboot after check.",
        "name": "fsck.minix - check consistency of Minix filesystem",
        "section": 8
    },
    {
        "command": "fsck.msdos",
        "description": "fsck.fat verifies the consistency of MS-DOS filesystems andoptionally tries to repair them.The following filesystem problems can be corrected (in thisorder):\u2022FAT contains invalid cluster numbers.Cluster is changed toEOF.\u2022File's cluster chain contains a loop.The loop is broken.\u2022Bad clusters (read errors).The clusters are marked bad andthey are removed from files owning them.This check isoptional.\u2022Directories with a large number of bad entries (probablycorrupt).The directory can be deleted.\u2022Files . and .. are non-directories.They can be deleted orrenamed.\u2022Directories . and .. in root directory.They are deleted.\u2022Bad filenames.They can be renamed.\u2022Duplicate directory entries.They can be deleted or renamed.\u2022Directories with non-zero size field.Size is set to zero.\u2022Directory . does not point to parent directory.The startpointer is adjusted.\u2022Directory .. does not point to parent of parent directory.The start pointer is adjusted.\u2022. and .. are not the two first entries in a non-rootdirectory.The entries are created, moving occupied slots ifnecessary.\u2022Start cluster number of a file is invalid.The file istruncated.\u2022File contains bad or free clusters.The file is truncated.\u2022File's cluster chain is longer than indicated by the sizefields.The file is truncated.\u2022Two or more files share the same cluster(s).All but one ofthe files are truncated.If the file being truncated is adirectory file that has already been read, the filesystemcheck is restarted after truncation.\u2022File's cluster chain is shorter than indicated by the sizefields.The file is truncated.\u2022Volume label in root directory or label in boot sector isinvalid.Invalid labels are removed.\u2022Volume label in root directory and label in boot sector aredifferent.Volume label from root directory is copied toboot sector.\u2022Clusters are marked as used but are not owned by a file.They are marked as free.Additionally, the following problems are detected, but notrepaired:\u2022Invalid parameters in boot sectorWhen fsck.fat checks a filesystem, it accumulates all changes inmemory and performs them only after all checks are complete.This can be disabled with the -w option.Two different variants of the FAT filesystem are supported.Standard is the FAT12, FAT16 and FAT32 filesystems as defined byMicrosoft and widely used on hard disks and removable media likeUSB sticks and SD cards.The other is the legacy Atari variantused on Atari ST.There are some minor differences in Atari format: Some bootsector fields are interpreted slightly different, and the specialFAT entries for end-of-file and bad cluster can be different.Under MS-DOS 0xfff8 is used for EOF and Atari employs 0xffff bydefault, but both systems recognize all values from 0xfff8\u20130xffffas end-of-file.MS-DOS uses only 0xfff7 for bad clusters, whereon Atari values 0xfff0\u20130xfff7 are for this purpose (but thestandard value is still 0xfff7).",
        "name": "fsck.fat - check and repair MS-DOS FAT filesystems",
        "section": 8
    },
    {
        "command": "fsck.vfat",
        "description": "fsck.fat verifies the consistency of MS-DOS filesystems andoptionally tries to repair them.The following filesystem problems can be corrected (in thisorder):\u2022FAT contains invalid cluster numbers.Cluster is changed toEOF.\u2022File's cluster chain contains a loop.The loop is broken.\u2022Bad clusters (read errors).The clusters are marked bad andthey are removed from files owning them.This check isoptional.\u2022Directories with a large number of bad entries (probablycorrupt).The directory can be deleted.\u2022Files . and .. are non-directories.They can be deleted orrenamed.\u2022Directories . and .. in root directory.They are deleted.\u2022Bad filenames.They can be renamed.\u2022Duplicate directory entries.They can be deleted or renamed.\u2022Directories with non-zero size field.Size is set to zero.\u2022Directory . does not point to parent directory.The startpointer is adjusted.\u2022Directory .. does not point to parent of parent directory.The start pointer is adjusted.\u2022. and .. are not the two first entries in a non-rootdirectory.The entries are created, moving occupied slots ifnecessary.\u2022Start cluster number of a file is invalid.The file istruncated.\u2022File contains bad or free clusters.The file is truncated.\u2022File's cluster chain is longer than indicated by the sizefields.The file is truncated.\u2022Two or more files share the same cluster(s).All but one ofthe files are truncated.If the file being truncated is adirectory file that has already been read, the filesystemcheck is restarted after truncation.\u2022File's cluster chain is shorter than indicated by the sizefields.The file is truncated.\u2022Volume label in root directory or label in boot sector isinvalid.Invalid labels are removed.\u2022Volume label in root directory and label in boot sector aredifferent.Volume label from root directory is copied toboot sector.\u2022Clusters are marked as used but are not owned by a file.They are marked as free.Additionally, the following problems are detected, but notrepaired:\u2022Invalid parameters in boot sectorWhen fsck.fat checks a filesystem, it accumulates all changes inmemory and performs them only after all checks are complete.This can be disabled with the -w option.Two different variants of the FAT filesystem are supported.Standard is the FAT12, FAT16 and FAT32 filesystems as defined byMicrosoft and widely used on hard disks and removable media likeUSB sticks and SD cards.The other is the legacy Atari variantused on Atari ST.There are some minor differences in Atari format: Some bootsector fields are interpreted slightly different, and the specialFAT entries for end-of-file and bad cluster can be different.Under MS-DOS 0xfff8 is used for EOF and Atari employs 0xffff bydefault, but both systems recognize all values from 0xfff8\u20130xffffas end-of-file.MS-DOS uses only 0xfff7 for bad clusters, whereon Atari values 0xfff0\u20130xfff7 are for this purpose (but thestandard value is still 0xfff7).",
        "name": "fsck.fat - check and repair MS-DOS FAT filesystems",
        "section": 8
    },
    {
        "command": "fsck.xfs",
        "description": "fsck.xfs is called by the generic Linux fsck(8) program atstartup to check and repair an XFS filesystem.XFS is ajournaling filesystem and performs recovery at mount(8) time ifnecessary, so fsck.xfs simply exits with a zero exit status.If you wish to check the consistency of an XFS filesystem, orrepair a damaged or corrupt XFS filesystem, see xfs_repair(8).However, the system administrator can force fsck.xfs to runxfs_repair(8) at boot time by creating a /forcefsck file orbooting the system with \"fsck.mode=force\" on the kernel commandline.",
        "name": "fsck.xfs - do nothing, successfully",
        "section": 8
    },
    {
        "command": "fsfreeze",
        "description": "fsfreeze suspends or resumes access to a filesystem.fsfreeze halts any new access to the filesystem and creates astable image on disk. fsfreeze is intended to be used withhardware RAID devices that support the creation of snapshots.fsfreeze is unnecessary for device-mapper devices. Thedevice-mapper (and LVM) automatically freezes a filesystem on thedevice when a snapshot creation is requested. For more detailssee the dmsetup(8) man page.The mountpoint argument is the pathname of the directory wherethe filesystem is mounted. The filesystem must be mounted to befrozen (see mount(8)).Note that access-time updates are also suspended if thefilesystem is mounted with the traditional atime behavior (mountoption strictatime, for more details see mount(8)).",
        "name": "fsfreeze - suspend access to a filesystem (Ext3/4, ReiserFS, JFS,XFS)",
        "section": 8
    },
    {
        "command": "fstrim",
        "description": "fstrim is used on a mounted filesystem to discard (or \"trim\")blocks which are not in use by the filesystem. This is useful forsolid-state drives (SSDs) and thinly-provisioned storage.By default, fstrim will discard all unused blocks in thefilesystem. Options may be used to modify this behavior based onrange or size, as explained below.The mountpoint argument is the pathname of the directory wherethe filesystem is mounted and is required when -A, -a, --fstab,or --all are unspecified.Running fstrim frequently, or even using mount -o discard, mightnegatively affect the lifetime of poor-quality SSD devices. Formost desktop and server systems a sufficient trimming frequencyis once a week. Note that not all devices support a queued trim,so each trim command incurs a performance penalty on whateverelse might be trying to use the disk at the time.",
        "name": "fstrim - discard unused blocks on a mounted filesystem",
        "section": 8
    },
    {
        "command": "fullreport",
        "description": "lvm fullreport produces formatted output about PVs, PV segments,VGs, LVs and LV segments. The information is all gatheredtogether for each VG (under a per-VG lock) so it is consistent.Information gathered from separate calls to vgs, pvs, and lvs canbe inconsistent if information changes between commands.",
        "name": "lvm fullreport \u2014 Display full report",
        "section": 8
    },
    {
        "command": "fuse",
        "description": "FUSE (Filesystem in Userspace) is a simple interface foruserspace programs to export a virtual filesystem to the Linuxkernel. FUSE also aims to provide a secure method for nonprivileged users to create and mount their own filesystemimplementations.",
        "name": "fuse - configuration and mount options for FUSE file systems",
        "section": 8
    },
    {
        "command": "fw",
        "description": "the fw filter allows one to classify packets based on apreviously set fwmark by iptables.If the masked value of thefwmark matches the filter's masked handle, the filter matches. Bydefault, all 32 bits of the handle and the fwmark are masked.iptables allows one to mark single packets with the MARK target,or whole connections using CONNMARK.The benefit of using thisfilter instead of doing the heavy-lifting with tc itself is thaton one hand it might be convenient to keep packet filtering andclassification in one place, possibly having to match a packetjust once, and on the other users familiar with iptables but nottc will have a less hard time adding QoS to their setups.",
        "name": "fw - fwmark traffic control filter",
        "section": 8
    },
    {
        "command": "gate",
        "description": "GATE action allows specified ingress frames can be passed atspecific time slot, or be dropped at specific time slot. Tcfilter filters the ingress frames, then tc gate action wouldspecify which time slot and how many bytes these frames can bepassed to device and which time slot frames would be dropped.Gate action also assign a base-time to tell when the entry liststart.Then gate action would start to repeat the gate entrylist cyclically at the start base-time.For the softwaresimulation, gate action requires the user assign reference timeclock type.",
        "name": "gate - Stream Gate Action",
        "section": 8
    },
    {
        "command": "genccode",
        "description": "genccode reads each of the supplied filename and writes out a Cfile containing a compilable definition of the data in the datafile.The C file name is made by taking the base name of thedata filename, replacing dots by underscores, and adding a .cfile extension.If the -a option is used, platform specific assembly code isgenerated instead of C code.Most C compilers will accept both Cand assembly files.Instead of writing a filename with a .c fileextension, a filename with a .s will be written instead.If genccode is called with no filename it terminates gracefully.",
        "name": "genccode - generate C or platform specific assembly code from anICU data file.",
        "section": 8
    },
    {
        "command": "gencmn",
        "description": "gencmn takes a set of files and packages them as an ICU memory-mappable data file. The resulting data file can then be useddirectly by ICU.gencmn reads a list of files to be packaged from either thesupplied listfilename file, or from its standard output. Itpackages all the files from the list that are not bigger thanmaxsize bytes, except if maxsize is 0, which indicates that thereis no size limit on files.",
        "name": "gencmn - generate an ICU memory-mappable data file",
        "section": 8
    },
    {
        "command": "genhomedircon",
        "description": "genhomedircon is a script that executes semodule to rebuild thecurrently active SELinux policy (without reloading it) and tocreate the labels for each user home directory based on directorypaths returned by calls to getpwent().The latter functionality depends on the \"usepasswd\" parameterbeing set to \"true\" (default) in /etc/selinux/semanage.conf.This script is usually executed by semanage although this defaultbehavior can be optionally modified by setting to \"true\" the\"disable-genhomedircon\" in /etc/selinux/semanage.conf.Directories can be excluded from the list of home directories bythe setting \"ignoredirs\" in /etc/selinux/semanage.conf.",
        "name": "genhomedircon - generate SELinux file context configurationentries for user home directories",
        "section": 8
    },
    {
        "command": "genl",
        "description": "The genl utility provides a simple frontend to the genericnetlink library. Although it's designed to support multipleOBJECTs, for now only the ctrl object is available, which is usedto query the generic netlink controller.ctrlThe generic netlink controller can be queried in various ways:helpThis command just prints a help text for the ctrl object.listShow the registered netlink users.monitorListen for generic netlink notifications.getQuery the controller for a given user, identified eitherby name or id.",
        "name": "genl - generic netlink utility frontend",
        "section": 8
    },
    {
        "command": "genpolbools",
        "description": "genpolbools rewrites an existing binary policy with differentboolean settings, generating a new binary policy.The booleansfile specifies the different boolean settings using name=valuelines, where value can be 0 or false to disable the boolean or 1or true to enable it.",
        "name": "genpolbools - Rewrite a binary policy with different booleansettings",
        "section": 8
    },
    {
        "command": "genpolusers",
        "description": "Given an existing binary policy file in-policy, generate a newbinary policy out-policy with an updated user configuration basedon any system.users and local.users files in the specifiedusersdir.",
        "name": "genpolusers - Generate new binary policy with updated userconfiguration",
        "section": 8
    },
    {
        "command": "gensprep",
        "description": "gensprep reads filtered RFC 3454 files and compiles theirinformation into a binary form.The resulting file, <name>.icu,can then be read directly by ICU, or used by pkgdata(8) forincorporation into a larger archive or library.The files read by gensprep are described in the FILES section.",
        "name": "gensprep - compile StringPrep data from files filtered byfilterRFC3454.pl",
        "section": 8
    },
    {
        "command": "getcap",
        "description": "getcap displays the name and capabilities of each specified file.",
        "name": "getcap - examine file capabilities",
        "section": 8
    },
    {
        "command": "getenforce",
        "description": "getenforce reports whether SELinux is enforcing, permissive, ordisabled.",
        "name": "getenforce - get the current mode of SELinux",
        "section": 8
    },
    {
        "command": "getkeycodes",
        "description": "The getkeycodes command prints the kernel scancode-to-keycodemapping table.",
        "name": "getkeycodes - print kernel scancode-to-keycode mapping table",
        "section": 8
    },
    {
        "command": "getpcaps",
        "description": "getpcaps displays the capabilities on the processes indicated bythe pid value(s) given on the command line.A pid of 0 displaysthe capabilities of the process that is running getpcaps itself.The capabilities are displayed in the cap_from_text(3) format.Optional arguments:--help or --usageDisplays usage information and exits.--ugly or --legacyDisplays output in a somewhat ugly legacy format.--verboseDisplays usage in a legacy-like format but not quite sougly in modern default terminal fonts.--iabDisplays IAB tuple capabilities from the process. Theoutput format here is the text format described incap_iab(3). Double quotes encase the regular processcapabilities and square brackets encase the IAB tuple.This format is also used by captree(8).",
        "name": "getpcaps - display process capabilities",
        "section": 8
    },
    {
        "command": "getsebool",
        "description": "getsebool reports where a particular SELinux boolean or allSELinux booleans are on or off In certain situations a booleancan be in one state with a pending change to the other state.getsebool will report this as a pending change.The pendingvalue indicates the value that will be applied upon the nextboolean commit.The setting of boolean values occurs in two stages; first thepending value is changed, then the booleans are committed,causing their active values to become their pending values.Thisallows a group of booleans to be changed in a single transaction,by setting all of their pending values as desired and thencommitting once.",
        "name": "getsebool - get SELinux boolean value(s)",
        "section": 8
    },
    {
        "command": "groupadd",
        "description": "The groupadd command creates a new group account using the valuesspecified on the command line plus the default values from thesystem. The new group will be entered into the system files asneeded.Groupnames may contain only lower and upper case letters, digits,underscores, or dashes. They can end with a dollar sign. Dashesare not allowed at the beginning of the groupname. Fully numericgroupnames and groupnames . or .. are also disallowed.Groupnames may only be up to 16 characters long.",
        "name": "groupadd - create a new group",
        "section": 8
    },
    {
        "command": "groupdel",
        "description": "The groupdel command modifies the system account files, deletingall entries that refer to GROUP. The named group must exist.",
        "name": "groupdel - delete a group",
        "section": 8
    },
    {
        "command": "groupmems",
        "description": "The groupmems command allows a user to administer their own groupmembership list without the requirement of superuser privileges.The groupmems utility is for systems that configure its users tobe in their own name sake primary group (i.e., guest / guest).Only the superuser, as administrator, can use groupmems to alterthe memberships of other groups.",
        "name": "groupmems - administer members of a user's primary group",
        "section": 8
    },
    {
        "command": "groupmod",
        "description": "The groupmod command modifies the definition of the specifiedGROUP by modifying the appropriate entry in the group database.",
        "name": "groupmod - modify a group definition on the system",
        "section": 8
    },
    {
        "command": "grpck",
        "description": "The grpck command verifies the integrity of the groupsinformation. It checks that all entries in /etc/group and/etc/gshadow have the proper format and contain valid data. Theuser is prompted to delete entries that are improperly formattedor which have other uncorrectable errors.Checks are made to verify that each entry has:\u2022the correct number of fields\u2022a unique and valid group name\u2022a valid group identifier (/etc/group only)\u2022a valid list of members and administrators\u2022a corresponding entry in the /etc/gshadow file (respectively/etc/group for the gshadow checks)The checks for correct number of fields and unique group name arefatal. If an entry has the wrong number of fields, the user willbe prompted to delete the entire line. If the user does notanswer affirmatively, all further checks are bypassed. An entrywith a duplicated group name is prompted for deletion, but theremaining checks will still be made. All other errors arewarnings and the user is encouraged to run the groupmod commandto correct the error.The commands which operate on the /etc/group and /etc/gshadowfiles are not able to alter corrupted or duplicated entries.grpck should be used in those circumstances to remove theoffending entries.",
        "name": "grpck - verify integrity of group files",
        "section": 8
    },
    {
        "command": "grpconv",
        "description": "The pwconv command creates shadow from passwd and an optionallyexisting shadow.The pwunconv command creates passwd from passwd and shadow andthen removes shadow.The grpconv command creates gshadow from group and an optionallyexisting gshadow.The grpunconv command creates group from group and gshadow andthen removes gshadow.These four programs all operate on the normal and shadow passwordand group files: /etc/passwd, /etc/group, /etc/shadow, and/etc/gshadow.Each program acquires the necessary locks before conversion.pwconv and grpconv are similar. First, entries in the shadowedfile which don't exist in the main file are removed. Then,shadowed entries which don't have `x' as the password in the mainfile are updated. Any missing shadowed entries are added.Finally, passwords in the main file are replaced with `x'. Theseprograms can be used for initial conversion as well to update theshadowed file if the main file is edited by hand.pwconv will use the values of PASS_MIN_DAYS, PASS_MAX_DAYS, andPASS_WARN_AGE from /etc/login.defs when adding new entries to/etc/shadow.Likewise pwunconv and grpunconv are similar. Passwords in themain file are updated from the shadowed file. Entries which existin the main file but not in the shadowed file are left alone.Finally, the shadowed file is removed. Some password aginginformation is lost by pwunconv. It will convert what it can.",
        "name": "pwconv, pwunconv, grpconv, grpunconv - convert to and from shadowpasswords and groups",
        "section": 8
    },
    {
        "command": "grpunconv",
        "description": "The pwconv command creates shadow from passwd and an optionallyexisting shadow.The pwunconv command creates passwd from passwd and shadow andthen removes shadow.The grpconv command creates gshadow from group and an optionallyexisting gshadow.The grpunconv command creates group from group and gshadow andthen removes gshadow.These four programs all operate on the normal and shadow passwordand group files: /etc/passwd, /etc/group, /etc/shadow, and/etc/gshadow.Each program acquires the necessary locks before conversion.pwconv and grpconv are similar. First, entries in the shadowedfile which don't exist in the main file are removed. Then,shadowed entries which don't have `x' as the password in the mainfile are updated. Any missing shadowed entries are added.Finally, passwords in the main file are replaced with `x'. Theseprograms can be used for initial conversion as well to update theshadowed file if the main file is edited by hand.pwconv will use the values of PASS_MIN_DAYS, PASS_MAX_DAYS, andPASS_WARN_AGE from /etc/login.defs when adding new entries to/etc/shadow.Likewise pwunconv and grpunconv are similar. Passwords in themain file are updated from the shadowed file. Entries which existin the main file but not in the shadowed file are left alone.Finally, the shadowed file is removed. Some password aginginformation is lost by pwunconv. It will convert what it can.",
        "name": "pwconv, pwunconv, grpconv, grpunconv - convert to and from shadowpasswords and groups",
        "section": 8
    },
    {
        "command": "gssd",
        "description": "To establish GSS security contexts using these credential files,the Linux kernel RPC client depends on a userspace daemon calledrpc.gssd.The rpc.gssd daemon uses the rpc_pipefs filesystem tocommunicate with the kernel.User CredentialsWhen a user authenticates using a command such as kinit(1), theresulting credential is stored in a file with a well-known nameconstructed using the user's UID.To interact with an NFS server on behalf of a particularKerberos-authenticated user, the Linux kernel RPC client requeststhat rpc.gssd initialize a security context with the credentialin that user's credential file.Typically, credential files are placed in /tmp.However,rpc.gssd can search for credential files in more than onedirectory.See the description of the -d option for details.Machine Credentialsrpc.gssd searches the default keytab, /etc/krb5.keytab, in thefollowing order for a principal and password to use whenestablishing the machine credential.For the search, rpc.gssdreplaces <hostname> and <REALM> with the local system's hostnameand Kerberos realm.<HOSTNAME>$@<REALM>root/<hostname>@<REALM>nfs/<hostname>@<REALM>host/<hostname>@<REALM>root/<anyname>@<REALM>nfs/<anyname>@<REALM>host/<anyname>@<REALM>rpc.gssd selects one of the <anyname> entries if it does not finda service principal matching the local hostname, e.g. if DHCPassigns the local hostname dynamically.The <anyname> facilityenables the use of the same keytab on multiple systems.However,using the same service principal to establish a machinecredential on multiple hosts can create unwanted securityexposures and is therefore not recommended.Note that <HOSTNAME>$@<REALM> is a user principal that enablesKerberized NFS when the local system is joined to an ActiveDirectory domain using Samba.The keytab provides the passwordfor this principal.You can specify a different keytab by using the -k option if/etc/krb5.keytab does not exist or does not provide one of theseprincipals.Credentials for UID 0UID 0 is a special case.By default rpc.gssd uses the system'smachine credentials for UID 0 accesses that require GSSauthentication.This limits the privileges of the root user whenaccessing network resources that require authentication.Specify the -n option when starting rpc.gssd if you'd like toforce the root user to obtain a user credential rather than usethe local system's machine credential.When -n is specified, the kernel continues to request a GSScontext established with a machine credential for NFSv4operations, such as SETCLIENTID or RENEW, that manage state.Ifrpc.gssd cannot obtain a machine credential (say, the localsystem has no keytab), NFSv4 operations that require machinecredentials will fail.Encryption typesA realm administrator can choose to add keys encoded in a numberof different encryption types to the local system's keytab.Forinstance, a host/ principal might have keys for the aes256-cts-hmac-sha1-96, aes128-cts-hmac-sha1-96, des3-cbc-sha1, andarcfour-hmac encryption types.This permits rpc.gssd to choosean appropriate encryption type that the target NFS serversupports.These encryption types are stronger than legacy single-DESencryption types.To interoperate in environments where serverssupport only weak encryption types, you can restrict your clientto use only single-DES encryption types by specifying the -loption when starting rpc.gssd.",
        "name": "rpc.gssd - RPCSEC_GSS daemon",
        "section": 8
    },
    {
        "command": "halt",
        "description": "poweroff, reboot, and halt may be used to power off, reboot, orhalt the machine. All three commands take the same options.",
        "name": "poweroff, reboot, halt - Power off, reboot, or halt the machine",
        "section": 8
    },
    {
        "command": "hdparm",
        "description": "hdparm provides a command line interface to various kernelinterfaces supported by the Linux SATA/PATA/SAS \"libata\"subsystem and the older IDE driver subsystem.Many newer (2008and later) USB drive enclosures now also support \"SAT\" (SCSI-ATACommand Translation) and therefore may also work with hdparm.E.g. recent WD \"Passport\" models and recent NexStar-3 enclosures.Some options may work correctly only with the latest kernels.",
        "name": "hdparm - get/set SATA/IDE device parameters",
        "section": 8
    },
    {
        "command": "hwclock",
        "description": "hwclock is an administration tool for the time clocks. It can:display the Hardware Clock time; set the Hardware Clock to aspecified time; set the Hardware Clock from the System Clock; setthe System Clock from the Hardware Clock; compensate for HardwareClock drift; correct the System Clock timescale; set the kernel\u2019stimezone, NTP timescale, and epoch (Alpha only); and predictfuture Hardware Clock values based on its drift rate.Since v2.26 important changes were made to the --hctosys functionand the --directisa option, and a new option --update-drift wasadded. See their respective descriptions below.",
        "name": "hwclock - time clocks utility",
        "section": 8
    },
    {
        "command": "i386",
        "description": "setarch modifies execution domains and process personality flags.The execution domains currently only affects the output of uname-m. For example, on an AMD64 system, running setarch i386 programwill cause program to see i686 instead of x86_64 as the machinetype. It can also be used to set various personality options. Thedefault program is /bin/sh.Since version 2.33 the arch command line argument is optional andsetarch may be used to change personality flags (ADDR_LIMIT_*,SHORT_INODE, etc) without modification of the execution domain.",
        "name": "setarch - change reported architecture in new program environmentand/or set personality flags",
        "section": 8
    },
    {
        "command": "ibacm",
        "description": "The IB ACM implements and provides a framework for name, address,and route (path) resolution services over InfiniBand.It isintended to address connection setup scalability issues runningMPI applications on large clusters.The IB ACM providesinformation needed to establish a connection, but does notimplement the CM protocol.A primary user of the ibacm service is the librdmacm library.This enables applications to make use of the ibacm servicewithout code changes or needing to be aware that the service isin use.librdmacm versions 1.0.12 - 1.0.15 can invoke IB ACMservices when built using the --with-ib_acm option.Version1.0.16 and newer of librdmacm will automatically use the IB ACMif it is installed.The IB ACM services tie in under therdma_resolve_addr, rdma_resolve_route, and rdma_getaddrinforoutines.For maximum benefit, the rdma_getaddrinfo routineshould be used, however existing applications should still seesignificant connection scaling benefits using the calls availablein librdmacm 1.0.11 and previous releases.The IB ACM is focused on being scalable, efficient, andextensible.It implements a plugin architecture that allows avendor to supply its proprietary provider in addition to thedefault provider.The current default provider implementationibacmp limits network traffic, SA interactions, and centralizedservices.Ibacmp supports multiple resolution protocols in orderto handle different fabric topologies.The IB ACM package is comprised of three components: the ibacmcore service, the default provider ibacmp shared library, and atest/configuration utility - ib_acme.All three are userspacecomponents and are available for Linux.Additional details aregiven below.",
        "name": "ibacm - address and route resolution services for InfiniBand.",
        "section": 8
    },
    {
        "command": "ibsrpdm",
        "description": "List InfiniBand SCSI RDMA Protocol (SRP) targets on an IB fabric.",
        "name": "ibsrpdm - Discover SRP targets on an InfiniBand Fabric",
        "section": 8
    },
    {
        "command": "iconvconfig",
        "description": "The iconv(3) function internally uses gconv modules to convert toand from a character set.A configuration file is used todetermine the needed modules for a conversion.Loading andparsing such a configuration file would slow down programs thatuse iconv(3), so a caching mechanism is employed.The iconvconfig program reads iconv module configuration filesand writes a fast-loading gconv module configuration cache file.In addition to the system provided gconv modules, the user canspecify custom gconv module directories with the environmentvariable GCONV_PATH.However, iconv module configuration cachingis used only when the environment variable GCONV_PATH is not set.",
        "name": "iconvconfig - create iconv module configuration cache",
        "section": 8
    },
    {
        "command": "icupkg",
        "description": "icupkg reads the input ICU .dat package file, modify it accordingto the options, swap it to the desired platform properties(charset & endianness), and optionally write the resulting ICU.dat package to the output file.Items are removed, then added,then extracted and listed.An ICU .dat package is written ifitems are removed or added, or if the input and output filenamesdiffer, or if the -w, --writepkg option is set.If the input filename is \"new\" then an empty package is created.If the output filename is missing, then it is automaticallygenerated from the input filename. If the input filename endswith an l, b, or e matching its platform properties, then theoutput filename will contain the letter from the -t, --typeoption.This tool can also be used to just swap a single ICU data file,replacing the former icuswap tool. For this mode, provide theinfilename (and optional outfilename) for a non-package ICU datafile.Allowed options include -t, -w, -s and -d The filenamescan be absolute, or relative to the source/dest dir paths.Otheroptions are not allowed in this mode.",
        "name": "icupkg - extract or modify an ICU .dat archive",
        "section": 8
    },
    {
        "command": "idmapd",
        "description": "rpc.idmapd is the NFSv4 ID <-> name mapping daemon.It providesfunctionality to the NFSv4 kernel client and server, to which itcommunicates via upcalls, by translating user and group IDs tonames, and vice versa.The system derives the user part of the string by performing apassword or group lookup.The lookup mechanism is configured in/etc/idmapd.confBy default, the domain part of the string is the system's DNSdomain name.It can also be specified in /etc/idmapd.conf if thesystem is multi-homed, or if the system's DNS domain name does notmatch the name of the system's Kerberos realm.When the domain is not specified in /etc/idmapd.conf the local DNSserver will be queried for the _nfsv4idmapdomain text record. Ifthe record exists that will be used as the domain. When the recorddoes not exist, the domain part of the DNS domain will used.Note that on more recent kernels only the NFSv4 server usesrpc.idmapd.The NFSv4 client instead uses nfsidmap(8), and onlyfalls back to rpc.idmapd if there was a problem running thenfsidmap(8) program.The options are as follows:-hDisplay usage message.-vIncreases the verbosity level (can be specifiedmultiple times).-fRuns rpc.idmapd in the foreground and prints alloutput to the terminal.-p pathSpecifies the location of the RPC pipefs to be path.The default value is \"/var/lib/nfs/rpc_pipefs\".-c pathUse configuration file path.This option isdeprecated.-CClient-only: perform no idmapping for any NFS server,even if one is detected.-SServer-only: perform no idmapping for any NFS client,even if one is detected.",
        "name": "rpc.idmapd \u2014 NFSv4 ID <-> Name Mapper",
        "section": 8
    },
    {
        "command": "ifconfig",
        "description": "Ifconfig is used to configure the kernel-resident networkinterfaces.It is used at boot time to set up interfaces asnecessary.After that, it is usually only needed when debuggingor when system tuning is needed.If no arguments are given, ifconfig displays the status of thecurrently active interfaces.If a single interface argument isgiven, it displays the status of the given interface only; if asingle -a argument is given, it displays the status of allinterfaces, even those that are down.Otherwise, it configuresan interface.",
        "name": "ifconfig - configure a network interface",
        "section": 8
    },
    {
        "command": "ifpps",
        "description": "ifpps is a small utility which periodically provides top-likenetworking and system statistics from the kernel. ifpps gathersits data directly from procfs files and does not make use of anyuser space monitoring libraries which would falsify statisticsunder high load.For instance, consider the following scenario: two directlyconnected Linux machines with Intel Core 2 Quad Q6600 2.40GHzCPUs, 4 GB RAM, and an Intel 82566DC-2 Gigabit Ethernet NIC areused for performance evaluation.One machine generates 64 bytenetwork packets by using the kernel space packet generator pktgenwith a maximum possible packet rate. The other machine displaysstatistics about incoming network packets by using i) iptraf(8)and ii) ifpps.iptraf which incorporates pcap(3) shows an average packet rate of246,000 pps while on the other hand ifpps shows an average packetrate of 1,378,000 pps. Hence, due to packet copies and deferringstatistics creation into user space, a measurement error ofapproximately 460 percent occurs. Tools like iptraf might displaymuch more information such as TCP per flow statistics (hence theuse of the pcap library).This is not possible with ifpps,because overall networking statistics are its focus; statistics,which are also fairly reliable under high packet load.ifpps also periodically displays CPU load, interrupt, softwareinterrupt data per sample interval as well as total interrupts,all per CPU. In case the number of CPUs exceeds 5 or the numberspecified by the user with the -n command line option, ifpps willonly display this number top heavy hitters. The topmost heavyhitter CPU will be marked with \u201c+\u201d.The least heavy hitter willalways be displayed and is marked with \u201c-\u201d. In addition, theaverage for all the above per-CPU data is shown. Optionally themedian values can be displayed using the -m command line option.ifpps also supports directly the gnuplot(1) data sample format.This facilitates creation of gnuplot figures from ifpps timeseries.",
        "name": "ifpps - top-like networking and system statistics",
        "section": 8
    },
    {
        "command": "ifstat",
        "description": "ifstat neatly prints out network interface statistics.Theutility keeps records of the previous data displayed in historyfiles and by default only shows difference between the last andthe current call.Location of the history files defaults to/tmp/.ifstat.u$UID but may be overridden with the IFSTAT_HISTORYenvironment variable. Similarly, the default location for xstat(extended stats) is /tmp/.<xstat name>_ifstat.u$UID.",
        "name": "ifstat - handy utility to read network interface statistics",
        "section": 8
    },
    {
        "command": "insmod",
        "description": "insmod is a trivial program to insert a module into the kernel.Most users will want to use modprobe(8) instead, which is moreclever and can handle module dependencies.Only the most general of error messages are reported: as the workof trying to link the module is now done inside the kernel, thedmesg usually gives more information about errors.",
        "name": "insmod - Simple program to insert a module into the Linux Kernel",
        "section": 8
    },
    {
        "command": "integritysetup",
        "description": "Integritysetup is used to configure dm-integrity manageddevice-mapper mappings.Device-mapper integrity target provides read-write transparentintegrity checking of block devices. The dm-integrity targetemulates an additional data integrity field per-sector. You canuse this additional field directly with integritysetup utility,or indirectly (for authenticated encryption) through cryptsetup.",
        "name": "integritysetup - manage dm-integrity (block level integrity)volumes",
        "section": 8
    },
    {
        "command": "iotop",
        "description": "iotop watches I/O usage information output by the Linux kernel(requires 2.6.20 or later) and displays a table of current I/Ousage by processes or threads on the system. At least theCONFIG_TASK_DELAY_ACCT, CONFIG_TASK_IO_ACCOUNTING,CONFIG_TASKSTATS and CONFIG_VM_EVENT_COUNTERS options need to beenabled in your Linux kernel build configuration and since Linuxkernel 5.14, the kernel.task_delayacct sysctl enabled.iotop displays columns for the I/O bandwidth read and written byeach process/thread during the sampling period. It also displaysthe percentage of time the thread/process spent while swapping inand while waiting on I/O. For each process, its I/O priority(class/level) is shown.In addition, the total I/O bandwidth read and written during thesampling period is displayed at the top of the interface.TotalDISK READ and Total DISK WRITE values represent total read andwrite bandwidth between processes and kernel threads on the oneside and kernel block device subsystem on the other. WhileCurrent DISK READ and Current DISK WRITE values representcorresponding bandwidths for current disk I/O between kernelblock device subsystem and underlying hardware (HDD, SSD, etc.).Thus Total and Current values may not be equal at any givenmoment of time due to data caching and I/O operations reorderingthat take place inside Linux kernel.Use the left and right arrows to change the sorting, r to reversethe sorting order, o to toggle the --only option, p to toggle the--processes option, a to toggle the --accumulated option, q toquit or i to change the priority of a thread or a process'sthread(s). Any other key will force a refresh.",
        "name": "iotop - simple top-like I/O monitor",
        "section": 8
    },
    {
        "command": "ip",
        "description": null,
        "name": "ip - show / manipulate routing, network devices, interfaces andtunnels",
        "section": 8
    },
    {
        "command": "ip-address",
        "description": "The address is a protocol (IPv4 or IPv6) address attached to anetwork device. Each device must have at least one address to usethe corresponding protocol. It is possible to have severaldifferent addresses attached to one device. These addresses arenot discriminated, so that the term alias is not quiteappropriate for them and we do not use it in this document.The ip address command displays addresses and their properties,adds new addresses and deletes old ones.ip address add - add new protocol address.dev IFNAMEthe name of the device to add the address to.local ADDRESS (default)the address of the interface. The format of the addressdepends on the protocol. It is a dotted quad for IP and asequence of hexadecimal halfwords separated by colons forIPv6. The ADDRESS may be followed by a slash and a decimalnumber which encodes the network prefix length.peer ADDRESSthe address of the remote endpoint for pointopointinterfaces.Again, the ADDRESS may be followed by a slashand a decimal number, encoding the network prefix length.If a peer address is specified, the local address cannothave a prefix length. The network prefix is associatedwith the peer rather than with the local address.broadcast ADDRESSthe broadcast address on the interface.It is possible to use the special symbols '+' and '-'instead of the broadcast address. In this case, thebroadcast address is derived by setting/resetting the hostbits of the interface prefix.label LABELEach address may be tagged with a label string.Themaximum allowed total length of label is 15 characters.scope SCOPE_VALUEthe scope of the area where this address is valid.Theavailable scopes are listed in file/etc/iproute2/rt_scopes.Predefined scope values are:global - the address is globally valid.site - (IPv6 only, deprecated) the address is sitelocal, i.e. it is valid inside this site.link - the address is link local, i.e. it is validonly on this device.host - the address is valid only inside this host.metric NUMBERpriority of prefix route associated with address.valid_lft LFTthe valid lifetime of this address; see section 5.5.4 ofRFC 4862. When it expires, the address is removed by thekernel.Defaults to forever.preferred_lft LFTthe preferred lifetime of this address; see section 5.5.4of RFC 4862. When it expires, the address is no longerused for new outgoing connections. Defaults to forever.home(IPv6 only) designates this address the \"home address\" asdefined in RFC 6275.mngtmpaddr(IPv6 only) make the kernel manage temporary addressescreated from this one as template on behalf of PrivacyExtensions (RFC3041). For this to become active, theuse_tempaddr sysctl setting has to be set to a valuegreater than zero.The given address needs to have aprefix length of 64. This flag allows to use privacyextensions in a manually configured network, just like ifstateless auto-configuration was active.nodad(IPv6 only) do not perform Duplicate Address Detection(RFC 4862) when adding this address.optimistic(IPv6 only) When performing Duplicate Address Detection,use the RFC 4429 optimistic variant.noprefixrouteDo not automatically create a route for the network prefixof the added address, and don't search for one to deletewhen removing the address. Changing an address to add thisflag will remove the automatically added prefix route,changing it to remove this flag will create the prefixroute automatically.autojoinJoining multicast groups on Ethernet level via ip maddrcommand does not work if connected to an Ethernet switchthat does IGMP snooping since the switch would notreplicate multicast packets on ports that did not haveIGMP reports for the multicast addresses.Linux VXLAN interfaces created via ip link add vxlan havethe group option that enables them to do the requiredjoin.Using the autojoin flag when adding a multicast addressenables similar functionality for Openvswitch VXLANinterfaces as well as other tunneling mechanisms that needto receive multicast traffic.proto ADDRPROTOthe protocol identifier of this route.ADDRPROTO may be anumber or a string from the file/etc/iproute2/rt_addrprotos.If the protocol ID is notgiven,ip assumes protocol 0. Several protocol values have afixed interpretation. Namely:kernel_lo - The ::1 address that kernel installson a loopback netdevice has thisprotocol valuekernel_ra - IPv6 addresses installed in responseto router advertisement messageskernel_ll - Link-local addresses have thisprotocol valueThe rest of the values are not reserved and theadministrator is free to assign (or not to assign)protocol tags.ip address delete - delete protocol addressArguments: coincide with the arguments of ip addr add.Thedevice name is a required argument. The rest are optional.If noarguments are given, the first address is deleted.ip address show - look at protocol addressesdev IFNAME (default)name of device.scope SCOPE_VALonly list addresses with this scope.to PREFIXonly list addresses matching this prefix.label PATTERNonly list addresses with labels matching the PATTERN.PATTERN is a usual shell style pattern.master DEVICEonly list interfaces enslaved to this master device.vrf NAMEonly list interfaces enslaved to this vrf.type TYPEonly list interfaces of the given type.Note that the type name is not checked against the list ofsupported types - instead it is sent as-is to the kernel.Later it is used to filter the returned interface list bycomparing it with the relevant attribute in case thekernel didn't filter already. Therefore any string isaccepted, but may lead to empty output.uponly list running interfaces.nomasteronly list interfaces with no master.dynamic and permanent(IPv6 only) only list addresses installed due to statelessaddress configuration or only list permanent (not dynamic)addresses. These two flags are inverses of each other, so-dynamic is equal to permanent and -permanent is equal todynamic.tentative(IPv6 only) only list addresses which have not yet passedduplicate address detection.-tentative(IPv6 only) only list addresses which are not in theprocess of duplicate address detection currently.deprecated(IPv6 only) only list deprecated addresses.-deprecated(IPv6 only) only list addresses not being deprecated.dadfailed(IPv6 only) only list addresses which have failedduplicate address detection.-dadfailed(IPv6 only) only list addresses which have not failedduplicate address detection.temporary or secondaryList temporary IPv6 or secondary IPv4 addresses only. TheLinux kernel shares a single bit for those, so they areactually aliases for each other although the meaningdiffers depending on address family.-temporary or -secondaryThese flags are aliases for primary.primaryList only primary addresses, in IPv6 exclude temporaryones. This flag is the inverse of temporary and secondary.-primaryThis is an alias for temporary or secondary.proto ADDRPROTOOnly show addresses with a given protocol, or those forwhich the kernel response did not include protocol. Seethe corresponding argument to ip addr add for detailsabout address protocols.ip address flush - flush protocol addressesThis command flushes the protocol addresses selected by somecriteria.This command has the same arguments as show except that type andmaster selectors are not supported.Another difference is thatit does not run when no arguments are given.Warning: This command and other flush commands are unforgiving.They will cruelly purge all the addresses.With the -statistics option, the command becomes verbose. Itprints out the number of deleted addresses and the number ofrounds made to flush the address list.If this option is giventwice, ip address flush also dumps all the deleted addresses inthe format described in the previous subsection.",
        "name": "ip-address - protocol address management",
        "section": 8
    },
    {
        "command": "ip-addrlabel",
        "description": "IPv6 address labels are used for address selection; they aredescribed in RFC 3484. Precedence is managed by userspace, andonly the label itself is stored in the kernel.ip addrlabel add - add an address labeladd an address label entry to the kernel.prefix PREFIXdev DEVthe outgoing interface.label NUMBERthe label for the prefix.0xffffffff is reserved.ip addrlabel del - delete an address labeldelete an address label entry from the kernel.Arguments:coincide with the arguments of ip addrlabel add but the label isnot required.ip addrlabel list - list address labelslist the current address label entries in the kernel.ip addrlabel flush - flush address labelsflush all address labels in the kernel. This does not restore anydefault settings.",
        "name": "ip-addrlabel - protocol address label management",
        "section": 8
    },
    {
        "command": "ip-fou",
        "description": "The ip fou commands are used to create and delete receive portsfor Foo-over-UDP (FOU) as well as Generic UDP Encapsulation(GUE).Foo-over-UDP allows encapsulating packets of an IP protocoldirectly over UDP. The receiver infers the protocol of a packetreceived on a FOU UDP port to be the protocol configured for theport.Generic UDP Encapsulation (GUE) encapsulates packets of an IPprotocol within UDP and an encapsulation header. Theencapsulation header contains the IP protocol number for theencapsulated packet.When creating a FOU or GUE receive port, the port number isspecified in PORT argument. If FOU is used, the IP protocolnumber associated with the port is specified in PROTO argument.You can bind a port to a local address/interface, by specifyingthe address in the local IFADDR argument or the device in theIFNAME argument. If you would like to connect the port, you canspecify the peer address in the peer IFADDR argument and peerport in the peer_port PORT argument.A FOU or GUE receive port is deleted by specifying PORT in thedelete command, as well as local address/interface or peeraddress/port (if set).",
        "name": "ip-fou - Foo-over-UDP receive port configurationip-gue - Generic UDP Encapsulation receive port configuration",
        "section": 8
    },
    {
        "command": "ip-gue",
        "description": "The ip fou commands are used to create and delete receive portsfor Foo-over-UDP (FOU) as well as Generic UDP Encapsulation(GUE).Foo-over-UDP allows encapsulating packets of an IP protocoldirectly over UDP. The receiver infers the protocol of a packetreceived on a FOU UDP port to be the protocol configured for theport.Generic UDP Encapsulation (GUE) encapsulates packets of an IPprotocol within UDP and an encapsulation header. Theencapsulation header contains the IP protocol number for theencapsulated packet.When creating a FOU or GUE receive port, the port number isspecified in PORT argument. If FOU is used, the IP protocolnumber associated with the port is specified in PROTO argument.You can bind a port to a local address/interface, by specifyingthe address in the local IFADDR argument or the device in theIFNAME argument. If you would like to connect the port, you canspecify the peer address in the peer IFADDR argument and peerport in the peer_port PORT argument.A FOU or GUE receive port is deleted by specifying PORT in thedelete command, as well as local address/interface or peeraddress/port (if set).",
        "name": "ip-fou - Foo-over-UDP receive port configurationip-gue - Generic UDP Encapsulation receive port configuration",
        "section": 8
    },
    {
        "command": "ip-ioam",
        "description": "The ip ioam command is used to configure IPv6 In-situ OAM (IOAM6)internal parameters, namely IOAM namespaces and schemas.Those parameters also include the mapping between an IOAMnamespace and an IOAM schema.",
        "name": "ip-ioam - IPv6 In-situ OAM (IOAM)",
        "section": 8
    },
    {
        "command": "ip-l2tp",
        "description": "The ip l2tp commands are used to establish static, or so-calledunmanaged L2TPv3 ethernet tunnels. For unmanaged tunnels, thereis no L2TP control protocol so no userspace daemon is required -tunnels are manually created by issuing commands at a localsystem and at a remote peer.L2TPv3 is suitable for Layer-2 tunneling. Static tunnels areuseful to establish network links across IP networks when thetunnels are fixed. L2TPv3 tunnels can carry data of more than onesession. Each session is identified by a session_id and itsparent tunnel's tunnel_id. A tunnel must be created before asession can be created in the tunnel.When creating an L2TP tunnel, the IP address of the remote peeris specified, which can be either an IPv4 or IPv6 address. Thelocal IP address to be used to reach the peer must also bespecified. This is the address on which the local system willlisten for and accept received L2TP data packets from the peer.L2TPv3 defines two packet encapsulation formats: UDP or IP. UDPencapsulation is most common. IP encapsulation uses a dedicatedIP protocol value to carry L2TP data without the overhead of UDP.Use IP encapsulation only when there are no NAT devices orfirewalls in the network path.When an L2TPv3 ethernet session is created, a virtual networkinterface is created for the session, which must then beconfigured and brought up, just like any other network interface.When data is passed through the interface, it is carried over theL2TP tunnel to the peer. By configuring the system's routingtables or adding the interface to a bridge, the L2TP interface islike a virtual wire (pseudowire) connected to the peer.Establishing an unmanaged L2TPv3 ethernet pseudowire involvesmanually creating L2TP contexts on the local system and at thepeer. Parameters used at each site must correspond or no datawill be passed. No consistency checks are possible since there isno control protocol used to establish unmanaged L2TP tunnels.Once the virtual network interface of a given L2TP session isconfigured and enabled, data can be transmitted, even if the peerisn't yet configured. If the peer isn't configured, the L2TP datapackets will be discarded by the peer.To establish an unmanaged L2TP tunnel, use l2tp add tunnel andl2tp add session commands described in this document. Thenconfigure and enable the tunnel's virtual network interface, asrequired.Note that unmanaged tunnels carry only ethernet frames. If youneed to carry PPP traffic (L2TPv2) or your peer doesn't supportunmanaged L2TPv3 tunnels, you will need an L2TP server whichimplements the L2TP control protocol. The L2TP control protocolallows dynamic L2TP tunnels and sessions to be established andprovides for detecting and acting upon network failures.ip l2tp add tunnel - add a new tunneltunnel_id IDset the tunnel id, which is a 32-bit integer value.Uniquely identifies the tunnel. The value used must matchthe peer_tunnel_id value being used at the peer.peer_tunnel_id IDset the peer tunnel id, which is a 32-bit integer valueassigned to the tunnel by the peer. The value used mustmatch the tunnel_id value being used at the peer.remote ADDRset the IP address of the remote peer. May be specified asan IPv4 address or an IPv6 address.local ADDRset the IP address of the local interface to be used forthe tunnel. This address must be the address of a localinterface. May be specified as an IPv4 address or an IPv6address.encap ENCAPset the encapsulation type of the tunnel.Valid values for encapsulation are: udp, ip.udp_sport PORTset the UDP source port to be used for the tunnel. Must bepresent when udp encapsulation is selected. Ignored whenip encapsulation is selected.udp_dport PORTset the UDP destination port to be used for the tunnel.Must be present when udp encapsulation is selected.Ignored when ip encapsulation is selected.udp_csum STATE(IPv4 only) control if IPv4 UDP checksums should becalculated and checked for the encapsulating UDP packets,when UDP encapsulating is selected.Default is off.Valid values are: on, off.udp6_csum_tx STATE(IPv6 only) control if IPv6 UDP checksums should becalculated for encapsulating UDP packets, when UDPencapsulating is selected.Default is on.Valid values are: on, off.udp6_csum_rx STATE(IPv6 only) control if IPv6 UDP checksums should bechecked for the encapsulating UDP packets, when UDPencapsulating is selected.Default is on.Valid values are: on, off.ip l2tp del tunnel - destroy a tunneltunnel_id IDset the tunnel id of the tunnel to be deleted. Allsessions within the tunnel must be deleted first.ip l2tp show tunnel - show information about tunnelstunnel_id IDset the tunnel id of the tunnel to be shown. If notspecified, information about all tunnels is printed.ip l2tp add session - add a new session to a tunnelname NAMEsets the session network interface name. Default isl2tpethN.tunnel_id IDset the tunnel id, which is a 32-bit integer value.Uniquely identifies the tunnel into which the session willbe created. The tunnel must already exist.session_id IDset the session id, which is a 32-bit integer value.Uniquely identifies the session being created. The valueused must match the peer_session_id value being used atthe peer.peer_session_id IDset the peer session id, which is a 32-bit integer valueassigned to the session by the peer. The value used mustmatch the session_id value being used at the peer.cookie HEXSTRsets an optional cookie value to be assigned to thesession. This is a 4 or 8 byte value, specified as 8 or 16hex digits, e.g. 014d3636deadbeef. The value must matchthe peer_cookie value set at the peer. The cookie value iscarried in L2TP data packets and is checked for expectedvalue at the peer. Default is to use no cookie.peer_cookie HEXSTRsets an optional peer cookie value to be assigned to thesession. This is a 4 or 8 byte value, specified as 8 or 16hex digits, e.g. 014d3636deadbeef. The value must matchthe cookie value set at the peer. It tells the localsystem what cookie value to expect to find in receivedL2TP packets. Default is to use no cookie.l2spec_type L2SPECTYPEset the layer2specific header type of the session.Valid values are: none, default.seq SEQcontrols sequence numbering to prevent or detect out oforder packets.send puts a sequence number in the defaultlayer2specific header of each outgoing packet.recvreorder packets if they are received out of order.Default is none.Valid values are: none, send, recv, both.ip l2tp del session - destroy a sessiontunnel_id IDset the tunnel id in which the session to be deleted islocated.session_id IDset the session id of the session to be deleted.ip l2tp show session - show information about sessionstunnel_id IDset the tunnel id of the session(s) to be shown. If notspecified, information about sessions in all tunnels isprinted.session_id IDset the session id of the session to be shown. If notspecified, information about all sessions is printed.",
        "name": "ip-l2tp - L2TPv3 static unmanaged tunnel configuration",
        "section": 8
    },
    {
        "command": "ip-link",
        "description": "ip link add - add virtual linklink DEVICEspecifies the physical device to act operate on.NAME specifies the name of the new virtual device.TYPE specifies the type of the new device.Link types:amt - Automatic Multicast Tunneling (AMT)bareudp - Bare UDP L3 encapsulation supportbond - Bonding devicebridge - Ethernet Bridge devicecan - Controller Area Networkdsa - Distributed Switch Architecturedummy - Dummy network interfaceerspan - Encapsulated Remote SPAN over GRE andIPv4geneve - GEneric NEtwork VirtualizationEncapsulationgre - Virtual tunnel interface GRE over IPv4gretap - Virtual L2 tunnel interface GRE over IPv4gtp - GPRS Tunneling Protocolhsr - High-availability Seamless Redundancy deviceifb - Intermediate Functional Block deviceip6erspan - Encapsulated Remote SPAN over GRE andIPv6ip6gre - Virtual tunnel interface GRE over IPv6ip6gretap - Virtual L2 tunnel interface GRE overIPv6ip6tnl - Virtual tunnel interface IPv4|IPv6 overIPv6ipip - Virtual tunnel interface IPv4 over IPv4ipoib - IP over Infiniband deviceipvlan - Interface for L3 (IPv6/IPv4) based VLANsipvtap - Interface for L3 (IPv6/IPv4) based VLANsand TAPlowpan - Interface for 6LoWPAN (IPv6) over IEEE802.15.4 / Bluetoothmacsec - Interface for IEEE 802.1AE MAC Security(MACsec)macvlan - Virtual interface base on link layeraddress (MAC)macvtap - Virtual interface based on link layeraddress (MAC) and TAP.netdevsim - Interface for netdev API testsnlmon - Netlink monitoring devicermnet - Qualcomm rmnet devicesit - Virtual tunnel interface IPv6 over IPv4vcan - Virtual Controller Area Network interfaceveth - Virtual ethernet interfacevirt_wifi - rtnetlink wifi simulation devicevlan - 802.1q tagged virtual LAN interfacevrf - Interface for L3 VRF domainsvti - Virtual tunnel interfacevxcan - Virtual Controller Area Network tunnelinterfacevxlan - Virtual eXtended LANxfrm - Virtual xfrm interfacenumtxqueues QUEUE_COUNTspecifies the number of transmit queues for new device.numrxqueues QUEUE_COUNTspecifies the number of receive queues for new device.gso_max_size BYTESspecifies the recommended maximum size of a GenericSegment Offload packet the new device should accept. Thisis also used to enable BIG TCP for IPv6 on this devicewhen the size is greater than 65536.gso_ipv4_max_size BYTESspecifies the recommended maximum size of a IPv4 GenericSegment Offload packet the new device should accept. Thisis especially used to enable BIG TCP for IPv4 on thisdevice by setting to a size greater than 65536.gso_max_segs SEGMENTSspecifies the recommended maximum number of a GenericSegment Offload segments the new device should accept.gro_max_size BYTESspecifies the maximum size of a packet built by GRO stackon this device. This is also used for BIG TCP to allow thesize of a merged IPv6 GSO packet on this device greaterthan 65536.gro_ipv4_max_size BYTESspecifies the maximum size of a IPv4 packet built by GROstack on this device. This is especially used for BIG TCPto allow the size of a merged IPv4 GSO packet on thisdevice greater than 65536.index IDXspecifies the desired index of the new virtual device. Thelink creation fails, if the index is busy.netns{ PID | NETNSNAME | NETNSFILE }create the device in the network namespace associated withprocess PID or the name NETNSNAME or the file NETNSFILE.VLAN Type SupportFor a link of type VLAN the following additional argumentsare supported:ip link add link DEVICE name NAME type vlan [ protocolVLAN_PROTO ] id VLANID [ reorder_hdr { on | off } ] [ gvrp{ on | off } ] [ mvrp { on | off } ] [ loose_binding { on| off } ] [ bridge_binding { on | off } ] [ ingress-qos-map QOS-MAP ] [ egress-qos-map QOS-MAP ]protocol VLAN_PROTO - either 802.1Q or 802.1ad.id VLANID - specifies the VLAN Identifier to use.Note that numbers with a leading \" 0 \" or \" 0x \"are interpreted as octal or hexadecimal,respectively.reorder_hdr { on | off } - specifies whetherethernet headers are reordered or not (default ison).If reorder_hdr is on then VLAN header will benot inserted immediately but only beforepassing to the physical device (if this devicedoes not support VLAN offloading), the similaron the RX direction - by default the packetwill be untagged before being received by VLANdevice. Reordering allows one to acceleratetagging on egress and to hide VLAN header oningress so the packet looks like regularEthernet packet, at the same time it might beconfusing for packet capture as the VLANheader does not exist within the packet.VLAN offloading can be checked by ethtool(8):ethtool -k <phy_dev> | grep tx-vlan-offloadwhere <phy_dev> is the physical device towhich VLAN device is bound.gvrp { on | off } - specifies whether this VLANshould be registered using GARP VLAN RegistrationProtocol.mvrp { on | off } - specifies whether this VLANshould be registered using Multiple VLANRegistration Protocol.loose_binding { on | off } - specifies whether theVLAN device state is bound to the physical devicestate.bridge_binding { on | off } - specifies whetherthe VLAN device link state tracks the state ofbridge ports that are members of the VLAN.ingress-qos-map QOS-MAP - defines a mapping ofVLAN header prio field to the Linux internalpacket priority on incoming frames. The format isFROM:TO with multiple mappings separated byspaces.egress-qos-map QOS-MAP - defines a mapping ofLinux internal packet priority to VLAN header priofield but for outgoing frames. The format is thesame as for ingress-qos-map.Linux packet priority can be set byiptables(8):iptables -t mangle -A POSTROUTING [...] -jCLASSIFY --set-class 0:4and this \"4\" priority can be used in theegress qos mapping to set VLAN prio \"5\":ip link set veth0.10 type vlan egress 4:5VXLAN Type SupportFor a link of type VXLAN the following additionalarguments are supported:ip link add DEVICE type vxlan id VNI [ dev PHYS_DEV] [ {group | remote } IPADDR ] [ local { IPADDR | any } ] [ ttlTTL ] [ tos TOS ] [ df DF ] [ flowlabel FLOWLABEL ] [dstport PORT ] [ srcport MIN MAX ] [ [no]learning ] [[no]proxy ] [ [no]rsc ] [ [no]l2miss ] [ [no]l3miss ] [[no]udpcsum ] [ [no]udp6zerocsumtx ] [ [no]udp6zerocsumrx] [ ageing SECONDS ] [ maxaddress NUMBER ] [ [no]external] [ gbp ] [ gpe ] [ [no]vnifilter ]id VNI - specifies the VXLAN Network Identifier(or VXLAN Segment Identifier) to use.dev PHYS_DEV - specifies the physical device touse for tunnel endpoint communication.group IPADDR - specifies the multicast IP addressto join.This parameter cannot be specified withthe remote parameter.remote IPADDR - specifies the unicast destinationIP address to use in outgoing packets when thedestination link layer address is not known in theVXLAN device forwarding database. This parametercannot be specified with the group parameter.local IPADDR - specifies the source IP address touse in outgoing packets.ttl TTL - specifies the TTL value to use inoutgoing packets.tos TOS - specifies the TOS value to use inoutgoing packets.df DF - specifies the usage of the Don't Fragmentflag (DF) bit in outgoing packets with IPv4headers. The value inherit causes the bit to becopied from the original IP header. The valuesunset and set cause the bit to be always unset oralways set, respectively. By default, the bit isnot set.flowlabel FLOWLABEL - specifies the flow label touse in outgoing packets.dstport PORT - specifies the UDP destination portto communicate to the remoteVXLAN tunnel endpoint.srcport MIN MAX - specifies the range of portnumbers to use as UDP source ports to communicateto the remote VXLAN tunnel endpoint.[no]learning - specifies if unknown source linklayer addresses and IP addresses are entered intothe VXLAN device forwarding database.[no]rsc - specifies if route short circuit isturned on.[no]proxy - specifies ARP proxy is turned on.[no]l2miss - specifies if netlink LLADDR missnotifications are generated.[no]l3miss - specifies if netlink IP ADDR missnotifications are generated.[no]udpcsum - specifies if UDP checksum iscalculated for transmitted packets over IPv4.[no]udp6zerocsumtx - skip UDP checksum calculationfor transmitted packets over IPv6.[no]udp6zerocsumrx - allow incoming UDP packetsover IPv6 with zero checksum field.ageing SECONDS - specifies the lifetime in secondsof FDB entries learnt by the kernel.maxaddress NUMBER - specifies the maximum numberof FDB entries.[no]external - specifies whether an externalcontrol plane (e.g. ip route encap) or theinternal FDB should be used.[no]vnifilter - specifies whether the vxlan deviceis capable of vni filtering. Only works with avxlan device with external flag set. once enabled,bridge vni command is used to manage the vnifiltering table on the device. The device can onlyreceive packets with vni's configured in the vnifiltering table.gbp - enables the Group Policy extension (VXLAN-GBP).Allows one to transport group policy contextacross VXLAN network peers.If enabled,includes the mark of a packet in the VXLANheader for outgoing packets and fills thepacket mark based on the information found inthe VXLAN header for incoming packets.Format of upper 16 bits of packet mark(flags);+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|-|-|-|-|-|-|-|-|-|D|-|-|A|-|-|-|+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+D := Don't Learn bit. When set, this bitindicates that the egress VTEP MUST NOTlearn the source address of the encapsulatedframe.A := Indicates that the group policy hasalready been applied to this packet.Policies MUST NOT be applied by devices whenthe A bit is set.Format of lower 16 bits of packet mark (policyID):+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+|Group Policy ID|+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+Example:iptables -A OUTPUT [...] -j MARK --set-mark0x800FFgpe - enables the Generic Protocol extension(VXLAN-GPE). Currently, this is only supportedtogether with the external keyword.VETH, VXCAN Type SupportFor a link of types VETH/VXCAN the following additionalarguments are supported:ip link add DEVICE type { veth | vxcan } [ peer name NAME]peer name NAME - specifies the virtual pair devicename of the VETH/VXCAN tunnel.IPIP, SIT Type SupportFor a link of type IPIPorSIT the following additionalarguments are supported:ip link add DEVICE type { ipip | sit }remote ADDR localADDR [ encap { fou | gue | none } ] [ encap-sport { PORT |auto } ] [ encap-dport PORT ] [ [no]encap-csum ] [[no]encap-remcsum ] [mode{ ip6ip | ipip | mplsip | any} ] [ external ]remote ADDR - specifies the remote address of thetunnel.local ADDR - specifies the fixed local address fortunneled packets.It must be an address onanother interface on this host.encap { fou | gue | none } - specifies type ofsecondary UDP encapsulation. \"fou\" indicates Foo-Over-UDP, \"gue\" indicates Generic UDPEncapsulation.encap-sport { PORT | auto } - specifies the sourceport in UDP encapsulation.PORT indicates theport by number, \"auto\" indicates that the portnumber should be chosen automatically (the kernelpicks a flow based on the flow hash of theencapsulated packet).[no]encap-csum - specifies if UDP checksums areenabled in the secondary encapsulation.[no]encap-remcsum - specifies if Remote ChecksumOffload is enabled. This is only applicable forGeneric UDP Encapsulation.mode { ip6ip | ipip | mplsip | any } - specifiesmode in which device should run. \"ip6ip\" indicatesIPv6-Over-IPv4, \"ipip\" indicates \"IPv4-Over-IPv4\",\"mplsip\" indicates MPLS-Over-IPv4, \"any\" indicatesIPv6, IPv4 or MPLS Over IPv4. Supported for SITwhere the default is \"ip6ip\" and IPIP where thedefault is \"ipip\".IPv6-Over-IPv4 is notsupported for IPIP.external - make this tunnel externally controlled(e.g. ip route encap).GRE Type SupportFor a link of type GRE or GRETAP the following additionalarguments are supported:ip link add DEVICE type { gre | gretap }remote ADDRlocal ADDR [ [no][i|o]seq ] [ [i|o]key KEY | no[i|o]key ][ [no][i|o]csum ] [ ttl TTL ] [ tos TOS ] [ [no]pmtudisc ][ [no]ignore-df ] [ dev PHYS_DEV ] [ encap { fou | gue |none } ] [ encap-sport { PORT | auto } ] [ encap-dportPORT ] [ [no]encap-csum ] [ [no]encap-remcsum ] [ external]remote ADDR - specifies the remote address of thetunnel.local ADDR - specifies the fixed local address fortunneled packets.It must be an address onanother interface on this host.[no][i|o]seq - serialize packets.The oseq flagenables sequencing of outgoing packets.The iseqflag requires that all input packets areserialized.[i|o]key KEY | no[i|o]key - use keyed GRE with keyKEY. KEY is either a number or an IPv4 address-like dotted quad.The key parameter specifies thesame key to use in both directions.The ikey andokey parameters specify different keys for inputand output.[no][i|o]csum - generate/require checksums fortunneled packets.The ocsum flag calculateschecksums for outgoing packets.The icsum flagrequires that all input packets have the correctchecksum. The csum flag is equivalent to thecombination icsum ocsum .ttl TTL - specifies the TTL value to use inoutgoing packets.tos TOS - specifies the TOS value to use inoutgoing packets.[no]pmtudisc - enables/disables Path MTU Discoveryon this tunnel.It is enabled by default. Notethat a fixed ttl is incompatible with this option:tunneling with a fixed ttl always makes pmtudiscovery.[no]ignore-df - enables/disables IPv4 DFsuppression on this tunnel.Normally datagramsthat exceed the MTU will be fragmented; thepresence of the DF flag inhibits this, resultinginstead in an ICMP Unreachable (FragmentationRequired) message.Enabling this attribute causesthe DF flag to be ignored.dev PHYS_DEV - specifies the physical device touse for tunnel endpoint communication.encap { fou | gue | none } - specifies type ofsecondary UDP encapsulation. \"fou\" indicates Foo-Over-UDP, \"gue\" indicates Generic UDPEncapsulation.encap-sport { PORT | auto } - specifies the sourceport in UDP encapsulation.PORT indicates theport by number, \"auto\" indicates that the portnumber should be chosen automatically (the kernelpicks a flow based on the flow hash of theencapsulated packet).[no]encap-csum - specifies if UDP checksums areenabled in the secondary encapsulation.[no]encap-remcsum - specifies if Remote ChecksumOffload is enabled. This is only applicable forGeneric UDP Encapsulation.external - make this tunnel externally controlled(e.g. ip route encap).IP6GRE/IP6GRETAP Type SupportFor a link of type IP6GRE/IP6GRETAP the followingadditional arguments are supported:ip link add DEVICE type { ip6gre | ip6gretap } remote ADDRlocal ADDR [ [no][i|o]seq ] [ [i|o]key KEY | no[i|o]key ][ [no][i|o]csum ] [ hoplimit TTL ] [ encaplimit ELIM ] [tclass TCLASS ] [ flowlabel FLOWLABEL ] [ dscp inherit ] [[no]allow-localremote ] [ dev PHYS_DEV ] [ external ]remote ADDR - specifies the remote IPv6 address ofthe tunnel.local ADDR - specifies the fixed local IPv6address for tunneled packets.It must be anaddress on another interface on this host.[no][i|o]seq - serialize packets.The oseq flagenables sequencing of outgoing packets.The iseqflag requires that all input packets areserialized.[i|o]key KEY | no[i|o]key - use keyed GRE with keyKEY. KEY is either a number or an IPv4 address-like dotted quad.The key parameter specifies thesame key to use in both directions.The ikey andokey parameters specify different keys for inputand output.[no][i|o]csum - generate/require checksums fortunneled packets.The ocsum flag calculateschecksums for outgoing packets.The icsum flagrequires that all input packets have the correctchecksum. The csum flag is equivalent to thecombination icsum ocsum.hoplimit TTL - specifies Hop Limit value to use inoutgoing packets.encaplimit ELIM - specifies a fixed encapsulationlimit. Default is 4.flowlabel FLOWLABEL - specifies a fixed flowlabel.[no]allow-localremote - specifies whether to allowremote endpoint to have an address configured onlocal host.tclass TCLASS - specifies the traffic class fieldon tunneled packets, which can be specified aseither a two-digit hex value (e.g. c0) or apredefined string (e.g. internet).The valueinherit causes the field to be copied from theoriginal IP header. The values inherit/STRING orinherit/00..ff will set the field to STRING or00..ff when tunneling non-IP packets. The defaultvalue is 00.external - make this tunnel externally controlled(or not, which is the default).In the kernel,this is referred to as collect metadata mode.This flag is mutually exclusive with the remote,local, seq, key, csum, hoplimit, encaplimit,flowlabel and tclass options.IPoIB Type SupportFor a link of type IPoIB the following additionalarguments are supported:ip link add DEVICE name NAME type ipoib [ pkey PKEY ] [mode MODE ]pkey PKEY - specifies the IB P-Key to use.mode MODE - specifies the mode (datagram orconnected) to use.ERSPAN Type SupportFor a link of type ERSPAN/IP6ERSPAN the followingadditional arguments are supported:ip link add DEVICE type { erspan | ip6erspan } remote ADDRlocal ADDR seq key KEY erspan_ver version [ erspan IDX ] [erspan_dir { ingress | egress } ] [ erspan_hwid hwid ] [[no]allow-localremote ] [ external ]remote ADDR - specifies the remote address of thetunnel.local ADDR - specifies the fixed local address fortunneled packets.It must be an address onanother interface on this host.erspan_ver version - specifies the ERSPAN versionnumber.version indicates the ERSPAN version tobe created: 0 for version 0 type I, 1 for version1 (type II) or 2 for version 2 (type III).erspan IDX - specifies the ERSPAN v1 index field.IDX indicates a 20 bit index/port numberassociated with the ERSPAN traffic's source portand direction.erspan_dir { ingress | egress } - specifies theERSPAN v2 mirrored traffic's direction.erspan_hwid hwid - an unique identifier of anERSPAN v2 engine within a system.hwid is a 6-bitvalue for users to configure.[no]allow-localremote - specifies whether to allowremote endpoint to have an address configured onlocal host.external - make this tunnel externally controlled(or not, which is the default).In the kernel,this is referred to as collect metadata mode.This flag is mutually exclusive with the remote,local, erspan_ver, erspan, erspan_dir anderspan_hwid options.GENEVE Type SupportFor a link of type GENEVE the following additionalarguments are supported:ip link add DEVICE type geneve id VNI remote IPADDR [ ttlTTL ] [ tos TOS ] [ df DF ] [ flowlabel FLOWLABEL ] [dstport PORT ] [ [no]external ] [ [no]udpcsum ] [[no]udp6zerocsumtx ] [ [no]udp6zerocsumrx ] [innerprotoinherit ]id VNI - specifies the Virtual Network Identifierto use.remote IPADDR - specifies the unicast destinationIP address to use in outgoing packets.ttl TTL - specifies the TTL value to use inoutgoing packets. \"0\" or \"auto\" means use whateverdefault value, \"inherit\" means inherit the innerprotocol's ttl. Default option is \"0\".tos TOS - specifies the TOS value to use inoutgoing packets.df DF - specifies the usage of the Don't Fragmentflag (DF) bit in outgoing packets with IPv4headers. The value inherit causes the bit to becopied from the original IP header. The valuesunset and set cause the bit to be always unset oralways set, respectively. By default, the bit isnot set.flowlabel FLOWLABEL - specifies the flow label touse in outgoing packets.dstport PORT - select a destination port otherthan the default of 6081.[no]external - make this tunnel externallycontrolled (or not, which is the default). Thisflag is mutually exclusive with the id, remote,ttl, tos and flowlabel options.[no]udpcsum - specifies if UDP checksum iscalculated for transmitted packets over IPv4.[no]udp6zerocsumtx - skip UDP checksum calculationfor transmitted packets over IPv6.[no]udp6zerocsumrx - allow incoming UDP packetsover IPv6 with zero checksum field.innerprotoinherit - use IPv4/IPv6 as innerprotocol instead of Ethernet.Bareudp Type SupportFor a link of type Bareudp the following additionalarguments are supported:ip link add DEVICE type bareudp dstport PORT ethertypePROTO [ srcportmin PORT ] [ [no]multiproto ]dstport PORT - specifies the destination port forthe UDP tunnel.ethertype PROTO - specifies the ethertype of theL3 protocol being tunnelled.ethertype can begiven as plain Ethernet protocol number or usingthe protocol name (\"ipv4\", \"ipv6\", \"mpls_uc\",etc.).srcportmin PORT - selects the lowest value of theUDP tunnel source port range.[no]multiproto - activates support for protocolssimilar to the one specified by ethertype.Whenethertype is \"mpls_uc\" (that is, unicast MPLS),this allows the tunnel to also handle multicastMPLS.When ethertype is \"ipv4\", this allows thetunnel to also handle IPv6. This option isdisabled by default.AMT Type SupportFor a link of type AMT the following additional argumentsare supported:ip link add DEVICE type AMT discovery IPADDR mode {gateway | relay } local IPADDR dev PHYS_DEV [ relay_portPORT ] [ gateway_port PORT ] [ max_tunnels NUMBER ]discovery IPADDR - specifies the unicast discoveryIP address to use to find remote IP address.mode { gateway | relay } - specifies the role ofAMT, Gateway or Relaylocal IPADDR - specifies the source IP address touse in outgoing packets.dev PHYS_DEV - specifies the underlying physicalinterface from which transform traffic is sent andreceived.relay_port PORT - specifies the UDP Relay port tocommunicate to the Relay.gateway_port PORT - specifies the UDP Gateway portto communicate to the Gateway.max_tunnels NUMBER - specifies the maximum numberof tunnels.MACVLAN and MACVTAP Type SupportFor a link of type MACVLAN or MACVTAP the followingadditional arguments are supported:ip link add link DEVICE name NAME type { macvlan | macvtap} mode { private | vepa | bridge | passthru[ nopromisc ]| source [ nodst ] }[ bcqueuelen { LENGTH } ][ bclimLIMIT ]type { macvlan | macvtap } - specifies the linktype to use.macvlan creates just a virtualinterface, while macvtap in addition creates acharacter device /dev/tapX to be used just like atuntap device.mode private - Do not allow communication betweenmacvlan instances on the same physical interface,even if the external switch supports hairpin mode.mode vepa - Virtual Ethernet Port Aggregator mode.Data from one macvlan instance to the other on thesame physical interface is transmitted over thephysical interface. Either the attached switchneeds to support hairpin mode, or there must be aTCP/IP router forwarding the packets in order toallow communication. This is the default mode.mode bridge - In bridge mode, all endpoints aredirectly connected to each other, communication isnot redirected through the physical interface'speer.mode passthru [ nopromisc ] - This mode gives morepower to a single endpoint, usually in macvtapmode. It is not allowed for more than one endpointon the same physical interface. All traffic willbe forwarded to this endpoint, allowing virtioguests to change MAC address or set promiscuousmode in order to bridge the interface or createvlan interfaces on top of it. By default, thismode forces the underlying interface intopromiscuous mode. Passing the nopromisc flagprevents this, so the promisc flag may becontrolled using standard tools.mode source [ nodst ] - allows one to set a listof allowed mac address, which is used to matchagainst source mac address from received frames onunderlying interface. This allows creating macbased VLAN associations, instead of standard portor tag based. The feature is useful to deploy802.1x mac based behavior, where drivers ofunderlying interfaces doesn't allows that. Bydefault, packets are also considered (duplicated)for destination-based MACVLAN. Passing the nodstflag stops matching packets from also goingthrough the destination-based flow.bcqueuelen { LENGTH } - Set the length of the RXqueue used to process broadcast and multicastpackets.LENGTH must be a positive integer in therange [0-4294967295].Setting a length of 0 willeffectively drop all broadcast/multicast traffic.If not specified the macvlan driver default (1000)is used.Note that all macvlans that share thesame underlying device are using the same queue.The parameter here is a request, the actual queuelength used will be the maximum length that anymacvlan interface has requested.When listingdevice parameters both the bcqueuelen parameter aswell as the actual used bcqueuelen are listed tobetter help the user understand the setting.bclim LIMIT - Set the threshold for broadcastqueueing.LIMIT must be a 32-bit integer.Setting this to -1 disables broadcast queueingaltogether.Otherwise a multicast address will bequeued as broadcast if the number of devices usingit is greater than the given value.High-availability Seamless Redundancy (HSR) SupportFor a link of type HSR the following additional argumentsare supported:ip link add link DEVICE name NAME type hsr slave1SLAVE1-IF slave2 SLAVE2-IF [ supervision ADDR-BYTE ] [version { 0 | 1 } [ proto { 0 | 1 } ]type hsr - specifies the link type to use, hereHSR.slave1 SLAVE1-IF - Specifies the physical deviceused for the first of the two ring ports.slave2 SLAVE2-IF - Specifies the physical deviceused for the second of the two ring ports.supervision ADDR-BYTE - The last byte of themulticast address used for HSR supervision frames.Default option is \"0\", possible values 0-255.version { 0 | 1 } - Selects the protocol versionof the interface. Default option is \"0\", whichcorresponds to the 2010 version of the HSRstandard. Option \"1\" activates the 2012 version.proto { 0 | 1 } - Selects the protocol at theinterface. Default option is \"0\", whichcorresponds to the HSR standard. Option \"1\"activates the Parallel Redundancy Protocol (PRP).BRIDGE Type SupportFor a link of type BRIDGE the following additionalarguments are supported:ip link add DEVICE type bridge [ ageing_time AGEING_TIME ][ group_fwd_mask MASK ] [ group_address ADDRESS ] [forward_delay FORWARD_DELAY ] [ hello_time HELLO_TIME ] [max_age MAX_AGE ] [ stp_state STP_STATE ] [ priorityPRIORITY ] [ no_linklocal_learn NO_LINKLOCAL_LEARN ] [vlan_filtering VLAN_FILTERING ] [ vlan_protocolVLAN_PROTOCOL ] [ vlan_default_pvid VLAN_DEFAULT_PVID ] [vlan_stats_enabled VLAN_STATS_ENABLED ] [vlan_stats_per_port VLAN_STATS_PER_PORT ] [ mcast_snoopingMULTICAST_SNOOPING ] [ mcast_vlan_snoopingMULTICAST_VLAN_SNOOPING ] [ mcast_router MULTICAST_ROUTER] [ mcast_query_use_ifaddr MCAST_QUERY_USE_IFADDR ] [mcast_querier MULTICAST_QUERIER ] [ mcast_hash_elasticityHASH_ELASTICITY ] [ mcast_hash_max HASH_MAX ] [mcast_last_member_count LAST_MEMBER_COUNT ] [mcast_startup_query_count STARTUP_QUERY_COUNT ] [mcast_last_member_interval LAST_MEMBER_INTERVAL ] [mcast_membership_interval MEMBERSHIP_INTERVAL ] [mcast_querier_interval QUERIER_INTERVAL ] [mcast_query_interval QUERY_INTERVAL ] [mcast_query_response_interval QUERY_RESPONSE_INTERVAL ] [mcast_startup_query_interval STARTUP_QUERY_INTERVAL ] [mcast_stats_enabled MCAST_STATS_ENABLED ] [mcast_igmp_version IGMP_VERSION ] [ mcast_mld_versionMLD_VERSION ] [ nf_call_iptables NF_CALL_IPTABLES ] [nf_call_ip6tables NF_CALL_IP6TABLES ] [ nf_call_arptablesNF_CALL_ARPTABLES ]ageing_time AGEING_TIME - configure the bridge'sFDB entries ageing time, ie the number of secondsa MAC address will be kept in the FDB after apacket has been received from that address. afterthis time has passed, entries are cleaned up.group_fwd_mask MASK - set the group forward mask.This is the bitmask that is applied to decidewhether to forward incoming frames destined tolink-local addresses, ie addresses of the form01:80:C2:00:00:0X (defaults to 0, ie the bridgedoes not forward any link-local frames).group_address ADDRESS - set the MAC address of themulticast group this bridge uses for STP.Theaddress must be a link-local address in standardEthernet MAC address format, ie an address of theform 01:80:C2:00:00:0X, with Xin [0, 4..f].forward_delay FORWARD_DELAY - set the forwardingdelay in seconds, ie the time spent in LISTENINGstate (before moving to LEARNING) and in LEARNINGstate (before moving to FORWARDING). Only relevantif STP is enabled. Valid values are between 2 and30.hello_time HELLO_TIME - set the time in secondsbetween hello packets sent by the bridge, when itis a root bridge or a designated bridges.Onlyrelevant if STP is enabled. Valid values arebetween 1 and 10.max_age MAX_AGE - set the hello packet timeout, iethe time in seconds until another bridge in thespanning tree is assumed to be dead, afterreception of its last hello message. Only relevantif STP is enabled. Valid values are between 6 and40.stp_state STP_STATE - turn spanning tree protocolon (STP_STATE > 0) or off (STP_STATE == 0).forthis bridge.priority PRIORITY - set this bridge's spanningtree priority, used during STP root bridgeelection.PRIORITY is a 16bit unsigned integer.no_linklocal_learn NO_LINKLOCAL_LEARN - turn link-local learning on (NO_LINKLOCAL_LEARN == 0) or off(NO_LINKLOCAL_LEARN > 0).When disabled, thebridge will not learn from link-local frames(default: enabled).vlan_filtering VLAN_FILTERING - turn VLANfiltering on (VLAN_FILTERING > 0) or off(VLAN_FILTERING == 0).When disabled, the bridgewill not consider the VLAN tag when handlingpackets.vlan_protocol { 802.1Q | 802.1ad } - set theprotocol used for VLAN filtering.vlan_default_pvid VLAN_DEFAULT_PVID - set thedefault PVID (native/untagged VLAN ID) for thisbridge.vlan_stats_enabled VLAN_STATS_ENABLED - enable(VLAN_STATS_ENABLED == 1) or disable(VLAN_STATS_ENABLED == 0) per-VLAN statsaccounting.vlan_stats_per_port VLAN_STATS_PER_PORT - enable(VLAN_STATS_PER_PORT == 1) or disable(VLAN_STATS_PER_PORT == 0) per-VLAN per-port statsaccounting. Can be changed only when there are noport VLANs configured.mcast_snooping MULTICAST_SNOOPING - turn multicastsnooping on (MULTICAST_SNOOPING > 0) or off(MULTICAST_SNOOPING == 0).mcast_vlan_snooping MULTICAST_VLAN_SNOOPING - turnmulticast VLAN snooping on(MULTICAST_VLAN_SNOOPING > 0) or off(MULTICAST_VLAN_SNOOPING == 0).mcast_router MULTICAST_ROUTER - set bridge'smulticast router if IGMP snooping is enabled.MULTICAST_ROUTER is an integer value having thefollowing meaning:0 - disabled.1 - automatic (queried).2 - permanently enabled.mcast_query_use_ifaddr MCAST_QUERY_USE_IFADDR -whether to use the bridge's own IP address assource address for IGMP queries(MCAST_QUERY_USE_IFADDR > 0) or the default of0.0.0.0 (MCAST_QUERY_USE_IFADDR == 0).mcast_querier MULTICAST_QUERIER - enable(MULTICAST_QUERIER > 0) or disable(MULTICAST_QUERIER == 0) IGMP querier, ie sendingof multicast queries by the bridge (default:disabled).mcast_querier_interval QUERIER_INTERVAL - intervalbetween queries sent by other routers. if noqueries are seen after this delay has passed, thebridge will start to send its own queries (as ifmcast_querier was enabled).mcast_hash_elasticity HASH_ELASTICITY - setmulticast database hash elasticity, ie the maximumchain length in the multicast hash table (defaultsto 4).mcast_hash_max HASH_MAX - set maximum size ofmulticast hash table (defaults to 512, value mustbe a power of 2).mcast_last_member_count LAST_MEMBER_COUNT - setmulticast last member count, ie the number ofqueries the bridge will send before stoppingforwarding a multicast group after a \"leave\"message has been received (defaults to 2).mcast_last_member_interval LAST_MEMBER_INTERVAL -interval between queries to find remaining membersof a group, after a \"leave\" message is received.mcast_startup_query_count STARTUP_QUERY_COUNT -set the number of IGMP queries to send duringstartup phase (defaults to 2).mcast_startup_query_intervalSTARTUP_QUERY_INTERVAL - interval between queriesin the startup phase.mcast_query_interval QUERY_INTERVAL - intervalbetween queries sent by the bridge after the endof the startup phase.mcast_query_response_intervalQUERY_RESPONSE_INTERVAL - set the Max ResponseTime/Maximum Response Delay for IGMP/MLD queriessent by the bridge.mcast_membership_interval MEMBERSHIP_INTERVAL -delay after which the bridge will leave a group,if no membership reports for this group arereceived.mcast_stats_enabled MCAST_STATS_ENABLED - enable(MCAST_STATS_ENABLED > 0) or disable(MCAST_STATS_ENABLED == 0) multicast (IGMP/MLD)stats accounting.mcast_igmp_version IGMP_VERSION - set the IGMPversion.mcast_mld_version MLD_VERSION - set the MLDversion.nf_call_iptables NF_CALL_IPTABLES - enable(NF_CALL_IPTABLES > 0) or disable(NF_CALL_IPTABLES == 0) iptables hooks on thebridge.nf_call_ip6tables NF_CALL_IP6TABLES - enable(NF_CALL_IP6TABLES > 0) or disable(NF_CALL_IP6TABLES == 0) ip6tables hooks on thebridge.nf_call_arptables NF_CALL_ARPTABLES - enable(NF_CALL_ARPTABLES > 0) or disable(NF_CALL_ARPTABLES == 0) arptables hooks on thebridge.MACsec Type SupportFor a link of type MACsec the following additionalarguments are supported:ip link add link DEVICE name NAME type macsec [ [ address<lladdr> ] port PORT | sci SCI ] [ cipher CIPHER_SUITE ] [icvlen { 8..16 } ] [ encrypt { on | off } ] [ send_sci {on | off } ] [ end_station { on | off } ] [ scb { on | off} ] [ protect { on | off } ] [ replay { on | off } window{ 0..2^32-1 } ] [ validate { strict | check | disabled } ][ encodingsa { 0..3 } ]address <lladdr> - sets the system identifiercomponent of secure channel for this MACsecdevice.port PORT - sets the port number component ofsecure channel for this MACsec device, in a rangefrom 1 to 65535 inclusive. Numbers with a leading\" 0 \" or \" 0x \" are interpreted as octal andhexadecimal, respectively.sci SCI - sets the secure channel identifier forthis MACsec device.SCI is a 64bit wide number inhexadecimal format.cipher CIPHER_SUITE - defines the cipher suite touse.icvlen LENGTH - sets the length of the IntegrityCheck Value (ICV).encrypt on or encrypt off - switches betweenauthenticated encryption, or authenticity modeonly.send_sci on or send_sci off - specifies whetherthe SCI is included in every packet, or only whenit is necessary.end_station on or end_station off - sets the EndStation bit.scb on or scb off - sets the Single Copy Broadcastbit.protect on or protect off - enables MACsecprotection on the device.replay on or replay off - enables replayprotection on the device.window SIZE - sets the size of the replaywindow.validate strict or validate check or validatedisabled - sets the validation mode on the device.encodingsa AN - sets the active secure associationfor transmission.VRF Type SupportFor a link of type VRF the following additional argumentsare supported:ip link add DEVICE type vrf table TABLEtable table id associated with VRF deviceRMNET Type SupportFor a link of type RMNET the following additionalarguments are supported:ip link add link DEVICE name NAME type rmnet mux_id MUXIDmux_id MUXID - specifies the mux identifier forthe rmnet device, possible values 1-254.XFRM Type SupportFor a link of type XFRM the following additional argumentsare supported:ip link add DEVICE type xfrm dev PHYS_DEV [ if_id IF_ID ][ external ]dev PHYS_DEV - specifies the underlying physicalinterface from which transform traffic is sent andreceived.if_id IF-ID - specifies the hexadecimal lookup keyused to send traffic to and from specific xfrmpolicies. Policies must be configured with thesame key. If not set, the key defaults to 0 andwill match any policies which similarly do nothave a lookup key configuration.external - make this device externally controlled.This flag is mutually exclusive with the dev andif_id options.GTP Type SupportFor a link of type GTP the following additional argumentsare supported:ip link add DEVICE type gtp role ROLE hsize HSIZErole ROLE - specifies the role of the GTP device,either sgsn or ggsnhsize HSIZE - specifies size of the hashtablewhich stores PDP contextsrestart_count RESTART_COUNT - GTP instance restartcounterip link delete - delete virtual linkdev DEVICEspecifies the virtual device to act operate on.group GROUPspecifies the group of virtual links to delete. Group 0 isnot allowed to be deleted since it is the default group.type TYPEspecifies the type of the device.ip link set - change device attributesWarning: If multiple parameter changes are requested, ip abortsimmediately after any of the changes have failed.This is theonly case when ip can move the system to an unpredictable state.The solution is to avoid changing several parameters with one iplink set call.The modifier change is equivalent to set.dev DEVICEDEVICE specifies network device to operate on. Whenconfiguring SR-IOV Virtual Function (VF) devices, thiskeyword should specify the associated Physical Function(PF) device.group GROUPGROUP has a dual role: If both group and dev are present,then move the device to the specified group. If only agroup is specified, then the command operates on alldevices in that group.up and downchange the state of the device to UP or DOWN.arp on or arp offchange the NOARP flag on the device.multicast on or multicast offchange the MULTICAST flag on the device.allmulticast on or allmulticast offchange the ALLMULTI flag on the device. When enabled,instructs network driver to retrieve all multicast packetsfrom the network to the kernel for further processing.promisc on or promisc offchange the PROMISC flag on the device. When enabled,activates promiscuous operation of the network device.trailers on or trailers offchange the NOTRAILERS flag on the device, NOT used by theLinux and exists for BSD compatibility.protodown on or protodown offchange the PROTODOWN state on the device. Indicates that aprotocol error has been detected on the port. Switchdrivers can react to this error by doing a phys down onthe switch port.protodown_reason PREASON on or offset PROTODOWN reasons on the device. protodown reason bitnames can be enumerated under/etc/iproute2/protodown_reasons.d/. possible reasons bits0-31dynamic on or dynamic offchange the DYNAMIC flag on the device. Indicates thataddress can change when interface goes down (currently NOTused by the Linux).name NAMEchange the name of the device. This operation is notrecommended if the device is running or has some addressesalready configured.txqueuelen NUMBERtxqlen NUMBERchange the transmit queue length of the device.mtu NUMBERchange the MTU of the device.address LLADDRESSchange the station address of the interface.broadcast LLADDRESSbrd LLADDRESSpeer LLADDRESSchange the link layer broadcast address or the peeraddress when the interface is POINTOPOINT.netns{ PID | NETNSNAME | NETNSFILE }move the device to the network namespace associated withprocess PID or the name NETNSNAME or the file NETNSFILE.Some devices are not allowed to change network namespace:loopback, bridge, wireless. These are network namespacelocal devices. In such case ip tool will return \"Invalidargument\" error. It is possible to find out if device islocal to a single network namespace by checking netns-local flag in the output of the ethtool:ethtool -k DEVICETo change network namespace for wireless devices the iwtool can be used. But it allows one to change networknamespace only for physical devices and by process PID.alias NAMEgive the device a symbolic name for easy reference.group GROUPspecify the group the device belongs to.The availablegroups are listed in file /etc/iproute2/group.vf NUM specify a Virtual Function device to be configured. Theassociated PF device must be specified using the devparameter.mac LLADDRESS - change the station address for thespecified VF. The vf parameter must be specified.vlan VLANID - change the assigned VLAN for thespecified VF. When specified, all traffic sentfrom the VF will be tagged with the specified VLANID. Incoming traffic will be filtered for thespecified VLAN ID, and will have all VLAN tagsstripped before being passed to the VF. Settingthis parameter to 0 disables VLAN tagging andfiltering. The vf parameter must be specified.qos VLAN-QOS - assign VLAN QOS (priority) bits forthe VLAN tag. When specified, all VLAN tagstransmitted by the VF will include the specifiedpriority bits in the VLAN tag. If not specified,the value is assumed to be 0. Both the vf and vlanparameters must be specified. Setting both vlanand qos as 0 disables VLAN tagging and filteringfor the VF.proto VLAN-PROTO - assign VLAN PROTOCOL for theVLAN tag, either 802.1Q or 802.1ad.Setting to802.1ad, all traffic sent from the VF will betagged with VLAN S-Tag.Incoming traffic willhave VLAN S-Tags stripped before being passed tothe VF.Setting to 802.1ad also enables an optionto concatenate another VLAN tag, so both S-TAG andC-TAG will be inserted/stripped foroutgoing/incoming traffic, respectively.If notspecified, the value is assumed to be 802.1Q. Boththe vf and vlan parameters must be specified.rate TXRATE -- change the allowed transmitbandwidth, in Mbps, for the specified VF.Settingthis parameter to 0 disables rate limiting.vfparameter must be specified.Please use new APImax_tx_rate option instead.max_tx_rate TXRATE - change the allowed maximumtransmit bandwidth, in Mbps, for the specified VF.Setting this parameter to 0 disables ratelimiting.vf parameter must be specified.min_tx_rate TXRATE - change the allowed minimumtransmit bandwidth, in Mbps, for the specified VF.Minimum TXRATE should be always <= Maximum TXRATE.Setting this parameter to 0 disables ratelimiting.vf parameter must be specified.spoofchk on|off - turn packet spoof checking on oroff for the specified VF.query_rss on|off - toggle the ability of queryingthe RSS configuration of a specific VF. VF RSSinformation like RSS hash key may be consideredsensitive on some devices where this informationis shared between VF and PF and thus its queryingmay be prohibited by default.state auto|enable|disable - set the virtual linkstate as seen by the specified VF. Setting to automeans a reflection of the PF link state, enablelets the VF to communicate with other VFs on thishost even if the PF link state is down, disablecauses the HW to drop any packets sent by the VF.trust on|off - trust the specified VF user. Thisenables that VF user can set a specific featurewhich may impact security and/or performance.(e.g. VF multicast promiscuous mode)node_guid eui64 - configure node GUID forInfiniband VFs.port_guid eui64 - configure port GUID forInfiniband VFs.xdp object | pinned | offset (or unset) a XDP (\"eXpress Data Path\") BPF program torun on every packet at driver level.ip link output willindicate a xdp flag for the networking device. If thedriver does not have native XDP support, the kernel willfall back to a slower, driver-independent \"generic\" XDPvariant. The ip link output will in that case indicatexdpgeneric instead of xdp only. If the driver does havenative XDP support, but the program is loaded underxdpgeneric object | pinned then the kernel will use thegeneric XDP variant instead of the native one.xdpdrv hasthe opposite effect of requestsing that the automaticfallback to the generic XDP variant be disabled and incase driver is not XDP-capable error should be returned.xdpdrv also disables hardware offloads.xdpoffload in iplink output indicates that the program has been offloadedto hardware and can also be used to request the \"offload\"mode, much like xdpgeneric it forces program to beinstalled specifically in HW/FW of the apater.off (or none ) - Detaches any currently attached XDP/BPFprogram from the given device.object FILE - Attaches a XDP/BPF program to the givendevice. The FILE points to a BPF ELF file (f.e. generatedby LLVM) that contains the BPF program code, mapspecifications, etc. If a XDP/BPF program is alreadyattached to the given device, an error will be thrown. Ifno XDP/BPF program is currently attached, the devicesupports XDP and the program from the BPF ELF file passesthe kernel verifier, then it will be attached to thedevice. If the option -force is passed to ip then anyprior attached XDP/BPF program will be atomicallyoverridden and no error will be thrown in this case. If nosection option is passed, then the default section name(\"prog\") will be assumed, otherwise the provided sectionname will be used. If no verbose option is passed, then averifier log will only be dumped on load error.See alsoEXAMPLES section for usage examples.section NAME - Specifies a section name that contains theBPF program code. If no section name is specified, thedefault one (\"prog\") will be used. This option is to bepassed with the object option.program NAME - Specifies the BPF program name that need tobe attached. When the program name is specified, thesection name parameter will be ignored. This option onlyworks when iproute2 build with libbpf support.verbose - Act in verbose mode. For example, even in caseof success, this will print the verifier log in case aprogram was loaded from a BPF ELF file.pinned FILE - Attaches a XDP/BPF program to the givendevice. The FILE points to an already pinned BPF programin the BPF file system. The option section doesn't applyhere, but otherwise semantics are the same as with theoption object described already.master DEVICEset master device of the device (enslave device).nomasterunset master device of the device (release device).addrgenmode eui64|none|stable_secret|randomset the IPv6 address generation modeeui64 - use a Modified EUI-64 format interface identifiernone - disable automatic address generationstable_secret - generate the interface identifier based ona preset/proc/sys/net/ipv6/conf/{default,DEVICE}/stable_secretrandom - like stable_secret, but auto-generate a newrandom secret if none is setlink-netnsidset peer netnsid for a cross-netns interfacetype ETYPE TYPE_ARGSChange type-specific settings. For a list of supportedtypes and arguments refer to the description of ip linkadd above. In addition to that, it is possible tomanipulate settings to slave devices:Bridge Slave SupportFor a link with master bridge the following additionalarguments are supported:ip link set type bridge_slave [ fdb_flush ] [ state STATE] [ priority PRIO ] [ cost COST ] [ guard { on | off } ] [hairpin { on | off } ] [ fastleave { on | off } ] [root_block { on | off } ] [ learning { on | off } ] [flood { on | off } ] [ proxy_arp { on | off } ] [proxy_arp_wifi { on | off } ] [ mcast_routerMULTICAST_ROUTER ] [ mcast_fast_leave { on | off} ] [bcast_flood { on | off } ] [ mcast_flood { on | off } ] [mcast_to_unicast { on | off } ] [ group_fwd_mask MASK ] [neigh_suppress { on | off } ] [ neigh_vlan_suppress { on |off } ] [ vlan_tunnel { on | off } ] [ isolated { on | off} ] [ locked { on | off } ] [ mab { on | off } ] [backup_port DEVICE ] [ nobackup_port ]fdb_flush - flush bridge slave's fdb dynamicentries.state STATE - Set port state.STATE is a numberrepresenting the following states: 0 (disabled), 1(listening), 2 (learning), 3 (forwarding), 4(blocking).priority PRIO - set port priority (allowed valuesare between 0 and 63, inclusively).cost COST - set port cost (allowed values arebetween 1 and 65535, inclusively).guard { on | off } - block incoming BPDU packetson this port.hairpin { on | off } - enable hairpin mode on thisport. This will allow incoming packets on thisport to be reflected back.fastleave { on | off } - enable multicast fastleave on this port.root_block { on | off } - block this port frombecoming the bridge's root port.learning { on | off } - allow MAC address learningon this port.flood { on | off } - open the flood gates on thisport, i.e. forward all unicast frames to this portalso. Requires proxy_arp and proxy_arp_wifi to beturned off.proxy_arp { on | off } - enable proxy ARP on thisport.proxy_arp_wifi { on | off } - enable proxy ARP onthis port which meets extended requirements byIEEE 802.11 and Hotspot 2.0 specifications.mcast_router MULTICAST_ROUTER - configure thisport for having multicast routers attached. A portwith a multicast router will receive all multicasttraffic.MULTICAST_ROUTER may be either 0 todisable multicast routers on this port, 1 to letthe system detect the presence of routers (this isthe default), 2 to permanently enable multicasttraffic forwarding on this port or 3 to enablemulticast routers temporarily on this port, notdepending on incoming queries.mcast_fast_leave { on | off } - this is a synonymto the fastleave option above.bcast_flood { on | off } - controls flooding ofbroadcast traffic on the given port. By defaultthis flag is on.mcast_flood { on | off } - controls whether agiven port will flood multicast traffic for whichthere is no MDB entry. By default this flag is on.mcast_to_unicast { on | off } - controls whether agiven port will replicate packets using unicastinstead of multicast. By default this flag is off.group_fwd_mask MASK - set the group forward mask.This is the bitmask that is applied to decidewhether to forward incoming frames destined tolink-local addresses, ie addresses of the form01:80:C2:00:00:0X (defaults to 0, ie the bridgedoes not forward any link-local frames coming onthis port).neigh_suppress { on | off } - controls whetherneigh discovery (arp and nd) proxy and suppressionis enabled on the port. By default this flag isoff.neigh_vlan_suppress { on | off } - controlswhether per-VLAN neigh discovery (arp and nd)proxy and suppression is enabled on the port. Whenon, the bridge link option neigh_suppress has noeffect and the per-VLAN state is set using thebridge vlan option neigh_suppress. By default thisflag is off.vlan_tunnel { on | off } - controls whether vlanto tunnel mapping is enabled on the port. Bydefault this flag is off.locked { on | off } - controls whether a port islocked or not. When locked, non-link-local framesreceived through the port are dropped unless anFDB entry with the MAC source address points tothe port. The common use case is IEEE 802.1X wherehosts can authenticate themselves by exchangingEAPOL frames with an authenticator. Afterauthentication is complete, the user space controlplane can install a matching FDB entry to allowtraffic from the host to be forwarded by thebridge. When learning is enabled on a locked port,the no_linklocal_learn bridge option needs to beon to prevent the bridge from learning fromreceived EAPOL frames. By default this flag isoff.mab { on | off } - controls whether MACAuthentication Bypass (MAB) is enabled on the portor not.MAB can only be enabled on a locked portthat has learning enabled. When enabled, FDBentries are learned from received traffic and havethe \"locked\" FDB flag set. The flag can only beset by the kernel and it indicates that the FDBentry cannot be used to authenticate thecorresponding host. User space can decide toauthenticate the host by replacing the FDB entryand clearing the \"locked\" FDB flag. Locked FDBentries can roam to unlocked (authorized) ports inwhich case the \"locked\" flag is cleared. FDBentries cannot roam to locked ports regardless ofMAB being enabled or not. Therefore, locked FDBentries are only created if an FDB entry with thegiven {MAC, VID} does not already exist.Thisbehavior prevents unauthenticated hosts fromdisrupting traffic destined to alreadyauthenticated hosts. Locked FDB entries act likeregular dynamic entries with respect to forwardingand aging. By default this flag is off.backup_port DEVICE - if the port loses carrier alltraffic will be redirected to the configuredbackup portnobackup_port - removes the currently configuredbackup portBonding Slave SupportFor a link with master bond the following additionalarguments are supported:ip link set type bond_slave [ queue_id ID ] [ prioPRIORITY ]queue_id ID - set the slave's queue ID (a 16bitunsigned value).prio PRIORITY - set the slave's priority foractive slave re-selection during failover (a 32bitsigned value). This option only valid for active-backup(1), balance-tlb (5) and balance-alb (6)mode.MACVLAN and MACVTAP SupportModify list of allowed macaddr for link in source mode.ip link set type { macvlan | macvap } [ macaddr COMMANDMACADDR ...]Commands:add - add MACADDR to allowed listset - replace allowed listdel - remove MACADDR from allowed listflush - flush whole allowed listUpdate the broadcast/multicast queue length.ip link set type { macvlan | macvap } [ bcqueuelenLENGTH] [ bclim LIMIT ]bcqueuelen LENGTH - Set the length of the RX queueused to process broadcast and multicast packets.LENGTH must be a positive integer in the range[0-4294967295].Setting a length of 0 willeffectively drop all broadcast/multicast traffic.If not specified the macvlan driver default (1000)is used.Note that all macvlans that share thesame underlying device are using the same queue.The parameter here is a request, the actual queuelength used will be the maximum length that anymacvlan interface has requested.When listingdevice parameters both the bcqueuelen parameter aswell as the actual used bcqueuelen are listed tobetter help the user understand the setting.bclim LIMIT - Set the threshold for broadcastqueueing.LIMIT must be a 32-bit integer.Setting this to -1 disables broadcast queueingaltogether.Otherwise a multicast address will bequeued as broadcast if the number of devices usingit is greater than the given value.DSA user port supportFor a link having the DSA user port type, the followingadditional arguments are supported:ip link set type dsa [ conduit DEVICE ]conduit DEVICE - change the DSA conduit (hostnetwork interface) responsible for handling thelocally terminated traffic for the given DSAswitch user port. For a description of whichnetwork interfaces are suitable for serving asconduit interfaces of this user port, please seehttps://www.kernel.org/doc/html/latest/networking/dsa/configuration.html#affinity-of-user-ports-to-cpu-portsas well as what is supported by the driver in use.master DEVICE - this is a synonym for \"conduit\".ip link show - display device attributesdev NAME (default)NAME specifies the network device to show.group GROUPGROUP specifies what group of devices to show.uponly display running interfaces.master DEVICEDEVICE specifies the master device which enslaves devicesto show.vrf NAMENAME specifies the VRF which enslaves devices to show.type TYPETYPE specifies the type of devices to show.Note that the type name is not checked against the list ofsupported types - instead it is sent as-is to the kernel.Later it is used to filter the returned interface list bycomparing it with the relevant attribute in case thekernel didn't filter already. Therefore any string isaccepted, but may lead to empty output.nomasteronly show devices with no masterip link xstats - display extended statisticstype TYPETYPE specifies the type of devices to display extendedstatistics for.ip link afstats - display address-family specific statisticsdev DEVICEDEVICE specifies the device to display address-familystatistics for.ip link help - display helpTYPE specifies which help of link type to display.GROUPmay be a number or a string from the file /etc/iproute2/groupwhich can be manually filled.",
        "name": "ip-link - network device configuration",
        "section": 8
    },
    {
        "command": "ip-macsec",
        "description": "The ip macsec commands are used to configure transmit secureassociations and receive secure channels and their secureassociations on a MACsec device created with the ip link addcommand using the macsec type.",
        "name": "ip-macsec - MACsec device configuration",
        "section": 8
    },
    {
        "command": "ip-maddress",
        "description": "maddress objects are multicast addresses.ip maddress show - list multicast addressesdev NAME (default)the device name.ip maddress add - add a multicast addressip maddress delete - delete a multicast addressThese commands attach/detach a static link-layer multicastaddress to listen on the interface.Note that it isimpossible to join protocol multicast groups statically.This command only manages link-layer addresses.address LLADDRESS (default)the link-layer multicast address.dev NAMEthe device to join/leave this multicast address.",
        "name": "ip-maddress - multicast addresses management",
        "section": 8
    },
    {
        "command": "ip-monitor",
        "description": "The ip utility can monitor the state of devices, addresses androutes continuously. This option has a slightly different format.Namely, the monitor command is the first in the command line andthen the object list follows:ip monitor [ all | OBJECT-LIST ] [ file FILENAME ] [ label ] [all-nsid ] [ dev DEVICE ]OBJECT-LIST is the list of object types that we want to monitor.It may contain link, address, route, mroute, prefix, neigh,netconf, rule, stats, nsid and nexthop.If no file argument isgiven, ip opens RTNETLINK, listens on it and dumps state changesin the format described in previous sections.If the label option is set, a prefix is displayed before eachmessage to show the family of the message. For example:[NEIGH]10.16.0.112 dev eth0 lladdr 00:04:23:df:2f:d0 REACHABLE[LINK]3: eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_faststate DOWN group defaultlink/ether 52:54:00:12:34:57 brd ff:ff:ff:ff:ff:ffIf the all-nsid option is set, the program listens to all networknamespaces that have a nsid assigned into the network namespacewere the program is running.A prefix is displayed to show thenetwork namespace where the message originates. Example:[nsid 0]10.16.0.112 dev eth0 lladdr 00:04:23:df:2f:d0 REACHABLEIf the file option is given, the program does not listen onRTNETLINK, but opens the given file, and dumps its contents. Thefile should contain RTNETLINK messages saved in binary format.Such a file can be generated with the rtmon utility. This utilityhas a command line syntax similar to ip monitor.Ideally, rtmonshould be started before the first network configuration commandis issued. F.e. if you insert:rtmon file /var/log/rtmon.login a startup script, you will be able to view the full historylater.Nevertheless, it is possible to start rtmon at any time.Itprepends the history with the state snapshot dumped at the momentof starting.If the dev option is given, the program prints only eventsrelated to this device.",
        "name": "ip-monitor, rtmon - state monitoring",
        "section": 8
    },
    {
        "command": "ip-mptcp",
        "description": "MPTCP is a transport protocol built on top of TCP that allows TCPconnections to use multiple paths to maximize resource usage andincrease redundancy. The ip-mptcp sub-commands allow configuringseveral aspects of the MPTCP path manager, which is in charge ofsubflows creation:The endpoint object specifies the IP addresses that will be usedand/or announced for additional subflows:ip mptcp endpoint addadd new MPTCP endpointip mptcp endpoint deletedelete existing MPTCP endpointip mptcp endpoint showget existing MPTCP endpointip mptcp endpoint flushflush all existing MPTCP endpointsIFADDR An IPv4 or IPv6 address. When used with the delete idoperation, an IFADDR is only included when the ID is 0.PORTWhen a port number is specified, incoming MPTCP subflowsfor already established MPTCP sockets will be accepted onthe specified port, regardless the original listener portaccepting the first MPTCP subflow and/or this peer beingactually on the client side.IDis a unique numeric identifier for the given endpointsignal The endpoint will be announced/signaled to each peer viaan MPTCP ADD_ADDR sub-option. Upon reception of anADD_ADDR sub-option, the peer can try to create additionalsubflows, see ADD_ADDR_ACCEPTED_NR.subflowIf additional subflow creation is allowed by the MPTCPlimits, the MPTCP path manager will try to create anadditional subflow using this endpoint as the sourceaddress after the MPTCP connection is established.backup If this is a subflow endpoint, the subflows created usingthis endpoint will have the backup flag set during theconnection process. This flag instructs the peer to onlysend data on a given subflow when all non-backup subflowsare unavailable. This does not affect outgoing data, wheresubflow priority is determined by the backup/non-backupflag received from the peerfullmeshIf this is a subflow endpoint and additional subflowcreation is allowed by the MPTCP limits, the MPTCP pathmanager will try to create an additional subflow for eachknown peer address, using this endpoint as the sourceaddress. This will occur after the MPTCP connection isestablished. If the peer did not announce any additionaladdresses using the MPTCP ADD_ADDR sub-option, this willbehave the same as a plain subflow endpoint. When the peerdoes announce addresses, each received ADD_ADDR sub-optionwill trigger creation of an additional subflow to generatea full mesh topology.The limits object specifies the constraints for subflowcreations:ip mptcp limits showget current MPTCP subflow creation limitsip mptcp limits setchange the MPTCP subflow creation limitsSUBFLOW_NRspecifies the maximum number of additional subflowsallowed for each MPTCP connection. Additional subflows canbe created due to: incoming accepted ADD_ADDR sub-option,local subflow endpoints, additional subflows started bythe peer.ADD_ADDR_ACCEPTED_NRspecifies the maximum number of incoming ADD_ADDR sub-options accepted for each MPTCP connection. Afterreceiving the specified number of ADD_ADDR sub-options,any other incoming one will be ignored for the MPTCPconnection lifetime. When an ADD_ADDR sub-option isaccepted and there are no local fullmesh endpoints, theMPTCP path manager will try to create a new subflow usingthe address in the ADD_ADDR sub-option as the destinationaddress and a source address determined using localrouting resolution When fullmesh endpoints are available,the MPTCP path manager will try to create new subflowsusing each fullmesh endpoint as a source address and thepeer's ADD_ADDR address as the destination.In both casesthe SUBFLOW_NR limit is enforced.monitor displays creation and deletion of MPTCP connections aswell as addition or removal of remote addresses and subflows.",
        "name": "ip-mptcp - MPTCP path manager configuration",
        "section": 8
    },
    {
        "command": "ip-mroute",
        "description": "mroute objects are multicast routing cache entries created by auser-level mrouting daemon (f.e.pimd or mrouted ).Due to the limitations of the current interface to the multicastrouting engine, it is impossible to change mroute objectsadministratively, so we can only display them. This limitationwill be removed in the future.ip mroute show - list mroute cache entriesto PREFIX (default)the prefix selecting the destination multicast addressesto list.iif NAMEthe interface on which multicast packets are received.from PREFIXthe prefix selecting the IP source addresses of themulticast route.table TABLE_IDthe table id selecting the multicast table. It can belocal, main, default, all or a number.",
        "name": "ip-mroute - multicast routing cache management",
        "section": 8
    },
    {
        "command": "ip-neighbour",
        "description": "The ip neigh command manipulates neighbour objects that establishbindings between protocol addresses and link layer addresses forhosts sharing the same link.Neighbour entries are organizedinto tables. The IPv4 neighbour table is also known by anothername - the ARP table.The corresponding commands display neighbour bindings and theirproperties, add new neighbour entries and delete old ones.ip neighbour addadd a new neighbour entryip neighbour changechange an existing entryip neighbour replaceadd a new entry or change an existing oneThese commands create new neighbour records or updateexisting ones.to ADDRESS (default)the protocol address of the neighbour. It is eitheran IPv4 or IPv6 address.dev NAMEthe interface to which this neighbour is attached.proxyindicates whether we are proxying for thisneighbour entryrouter indicates whether neighbour is a routerusethis neigh entry is in \"use\". This option can beused to indicate to the kernel that a controller isusing this dynamic entry. If the entry does notexist, the kernel will resolve it. If it exists, anattempt to refresh the neighbor entry will betriggered.managedthis neigh entry is \"managed\". This option can beused to indicate to the kernel that a controller isusing this dynamic entry. In contrast to \"use\", ifthe entry does not exist, the kernel will resolveit and periodically attempt to auto-refresh theneighbor entry such that it remains in resolvedstate when possible.extern_learnthis neigh entry was learned externally. Thisoption can be used to indicate to the kernel thatthis is a controller learnt dynamic entry.Kernelwill not gc such an entry.lladdr LLADDRESSthe link layer address of the neighbour.LLADDRESScan also be null.nud STATEthe state of the neighbour entry.nud is anabbreviation for 'Neighbour UnreachabilityDetection'.The state can take one of thefollowing values:permanentthe neighbour entry is valid forever and canbe only be removed administratively.noarpthe neighbour entry is valid. No attempts tovalidate this entry will be made but it canbe removed when its lifetime expires.reachablethe neighbour entry is valid until thereachability timeout expires.stalethe neighbour entry is valid but suspicious.This option to ip neigh does not change theneighbour state if it was valid and theaddress is not changed by this command.nonethis is a pseudo state used when initiallycreating a neighbour entry or after tryingto remove it before it becomes free to doso.incompletethe neighbour entry has not (yet) beenvalidated/resolved.delayneighbor entry validation is currentlydelayed.probeneighbor is being probed.failed max number of probes exceeded withoutsuccess, neighbor validation has ultimatelyfailed.ip neighbour deletedelete a neighbour entryThe arguments are the same as with ip neigh add, exceptthat lladdr and nud are ignored.Warning: Attempts to delete or manually change a noarpentry created by the kernel may result in unpredictablebehaviour.Particularly, the kernel may try to resolvethis address even on a NOARP interface or if the addressis multicast or broadcast.ip neighbour showlist neighbour entriesto ADDRESS (default)the prefix selecting the neighbours to list.dev NAMEonly list the neighbours attached to this device.vrf NAMEonly list the neighbours for given VRF.nomasteronly list neighbours attached to an interface withno master.proxylist neighbour proxies.unused only list neighbours which are not currently inuse.nud STATEonly list neighbour entries in this state.NUD_STATE takes values listed below or the specialvalue all which means all states. This option mayoccur more than once.If this option is absent, iplists all entries except for none and noarp.ip neighbour flushflush neighbour entriesThis command has the same arguments as show.Thedifferences are that it does not run when no arguments aregiven, and that the default neighbour states to be flusheddo not include permanent and noarp.With the -statistics option, the command becomes verbose.It prints out the number of deleted neighbours and thenumber of rounds made to flush the neighbour table. If theoption is given twice, ip neigh flush also dumps all thedeleted neighbours.ip neigh getlookup a neighbour entry to a destination given a deviceproxyindicates whether we should lookup a proxyneighbour entryto ADDRESS (default)the prefix selecting the neighbour to query.dev NAMEget neighbour entry attached to this device.",
        "name": "ip-neighbour - neighbour/arp tables management.",
        "section": 8
    },
    {
        "command": "ip-netconf",
        "description": "The ip netconf utility can monitor IPv4 and IPv6 parameters (see/proc/sys/net/ipv[4|6]/conf/[all|DEV]/) like forwarding,rp_filter, proxy_neigh, ignore_routes_with_linkdown ormc_forwarding status.If no interface is specified, the entry all is displayed.ip netconf show - display network parametersdev NAMEthe name of the device to display network parameters for.",
        "name": "ip-netconf - network configuration monitoring",
        "section": 8
    },
    {
        "command": "ip-netns",
        "description": "A network namespace is logically another copy of the networkstack, with its own routes, firewall rules, and network devices.By default a process inherits its network namespace from itsparent. Initially all the processes share the same defaultnetwork namespace from the init process.By convention a named network namespace is an object at/var/run/netns/NAME that can be opened. The file descriptorresulting from opening /var/run/netns/NAME refers to thespecified network namespace. Holding that file descriptor openkeeps the network namespace alive. The file descriptor can beused with the setns(2) system call to change the networknamespace associated with a task.For applications that are aware of network namespaces, theconvention is to look for global network configuration filesfirst in /etc/netns/NAME/ then in /etc/.For example, if youwant a different version of /etc/resolv.conf for a networknamespace used to isolate your vpn you would name it/etc/netns/myvpn/resolv.conf.ip netns exec automates handling of this configuration, fileconvention for network namespace unaware applications, bycreating a mount namespace and bind mounting all of the pernetwork namespace configure files into their traditional locationin /etc.ip netns list - show all of the named network namespacesThis command displays all of the network namespaces in/var/run/netnsip netns add NAME - create a new named network namespaceIf NAME is available in /var/run/netns this commandcreates a new network namespace and assigns NAME.ip netns attach NAME PID - create a new named network namespaceIf NAME is available in /var/run/netns this commandattaches the network namespace of the process PID to NAMEas if it were created with ip netns.ip [-all] netns delete [ NAME ] - delete the name of a networknamespace(s)If NAME is present in /var/run/netns it is umounted andthe mount point is removed. If this is the last user ofthe network namespace the network namespace will be freedand all physical devices will be moved to the default one,otherwise the network namespace persists until it has nomore users. ip netns delete may fail if the mount point isin use in another mount namespace.If -all option was specified then all the networknamespace names will be removed.It is possible to lose the physical device when it wasmoved to netns and then this netns was deleted with arunning process:$ ip netns add net0$ ip link set dev eth0 netns net0$ ip netns exec net0 SOME_PROCESS_IN_BACKGROUND$ ip netns del net0and eth0 will appear in the default netns only afterSOME_PROCESS_IN_BACKGROUND will exit or will be killed. Toprevent this the processes running in net0 should bekilled before deleting the netns:$ ip netns pids net0 | xargs kill$ ip netns del net0ip netns set NAME NETNSID - assign an id to a peer networknamespaceThis command assigns a id to a peer network namespace.This id is valid only in the current network namespace.If the keyword \"auto\" is specified an available nsid willbe chosen.This id will be used by the kernel in somenetlink messages. If no id is assigned when the kernelneeds it, it will be automatically assigned by the kernel.Once it is assigned, it's not possible to change it.ip netns identify [PID] - Report network namespaces names forprocessThis command walks through /var/run/netns and finds allthe network namespace names for network namespace of thespecified process, if PID is not specified then thecurrent process will be used.ip netns pids NAME - Report processes in the named networknamespaceThis command walks through proc and finds all of theprocess who have the named network namespace as theirprimary network namespace.ip [-all] netns exec [ NAME ] cmd ... - Run cmd in the namednetwork namespaceThis command allows applications that are networknamespace unaware to be run in something other than thedefault network namespace with all of the configurationfor the specified network namespace appearing in thecustomary global locations. A network namespace and bindmounts are used to move files from their network namespacespecific location to their default locations withoutaffecting other processes.If -all option was specified then cmd will be executedsynchronously on the each named network namespace even ifcmd fails on some of them. Network namespace name isprinted on each cmd executing.ip netns monitor - Report as network namespace names are addedand deletedThis command watches network namespace name addition anddeletion events and prints a line for each event it sees.ip netns list-id [target-nsid POSITIVE-INT] [nsid POSITIVE-INT] -list network namespace ids (nsid)Network namespace ids are used to identify a peer networknamespace. This command displays nsids of the currentnetwork namespace and provides the corresponding iproute2netns name (from /var/run/netns) if any.The target-nsid option enables to display nsids of thespecified network namespace instead of the current networknamespace. This target-nsid is a nsid from the currentnetwork namespace.The nsid option enables to display only this nsid. It is ansid from the current network namespace. In combinationwith the target-nsid option, it enables to convert aspecific nsid from the current network namespace to a nsidof the target-nsid network namespace.",
        "name": "ip-netns - process network namespace management",
        "section": 8
    },
    {
        "command": "ip-nexthop",
        "description": "ip nexthop is used to manipulate entries in the kernel's nexthoptables.ip nexthop add id IDadd new nexthop entryip nexthop replace id IDchange the configuration of a nexthop or add new onevia [ FAMILY ] ADDRESSthe address of the nexthop router, in the addressfamily FAMILY.Address family must match addressfamily of nexthop instance.dev NAMEis the output device.onlink pretend that the nexthop is directly attached tothis link, even if it does not match any interfaceprefix.encap ENCAPTYPE ENCAPHDRattach tunnel encapsulation attributes to thisroute.ENCAPTYPE is a string specifying the supportedencapsulation type. Namely:mpls - encapsulation type MPLSENCAPHDR is a set of encapsulation attributesspecific to the ENCAPTYPE.mplsMPLSLABEL - mpls label stack with labelsseparated by /ttl TTL - TTL to use for MPLS header or 0to inherit from IP headergroup GROUP [ type TYPE [ TYPE_ARGS ] ]create a nexthop group. Group specification is idwith an optional weight (id,weight) and a '/' as aseparator between entries.TYPE is a string specifying the nexthop group type.Namely:mpath - Multipath nexthop group backed bythe hash-threshold algorithm. The defaultwhen the type is unspecified.resilient - Resilient nexthop group. Groupis resilient to addition and deletion ofnexthops.TYPE_ARGS is a set of attributes specific to theTYPE.resilientbuckets BUCKETS - Number of nexthopbuckets. Cannot be changed for anexisting groupidle_timer IDLE - Time in seconds inwhich a nexthop bucket does not seetraffic and is therefore considered idle.Default is 120 secondsunbalanced_timer UNBALANCED - Time inseconds in which a nexthop group isunbalanced and is therefore consideredunbalanced. The kernel will try torebalance unbalanced groups, which mightresult in some flows being reset. A valueof 0 means that no rebalancing will takeplace. Default is 0 secondsblackholecreate a blackhole nexthopfdbnexthop and nexthop groups for use with layer-2 fdbentries.A fdb nexthop group can only have fdbnexthops.Example: Used to represent a vxlanremote vtep ip. layer-2 vxlan fdb entry pointing toan ecmp nexthop group containing multiple remotevtep ips.ip nexthop delete id IDdelete nexthop with given id.ip nexthop showshow the contents of the nexthop table or the nexthopsselected by some criteria.dev DEVshow the nexthops using the given device.vrf NAMEshow the nexthops using devices associated with thevrf namemaster DEVshow the nexthops using devices enslaved to givenmaster devicegroups show only nexthop groupsfdbshow only fdb nexthops and nexthop groupsip nexthop flushflushes nexthops selected by some criteria. Criteriaoptions are the same as show.ip nexthop get id IDget a single nexthop by idip nexthop bucket showshow the contents of the nexthop bucket table or thenexthop buckets selected by some criteria.id IDshow the nexthop buckets that belong to a nexthopgroup with a given idnhid IDshow the nexthop buckets that hold a nexthop with agiven iddev DEVshow the nexthop buckets using the given devicevrf NAMEshow the nexthop buckets using devices associatedwith the vrf namemaster DEVshow the nexthop buckets using devices enslaved togiven master deviceip nexthop bucket get id ID index INDEXget a single nexthop bucket by nexthop group id and bucketindex",
        "name": "ip-nexthop - nexthop object management",
        "section": 8
    },
    {
        "command": "ip-ntable",
        "description": "ip ntable controls the parameters for the neighbour tables.ip ntable show - list the ip neighbour tablesThis commands displays neighbour table parameters and statistics.dev DEVonly list the table attached to this device.name NAMEonly lists the table with the given name.ip ntable change - modify table parameterThis command allows modifying table parameters such as timers andqueue lengths.name NAMEthe name of the table to modify.dev DEVthe name of the device to modify the table values.",
        "name": "ip-ntable - neighbour table configuration",
        "section": 8
    },
    {
        "command": "ip-route",
        "description": "ip route is used to manipulate entries in the kernel routingtables.Route types:unicast - the route entry describes real paths to thedestinations covered by the route prefix.unreachable - these destinations are unreachable. Packetsare discarded and the ICMP message host unreachable isgenerated.The local senders get an EHOSTUNREACH error.blackhole - these destinations are unreachable. Packetsare discarded silently.The local senders get an EINVALerror.prohibit - these destinations are unreachable. Packetsare discarded and the ICMP message communicationadministratively prohibited is generated. The localsenders get an EACCES error.local - the destinations are assigned to this host. Thepackets are looped back and delivered locally.broadcast - the destinations are broadcast addresses. Thepackets are sent as link broadcasts.throw - a special control route used together with policyrules. If such a route is selected, lookup in this tableis terminated pretending that no route was found. Withoutpolicy routing it is equivalent to the absence of theroute in the routing table. The packets are dropped andthe ICMP message net unreachable is generated. The localsenders get an ENETUNREACH error.nat - a special NAT route. Destinations covered by theprefix are considered to be dummy (or external) addresseswhich require translation to real (or internal) onesbefore forwarding. The addresses to translate to areselected with the attribute via.Warning: Route NAT isno longer supported in Linux 2.6.anycast - not implemented the destinations are anycastaddresses assigned to this host. They are mainlyequivalent to local with one difference: such addressesare invalid when used as the source address of anypacket.multicast - a special type used for multicast routing. Itis not present in normal routing tables.Route tables: Linux-2.x can pack routes into several routingtables identified by a number in the range from 1 to 2^32-1 or byname from the file /etc/iproute2/rt_tables By default all normalroutes are inserted into the main table (ID 254) and the kernelonly uses this table when calculating routes.Values (0, 253,254, and 255) are reserved for built-in use.Actually, one other table always exists, which is invisible buteven more important. It is the local table (ID 255). This tableconsists of routes for local and broadcast addresses. The kernelmaintains this table automatically and the administrator usuallyneed not modify it or even look at it.The multiple routing tables enter the game when policy routing isused.ip route addadd new routeip route changechange routeip route replacechange or add new oneto TYPE PREFIX (default)the destination prefix of the route. If TYPE isomitted, ip assumes type unicast.Other values ofTYPE are listed above.PREFIX is an IP or IPv6address optionally followed by a slash and theprefix length. If the length of the prefix ismissing, ip assumes a full-length host route. Thereis also a special PREFIX default - which isequivalent to IP 0/0 or to IPv6 ::/0.tos TOSdsfield TOSthe Type Of Service (TOS) key. This key has noassociated mask and the longest match is understoodas: First, compare the TOS of the route and of thepacket. If they are not equal, then the packet maystill match a route with a zero TOS.TOS is eitheran 8 bit hexadecimal number or an identifier from/etc/iproute2/rt_dsfield.metric NUMBERpreference NUMBERthe preference value of the route.NUMBER is anarbitrary 32bit number, where routes with lowervalues are preferred.table TABLEIDthe table to add this route to.TABLEID may be anumber or a string from the file/etc/iproute2/rt_tables.If this parameter isomitted, ip assumes the main table, with theexception of local, broadcast and nat routes, whichare put into the local table by default.vrf NAMEthe vrf name to add this route to. Implicitly meansthe table associated with the VRF.dev NAMEthe output device name.via [ FAMILY ] ADDRESSthe address of the nexthop router, in the addressfamily FAMILY.Actually, the sense of this fielddepends on the route type.For normal unicastroutes it is either the true next hop router or, ifit is a direct route installed in BSD compatibilitymode, it can be a local address of the interface.For NAT routes it is the first address of the blockof translated IP destinations.src ADDRESSthe source address to prefer when sending to thedestinations covered by the route prefix.realm REALMIDthe realm to which this route is assigned.REALMIDmay be a number or a string from the file/etc/iproute2/rt_realms.mtu MTUmtu lock MTUthe MTU along the path to the destination. If themodifier lock is not used, the MTU may be updatedby the kernel due to Path MTU Discovery. If themodifier lock is used, no path MTU discovery willbe tried, all packets will be sent without the DFbit in IPv4 case or fragmented to MTU for IPv6.window NUMBERthe maximal window for TCP to advertise to thesedestinations, measured in bytes. It limits maximaldata bursts that our TCP peers are allowed to sendto us.rtt TIMEthe initial RTT ('Round Trip Time') estimate. If nosuffix is specified the units are raw values passeddirectly to the routing code to maintaincompatibility with previous releases.Otherwise ifa suffix of s, sec or secs is used to specifyseconds and ms, msec or msecs to specifymilliseconds.rttvar TIME (Linux 2.3.15+ only)the initial RTT variance estimate. Values arespecified as with rtt above.rto_min TIME (Linux 2.6.23+ only)the minimum TCP Retransmission TimeOut to use whencommunicating with this destination. Values arespecified as with rtt above.ssthresh NUMBER (Linux 2.3.15+ only)an estimate for the initial slow start threshold.cwnd NUMBER (Linux 2.3.15+ only)the clamp for congestion window. It is ignored ifthe lock flag is not used.initcwnd NUMBER (Linux 2.5.70+ only)the initial congestion window size for connectionsto this destination.Actual window size is thisvalue multiplied by the MSS (``Maximal SegmentSize'') for same connection. The default is zero,meaning to use the values specified in RFC2414.initrwnd NUMBER (Linux 2.6.33+ only)the initial receive window size for connections tothis destination.Actual window size is this valuemultiplied by the MSS of the connection.Thedefault value is zero, meaning to use Slow Startvalue.features FEATURES (Linux3.18+only)Enable or disable per-route features. Onlyavailable feature at this time is ecn to enableexplicit congestion notification when initiatingconnections to the given destination network.Whenresponding to a connection request from the givennetwork, ecn will also be used even if thenet.ipv4.tcp_ecn sysctl is set to 0.quickack BOOL (Linux 3.11+ only)Enable or disable quick ack for connections to thisdestination.fastopen_no_cookie BOOL (Linux 4.15+ only)Enable TCP Fastopen without a cookie forconnections to this destination.congctl NAME (Linux 3.20+ only)congctl lock NAME (Linux 3.20+ only)Sets a specific TCP congestion control algorithmonly for a given destination.If not specified,Linux keeps the current global default TCPcongestion control algorithm, or the one set fromthe application. If the modifier lock is not used,an application may nevertheless overwrite thesuggested congestion control algorithm for thatdestination. If the modifier lock is used, then anapplication is not allowed to overwrite thespecified congestion control algorithm for thatdestination, thus it will be enforced/guaranteed touse the proposed algorithm.advmss NUMBER (Linux 2.3.15+ only)the MSS ('Maximal Segment Size') to advertise tothese destinations when establishing TCPconnections. If it is not given, Linux uses adefault value calculated from the first hop deviceMTU.(If the path to these destination isasymmetric, this guess may be wrong.)reordering NUMBER (Linux 2.3.15+ only)Maximal reordering on the path to this destination.If it is not given, Linux uses the value selectedwith sysctl variable net/ipv4/tcp_reordering.nexthop NEXTHOPthe nexthop of a multipath route.NEXTHOP is acomplex value with its own syntax similar to thetop level argument lists:via [ FAMILY ] ADDRESS - is the nexthoprouter.dev NAME - is the output device.weight NUMBER - is a weight for thiselement of a multipath route reflecting itsrelative bandwidth or quality.The internal buffer used in iproute2 limits themaximum number of nexthops that may be specified inone go. If only ADDRESS is given, the currentbuffer size allows for 144 IPv6 nexthops and 253IPv4 ones. For IPv4, this effectively limits thenumber of nexthops possible per route. With IPv6,further nexthops may be appended to the same routevia ip route append command.scope SCOPE_VALthe scope of the destinations covered by the routeprefix.SCOPE_VAL may be a number or a string fromthe file /etc/iproute2/rt_scopes.If thisparameter is omitted, ip assumes scope global forall gatewayed unicast routes, scope link for directunicast and broadcast routes and scope host forlocal routes.protocol RTPROTOthe routing protocol identifier of this route.RTPROTO may be a number or a string from the file/etc/iproute2/rt_protos.If the routing protocolID is not given, ip assumes protocol boot (i.e. itassumes the route was added by someone who doesn'tunderstand what they are doing). Several protocolvalues have a fixed interpretation.Namely:redirect - the route was installed due toan ICMP redirect.kernel - the route was installed by thekernel during autoconfiguration.boot - the route was installed during thebootup sequence.If a routing daemonstarts, it will purge all of them.static - the route was installed by theadministrator to override dynamic routing.Routing daemon will respect them and,probably, even advertise them to its peers.ra - the route was installed by RouterDiscovery protocol.The rest of the values are not reserved and theadministrator is free to assign (or not to assign)protocol tags.onlink pretend that the nexthop is directly attached tothis link, even if it does not match any interfaceprefix.pref PREFthe IPv6 route preference.PREF is a stringspecifying the route preference as defined inRFC4191 for Router Discovery messages. Namely:low - the route has a lowest prioritymedium - the route has a default priorityhigh - the route has a highest prioritynhid IDuse nexthop object with given id as nexthopspecification.encap ENCAPTYPE ENCAPHDRattach tunnel encapsulation attributes to thisroute.ENCAPTYPE is a string specifying the supportedencapsulation type. Namely:mpls - encapsulation type MPLSip - IP encapsulation (Geneve, GRE, VXLAN,...)bpf - Execution of BPF programseg6 - encapsulation type IPv6 SegmentRoutingseg6local - local SRv6 segment processingioam6 - encapsulation type IPv6 IOAMxfrm - encapsulation type XFRMENCAPHDR is a set of encapsulation attributesspecific to the ENCAPTYPE.mplsMPLSLABEL - mpls label stack with labelsseparated by /ttl TTL - TTL to use for MPLS header or 0to inherit from IP headeripid TUNNEL_ID dst REMOTE_IP [ src SRC ] [tos TOS ] [ ttl TTL ] [ key ] [ csum ] [seq ]bpfin PROG - BPF program to execute forincoming packetsout PROG - BPF program to execute foroutgoing packetsxmit PROG - BPF program to execute fortransmitted packetsheadroom SIZE - Size of header BPFprogram will attach (xmit)seg6mode inline - Directly insert SegmentRouting Header after IPv6 headermode encap - Encapsulate packet in anouter IPv6 header with SRHmode encap.red - Encapsulate packet in anouter IPv6 header with SRH applying thereduced segment list. When there is onlyone segment and the HMAC is not present,the SRH is omitted.mode l2encap - Encapsulate ingress L2frame within an outer IPv6 header and SRHmode l2encap.red - Encapsulate ingress L2frame within an outer IPv6 header and SRHapplying the reduced segment list. Whenthere is only one segment and the HMAC isnot present, the SRH is omitted.SEGMENTS - List of comma-separated IPv6addressesKEYID - Numerical value in decimalrepresentation. See ip-sr(8).seg6localSEG6_ACTION [ SEG6_ACTION_PARAM ] [ count] - Operation to perform on matchingpackets. The optional count attribute isused to collect statistics on theprocessing of actions.Three countersare implemented: 1) packets correctlyprocessed; 2) bytes correctly processed;3) packets that cause a processing error(i.e., missing SID List, wrong SID List,etc). To retrieve the counters related toan action use the -s flag in the showcommand.The following actions arecurrently supported (Linux 4.14+ only).End [ flavors FLAVORS ] - Regular SRv6processing as intermediate segmentendpoint.This action only acceptspackets with a non-zero Segments Leftvalue. Other matching packets aredropped. The presence of flavors canchange the regular processing of an Endbehavior according to the user-providedFlavor operations and informationcarried in the packet.See Flavorsparameters section.End.X nh6 NEXTHOP - Regular SRv6processing as intermediate segmentendpoint.Additionally, forwardprocessed packets to given next-hop.This action only accepts packets with anon-zero Segments Left value. Othermatching packets are dropped.End.DX6 nh6 NEXTHOP - Decapsulate innerIPv6 packet and forward it to thespecified next-hop. If the argument isset to ::, then the next-hop isselected according to the localselection rules. This action onlyaccepts packets with either a zeroSegments Left value or no SRH at all,and an inner IPv6 packet. Othermatching packets are dropped.End.DT6 { table | vrftable } TABLEID -Decapsulate the inner IPv6 packet andforward it according to the specifiedlookup table.TABLEID is either anumber or a string from the file/etc/iproute2/rt_tables.If vrftableis used, the argument must be a VRFdevice associated with the table id.Moreover, the VRF table associated withthe table id must be configured withthe VRF strict mode turned on(net.vrf.strict_mode=1). This actiononly accepts packets with either a zeroSegments Left value or no SRH at all,and an inner IPv6 packet. Othermatching packets are dropped.End.DT4 vrftable TABLEID - Decapsulatethe inner IPv4 packet and forward itaccording to the specified lookuptable.TABLEID is either a number or astring from the file/etc/iproute2/rt_tables.The argumentmust be a VRF device associated withthe table id.Moreover, the VRF tableassociated with the table id must beconfigured with the VRF strict modeturned on (net.vrf.strict_mode=1). Thisaction only accepts packets with eithera zero Segments Left value or no SRH atall, and an inner IPv4 packet. Othermatching packets are dropped.End.DT46 vrftable TABLEID - Decapsulatethe inner IPv4 or IPv6 packet andforward it according to the specifiedlookup table.TABLEID is either anumber or a string from the file/etc/iproute2/rt_tables.The argumentmust be a VRF device associated withthe table id.Moreover, the VRF tableassociated with the table id must beconfigured with the VRF strict modeturned on (net.vrf.strict_mode=1). Thisaction only accepts packets with eithera zero Segments Left value or no SRH atall, and an inner IPv4 or IPv6 packet.Other matching packets are dropped.End.B6 srh segs SEGMENTS [ hmac KEYID ]- Insert the specified SRH immediatelyafter the IPv6 header, update the DAwith the first segment of the newlyinserted SRH, then forward theresulting packet. The original SRH isnot modified. This action only acceptspackets with a non-zero Segments Leftvalue. Other matching packets aredropped.End.B6.Encaps srh segs SEGMENTS [ hmacKEYID ] - Regular SRv6 processing asintermediate segment endpoint.Additionally, encapsulate the matchingpacket within an outer IPv6 headerfollowed by the specified SRH. Thedestination address of the outer IPv6header is set to the first segment ofthe new SRH. The source address is setas described in ip-sr(8).Flavors parametersThe flavors represent additionaloperations that can modify or extend asubset of the existing behaviors.flavors OPERATION[,OPERATION][ATTRIBUTES]OPERATION := { psp | usp | usd |next-csid }ATTRIBUTES := { KEY VALUE } [ATTRIBUTES ]KEY := { lblen | nflen }psp - The Penultimate Segment Pop(PSP) copies the last SID from theSID List (carried by the outermostSRH) into the IPv6 DestinationAddress (DA) and removes (i.e. pops)the SRH from the IPv6 header.ThePSP operation takes place only at apenultimate SR Segment Endpoint node(e.g., the Segment Left must be one)and does not happen at non-penultimate endpoint nodes.usp - Ultimate Segment Pop of the SRH(not yet supported in kernel)usd - Ultimate Segment Decapsulation(not yet supported in kernel)next-csid - The NEXT-C-SID mechanismoffers the possibility of encodingseveral SRv6 segments within a single128 bit SID address. The NEXT-C-SIDflavor can be configured to supportuser-provided Locator-Block andLocator-Node Function lengths. IfLocator-Block and/or Locator-NodeFunction lengths are not provided bythe user during configuration of anSRv6 End behavior instance with NEXT-C-SID flavor, the default value is32-bit for Locator-Block and 16-bitfor Locator-Node Function.lblen VALUE - defines the Locator-Block length for NEXT-C-SID flavor.The Locator-Block length must begreater than 0 and evenly divisibleby 8. This attribute can be used onlywith NEXT-C-SID flavor.nflen VALUE - defines the Locator-Node Function length for NEXT-C-SIDflavors. The Locator-Node Functionlength must be greater than 0 andevenly divisible by 8. This attributecan be used only with NEXT-C-SIDflavor.ioam6freq K/N - Inject IOAM in K packetsevery N packets (default is 1/1).mode inline - Directly insert IOAMafter IPv6 header (default mode).mode encap - Encapsulate packet in anouter IPv6 header with IOAM.mode auto - Automatically use inlinemode for local packets and encap modefor in-transit packets.tundst ADDRESS - IPv6 address of thetunnel destination (outer header), notused with inline mode.type IOAM6_TRACE_TYPE - List of IOAMdata required in the trace, representedby a bitfield (24 bits).ns IOAM6_NAMESPACE - Numerical value torepresent an IOAM namespace. Seeip-ioam(8).size IOAM6_TRACE_SIZE - Size, inoctets, of the pre-allocated trace datablock.xfrmif_id IF_ID[ link_dev LINK_DEV ]expires TIME (Linux 4.4+ only)the route will be deleted after the expires time.Only support IPv6 at present.ttl-propagate { enabled | disabled }Control whether TTL should be propagated from anyencap into the un-encapsulated packet, overridingany global configuration. Only supported for MPLSat present.ip route deletedelete routeip route del has the same arguments as ip route add, buttheir semantics are a bit different.Key values (to, tos, preference and table) select theroute to delete. If optional attributes are present, ipverifies that they coincide with the attributes of theroute to delete.If no route with the given key andattributes was found, ip route del fails.ip route showlist routesthe command displays the contents of the routing tables orthe route(s) selected by some criteria.to SELECTOR (default)only select routes from the given range ofdestinations.SELECTOR consists of an optionalmodifier (root, match or exact) and a prefix.rootPREFIX selects routes with prefixes not shorterthan PREFIX.F.e.root 0/0 selects the entirerouting table.match PREFIX selects routes withprefixes not longer than PREFIX.F.e.match10.0/16 selects 10.0/16, 10/8 and 0/0, but it doesnot select 10.1/16 and 10.0.0/24.And exact PREFIX(or just PREFIX) selects routes with this exactprefix. If neither of these options are present, ipassumes root 0/0 i.e. it lists the entire table.tos TOSdsfield TOSonly select routes with the given TOS.table TABLEIDshow the routes from this table(s). The defaultsetting is to show table main.TABLEID may eitherbe the ID of a real table or one of the specialvalues:all - list all of the tables.cache - dump the routing cache.vrf NAMEshow the routes for the table associated with thevrf nameclonedcached list cloned routes i.e. routes which weredynamically forked from other routes because someroute attribute (f.e. MTU) was updated.Actually,it is equivalent to table cache.from SELECTORthe same syntax as for to, but it binds the sourceaddress range rather than destinations.Note thatthe from option only works with cloned routes.protocol RTPROTOonly list routes of this protocol.scope SCOPE_VALonly list routes with this scope.type TYPEonly list routes of this type.dev NAMEonly list routes going via this device.via [ FAMILY ] PREFIXonly list routes going via the nexthop routersselected by PREFIX.src PREFIXonly list routes with preferred source addressesselected by PREFIX.realm REALMIDrealms FROMREALM/TOREALMonly list routes with these realms.ip route flushflush routing tablesthis command flushes routes selected by some criteria.The arguments have the same syntax and semantics as thearguments of ip route show, but routing tables are notlisted but purged. The only difference is the defaultaction: show dumps all the IP main routing table but flushprints the helper page.With the -statistics option, the command becomes verbose.It prints out the number of deleted routes and the numberof rounds made to flush the routing table. If the optionis given twice, ip route flush also dumps all the deletedroutes in the format described in the previous subsection.ip route getget a single routethis command gets a single route to a destination andprints its contents exactly as the kernel sees it.fibmatchReturn full fib lookup matched route. Default is toreturn the resolved dst entryto ADDRESS (default)the destination address.from ADDRESSthe source address.tos TOSdsfield TOSthe Type Of Service.iif NAMEthe device from which this packet is expected toarrive.oif NAMEforce the output device on which this packet willbe routed.mark MARKthe firewall mark (fwmark)vrf NAMEforce the vrf device on which this packet will berouted.ipproto PROTOCOLip protocol as seen by the route lookupsport NUMBERsource port as seen by the route lookupdport NUMBERdestination port as seen by the route lookupconnectedif no source address (option from) was given,relookup the route with the source set to thepreferred address received from the first lookup.If policy routing is used, it may be a differentroute.Note that this operation is not equivalent to ip routeshow.show shows existing routes.get resolves them andcreates new clones if necessary. Essentially, get isequivalent to sending a packet along this path.If theiif argument is not given, the kernel creates a route tooutput packets towards the requested destination.This isequivalent to pinging the destination with a subsequent iproute ls cache, however, no packets are actually sent.With the iif argument, the kernel pretends that a packetarrived from this interface and searches for a path toforward the packet.ip route savesave routing table information to stdoutThis command behaves like ip route show except that theoutput is raw data suitable for passing to ip routerestore.ip route restorerestore routing table information from stdinThis command expects to read a data stream as returnedfrom ip route save.It will attempt to restore therouting table information exactly as it was at the time ofthe save, so any translation of information in the stream(such as device indexes) must be done first. Any existingroutes are left unchanged. Any routes specified in thedata stream that already exist in the table will beignored.",
        "name": "ip-route - routing table management",
        "section": 8
    },
    {
        "command": "ip-rule",
        "description": "ip rule manipulates rules in the routing policy database thatcontrols the route selection algorithm.Classic routing algorithms used in the Internet make routingdecisions based only on the destination address of packets (andin theory, but not in practice, on the TOS field).In some circumstances, we want to route packets differentlydepending not only on destination addresses but also on otherpacket fields: source address, IP protocol, transport protocolports or even packet payload.This task is called 'policyrouting'.To solve this task, the conventional destination based routingtable, ordered according to the longest match rule, is replacedwith a 'routing policy database' (or RPDB), which selects routesby executing some set of rules.Each policy routing rule consists of a selector and an actionpredicate.The RPDB is scanned in order of decreasing priority(note that a lower number means higher priority, see thedescription of PREFERENCE below). The selector of each rule isapplied to {source address, destination address, incominginterface, tos, fwmark} and, if the selector matches the packet,the action is performed. The action predicate may return withsuccess.In this case, it will either give a route or failureindication and the RPDB lookup is terminated. Otherwise, the RPDBprogram continues with the next rule.Semantically, the natural action is to select the nexthop and theoutput device.At startup time the kernel configures the default RPDB consistingof three rules:1.Priority: 0, Selector: match anything, Action: lookuprouting table local (ID 255).The local table is aspecial routing table containing high priority controlroutes for local and broadcast addresses.2.Priority: 32766, Selector: match anything, Action: lookuprouting table main (ID 254).The main table is the normalrouting table containing all non-policy routes. This rulemay be deleted and/or overridden with other ones by theadministrator.3.Priority: 32767, Selector: match anything, Action: lookuprouting table default (ID 253).The default table isempty. It is reserved for some post-processing if noprevious default rules selected the packet.This rule mayalso be deleted.Each RPDB entry has additional attributes. F.e. each rule has apointer to some routing table. NAT and masquerading rules have anattribute to select new IP address to translate/masquerade.Besides that, rules have some optional attributes, which routeshave, namely realms.These values do not override thosecontained in the routing tables. They are only used if the routedid not select any attributes.The RPDB may contain rules of the following types:unicast - the rule returns the route found in the routingtable referenced by the rule.blackhole - the rule causes a silent drop the packet.unreachable - the rule generates a 'Network isunreachable' error.prohibit - the rule generates 'Communication isadministratively prohibited' error.nat - the rule translates the source address of the IPpacket into some other value.ip rule add - insert a new ruleip rule delete - delete a ruletype TYPE (default)the type of this rule. The list of valid types wasgiven in the previous subsection.from PREFIXselect the source prefix to match.to PREFIXselect the destination prefix to match.iif NAMEselect the incoming device to match. If theinterface is loopback, the rule only matchespackets originating from this host. This means thatyou may create separate routing tables forforwarded and local packets and, hence, completelysegregate them.oif NAMEselect the outgoing device to match. The outgoinginterface is only available for packets originatingfrom local sockets that are bound to a device.tos TOSdsfield TOSselect the TOS value to match.fwmark MARKselect the fwmark value to match.uidrange NUMBER-NUMBERselect the uid value to match.ipproto PROTOCOLselect the ip protocol value to match.sport NUMBER | NUMBER-NUMBERselect the source port value to match. supportsport range.dport NUMBER | NUMBER-NUMBERselect the destination port value to match.supports port range.priority PREFERENCEthe priority of this rule.PREFERENCE is anunsigned integer value, higher number means lowerpriority, and rules get processed in order ofincreasing number. Each rule should have anexplicitly set unique priority value.The optionspreference and order are synonyms with priority.table TABLEIDthe routing table identifier to lookup if the ruleselector matches.It is also possible to uselookup instead of table.protocol PROTOthe routing protocol who installed the rule inquestion.As an example when zebra installs a ruleit would get RTPROT_ZEBRA as the installingprotocol.suppress_prefixlength NUMBERreject routing decisions that have a prefix lengthof NUMBER or less.suppress_ifgroup GROUPreject routing decisions that use a devicebelonging to the interface group GROUP.realms FROM/TORealms to select if the rule matched and therouting table lookup succeeded. Realm TO is onlyused if the route did not select any realm.nat ADDRESSThe base of the IP address block to translate (forsource addresses).The ADDRESS may be either thestart of the block of NAT addresses (selected byNAT routes) or a local host address (or even zero).In the last case the router does not translate thepackets, but masquerades them to this address.Using map-to instead of nat means the same thing.Warning: Changes to the RPDB made with thesecommands do not become active immediately. It isassumed that after a script finishes a batch ofupdates, it flushes the routing cache with ip routeflush cache.ip rule flush - also dumps all the deleted rules.protocol PROTOSelect the originating protocol.ip rule show - list rulesThis command has no arguments.The options list or lstare synonyms with show.ip rule saveprotocol PROTOSelect the originating protocol.save rules table information to stdoutThis command behaves like ip rule show except that theoutput is raw data suitable for passing to ip rulerestore.ip rule restorerestore rules table information from stdinThis command expects to read a data stream as returnedfrom ip rule save.It will attempt to restore the rulestable information exactly as it was at the time of thesave. Any rules already in the table are left unchanged,and duplicates are not ignored.",
        "name": "ip-rule - routing policy database management",
        "section": 8
    },
    {
        "command": "ip-sr",
        "description": "The ip sr command is used to configure IPv6 Segment Routing(SRv6) internal parameters.Those parameters include the mapping between an HMAC key ID andits associated hashing algorithm and secret, and the IPv6 addressto use as source for encapsulated packets.The ip sr hmac set command prompts for a passphrase that will beused as the HMAC secret for the corresponding key ID. A blankpassphrase removes the mapping.The currently supportedalgorithms for ALGO are sha1 and sha256.If the tunnel source is set to the address :: (which is thedefault), then an address of the egress interface will beselected. As this operation may hinder performances, it isrecommended to set a non-default address.",
        "name": "ip-sr - IPv6 Segment Routing management",
        "section": 8
    },
    {
        "command": "ip-stats",
        "description": "ip stats setis used for toggling whether a certain HW statistics suiteis collected on a given netdevice. The followingstatistics suites are supported:l3_stats L3 stats reflect traffic that takes place in a HWdevice on an object that corresponds to the givensoftware netdevice.ip stats showis used for showing stats on a given netdevice, or dumpingstatistics across all netdevices. By default, all statsare requested. It is possible to filter which stats arerequested by using the group and subgroup keywords.It is possible to specify several groups, or severalsubgroups for one group. When no subgroups are given for agroup, all the subgroups are requested.The following groups are recognized:group link - Link statistics. The same suite that \"ip -slink show\" shows.group offload - A group that contains a number of HW-oriented statistics. See below for individualsubgroups within this group.group xstats - Extended statistics. A subgroup identifiesthe type of netdevice to show the statistics for.group xstats_slave - Extended statistics for the slave ofa netdevice of a given type. A subgroup identifiesthe type of master netdevice.group afstats - A group for address-family specificnetdevice statistics.group offload subgroups:subgroup cpu_hit - The cpu_hit statistics suite is usefulon hardware netdevices. The link statistics onthese devices reflect both the hardware- andsoftware-datapath traffic. The cpu_hit statisticsthen only reflect software-datapath traffic.subgroup hw_stats_info - This suite does not includetraffic statistics, but rather communicates thestate of other statistics. Through this subgroup,it is possible to discover whether a givenstatistic was enabled, and when it was, whether anydevice driver actually configured its device tocollect these statistics. For example, l3_stats wasenabled in the following case, but no driver hasinstalled it:# ip stats show dev swp1 group offload subgrouphw_stats_info56: swp1: group offload subgroup hw_stats_infol3_stats on used offAfter an L3 address is added to the netdevice, thecounter will be installed:# ip addr add dev swp1 192.0.2.1/28# ip stats show dev swp1 group offload subgrouphw_stats_info56: swp1: group offload subgroup hw_stats_infol3_stats on used onsubgroup l3_stats - These statistics reflect L3 trafficthat takes place in HW on an object thatcorresponds to the netdevice. Note that this suiteis disabled by default and needs to be firstenabled through ip stats set.For example:# ip stats show dev swp2.200 group offload subgroupl3_stats112: swp2.200: group offload subgroup l3_stats onused onRX:bytes packets errors droppedmcast890072203TX:bytes packets errors dropped71765800Note how the l3_stats_info for the selected groupis also part of the dump.group xstats and group xstats_slave subgroups:subgroup bridge [ suite stp ] [ suite mcast ] - Statisticsfor STP and, respectively, IGMP / MLD (under thekeyword mcast) traffic on bridges and their slaves.subgroup bond [ suite 802.3ad ] - Statistics for LACPtraffic on bond devices and their slaves.group afstats subgroups:subgroup mpls - Statistics for MPLS traffic seen on thenetdevice. For example:# ip stats show dev veth01 group afstats subgroupmpls3: veth01: group afstats subgroup mplsRX: bytes packets errors dropped noroute00000TX: bytes packets errors dropped216200",
        "name": "ip-stats - manage and show interface statistics",
        "section": 8
    },
    {
        "command": "ip-tcp_metrics",
        "description": "ip tcp_metrics is used to manipulate entries in the kernel thatkeep TCP information for IPv4 and IPv6 destinations. The entriesare created when TCP sockets want to share information fordestinations and are stored in a cache keyed by the destinationaddress. The saved information may include values for metrics(initially obtained from routes), recent TSVAL for TIME-WAITrecycling purposes, state for the Fast Open feature, etc.Forperformance reasons the cache can not grow above configured limitand the older entries are replaced with fresh information,sometimes reclaimed and used for new destinations. The kernelnever removes entries, they can be flushed only with this tool.ip tcp_metrics show - show cached entriesaddress PREFIX (default)IPv4/IPv6 prefix or address. If no prefix is provided allentries are shown.The output may contain the following information:age <S.MMM>sec - time after the entry was created, reset orupdated with metrics from sockets. The entry is reset andrefreshed on use with metrics from route if the metrics are notupdated in last hour. Not all cached values reset the age onupdate.cwnd <N> - CWND metric valuefo_cookie <HEX-STRING> - Cookie value received in SYN-ACK to beused by Fast Open for next SYNsfo_mss <N> - MSS value received in SYN-ACK to be used by FastOpen for next SYNsfo_syn_drops <N>/<S.MMM>sec ago - Number of drops of initialoutgoing Fast Open SYNs with data detected by monitoring thereceived SYN-ACK after SYN retransmission.The seconds show thetime after last SYN drop and together with the drop count can beused to disable Fast Open for some time.reordering <N> - Reordering metric valuertt <N>us - RTT metric valuerttvar <N>us - RTTVAR metric valuessthresh <SSTHRESH> - SSTHRESH metric valuetw_ts <TSVAL>/<SEC>sec ago - recent TSVAL and the seconds aftersaving it into TIME-WAIT socketip tcp_metrics delete - delete single entryaddress ADDRESS (default)IPv4/IPv6 address. The address is a required argument.ip tcp_metrics flush - flush entriesThis command flushes the entries selected by some criteria.This command has the same arguments as show.",
        "name": "ip-tcp_metrics - management for TCP Metrics",
        "section": 8
    },
    {
        "command": "ip-token",
        "description": "IPv6 tokenized interface identifier support is used for assigningwell-known host-part addresses to nodes whilst still obtaining aglobal network prefix from Router advertisements. The primarytarget for tokenized identifiers are server platforms whereaddresses are usually manually configured, rather than usingDHCPv6 or SLAAC. By using tokenized identifiers, hosts can stilldetermine their network prefix by use of SLAAC, but more readilybe automatically renumbered should their network prefix change[1]. Tokenized IPv6 Identifiers are described in the draft [1]:<draft-chown-6man-tokenised-ipv6-identifiers-02>.ip token set - set an interface tokenset the interface token to the kernel.TOKENthe interface identifier token address.dev DEVthe networking interface.ip token del - delete an interface tokendelete the interface token from the kernel.dev DEVthe networking interface.ip token get - get the interface token from the kernelshow a tokenized interface identifier of a particular networkingdevice.Arguments: coincide with the arguments of ip token setbut the TOKEN must be left out.ip token list - list all interface tokenslist all tokenized interface identifiers for the networkinginterfaces from the kernel.",
        "name": "ip-token - tokenized interface identifier support",
        "section": 8
    },
    {
        "command": "ip-tunnel",
        "description": "tunnel objects are tunnels, encapsulating packets in IP packetsand then sending them over the IP infrastructure.Theencapsulating (or outer) address family is specified by the -foption. The default is IPv4.ip tunnel addadd a new tunnelip tunnel changechange an existing tunnelip tunnel deletedestroy a tunnelname NAME (default)select the tunnel device name.mode MODEset the tunnel mode. Available modes depend on theencapsulating address family.Modes for IPv4 encapsulation available: ipip, sit,isatap, vti, and gre.Modes for IPv6 encapsulation available: ip6ip6,ipip6, ip6gre, vti6, and any.remote ADDRESSset the remote endpoint of the tunnel.local ADDRESSset the fixed local address for tunneled packets.It must be an address on another interface of thishost.ttl Nhoplimit Nset a fixed TTL (IPv4) or hoplimit (IPv6) N ontunneled packets.N is a number in the range1--255. 0 is a special value meaning that packetsinherit the TTL value.The default value for IPv4tunnels is: inherit.The default value for IPv6tunnels is: 64.tos Tdsfield Ttclass Tset the type of service (IPv4) or traffic class(IPv6) field on tunneled packets, which can bespecified as either a two-digit hex value (e.g. c0)or a predefined string (e.g. internet).The valueinherit causes the field to be copied from theoriginal IP header. The values inherit/STRING orinherit/00..ff will set the field to STRING or00..ff when tunneling non-IP packets. The defaultvalue is 00.dev NAMEbind the tunnel to the device NAME so that tunneledpackets will only be routed via this device andwill not be able to escape to another device whenthe route to endpoint changes.nopmtudiscdisable Path MTU Discovery on this tunnel.It isenabled by default. Note that a fixed ttl isincompatible with this option: tunneling with afixed ttl always makes pmtu discovery.ignore-dfenable IPv4 DF suppression on this tunnel.Normally datagrams that exceed the MTU will befragmented; the presence of the DF flag inhibitsthis, resulting instead in an ICMP Unreachable(Fragmentation Required) message.Enabling thisattribute causes the DF flag to be ignored.key Kikey Kokey K ( only GRE tunnels ) use keyed GRE with key K. K iseither a number or an IP address-like dotted quad.The key parameter sets the key to use in bothdirections.The ikey and okey parameters setdifferent keys for input and output.csum, icsum, ocsum( only GRE tunnels ) generate/require checksums fortunneled packets.The ocsum flag calculateschecksums for outgoing packets.The icsum flagrequires that all input packets have the correctchecksum. The csum flag is equivalent to thecombination icsum ocsum.seq, iseq, oseq( only GRE tunnels ) serialize packets.The oseqflag enables sequencing of outgoing packets.Theiseq flag requires that all input packets areserialized.The seq flag is equivalent to thecombination iseq oseq.It doesn't work. Don't useit.encaplimit ELIM( only IPv6 tunnels ) set a fixed encapsulationlimit. Default is 4.flowlabel FLOWLABEL( only IPv6 tunnels ) set a fixed flowlabel.allow-localremote( only IPv6 tunnels ) allow remote endpoint on thelocal host.ip tunnel prlpotential router list (ISATAP only)dev NAMEmandatory device name.prl-default ADDRprl-nodefault ADDRprl-delete ADDRAdd or delete ADDR as a potential router or defaultrouter.ip tunnel showlist tunnels This command has no arguments.",
        "name": "ip-tunnel - tunnel configuration",
        "section": 8
    },
    {
        "command": "ip-vrf",
        "description": "A VRF provides traffic isolation at layer 3 for routing, similarto how a VLAN is used to isolate traffic at layer 2.Fundamentally, a VRF is a separate routing table. Network devicesare associated with a VRF by enslaving the device to the VRF. Atthat point network addresses assigned to the device are local tothe VRF with host and connected routes moved to the tableassociated with the VRF.A process can specify a VRF using several APIs -- binding thesocket to the VRF device using SO_BINDTODEVICE, setting the VRFassociation using IP_UNICAST_IF or IPV6_UNICAST_IF, or specifyingthe VRF for a specific message using IP_PKTINFO or IPV6_PKTINFO.By default a process is not bound to any VRF. An association canbe set explicitly by making the program use one of the APIsmentioned above or implicitly using a helper to setSO_BINDTODEVICE for all IPv4 and IPv6 sockets (AF_INET andAF_INET6) when the socket is created. This ip-vrf command is ahelper to run a command against a specific VRF with the VRFassociation inherited parent to child.ip vrf show [ NAME ] - Show all configured VRFThis command lists all VRF and their corresponding tableids. If NAME is given, then only that VRF and table id isshown. The latter command is useful for scripting wherethe table id for a VRF is needed.ip vrf exec [ NAME ] cmd ... - Run cmd against the named VRFThis command allows applications that are VRF unaware tobe run against a VRF other than the default VRF (maintable). A command can be run against the default VRF bypassing the \"default\" as the VRF name. This is useful ifthe current shell is associated with another VRF (e.g,Management VRF).This command requires the system to be booted with cgroupv2 (e.g. with systemd, addsystemd.unified_cgroup_hierarchy=1 to the kernel commandline).This command also requires to be ran as root or with theCAP_SYS_ADMIN, CAP_NET_ADMIN and CAP_DAC_OVERRIDEcapabilities. If built with libcap and if capabilities areadded to the ip binary program via setcap, the programwill drop them as the first thing when invoked, unless thecommand is vrf exec.NOTE: capabilities will NOT be dropped if CAP_NET_ADMIN isset to INHERITABLE to avoid breaking programs with ambientcapabilities that call ip.Do not set the INHERITABLEflag on the ip binary itself.ip vrf identify [PID] - Report VRF association for processThis command shows the VRF association of the specifiedprocess. If PID is not specified then the id of thecurrent process is used.ip vrf pids NAME - Report processes associated with the named VRFThis command shows all process ids that are associatedwith the given VRF.",
        "name": "ip-vrf - run a command against a vrf",
        "section": 8
    },
    {
        "command": "ip-xfrm",
        "description": "xfrm is an IP framework for transforming packets (such asencrypting their payloads). This framework is used to implementthe IPsec protocol suite (with the state object operating on theSecurity Association Database, and the policy object operating onthe Security Policy Database). It is also used for the IP PayloadCompression Protocol and features of Mobile IPv6.ip xfrm state addadd new state into xfrmip xfrm state updateupdate existing state in xfrmip xfrm state allocspiallocate an SPI valueip xfrm state deletedelete existing state in xfrmip xfrm state getget existing state in xfrmip xfrm state deletealldelete all existing state in xfrmip xfrm state listprint out the list of existing state in xfrmip xfrm state flushflush all state in xfrmip xfrm state countcount all existing state in xfrmIDis specified by a source address, destination address,transform protocol XFRM-PROTO, and/or Security ParameterIndex SPI.(For IP Payload Compression, the CompressionParameter Index or CPI is used for SPI.)XFRM-PROTOspecifies a transform protocol: IPsec EncapsulatingSecurity Payload (esp), IPsec Authentication Header (ah),IP Payload Compression (comp), Mobile IPv6 Type 2 RoutingHeader (route2), or Mobile IPv6 Home Address Option (hao).ALGO-LISTcontains one or more algorithms to use. Each algorithmALGO is specified by:\u2022the algorithm type: encryption (enc),authentication (auth or auth-trunc), authenticatedencryption with associated data (aead), orcompression (comp)\u2022the algorithm name ALGO-NAME (see below)\u2022(for all except comp) the keying material ALGO-KEYMAT, which may include both a key and a salt ornonce value; refer to the corresponding RFC\u2022(for auth-trunc only) the truncation length ALGO-TRUNC-LEN in bits\u2022(for aead only) the Integrity Check Value lengthALGO-ICV-LEN in bitsEncryption algorithms include ecb(cipher_null), cbc(des),cbc(des3_ede), cbc(cast5), cbc(blowfish), cbc(aes),cbc(serpent), cbc(camellia), cbc(twofish), andrfc3686(ctr(aes)).Authentication algorithms include digest_null, hmac(md5),hmac(sha1), hmac(sha256), hmac(sha384), hmac(sha512),hmac(rmd160), and xcbc(aes).Authenticated encryption with associated data (AEAD)algorithms include rfc4106(gcm(aes)), rfc4309(ccm(aes)),and rfc4543(gcm(aes)).Compression algorithms include deflate, lzs, and lzjh.MODEspecifies a mode of operation for the transform protocol.IPsec and IP Payload Compression modes are transport,tunnel, and (for IPsec ESP only) Bound End-to-End Tunnel(beet).Mobile IPv6 modes are route optimization (ro) andinbound trigger (in_trigger).FLAG-LISTcontains one or more of the following optional flags:noecn, decap-dscp, nopmtudisc, wildrecv, icmp, af-unspec,align4, or esn.SELECTORselects the traffic that will be controlled by the policy,based on the source address, the destination address, thenetwork device, and/or UPSPEC.UPSPEC selects traffic by protocol. For the tcp, udp, sctp, ordccp protocols, the source and destination port canoptionally be specified.For the icmp, ipv6-icmp, ormobility-header protocols, the type and code numbers canoptionally be specified.For the gre protocol, the keycan optionally be specified as a dotted-quad or number.Other protocols can be selected by name or number PROTO.LIMIT-LISTsets limits in seconds, bytes, or numbers of packets.ENCAPencapsulates packets with protocol espinudp, espinudp-nonike, or espintcp, using source port SPORT, destinationport DPORT , and original address OADDR.MARKused to match xfrm policies and statesOUTPUT-MARKused to set the output mark to influence the routing ofthe packets emitted by the stateIF-IDxfrm interface identifier used to in both xfrm policiesand statesDEVNetwork interface name used to offload policies and statesip xfrm policy addadd a new policyip xfrm policy updateupdate an existing policyip xfrm policy deletedelete an existing policyip xfrm policy getget an existing policyip xfrm policy deletealldelete all existing xfrm policiesip xfrm policy listprint out the list of xfrm policiesip xfrm policy flushflush policiesnosock filter (remove) all socket policies from the output.SELECTORselects the traffic that will be controlled by the policy,based on the source address, the destination address, thenetwork device, and/or UPSPEC.UPSPEC selects traffic by protocol. For the tcp, udp, sctp, ordccp protocols, the source and destination port canoptionally be specified.For the icmp, ipv6-icmp, ormobility-header protocols, the type and code numbers canoptionally be specified.For the gre protocol, the keycan optionally be specified as a dotted-quad or number.Other protocols can be selected by name or number PROTO.DIRselects the policy direction as in, out, or fwd.CTXsets the security context.PTYPEcan be main (default) or sub.ACTION can be allow (default) or block.PRIORITYis a number that defaults to zero.FLAG-LISTcontains one or both of the following optional flags:local or icmp.LIMIT-LISTsets limits in seconds, bytes, or numbers of packets.TMPL-LISTis a template list specified using ID, MODE, REQID, and/orLEVEL.IDis specified by a source address, destination address,transform protocol XFRM-PROTO, and/or Security ParameterIndex SPI.(For IP Payload Compression, the CompressionParameter Index or CPI is used for SPI.)XFRM-PROTOspecifies a transform protocol: IPsec EncapsulatingSecurity Payload (esp), IPsec Authentication Header (ah),IP Payload Compression (comp), Mobile IPv6 Type 2 RoutingHeader (route2), or Mobile IPv6 Home Address Option (hao).MODEspecifies a mode of operation for the transform protocol.IPsec and IP Payload Compression modes are transport,tunnel, and (for IPsec ESP only) Bound End-to-End Tunnel(beet).Mobile IPv6 modes are route optimization (ro) andinbound trigger (in_trigger).LEVELcan be required (default) or use.ip xfrm policy countcount existing policiesUse one or more -s options to display more details, includingpolicy hash table information.ip xfrm policy setconfigure the policy hash tableSecurity policies whose address prefix lengths are greater thanor equal policy hash table thresholds are hashed. Others arestored in the policy_inexact chained list.LBITSspecifies the minimum local address prefix length ofpolicies that are stored in the Security Policy Databasehash table.RBITSspecifies the minimum remote address prefix length ofpolicies that are stored in the Security Policy Databasehash table.ip xfrm monitorstate monitoring for xfrm objectsThe xfrm objects to monitor can be optionally specified.If the all-nsid option is set, the program listens to all networknamespaces that have a nsid assigned into the network namespacewere the program is running.A prefix is displayed to show thenetwork namespace where the message originates. Example:[nsid 1]Flushed state proto 0",
        "name": "ip-xfrm - transform configuration",
        "section": 8
    },
    {
        "command": "ip6tables",
        "description": "Iptables and ip6tables are used to set up, maintain, and inspectthe tables of IPv4 and IPv6 packet filter rules in the Linuxkernel.Several different tables may be defined.Each tablecontains a number of built-in chains and may also contain user-defined chains.Each chain is a list of rules which can match a set of packets.Each rule specifies what to do with a packet that matches.Thisis called a `target', which may be a jump to a user-defined chainin the same table.",
        "name": "iptables/ip6tables \u2014 administration tool for IPv4/IPv6 packetfiltering and NAT",
        "section": 8
    },
    {
        "command": "ip6tables-apply",
        "description": "iptables-apply will try to apply a new rulesfile (as output byiptables-save, read by iptables-restore) or run a command toconfigure iptables and then prompt the user whether the changesare okay. If the new iptables rules cut the existing connection,the user will not be able to answer affirmatively. In this case,the script rolls back to the previous working iptables rulesafter the timeout expires.Successfully applied rules can also be written to savefile andlater used to roll back to this state. This can be used toimplement a store last good configuration mechanism whenexperimenting with an iptables setup script: iptables-apply -w/etc/network/iptables.up.rules -c /etc/network/iptables.up.runWhen called as ip6tables-apply, the script will useip6tables-save/-restore and IPv6 default values instead. Defaultvalue for rulesfile is '/etc/network/iptables.up.rules'.",
        "name": "iptables-apply - a safer way to update iptables remotely",
        "section": 8
    },
    {
        "command": "ip6tables-restore",
        "description": "iptables-restore and ip6tables-restore are used to restore IP andIPv6 Tables from data specified on STDIN or in file. Use I/Oredirection provided by your shell to read from a file or specifyfile as an argument.-c, --countersrestore the values of all packet and byte counters-h, --helpPrint a short option summary.-n, --noflushdon't flush the previous contents of the table. If notspecified, both commands flush (delete) all previouscontents of the respective table.-t, --testOnly parse and construct the ruleset, but do not commitit.-v, --verbosePrint additional debug info during ruleset processing.Specify multiple times to increase debug level.-V, --versionPrint the program version number.-w, --wait [seconds]Wait for the xtables lock.To prevent multiple instancesof the program from running concurrently, an attempt willbe made to obtain an exclusive lock at launch.Bydefault, the program will exit if the lock cannot beobtained.This option will make the program wait(indefinitely or for optional seconds) until the exclusivelock can be obtained.-M, --modprobe modprobe_programSpecify the path to the modprobe program. By default,iptables-restore will inspect /proc/sys/kernel/modprobe todetermine the executable's path.-T, --table nameRestore only the named table even if the input streamcontains other ones.",
        "name": "iptables-restore \u2014 Restore IP Tablesip6tables-restore \u2014 Restore IPv6 Tables",
        "section": 8
    },
    {
        "command": "ip6tables-save",
        "description": "iptables-save and ip6tables-save are used to dump the contents ofIP or IPv6 Table in easily parseable format either to STDOUT orto a specified file.-M, --modprobe modprobe_programSpecify the path to the modprobe program. By default,iptables-save will inspect /proc/sys/kernel/modprobe todetermine the executable's path.-f, --file filenameSpecify a filename to log the output to. If not specified,iptables-save will log to STDOUT.-c, --countersinclude the current values of all packet and byte countersin the output-t, --table tablenamerestrict output to only one table. If the kernel isconfigured with automatic module loading, an attempt willbe made to load the appropriate module for that table ifit is not already there.If not specified, output includes all available tables.",
        "name": "iptables-save \u2014 dump iptables rulesip6tables-save \u2014 dump iptables rules",
        "section": 8
    },
    {
        "command": "ip6tables-translate",
        "description": "There is a set of tools to help the system administratortranslate a given ruleset from iptables(8), ip6tables(8) andebtables(8) to nftables(8).The available commands are:\u2022 iptables-translate\u2022 iptables-restore-translate\u2022 ip6tables-translate\u2022 ip6tables-restore-translate\u2022 ebtables-translate",
        "name": "iptables-translate \u2014 translation tool to migrate from iptables tonftablesip6tables-translate \u2014 translation tool to migrate from ip6tablesto nftablesebtables-translate \u2014 translation tool to migrate from ebtables tonftables",
        "section": 8
    },
    {
        "command": "ipmaddr",
        "description": "The ipmaddr command can perform one of the following operations:add - add a multicast addresschange - change a multicast addressdel - delete a multicast addressshow - list multicast addresses",
        "name": "ipmaddr - adds, changes, deletes, and displays multicastaddresses",
        "section": 8
    },
    {
        "command": "iptables",
        "description": "Iptables and ip6tables are used to set up, maintain, and inspectthe tables of IPv4 and IPv6 packet filter rules in the Linuxkernel.Several different tables may be defined.Each tablecontains a number of built-in chains and may also contain user-defined chains.Each chain is a list of rules which can match a set of packets.Each rule specifies what to do with a packet that matches.Thisis called a `target', which may be a jump to a user-defined chainin the same table.",
        "name": "iptables/ip6tables \u2014 administration tool for IPv4/IPv6 packetfiltering and NAT",
        "section": 8
    },
    {
        "command": "iptables-apply",
        "description": "iptables-apply will try to apply a new rulesfile (as output byiptables-save, read by iptables-restore) or run a command toconfigure iptables and then prompt the user whether the changesare okay. If the new iptables rules cut the existing connection,the user will not be able to answer affirmatively. In this case,the script rolls back to the previous working iptables rulesafter the timeout expires.Successfully applied rules can also be written to savefile andlater used to roll back to this state. This can be used toimplement a store last good configuration mechanism whenexperimenting with an iptables setup script: iptables-apply -w/etc/network/iptables.up.rules -c /etc/network/iptables.up.runWhen called as ip6tables-apply, the script will useip6tables-save/-restore and IPv6 default values instead. Defaultvalue for rulesfile is '/etc/network/iptables.up.rules'.",
        "name": "iptables-apply - a safer way to update iptables remotely",
        "section": 8
    },
    {
        "command": "iptables-extensions",
        "description": null,
        "name": "iptables-extensions \u2014 list of extensions in the standard iptablesdistribution",
        "section": 8
    },
    {
        "command": "iptables-restore",
        "description": "iptables-restore and ip6tables-restore are used to restore IP andIPv6 Tables from data specified on STDIN or in file. Use I/Oredirection provided by your shell to read from a file or specifyfile as an argument.-c, --countersrestore the values of all packet and byte counters-h, --helpPrint a short option summary.-n, --noflushdon't flush the previous contents of the table. If notspecified, both commands flush (delete) all previouscontents of the respective table.-t, --testOnly parse and construct the ruleset, but do not commitit.-v, --verbosePrint additional debug info during ruleset processing.Specify multiple times to increase debug level.-V, --versionPrint the program version number.-w, --wait [seconds]Wait for the xtables lock.To prevent multiple instancesof the program from running concurrently, an attempt willbe made to obtain an exclusive lock at launch.Bydefault, the program will exit if the lock cannot beobtained.This option will make the program wait(indefinitely or for optional seconds) until the exclusivelock can be obtained.-M, --modprobe modprobe_programSpecify the path to the modprobe program. By default,iptables-restore will inspect /proc/sys/kernel/modprobe todetermine the executable's path.-T, --table nameRestore only the named table even if the input streamcontains other ones.",
        "name": "iptables-restore \u2014 Restore IP Tablesip6tables-restore \u2014 Restore IPv6 Tables",
        "section": 8
    },
    {
        "command": "iptables-save",
        "description": "iptables-save and ip6tables-save are used to dump the contents ofIP or IPv6 Table in easily parseable format either to STDOUT orto a specified file.-M, --modprobe modprobe_programSpecify the path to the modprobe program. By default,iptables-save will inspect /proc/sys/kernel/modprobe todetermine the executable's path.-f, --file filenameSpecify a filename to log the output to. If not specified,iptables-save will log to STDOUT.-c, --countersinclude the current values of all packet and byte countersin the output-t, --table tablenamerestrict output to only one table. If the kernel isconfigured with automatic module loading, an attempt willbe made to load the appropriate module for that table ifit is not already there.If not specified, output includes all available tables.",
        "name": "iptables-save \u2014 dump iptables rulesip6tables-save \u2014 dump iptables rules",
        "section": 8
    },
    {
        "command": "iptables-translate",
        "description": "There is a set of tools to help the system administratortranslate a given ruleset from iptables(8), ip6tables(8) andebtables(8) to nftables(8).The available commands are:\u2022 iptables-translate\u2022 iptables-restore-translate\u2022 ip6tables-translate\u2022 ip6tables-restore-translate\u2022 ebtables-translate",
        "name": "iptables-translate \u2014 translation tool to migrate from iptables tonftablesip6tables-translate \u2014 translation tool to migrate from ip6tablesto nftablesebtables-translate \u2014 translation tool to migrate from ebtables tonftables",
        "section": 8
    },
    {
        "command": "iptraf",
        "description": "iptraf-ng is an ncurses-based IP LAN monitor that generatesvarious network statistics including TCP info, UDP counts, ICMPand OSPF information, Ethernet load info, node stats, IP checksumerrors, and others.If the iptraf-ng command is issued without any command-lineoptions, the program comes up in interactive mode, with thevarious facilities accessed through the main menu.",
        "name": "iptraf - Interactive Colorful IP LAN Monitor",
        "section": 8
    },
    {
        "command": "iptraf-ng",
        "description": "iptraf-ng is an ncurses-based IP LAN monitor that generatesvarious network statistics including TCP info, UDP counts, ICMPand OSPF information, Ethernet load info, node stats, IP checksumerrors, and others.If the iptraf-ng command is issued without any command-lineoptions, the program comes up in interactive mode, with thevarious facilities accessed through the main menu.",
        "name": "iptraf - Interactive Colorful IP LAN Monitor",
        "section": 8
    },
    {
        "command": "iptunnel",
        "description": "The iptunnel command creates configured tunnels for sending andreceiving IPv6 or IPv4 packets that are encapsulated as thepayload of an IPv4 datagram.The iptunnel command can perform one of the following operations:create - create a tunnel interface, which you must subsequentlyconfigure.change - change an existing tunnel interface.delete - delete a tunnel interface. You must disable the tunnelbefore you can delete it.show - show the tunnel attributes (name, tunnel end points, nexthop for tunneled packets).",
        "name": "iptunnel - creates, changes, deletes, and displays configuredtunnels",
        "section": 8
    },
    {
        "command": "isosize",
        "description": "This command outputs the length of an iso9660 filesystem that iscontained in the specified file. This file may be a normal fileor a block device (e.g. /dev/hdd or /dev/sr0). In the absence ofany options (and errors), it will output the size of the iso9660filesystem in bytes. This can now be a large number (>> 4 GB).",
        "name": "isosize - output the length of an iso9660 filesystem",
        "section": 8
    },
    {
        "command": "kbdrate",
        "description": "kbdrate is used to change the keyboard repeat rate and delaytime. The delay is the amount of time that a key must bedepressed before it will start to repeat.Using kbdrate without any options will reset the repeat rate to10.9 characters per second (cps) and the delay to 250milliseconds (ms) for Intel- and M68K-based systems.These arethe IBM defaults. On SPARC-based systems it will reset the repeatrate to 5 cps and the delay to 200 ms.",
        "name": "kbdrate - reset the keyboard repeat rate and delay time",
        "section": 8
    },
    {
        "command": "kernel-install",
        "description": "kernel-install is used to install and remove kernel and initrdimages [1] to and from the boot loader partition, referred to as$BOOT here. It will usually be one of /boot/, /efi/, or/boot/efi/, see below.kernel-install will run the executable files (\"plugins\") locatedin the directory /usr/lib/kernel/install.d/ and the localadministration directory /etc/kernel/install.d/. All files arecollectively sorted and executed in lexical order, regardless ofthe directory in which they live. However, files with identicalfilenames replace each other. Files in /etc/kernel/install.d/take precedence over files with the same name in/usr/lib/kernel/install.d/. This can be used to override asystem-supplied executables with a local file if needed; asymbolic link in /etc/kernel/install.d/ with the same name as anexecutable in /usr/lib/kernel/install.d/, pointing to /dev/null,disables the executable entirely. Executables must have theextension \".install\"; other extensions are ignored.An executable placed in these directories should return 0 onsuccess. It may also return 77 to cause the whole operation toterminate (executables later in lexical order will be skipped).",
        "name": "kernel-install - Add and remove kernel and initrd images to andfrom /boot",
        "section": 8
    },
    {
        "command": "kexec",
        "description": "kexec is a system call that enables you to load and boot intoanother kernel from the currently running kernel.kexec performsthe function of the boot loader from within the kernel. Theprimary difference between a standard system boot and a kexecboot is that the hardware initialization normally performed bythe BIOS or firmware (depending on architecture) is not performedduring a kexec boot. This has the effect of reducing the timerequired for a reboot.Make sure you have selected CONFIG_KEXEC=y when configuring thekernel. The CONFIG_KEXEC option enables the kexec system call.",
        "name": "kexec - directly boot into a new kernel",
        "section": 8
    },
    {
        "command": "key.dns_resolver",
        "description": "This program is invoked by request-key on behalf of the kernelwhen kernel services (such as NFS, CIFS and AFS) want to performa hostname lookup and the kernel does not have the key cached.It is not ordinarily intended to be called directly.There program has internal parameters that can be changed with aconfiguration file (see key.dns_resolver.conf(5) for moreinformation).The default configuration file is in /etc, butthis can be overridden with the -c flag.The program can be called in debugging mode to test itsfunctionality by passing a -D or --debug flag on the commandline.For this to work, the key description and the calloutinformation must be supplied.Verbosity can be increased bysupplying one or more -v flags.The program may also be called with --dump-config to show thevalues that configurable parameters will have after parsing theconfig file.",
        "name": "key.dns_resolver - upcall for request-key to handle dns_resolverkeys",
        "section": 8
    },
    {
        "command": "kmod",
        "description": "kmod is a multi-call binary which implements the programs used tocontrol Linux Kernel modules. Most users will only run it usingits other names.",
        "name": "kmod - Program to manage Linux Kernel modules",
        "section": 8
    },
    {
        "command": "lastlog",
        "description": "lastlog formats and prints the contents of the last login log/var/log/lastlog file. The login-name, port, and last login timewill be printed. The default (no flags) causes lastlog entries tobe printed, sorted by their order in /etc/passwd.",
        "name": "lastlog - reports the most recent login of all users or of agiven user",
        "section": 8
    },
    {
        "command": "ld-linux",
        "description": "The programs ld.so and ld-linux.so* find and load the sharedobjects (shared libraries) needed by a program, prepare theprogram to run, and then run it.Linux binaries require dynamic linking (linking at run time)unless the -static option was given to ld(1) during compilation.The program ld.so handles a.out binaries, a binary format usedlong ago.The program ld-linux.so* (/lib/ld-linux.so.1 forlibc5, /lib/ld-linux.so.2 for glibc2) handles binaries that arein the more modern ELF format.Both programs have the samebehavior, and use the same support files and programs (ldd(1),ldconfig(8), and /etc/ld.so.conf).When resolving shared object dependencies, the dynamic linkerfirst inspects each dependency string to see if it contains aslash (this can occur if a shared object pathname containingslashes was specified at link time).If a slash is found, thenthe dependency string is interpreted as a (relative or absolute)pathname, and the shared object is loaded using that pathname.If a shared object dependency does not contain a slash, then itis searched for in the following order:(1)Using the directories specified in the DT_RPATH dynamicsection attribute of the binary if present and DT_RUNPATHattribute does not exist.Use of DT_RPATH is deprecated.(2)Using the environment variable LD_LIBRARY_PATH, unless theexecutable is being run in secure-execution mode (seebelow), in which case this variable is ignored.(3)Using the directories specified in the DT_RUNPATH dynamicsection attribute of the binary if present.Suchdirectories are searched only to find those objects requiredby DT_NEEDED (direct dependencies) entries and do not applyto those objects' children, which must themselves have theirown DT_RUNPATH entries.This is unlike DT_RPATH, which isapplied to searches for all children in the dependency tree.(4)From the cache file /etc/ld.so.cache, which contains acompiled list of candidate shared objects previously foundin the augmented library path.If, however, the binary waslinked with the -z nodeflib linker option, shared objects inthe default paths are skipped.Shared objects installed inhardware capability directories (see below) are preferred toother shared objects.(5)In the default path /lib, and then /usr/lib.(On some64-bit architectures, the default paths for 64-bit sharedobjects are /lib64, and then /usr/lib64.)If the binary waslinked with the -z nodeflib linker option, this step isskipped.Dynamic string tokensIn several places, the dynamic linker expands dynamic stringtokens:\u2022In the environment variables LD_LIBRARY_PATH, LD_PRELOAD, andLD_AUDIT,\u2022inside the values of the dynamic section tags DT_NEEDED,DT_RPATH, DT_RUNPATH, DT_AUDIT, and DT_DEPAUDIT of ELFbinaries,\u2022in the arguments to the ld.so command line options --audit,--library-path, and --preload (see below), and\u2022in the filename arguments to the dlopen(3) and dlmopen(3)functions.The substituted tokens are as follows:$ORIGIN (or equivalently ${ORIGIN})This expands to the directory containing the program orshared object.Thus, an application located insomedir/app could be compiled withgcc -Wl,-rpath,'$ORIGIN/../lib'so that it finds an associated shared object insomedir/lib no matter where somedir is located in thedirectory hierarchy.This facilitates the creation of\"turn-key\" applications that do not need to be installedinto special directories, but can instead be unpacked intoany directory and still find their own shared objects.$LIB (or equivalently ${LIB})This expands to lib or lib64 depending on the architecture(e.g., on x86-64, it expands to lib64 and on x86-32, itexpands to lib).$PLATFORM (or equivalently ${PLATFORM})This expands to a string corresponding to the processortype of the host system (e.g., \"x86_64\").On somearchitectures, the Linux kernel doesn't provide a platformstring to the dynamic linker.The value of this string istaken from the AT_PLATFORM value in the auxiliary vector(see getauxval(3)).Note that the dynamic string tokens have to be quoted properlywhen set from a shell, to prevent their expansion as shell orenvironment variables.",
        "name": "ld.so, ld-linux.so - dynamic linker/loader",
        "section": 8
    },
    {
        "command": "ld-linux.so",
        "description": "The programs ld.so and ld-linux.so* find and load the sharedobjects (shared libraries) needed by a program, prepare theprogram to run, and then run it.Linux binaries require dynamic linking (linking at run time)unless the -static option was given to ld(1) during compilation.The program ld.so handles a.out binaries, a binary format usedlong ago.The program ld-linux.so* (/lib/ld-linux.so.1 forlibc5, /lib/ld-linux.so.2 for glibc2) handles binaries that arein the more modern ELF format.Both programs have the samebehavior, and use the same support files and programs (ldd(1),ldconfig(8), and /etc/ld.so.conf).When resolving shared object dependencies, the dynamic linkerfirst inspects each dependency string to see if it contains aslash (this can occur if a shared object pathname containingslashes was specified at link time).If a slash is found, thenthe dependency string is interpreted as a (relative or absolute)pathname, and the shared object is loaded using that pathname.If a shared object dependency does not contain a slash, then itis searched for in the following order:(1)Using the directories specified in the DT_RPATH dynamicsection attribute of the binary if present and DT_RUNPATHattribute does not exist.Use of DT_RPATH is deprecated.(2)Using the environment variable LD_LIBRARY_PATH, unless theexecutable is being run in secure-execution mode (seebelow), in which case this variable is ignored.(3)Using the directories specified in the DT_RUNPATH dynamicsection attribute of the binary if present.Suchdirectories are searched only to find those objects requiredby DT_NEEDED (direct dependencies) entries and do not applyto those objects' children, which must themselves have theirown DT_RUNPATH entries.This is unlike DT_RPATH, which isapplied to searches for all children in the dependency tree.(4)From the cache file /etc/ld.so.cache, which contains acompiled list of candidate shared objects previously foundin the augmented library path.If, however, the binary waslinked with the -z nodeflib linker option, shared objects inthe default paths are skipped.Shared objects installed inhardware capability directories (see below) are preferred toother shared objects.(5)In the default path /lib, and then /usr/lib.(On some64-bit architectures, the default paths for 64-bit sharedobjects are /lib64, and then /usr/lib64.)If the binary waslinked with the -z nodeflib linker option, this step isskipped.Dynamic string tokensIn several places, the dynamic linker expands dynamic stringtokens:\u2022In the environment variables LD_LIBRARY_PATH, LD_PRELOAD, andLD_AUDIT,\u2022inside the values of the dynamic section tags DT_NEEDED,DT_RPATH, DT_RUNPATH, DT_AUDIT, and DT_DEPAUDIT of ELFbinaries,\u2022in the arguments to the ld.so command line options --audit,--library-path, and --preload (see below), and\u2022in the filename arguments to the dlopen(3) and dlmopen(3)functions.The substituted tokens are as follows:$ORIGIN (or equivalently ${ORIGIN})This expands to the directory containing the program orshared object.Thus, an application located insomedir/app could be compiled withgcc -Wl,-rpath,'$ORIGIN/../lib'so that it finds an associated shared object insomedir/lib no matter where somedir is located in thedirectory hierarchy.This facilitates the creation of\"turn-key\" applications that do not need to be installedinto special directories, but can instead be unpacked intoany directory and still find their own shared objects.$LIB (or equivalently ${LIB})This expands to lib or lib64 depending on the architecture(e.g., on x86-64, it expands to lib64 and on x86-32, itexpands to lib).$PLATFORM (or equivalently ${PLATFORM})This expands to a string corresponding to the processortype of the host system (e.g., \"x86_64\").On somearchitectures, the Linux kernel doesn't provide a platformstring to the dynamic linker.The value of this string istaken from the AT_PLATFORM value in the auxiliary vector(see getauxval(3)).Note that the dynamic string tokens have to be quoted properlywhen set from a shell, to prevent their expansion as shell orenvironment variables.",
        "name": "ld.so, ld-linux.so - dynamic linker/loader",
        "section": 8
    },
    {
        "command": "ld.so",
        "description": "The programs ld.so and ld-linux.so* find and load the sharedobjects (shared libraries) needed by a program, prepare theprogram to run, and then run it.Linux binaries require dynamic linking (linking at run time)unless the -static option was given to ld(1) during compilation.The program ld.so handles a.out binaries, a binary format usedlong ago.The program ld-linux.so* (/lib/ld-linux.so.1 forlibc5, /lib/ld-linux.so.2 for glibc2) handles binaries that arein the more modern ELF format.Both programs have the samebehavior, and use the same support files and programs (ldd(1),ldconfig(8), and /etc/ld.so.conf).When resolving shared object dependencies, the dynamic linkerfirst inspects each dependency string to see if it contains aslash (this can occur if a shared object pathname containingslashes was specified at link time).If a slash is found, thenthe dependency string is interpreted as a (relative or absolute)pathname, and the shared object is loaded using that pathname.If a shared object dependency does not contain a slash, then itis searched for in the following order:(1)Using the directories specified in the DT_RPATH dynamicsection attribute of the binary if present and DT_RUNPATHattribute does not exist.Use of DT_RPATH is deprecated.(2)Using the environment variable LD_LIBRARY_PATH, unless theexecutable is being run in secure-execution mode (seebelow), in which case this variable is ignored.(3)Using the directories specified in the DT_RUNPATH dynamicsection attribute of the binary if present.Suchdirectories are searched only to find those objects requiredby DT_NEEDED (direct dependencies) entries and do not applyto those objects' children, which must themselves have theirown DT_RUNPATH entries.This is unlike DT_RPATH, which isapplied to searches for all children in the dependency tree.(4)From the cache file /etc/ld.so.cache, which contains acompiled list of candidate shared objects previously foundin the augmented library path.If, however, the binary waslinked with the -z nodeflib linker option, shared objects inthe default paths are skipped.Shared objects installed inhardware capability directories (see below) are preferred toother shared objects.(5)In the default path /lib, and then /usr/lib.(On some64-bit architectures, the default paths for 64-bit sharedobjects are /lib64, and then /usr/lib64.)If the binary waslinked with the -z nodeflib linker option, this step isskipped.Dynamic string tokensIn several places, the dynamic linker expands dynamic stringtokens:\u2022In the environment variables LD_LIBRARY_PATH, LD_PRELOAD, andLD_AUDIT,\u2022inside the values of the dynamic section tags DT_NEEDED,DT_RPATH, DT_RUNPATH, DT_AUDIT, and DT_DEPAUDIT of ELFbinaries,\u2022in the arguments to the ld.so command line options --audit,--library-path, and --preload (see below), and\u2022in the filename arguments to the dlopen(3) and dlmopen(3)functions.The substituted tokens are as follows:$ORIGIN (or equivalently ${ORIGIN})This expands to the directory containing the program orshared object.Thus, an application located insomedir/app could be compiled withgcc -Wl,-rpath,'$ORIGIN/../lib'so that it finds an associated shared object insomedir/lib no matter where somedir is located in thedirectory hierarchy.This facilitates the creation of\"turn-key\" applications that do not need to be installedinto special directories, but can instead be unpacked intoany directory and still find their own shared objects.$LIB (or equivalently ${LIB})This expands to lib or lib64 depending on the architecture(e.g., on x86-64, it expands to lib64 and on x86-32, itexpands to lib).$PLATFORM (or equivalently ${PLATFORM})This expands to a string corresponding to the processortype of the host system (e.g., \"x86_64\").On somearchitectures, the Linux kernel doesn't provide a platformstring to the dynamic linker.The value of this string istaken from the AT_PLATFORM value in the auxiliary vector(see getauxval(3)).Note that the dynamic string tokens have to be quoted properlywhen set from a shell, to prevent their expansion as shell orenvironment variables.",
        "name": "ld.so, ld-linux.so - dynamic linker/loader",
        "section": 8
    },
    {
        "command": "ldattach",
        "description": "The ldattach daemon opens the specified device file (which shouldrefer to a serial device) and attaches the line discipline ldiscto it for processing of the sent and/or received data. It thengoes into the background keeping the device open so that the linediscipline stays loaded.The line discipline ldisc may be specified either by name or bynumber.In order to detach the line discipline, kill(1) the ldattachprocess.With no arguments, ldattach prints usage information.",
        "name": "ldattach - attach a line discipline to a serial line",
        "section": 8
    },
    {
        "command": "ldconfig",
        "description": "ldconfig creates the necessary links and cache to the most recentshared libraries found in the directories specified on thecommand line, in the file /etc/ld.so.conf, and in the trusteddirectories, /lib and /usr/lib.On some 64-bit architecturessuch as x86-64, /lib and /usr/lib are the trusted directories for32-bit libraries, while /lib64 and /usr/lib64 are used for 64-bitlibraries.The cache is used by the run-time linker, ld.so or ld-linux.so.ldconfig checks the header and filenames of the libraries itencounters when determining which versions should have theirlinks updated.ldconfig should normally be run by the superuseras it may require write permission on some root owned directoriesand files.ldconfig will look only at files that are named lib*.so* (forregular shared objects) or ld-*.so* (for the dynamic loaderitself).Other files will be ignored.Also, ldconfig expects acertain pattern to how the symbolic links are set up, like thisexample, where the middle file (libfoo.so.1 here) is the SONAMEfor the library:libfoo.so -> libfoo.so.1 -> libfoo.so.1.12Failure to follow this pattern may result in compatibility issuesafter an upgrade.",
        "name": "ldconfig - configure dynamic linker run-time bindings",
        "section": 8
    },
    {
        "command": "libnss_myhostname.so.2",
        "description": "nss-myhostname is a plug-in module for the GNU Name ServiceSwitch (NSS) functionality of the GNU C Library (glibc),primarily providing hostname resolution for the locallyconfigured system hostname as returned by gethostname(2). Theprecise hostnames resolved by this module are:\u2022The local, configured hostname is resolved to all locallyconfigured IP addresses ordered by their scope, or \u2014 if noneare configured \u2014 the IPv4 address 127.0.0.2 (which is on thelocal loopback) and the IPv6 address ::1 (which is the localhost).\u2022The hostnames \"localhost\" and \"localhost.localdomain\" (aswell as any hostname ending in \".localhost\" or\".localhost.localdomain\") are resolved to the IP addresses127.0.0.1 and ::1.\u2022The hostname \"_gateway\" is resolved to all current defaultrouting gateway addresses, ordered by their metric. Thisassigns a stable hostname to the current gateway, useful forreferencing it independently of the current networkconfiguration state.\u2022The hostname \"_outbound\" is resolved to the local IPv4 andIPv6 addresses that are most likely used for communicationwith other hosts. This is determined by requesting a routingdecision to the configured default gateways from the kerneland then using the local IP addresses selected by thisdecision. This hostname is only available if there is atleast one local default gateway configured. This assigns astable hostname to the local outbound IP addresses, usefulfor referencing them independently of the current networkconfiguration state.Various software relies on an always-resolvable local hostname.When using dynamic hostnames, this is traditionally achieved bypatching /etc/hosts at the same time as changing the hostname.This is problematic since it requires a writable /etc/ filesystem and is fragile because the file might be edited by theadministrator at the same time. With nss-myhostname enabled,changing /etc/hosts is unnecessary, and on many systems, the filebecomes entirely optional.To activate the NSS modules, add \"myhostname\" to the linestarting with \"hosts:\" in /etc/nsswitch.conf.It is recommended to place \"myhostname\" after \"file\" and before\"dns\". This resolves well-known hostnames like \"localhost\" andthe machine hostnames locally. It is consistent with thebehaviour of nss-resolve, and still allows overriding via/etc/hosts.Please keep in mind that nss-myhostname (and nss-resolve) alsoresolve in the other direction \u2014 from locally attached IPaddresses to hostnames. If you rely on that lookup being providedby DNS, you might want to order things differently.",
        "name": "nss-myhostname, libnss_myhostname.so.2 - Hostname resolution forthe locally configured system hostname",
        "section": 8
    },
    {
        "command": "libnss_mymachines.so.2",
        "description": "nss-mymachines is a plug-in module for the GNU Name ServiceSwitch (NSS) functionality of the GNU C Library (glibc),providing hostname resolution for the names of containers runninglocally that are registered with systemd-machined.service(8). Thecontainer names are resolved to the IP addresses of the specificcontainer, ordered by their scope. This functionality onlyapplies to containers using network namespacing (see thedescription of --private-network in systemd-nspawn(1)). Note thatthe name that is resolved is the one registered withsystemd-machined, which may be different than the hostnameconfigured inside of the container.Note that this NSS module only makes available names of thecontainers running immediately below the current system context.It does not provide host name resolution for containers runningside-by-side with the invoking system context, or containersfurther up or down the container hierarchy. Or in other words, onthe host system it provides host name resolution for thecontainers running immediately below the host environment. Whenused inside a container environment however, it will not be ableto provide name resolution for containers running on the host (asthose are siblings and not children of the current containerenvironment), but instead only for nested containers runningimmediately below its own container environment.To activate the NSS module, add \"mymachines\" to the line startingwith \"hosts:\" in /etc/nsswitch.conf.It is recommended to place \"mymachines\" before the \"resolve\" or\"dns\" entry of the \"hosts:\" line of /etc/nsswitch.conf in orderto make sure that its mappings are preferred over other resolverssuch as DNS.",
        "name": "nss-mymachines, libnss_mymachines.so.2 - Hostname resolution forlocal container instances",
        "section": 8
    },
    {
        "command": "libnss_resolve.so.2",
        "description": "nss-resolve is a plug-in module for the GNU Name Service Switch(NSS) functionality of the GNU C Library (glibc) enabling it toresolve hostnames via the systemd-resolved(8) local network nameresolution service. It replaces the nss-dns plug-in module thattraditionally resolves hostnames via DNS.To activate the NSS module, add \"resolve [!UNAVAIL=return]\" tothe line starting with \"hosts:\" in /etc/nsswitch.conf.Specifically, it is recommended to place \"resolve\" early in/etc/nsswitch.conf's \"hosts:\" line. It should be before the\"files\" entry, since systemd-resolved supports /etc/hostsinternally, but with caching. To the contrary, it should be after\"mymachines\", to give hostnames given to local VMs and containersprecedence over names received over DNS. Finally, we recommendplacing \"dns\" somewhere after \"resolve\", to fall back to nss-dnsif systemd-resolved.service is not available.Note that systemd-resolved will synthesize DNS resource recordsin a few cases, for example for \"localhost\" and the current localhostname, see systemd-resolved(8) for the full list. Thisduplicates the functionality of nss-myhostname(8), but it isstill recommended (see examples below) to keep nss-myhostnameconfigured in /etc/nsswitch.conf, to keep those names resolveableif systemd-resolved is not running.Please keep in mind that nss-myhostname (and nss-resolve) alsoresolve in the other direction \u2014 from locally attached IPaddresses to hostnames. If you rely on that lookup being providedby DNS, you might want to order things differently.Communication between nss-resolve and systemd-resolved.servicetakes place via the /run/systemd/resolve/io.systemd.ResolveAF_UNIX socket.",
        "name": "nss-resolve, libnss_resolve.so.2 - Hostname resolution viasystemd-resolved.service",
        "section": 8
    },
    {
        "command": "libnss_systemd.so.2",
        "description": "nss-systemd is a plug-in module for the GNU Name Service Switch(NSS) functionality of the GNU C Library (glibc), providing UNIXuser and group name resolution for services implementing theUser/Group Record Lookup API via Varlink[1], such as the systemand service manager systemd(1) (for its DynamicUser= feature, seesystemd.exec(5) for details), systemd-homed.service(8), orsystemd-machined.service(8).This module also ensures that the root and nobody users andgroups (i.e. the users/groups with the UIDs/GIDs 0 and 65534)remain resolvable at all times, even if they aren't listed in/etc/passwd or /etc/group, or if these files are missing.This module preferably utilizes systemd-userdbd.service(8) forresolving users and groups, but also works without the servicerunning.To activate the NSS module, add \"systemd\" to the lines startingwith \"passwd:\", \"group:\", \"shadow:\" and \"gshadow:\" in/etc/nsswitch.conf.It is recommended to place \"systemd\" after the \"files\" or\"compat\" entry of the /etc/nsswitch.conf lines so that/etc/passwd, /etc/group, /etc/shadow and /etc/gshadow basedmappings take precedence.",
        "name": "nss-systemd, libnss_systemd.so.2 - UNIX user and group nameresolution for user/group lookup via Varlink",
        "section": 8
    },
    {
        "command": "linux32",
        "description": "setarch modifies execution domains and process personality flags.The execution domains currently only affects the output of uname-m. For example, on an AMD64 system, running setarch i386 programwill cause program to see i686 instead of x86_64 as the machinetype. It can also be used to set various personality options. Thedefault program is /bin/sh.Since version 2.33 the arch command line argument is optional andsetarch may be used to change personality flags (ADDR_LIMIT_*,SHORT_INODE, etc) without modification of the execution domain.",
        "name": "setarch - change reported architecture in new program environmentand/or set personality flags",
        "section": 8
    },
    {
        "command": "linux64",
        "description": "setarch modifies execution domains and process personality flags.The execution domains currently only affects the output of uname-m. For example, on an AMD64 system, running setarch i386 programwill cause program to see i686 instead of x86_64 as the machinetype. It can also be used to set various personality options. Thedefault program is /bin/sh.Since version 2.33 the arch command line argument is optional andsetarch may be used to change personality flags (ADDR_LIMIT_*,SHORT_INODE, etc) without modification of the execution domain.",
        "name": "setarch - change reported architecture in new program environmentand/or set personality flags",
        "section": 8
    },
    {
        "command": "lloadd",
        "description": "Lloadd is the stand-alone LDAP daemon. It listens for LDAPconnections on any number of ports (default 389), forwarding theLDAP operations it receives over these connections to be handledby the configured backends.lloadd is typically invoked at boottime, usually out of /etc/rc.local.Upon startup, lloaddnormally forks and disassociates itself from the invoking tty.If configured in the config file, the lloadd process will printits process ID (see getpid(2)) to a .pid file, as well as thecommand line options during invocation to an .args file (seelloadd.conf(5)).If the -d flag is given, even with a zeroargument, lloadd will not fork and disassociate from the invokingtty.See the \"OpenLDAP Administrator's Guide\" for more details onlloadd.",
        "name": "lloadd - LDAP Load Balancer Daemon",
        "section": 8
    },
    {
        "command": "lnstat",
        "description": "This manual page documents briefly the lnstat command.lnstat is a generalized and more feature-complete replacement forthe old rtstat program. It is commonly used to periodically printa selection of statistical values exported by the kernel.Inaddition to routing cache statistics, it supports any kind ofstatistics the linux kernel exports via a file in/proc/net/stat/.Each file in /proc/net/stat/ contains a header line listing thecolumn names.These names are used by lnstat as keys forselecting which statistics to print. For every CPU present in thesystem, a line follows which lists the actual values for eachcolumn of the file. lnstat sums these values up (which in factare counters) before printing them. After each interval, only thedifference to the last value is printed.Files and columns may be selected by using the -f and -kparameters. By default, all columns of all files are printed.",
        "name": "lnstat - unified linux network statistics",
        "section": 8
    },
    {
        "command": "load_policy",
        "description": "load_policy loads the installed policy file into the kernel.Theexisting policy boolean values are automatically preserved acrosspolicy reloads rather than being reset to the default values inthe policy file.",
        "name": "load_policy - load a new SELinux policy into the kernel",
        "section": 8
    },
    {
        "command": "loadunimap",
        "description": "The loadunimap command is obsolete - its function is now built-ininto setfont(8).However, for backwards compatibility it isstill available as a separate command.The program loadunimap loads the specified map in the kernelunicode-to-font mapping table.If no map is given def mappingtable is assumed.The default extension (that can be omitted) is.uni.If the -o oldmap option is given, the old map is saved in thefile specified.On Linux 2.6.1 and later one can specify the console device usingthe -C option.Usually one does not call loadunimap directly - its function isalso built into setfont(8).",
        "name": "loadunimap - load the kernel unicode-to-font mapping table",
        "section": 8
    },
    {
        "command": "logoutd",
        "description": "logoutd enforces the login time and port restrictions specifiedin /etc/porttime.logoutd should be started from /etc/rc. The/var/run/utmp file is scanned periodically and each user name ischecked to see if the named user is permitted on the named portat the current time. Any login session which is violating therestrictions in /etc/porttime is terminated.",
        "name": "logoutd - enforce login time restrictions",
        "section": 8
    },
    {
        "command": "logrotate",
        "description": "logrotate is designed to ease administration of systems thatgenerate large numbers of log files.It allows automaticrotation, compression, removal, and mailing of log files.Eachlog file may be handled daily, weekly, monthly, or when it growstoo large.Normally, logrotate is run as a daily cron job.It will notmodify a log more than once in one day unless the criterion forthat log is based on the log's size and logrotate is being runmore than once each day, or unless the -f or --force option isused.Any number of config files may be given on the command line.Later config files may override the options given in earlierfiles, so the order in which the logrotate config files arelisted is important.Normally, a single config file whichincludes any other config files which are needed should be used.See below for more information on how to use the includedirective to accomplish this.If a directory is given on thecommand line, every file in that directory is used as a configfile.If no command line arguments are given, logrotate will printversion and copyright information, along with a short usagesummary.If any errors occur while rotating logs, logrotate willexit with non-zero status, although the state file will beupdated.",
        "name": "logrotate \u2010 rotates, compresses, and mails system logs",
        "section": 8
    },
    {
        "command": "logsave",
        "description": "The logsave program will execute cmd_prog with the specifiedargument(s), and save a copy of its output to logfile.If thecontaining directory for logfile does not exist, logsave willaccumulate the output in memory until it can be written out.Acopy of the output will also be written to standard output.If cmd_prog is a single hyphen ('-'), then instead of executing aprogram, logsave will take its input from standard input and saveit in logfilelogsave is useful for saving the output of initial boot scriptsuntil the /var partition is mounted, so the output can be writtento /var/log.",
        "name": "logsave - save the output of a command in a logfile",
        "section": 8
    },
    {
        "command": "losetup",
        "description": "losetup is used to associate loop devices with regular files orblock devices, to detach loop devices, and to query the status ofa loop device. If only the loopdev argument is given, the statusof the corresponding loop device is shown. If no option is given,all loop devices are shown.Note that the old output format (i.e., losetup -a) withcomma-delimited strings is deprecated in favour of the --listoutput format.It\u2019s possible to create more independent loop devices for thesame backing file. This setup may be dangerous, can cause dataloss, corruption and overwrites. Use --nooverlap with --findduring setup to avoid this problem.The loop device setup is not an atomic operation when used with--find, and losetup does not protect this operation by any lock.The number of attempts is internally restricted to a maximum of16. It is recommended to use for example flock(1) to avoid acollision in heavily parallel use cases.",
        "name": "losetup - set up and control loop devices",
        "section": 8
    },
    {
        "command": "lpadmin",
        "description": "lpadmin configures printer and class queues provided by CUPS.Itcan also be used to set the server default printer or class.When specified before the -d, -p, or -x options, the -E optionforces encryption when connecting to the server.The first form of the command (-d) sets the default printer orclass to destination.Subsequent print jobs submitted via thelp(1) or lpr(1) commands will use this destination unless theuser specifies otherwise with the lpoptions(1) command.The second form of the command (-p) configures the named printeror class.The additional options are described below.The third form of the command (-x) deletes the printer or classdestination.Any jobs that are pending for the destination willbe removed and any job that is currently printed will be aborted.",
        "name": "lpadmin - configure cups printers and classes",
        "section": 8
    },
    {
        "command": "lpc",
        "description": "lpc provides limited control over printer and class queuesprovided by CUPS. It can also be used to query the state ofqueues.If no command is specified on the command-line, lpc displays aprompt and accepts commands from the standard input.COMMANDSThe lpc program accepts a subset of commands accepted by theBerkeley lpc program of the same name:exit Exits the command interpreter.help [command]? [command]Displays a short help message.quit Exits the command interpreter.status [queue]Displays the status of one or more printer or class queues.",
        "name": "lpc - line printer control program (deprecated)",
        "section": 8
    },
    {
        "command": "lpinfo",
        "description": "lpinfo lists the available devices or drivers known to the CUPSserver.The first form (-m) lists the available drivers, whilethe second form (-v) lists the available devices.",
        "name": "lpinfo - show available devices or drivers (deprecated)",
        "section": 8
    },
    {
        "command": "lpmove",
        "description": "lpmove moves the specified job or all jobs from source todestination. job can be the job ID number or the old destinationand job ID.",
        "name": "lpmove - move a job or all jobs to a new destination",
        "section": 8
    },
    {
        "command": "lsblk",
        "description": "lsblk lists information about all available or the specifiedblock devices. The lsblk command reads the sysfs filesystem andudev db to gather information. If the udev db is not available orlsblk is compiled without udev support, then it tries to readLABELs, UUIDs and filesystem types from the block device. In thiscase root permissions are necessary.By default, the command prints all block devices (except RAMdisks) in a tree-like format. The same device can be repeated inthe tree if it relates to other devices. The --merge option isrecommended for more complicated setups to gather groups ofdevices and describe complex N:M relationships.The default output, as well as the default output from optionslike --fs and --topology, is subject to change. So wheneverpossible, you should avoid using default outputs in your scripts.Always explicitly define expected columns by using --outputcolumns-list and --list in environments where a stable output isrequired.Use lsblk --help to get a list of all available columns.Note that lsblk might be executed in time when udev does not haveall information about recently added or modified devices yet. Inthis case it is recommended to use udevadm settle before lsblk tosynchronize with udev.The relationship between block devices and filesystems is notalways one-to-one. The filesystem may use more block devices, orthe same filesystem may be accessible by more paths. This is thereason why lsblk provides MOUNTPOINT and MOUNTPOINTS (pl.)columns. The column MOUNTPOINT displays only one mount point(usually the last mounted instance of the filesystem), and thecolumn MOUNTPOINTS displays by multi-line cell all mount pointsassociated with the device.",
        "name": "lsblk - list block devices",
        "section": 8
    },
    {
        "command": "lslocks",
        "description": "lslocks lists information about all the currently held file locksin a Linux system.Note that lslocks also lists OFD (Open File Description) locks,these locks are not associated with any process (PID is -1). OFDlocks are associated with the open file description on which theyare acquired. This lock type is available since Linux 3.15, seefcntl(2) for more details.",
        "name": "lslocks - list local system locks",
        "section": 8
    },
    {
        "command": "lsmod",
        "description": "lsmod is a trivial program which nicely formats the contents ofthe /proc/modules, showing what kernel modules are currentlyloaded.",
        "name": "lsmod - Show the status of modules in the Linux Kernel",
        "section": 8
    },
    {
        "command": "lsns",
        "description": "lsns lists information about all the currently accessiblenamespaces or about the given namespace. The namespace identifieris an inode number.The default output is subject to change. So whenever possible,you should avoid using default outputs in your scripts. Alwaysexplicitly define expected output mode (--tree or --list) andcolumns by using the --output option together with a columns listin environments where a stable output is required.The NSFS column, printed when net is specified for the --typeoption, is special; it uses multi-line cells. Use the option--nowrap to switch to \",\"-separated single-line representation.Note that lsns reads information directly from the /procfilesystem and for non-root users it may return incompleteinformation. The current /proc filesystem may be unshared andaffected by a PID namespace (see unshare --mount-proc for moredetails). lsns is not able to see persistent namespaces withoutprocesses where the namespace instance is held by a bind mount to/proc/pid/ns/type.",
        "name": "lsns - list namespaces",
        "section": 8
    },
    {
        "command": "lsof",
        "description": "Lsof revision 4.91 lists on its standard output file informationabout files opened by processes for the following UNIX dialects:Apple Darwin 9 and Mac OS X 10.[567]FreeBSD 8.[234], 9.0 and 1[012].0 for AMD64-based systemsLinux 2.1.72 and above for x86-based systemsSolaris 9, 10 and 11(See the DISTRIBUTION section of this manual page for informationon how to obtain the latest lsof revision.)An open file may be a regular file, a directory, a block specialfile, a character special file, an executing text reference, alibrary, a stream or a network file (Internet socket, NFS file orUNIX domain socket.)A specific file or all the files in a filesystem may be selected by path.Instead of a formatted display, lsof will produce output that canbe parsed by other programs.See the -F, option description, andthe OUTPUT FOR OTHER PROGRAMS section for more information.In addition to producing a single output list, lsof will run inrepeat mode.In repeat mode it will produce output, delay, thenrepeat the output operation until stopped with an interrupt orquit signal.See the +|-r [t[m<fmt>]] option description formore information.",
        "name": "lsof - list open files",
        "section": 8
    },
    {
        "command": "lspci",
        "description": "lspci is a utility for displaying information about PCI buses inthe system and devices connected to them.By default, it shows a brief list of devices. Use the optionsdescribed below to request either a more verbose output or outputintended for parsing by other programs.If you are going to report bugs in PCI device drivers or in lspciitself, please include output of \"lspci -vvx\" or even better\"lspci -vvxxx\" (however, see below for possible caveats).Some parts of the output, especially in the highly verbose modes,are probably intelligible only to experienced PCI hackers. Forexact definitions of the fields, please consult either the PCIspecifications or the header.h and /usr/include/linux/pci.hinclude files.Access to some parts of the PCI configuration space is restrictedto root on many operating systems, so the features of lspciavailable to normal users are limited. However, lspci tries itsbest to display as much as available and mark all otherinformation with <access denied> text.",
        "name": "lspci - list all PCI devices",
        "section": 8
    },
    {
        "command": "lsusb",
        "description": "lsusb is a utility for displaying information about USB buses inthe system and the devices connected to them. It uses udev'shardware database to associate a full human-readable name to thevendor ID and the product ID.",
        "name": "lsusb - list USB devices",
        "section": 8
    },
    {
        "command": "lttng-relayd",
        "description": "The Linux Trace Toolkit: next generation <https://lttng.org/> isan open source software package used for correlated tracing ofthe Linux kernel, user applications, and user libraries.LTTng consists of Linux kernel modules (for Linux kernel tracing)and dynamically loaded libraries (for user application andlibrary tracing).The LTTng relay daemon is responsible for receiving trace datafrom possibly remote LTTng session/consumer daemons and forwriting it to the local file system. The relay daemon alsoaccepts LTTng live connections from compatible viewers; this isthe official approach to viewing LTTng events as they areemitted.The relay daemon listens by default on all network interfaces togather trace data, but only on localhost for LTTng liveconnections.The relay daemon does not require any particular permissions, aslong as it can write to the output directory and listen on theconfigured ports. If a user is within a secured network and/orhas proper firewall settings, lttng-relayd can listen to LTTnglive connections from all network interfaces by specifying--live-port=tcp://localhost:5344.Once a trace has been streamed completely, the trace can beprocessed by any tool that can process an LTTng trace located onthe local file system.Output directoryBy default, the relay daemon writes the traces to:$LTTNG_HOME/lttng-traces/HOSTNAME/SESSION/DOMAINwith:HOSTNAMERemote hostname.SESSIONFull session name.DOMAINTracing domain.You can override the default output directory prefix($LTTNG_HOME/lttng-traces) with the --output option. The otherparts depend on the remote configuration.URL formatThe --control-port, --data-port, and --live-port options specifyURLs.The format of those URLs is:tcp://(HOST | IPADDR):PORTwith:(HOST | IPADDR)Binding hostname or IP address (IPv6 address must be enclosedin brackets ([ and ]); see RFC 2732<https://www.ietf.org/rfc/rfc2732.txt>).PORTTCP port.",
        "name": "lttng-relayd - LTTng 2 relay daemon",
        "section": 8
    },
    {
        "command": "lttng-sessiond",
        "description": "The Linux Trace Toolkit: next generation <https://lttng.org/> isan open source software package used for correlated tracing ofthe Linux kernel, user applications, and user libraries.LTTng consists of Linux kernel modules (for Linux kernel tracing)and dynamically loaded libraries (for user application andlibrary tracing).The LTTng session daemon is a tracing registry which allows theuser to interact with multiple tracers (kernel and user space)within the same container, a tracing session. Traces can begathered from the Linux kernel and/or from instrumentedapplications (see lttng-ust(3)). You can aggregate and read theevents of LTTng traces using babeltrace(1).To trace the Linux kernel, the session daemon needs to be runningas root. LTTng uses a tracing group to allow specific users tointeract with the root session daemon. The default tracing groupname is tracing. You can use the --group option to set thetracing group name to use.Session daemons can coexist. You can have a session daemonrunning as user Alice that can be used to trace her applicationsalongside a root session daemon or a session daemon running asuser Bob.The LTTng session daemon manages trace data consumer daemons byspawning them when necessary. You do not need to manage theconsumer daemons manually.NoteIt is highly recommended to start the session daemon at boottime for stable and long-term tracing.Automatic loading of tracing session configurationsWhen the session daemon starts, it automatically loads sessionconfiguration files.The following directories are searched, non-recursively, in thisorder for configuration files to load on launch:1. $LTTNG_HOME/.lttng/sessions/auto ($LTTNG_HOME defaults to$HOME)2. /usr/local/etc/lttng/sessions/autoNote that both the directory containing the tracing sessionconfigurations and the session daemon binary must share the sameUID for the configurations to be automatically loaded.The --load option overrides the default directories and the UIDcheck. The session daemon simply checks if the path is accessibleand tries to load every tracing session configuration in it. Whenthis option is specified, the default directories are NOTsearched for configuration files. When the option is notspecified, both default directories are searched forconfiguration files.If the --load option\u2019s argument is a directory, then all thetracing session configurations found in all the files in thisdirectory are loaded. If the argument is a file, then all thetracing session configurations found in this file are loaded.",
        "name": "lttng-sessiond - LTTng 2 tracing session daemon",
        "section": 8
    },
    {
        "command": "lvchange",
        "description": "lvchange changes LV attributes in the VG, changes LV activationin the kernel, and includes other utilities for LV maintenance.",
        "name": "lvchange \u2014 Change the attributes of logical volume(s)",
        "section": 8
    },
    {
        "command": "lvconvert",
        "description": "lvconvert changes the LV type and includes utilities for LV datamaintenance. The LV type controls data layout and redundancy.The LV type is also called the segment type or segtype.To display the current LV type, run the command:lvs -o name,segtype LVIn some cases, an LV is a single device mapper (dm) layer abovephysical devices.In other cases, hidden LVs (dm devices) arelayered between the visible LV and physical devices.LVs in themiddle layers are called sub LVs.A command run on a visible LVsometimes operates on a sub LV rather than the specified LV.Inother cases, a sub LV must be specified directly on the commandline.Sub LVs can be displayed with the command:lvs -aThe linear type is equivalent to the striped type when one stripeexists.In that case, the types can sometimes be usedinterchangeably.In most cases, the mirror type is deprecated and the raid1 typeshould be used.They are both implementations of mirroring.Striped raid types are raid0/raid0_meta, raid5 (an alias forraid5_ls), raid6 (an alias for raid6_zr) and raid10 (an alias forraid10_near).As opposed to mirroring, raid5 and raid6 stripe data andcalculate parity blocks. The parity blocks can be used for datablock recovery in case devices fail. A maximum number of onedevice in a raid5 LV may fail, and two in case of raid6. Stripedraid types typically rotate the parity and data blocks forperformance reasons, thus avoiding contention on a single device.Specific arrangements of parity and data blocks (layouts) can beused to optimize I/O performance, or to convert between raidlevels.See lvmraid(7) for more information.Layouts of raid5 rotating parity blocks can be: left-asymmetric(raid5_la), left-symmetric (raid5_ls with alias raid5), right-asymmetric (raid5_ra), right-symmetric (raid5_rs) and raid5_n,which doesn't rotate parity blocks. Layouts of raid6 are: zero-restart (raid6_zr with alias raid6), next-restart (raid6_nr), andnext-continue (raid6_nc).Layouts including _n allow for conversion between raid levels(raid5_n to raid6 or raid5_n to striped/raid0/raid0_meta).Additionally, special raid6 layouts for raid level conversionsbetween raid5 and raid6 are: raid6_ls_6, raid6_rs_6, raid6_la_6and raid6_ra_6. Those correspond to their raid5 counterparts(e.g. raid5_rs can be directly converted to raid6_rs_6 and vice-versa).raid10 (an alias for raid10_near) is currently limited to onedata copy and even number of sub LVs. This is a mirror grouplayout, thus a single sub LV may fail per mirror group withoutdata loss.Striped raid types support converting the layout, theirstripesize and their number of stripes.The striped raid types combined with raid1 allow for conversionfrom linear \u2192 striped/raid0/raid0_meta and vice-versa by e.g.linear \u2194 raid1 \u2194 raid5_n (then adding stripes) \u2194striped/raid0/raid0_meta.",
        "name": "lvconvert \u2014 Change logical volume layout",
        "section": 8
    },
    {
        "command": "lvcreate",
        "description": "lvcreate creates a new LV in a VG. For standard LVs, thisrequires allocating logical extents from the VG's free physicalextents. If there is not enough free space, the VG can beextended with other PVs (vgextend(8)), or existing LVs can bereduced or removed (lvremove(8), lvreduce(8)).To control which PVs a new LV will use, specify one or more PVsas position args at the end of the command line. lvcreate willallocate physical extents only from the specified PVs.lvcreate can also create snapshots of existing LVs, e.g. forbackup purposes. The data in a new snapshot LV represents thecontent of the original LV from the time the snapshot wascreated.RAID LVs can be created by specifying an LV type when creatingthe LV (see lvmraid(7)). Different RAID levels require differentnumbers of unique PVs be available in the VG for allocation.Thin pools (for thin provisioning) and cache pools (for caching)are represented by special LVs with types thin-pool andcache-pool (see lvmthin(7) and lvmcache(7)). The pool LVs are notusable as standard block devices, but the LV names act asreferences to the pools.Thin LVs are thinly provisioned from a thin pool, and are createdwith a virtual size rather than a physical size. A cache LV isthe combination of a standard LV with a cache pool, used to cacheactive portions of the LV to improve performance.VDO LVs are also provisioned volumes from a VDO pool, and arecreated with a virtual size rather than a physical size (seelvmvdo(7)).Usage notesIn the usage section below, --size Size can be replaced with--extents Number. See descriptions in the options section.In the usage section below, --name is omitted from the requiredoptions, even though it is typically used. When the name is notspecified, a new LV name is generated with the \"lvol\" prefix anda unique numeric suffix.In the usage section below, when creating a pool and the name isomitted the new LV pool name is generated with the \"vpool\" forvdo-poolsfor prefix and a unique numeric suffix.Pool name can be specified together with VG name i.e.:vg00/mythinpool.",
        "name": "lvcreate \u2014 Create a logical volume",
        "section": 8
    },
    {
        "command": "lvdisplay",
        "description": "lvdisplay shows the attributes of LVs, like size, read/writestatus, snapshot information, etc.lvs(8) is a preferred alternative that shows the same informationand more, using a more compact and configurable output format.",
        "name": "lvdisplay \u2014 Display information about a logical volume",
        "section": 8
    },
    {
        "command": "lvextend",
        "description": "lvextend extends the size of an LV. This requires allocatinglogical extents from the VG's free physical extents. If theextension adds a new LV segment, the new segment will use theexisting segment type of the LV.Extending a copy-on-write snapshot LV adds space for COW blocks.Use lvconvert(8) to change the number of data images in a RAID ormirrored LV.In the usage section below, --size Size can be replaced with--extents Number.See both descriptions the options section.",
        "name": "lvextend \u2014 Add space to a logical volume",
        "section": 8
    },
    {
        "command": "lvm",
        "description": "The Logical Volume Manager (LVM) provides tools to create virtualblock devices from physical devices.Virtual devices may beeasier to manage than physical devices, and can have capabilitiesbeyond what the physical devices provide themselves.A VolumeGroup (VG) is a collection of one or more physical devices, eachcalled a Physical Volume (PV).A Logical Volume (LV) is avirtual block device that can be used by the system orapplications.Each block of data in an LV is stored on one ormore PV in the VG, according to algorithms implemented by DeviceMapper (DM) in the kernel.The lvm command, and other commands listed below, are thecommand-line tools for LVM.A separate manual page describeseach command in detail.If lvm is invoked with no arguments it presents a readline prompt(assuming it was compiled with readline support).LVM commandsmay be entered interactively at this prompt with readlinefacilities including history and command name and optioncompletion.Refer to readline(3) for details.If lvm is invoked with argv[0] set to the name of a specific LVMcommand (for example by using a hard or soft link) it acts asthat command.On invocation, lvm requires that only the standard filedescriptors stdin, stdout and stderr are available.If othersare found, they get closed and messages are issued warning aboutthe leak.This warning can be suppressed by setting theenvironment variable LVM_SUPPRESS_FD_WARNINGS.Where commands take VG or LV names as arguments, the full pathname is optional.An LV called \"lvol0\" in a VG called \"vg0\" canbe specified as \"vg0/lvol0\".Where a list of VGs is required butis left empty, a list of all VGs will be substituted.Where alist of LVs is required but a VG is given, a list of all the LVsin that VG will be substituted.So lvdisplay vg0 will displayall the LVs in \"vg0\".Tags can also be used - see --addtagbelow.One advantage of using the built-in shell is that configurationinformation gets cached internally between commands.A file containing a simple script with one command per line canalso be given on the command line.The script can also beexecuted directly if the first line is #! followed by theabsolute path of lvm.Additional hyphens within option names are ignored.For example,--readonly and --read-only are both accepted.",
        "name": "lvm \u2014 LVM2 tools",
        "section": 8
    },
    {
        "command": "lvm-config",
        "description": "lvmconfig, lvm config, lvm dumpconfig (for compatibility reasons,to be phased out) produce formatted output from the LVMconfiguration tree. The sources of the configuration data includelvm.conf(5) and command line settings from --config.",
        "name": "lvmconfig \u2014 Display and manipulate configuration information",
        "section": 8
    },
    {
        "command": "lvm-dumpconfig",
        "description": "lvmconfig, lvm config, lvm dumpconfig (for compatibility reasons,to be phased out) produce formatted output from the LVMconfiguration tree. The sources of the configuration data includelvm.conf(5) and command line settings from --config.",
        "name": "lvmconfig \u2014 Display and manipulate configuration information",
        "section": 8
    },
    {
        "command": "lvm-fullreport",
        "description": "lvm fullreport produces formatted output about PVs, PV segments,VGs, LVs and LV segments. The information is all gatheredtogether for each VG (under a per-VG lock) so it is consistent.Information gathered from separate calls to vgs, pvs, and lvs canbe inconsistent if information changes between commands.",
        "name": "lvm fullreport \u2014 Display full report",
        "section": 8
    },
    {
        "command": "lvm-lvpoll",
        "description": "lvm lvpoll is an internal command used by lvmpolld(8) to monitorand complete lvconvert(8) and pvmove(8) operations. lvpoll itselfdoes not initiate these operations and should not normally needto be run directly.",
        "name": "lvm lvpoll \u2014 Continue already initiated poll operation on alogical volume",
        "section": 8
    },
    {
        "command": "lvm_import_vdo",
        "description": "lvm_import_vdo utility imports VDO volumes created and managed byvdo(8) manager into lvm2(8) managed VDO LV. This is realized bymoving VDO superblock by 2MiB and creating lvm2 metadata at thefront of this device. The operation is not reversible, thus afterconversion to lvm2 the access to VDO data is only possible withlvm2(8) commands, vdo(8) manager no longer control such volume.",
        "name": "lvm_import_vdo \u2014 utility to import VDO volumes into a new volumegroup.",
        "section": 8
    },
    {
        "command": "lvmconfig",
        "description": "lvmconfig, lvm config, lvm dumpconfig (for compatibility reasons,to be phased out) produce formatted output from the LVMconfiguration tree. The sources of the configuration data includelvm.conf(5) and command line settings from --config.",
        "name": "lvmconfig \u2014 Display and manipulate configuration information",
        "section": 8
    },
    {
        "command": "lvmdbusd",
        "description": "lvmdbusd is a service which provides a D-Bus API to the logicalvolume manager (LVM).Run lvmdbusd(8) as root.",
        "name": "lvmdbusd \u2014 LVM D-Bus daemon",
        "section": 8
    },
    {
        "command": "lvmdevices",
        "description": "The LVM devices file lists devices that lvm can use.The defaultfile is /etc/lvm/devices/system.devices, and the lvmdevices(8)command is used to add or remove device entries.If the filedoes not exist, or if lvm.conf includes use_devicesfile=0, thenlvm will not use a devices file.To use a device with lvm, add it to the devices file with thecommand lvmdevices --adddev, and to prevent lvm from seeing orusing a device, remove it from the devices file with lvmdevices--deldev.The vgimportdevices(8) command adds all PVs from a VGto the devices file, and updates the VG metadata to includedevice IDs of the PVs.Commands that add new devices to the devices file necessarilylook outside the existing devices file to find the devices beingadded.pvcreate, vgcreate, and vgextend also look outside thedevices file to create new PVs and add those PVs to the devicesfile.LVM records devices in the devices file using hardware-specificIDs, such as the WWID, and attempts to use subsystem-specific IDsfor virtual device types (which also aim to be as unique andstable as possible.) These device IDs are also written in the VGmetadata.When no hardware or virtual ID is available, lvm fallsback using the unstable device name as the device ID.Whendevnames are used as IDs, lvm performs extra scanning to finddevices if their devname changes, e.g. after reboot.When proper device IDs are used, an lvm command will not look atdevices outside the devices file, but when devnames are used as afallback, lvm will scan devices outside the devices file tolocate PVs on renamed devices.A config settingsearch_for_devnames can be used to control the scanning forrenamed devname entries.Related to the devices file, the new command option --devices<devnames> allows a list of devices to be specified for thecommand to use, overriding the devices file.The listed devicesact as a sort of devices file in terms of limiting which deviceslvm will see and use.Devices that are not listed will appear tobe missing to the lvm command.Multiple devices files can be kept /etc/lvm/devices, which allowslvm to be used with different sets of devices.For example,system devices do not need to be exposed to a specificapplication, and the application can use lvm on its own devicesthat are not exposed to the system.The option --devicesfile<filename> is used to select the devices file to use with thecommand.Without the option set, the default system devices fileis used.Setting --devicesfile \"\" causes lvm to not use a devices file.With no devices file, lvm will use any device on the system, andapplies the filter to limit the full set of system devices.Witha devices file, the regex filter is not used, and the filtersettings in lvm.conf or the command line are ignored.Thevgimportdevices command is one exception which does apply theregex filter when looking for a VG to import.If a devices file exists, lvm will use it, even if it's empty.An empty devices file means lvm will see no devices.If the system devices file does not yet exist, the pvcreate orvgcreate commands will create it if they see no existing VGs onthe system.lvmdevices --addev and vgimportdevices will alwayscreate a new devices file if it does not yet exist.It is recommended to use lvm commands to make changes to thedevices file to ensure proper updates.The device ID and device ID type are included in the VG metadataand can be reported with pvs -o deviceid,deviceidtype.(Notethat the lvmdevices command does not update VG metadata, butsubsequent lvm commands modifying the metadata will include thedevice ID.)Possible device ID types are:\u2022 sys_wwid uses the wwid reported by sysfs.This is the firstchoice for non-virtual devices.\u2022 sys_serial uses the serial number reported by sysfs.This isthe second choice for non-virtual devices.\u2022 mpath_uuid is used for dm multipath devices, reported by sysfs.\u2022 crypt_uuid is used for dm crypt devices, reported by sysfs.\u2022 md_uuid is used for md devices, reported by sysfs.\u2022 lvmlv_uuid is used if a PV is placed on top of an lvm LV,reported by sysfs.\u2022 loop_file is used for loop devices, the backing file namerepored by sysfs.\u2022 devname the device name is used if no other type applies.The default choice for device ID type can be overridden usinglvmdevices --addev --deviceidtype <type>.If the specified typeis available for the device it will be used, otherwise the devicewill be added using the type that would otherwise be chosen.",
        "name": "lvmdevices \u2014 Manage the devices file",
        "section": 8
    },
    {
        "command": "lvmdiskscan",
        "description": "lvmdiskscan scans all SCSI, (E)IDE disks, multiple devices and abunch of other block devices in the system looking for LVM PVs.The size reported is the real device size. Define a filter inlvm.conf(5) to restrict the scan to avoid a CD ROM, for example.This command is deprecated, use pvs instead.",
        "name": "lvmdiskscan \u2014 List devices that may be used as physical volumes",
        "section": 8
    },
    {
        "command": "lvmdump",
        "description": "lvmdump is a tool to dump various information concerning LVM2.By default, it creates a tarball suitable for submission alongwith a problem report.The content of the tarball is as follows:\u2022 dmsetup info\u2022 table of currently running processes\u2022 recent entries from /var/log/messages (containing systemmessages)\u2022 complete lvm configuration and cache (content of /etc/lvm)\u2022 list of device nodes present under /dev\u2022 list of files present /sys/block\u2022 list of files present /sys/devices/virtual/block\u2022 if enabled with -m, metadata dump will be also included\u2022 if enabled with -a, debug output of vgscan, pvscan and list ofall available volume groups, physical volumes and logicalvolumes will be included\u2022 if enabled with -l, lvmetad state if running\u2022 if enabled with -p, lvmpolld state if running\u2022 if enabled with -s, system info and context\u2022 if enabled with -u, udev info and context",
        "name": "lvmdump \u2014 create lvm2 information dumps for diagnostic purposes",
        "section": 8
    },
    {
        "command": "lvmlockctl",
        "description": "This command interacts with lvmlockd(8).",
        "name": "lvmlockctl \u2014 Control for lvmlockd",
        "section": 8
    },
    {
        "command": "lvmlockd",
        "description": "LVM commands use lvmlockd to coordinate access to shared storage.When LVM is used on devices shared by multiple hosts, locks will:\u2022 coordinate reading and writing of LVM metadata\u2022 validate caching of LVM metadata\u2022 prevent conflicting activation of logical volumeslvmlockd uses an external lock manager to perform basic locking.Lock manager (lock type) options are:\u2022 sanlock: places locks on disk within LVM storage.\u2022 dlm: uses network communication and a cluster manager.",
        "name": "lvmlockd \u2014 LVM locking daemon",
        "section": 8
    },
    {
        "command": "lvmpolld",
        "description": "lvmpolld is polling daemon for LVM. The daemon receives requestsfor polling of already initialised operations originating in LVM2command line tool.The requests for polling originate in thelvconvert, pvmove, lvchange or vgchange LVM2 commands.The purpose of lvmpolld is to reduce the number of spawnedbackground processes per otherwise unique polling operation.There should be only one. It also eliminates the possibility ofunsolicited termination of background process by externalfactors.lvmpolld is used by LVM only if it is enabled in lvm.conf(5) byspecifying the global/use_lvmpolld setting. If this is notdefined in the LVM configuration explicitly then default settingis used instead (see the output of lvmconfig --type defaultglobal/use_lvmpolld command).",
        "name": "lvmpolld \u2014 LVM poll daemon",
        "section": 8
    },
    {
        "command": "lvmsadc",
        "description": "lvmsadc is not supported under LVM2. The device-mapper statisticsfacility provides similar performance metrics using thedmstats(8) command.",
        "name": "lvmsadc \u2014 LVM system activity data collector",
        "section": 8
    },
    {
        "command": "lvmsar",
        "description": "lvmsar is not supported under LVM2. The device-mapper statisticsfacility provides similar performance metrics using thedmstats(8) command.",
        "name": "lvmsar \u2014 LVM system activity reporter",
        "section": 8
    },
    {
        "command": "lvpoll",
        "description": "lvm lvpoll is an internal command used by lvmpolld(8) to monitorand complete lvconvert(8) and pvmove(8) operations. lvpoll itselfdoes not initiate these operations and should not normally needto be run directly.",
        "name": "lvm lvpoll \u2014 Continue already initiated poll operation on alogical volume",
        "section": 8
    },
    {
        "command": "lvreduce",
        "description": "lvreduce reduces the size of an LV. The freed logical extents arereturned to the VG to be used by other LVs. A copy-on-writesnapshot LV can also be reduced if less space is needed to holdCOW blocks. Use lvconvert(8) to change the number of data imagesin a RAID or mirrored LV.Be careful when reducing an LV's size, because data in thereduced area is lost. Ensure that any file system on the LV isresized before running lvreduce so that the removed extents arenot in use by the file system.Sizes will be rounded if necessary. For example, the LV size mustbe an exact number of extents, and the size of a striped segmentmust be a multiple of the number of stripes.In the usage section below, --size Size can be replaced with--extents Number.See both descriptions the options section.",
        "name": "lvreduce \u2014 Reduce the size of a logical volume",
        "section": 8
    },
    {
        "command": "lvremove",
        "description": "lvremove removes one or more LVs. For standard LVs, this returnsthe logical extents that were used by the LV to the VG for use byother LVs.Confirmation will be requested before deactivating any active LVprior to removal.LVs cannot be deactivated or removed whilethey are open (e.g.if they contain a mounted filesystem).Removing an origin LV will also remove all dependent snapshots.When a single force option is used, LVs are removed withoutconfirmation, and the command will try to deactivate unused LVs.To remove damaged LVs, two force options may be required (-ff).Historical LVsIf the configuration setting metadata/record_lvs_history isenabled and the LV being removed forms part of the history of atleast one LV that is still present, then a simplifiedrepresentation of the LV will be retained. This includes the timeof removal (lv_time_removed reporting field), creation time(lv_time), name (lv_name), LV uuid (lv_uuid) and VG name(vg_name). This allows later reporting to see the ancestry chainof thin snapshot volumes, even after some intermediate LVs havebeen removed. The names of such historical LVs acquire a hyphenas a prefix (e.g. '-lvol1') and cannot be reactivated.Uselvremove a second time, with the hyphen, to remove the record ofthe former LV completely.",
        "name": "lvremove \u2014 Remove logical volume(s) from the system",
        "section": 8
    },
    {
        "command": "lvrename",
        "description": "lvrename renames an existing LV or a historical LV (see lvremovefor historical LV information.)",
        "name": "lvrename \u2014 Rename a logical volume",
        "section": 8
    },
    {
        "command": "lvresize",
        "description": "lvresize resizes an LV in the same way as lvextend and lvreduce.See lvextend(8) and lvreduce(8) for more information.In the usage section below, --size Size can be replaced with--extents Number.See both descriptions the options section.",
        "name": "lvresize \u2014 Resize a logical volume",
        "section": 8
    },
    {
        "command": "lvs",
        "description": "lvs produces formatted output about LVs.",
        "name": "lvs \u2014 Display information about logical volumes",
        "section": 8
    },
    {
        "command": "lvscan",
        "description": "lvscan scans all VGs or all supported LVM block devices in thesystem for LVs. The output consists of one line for each LVindicating whether or not it is active, a snapshot or origin, thesize of the device and its allocation policy. Use lvs(8) orlvdisplay(8) to obtain more comprehensive information about LVs.",
        "name": "lvscan \u2014 List all logical volumes in all volume groups",
        "section": 8
    },
    {
        "command": "mail.lmtp",
        "description": "mail.lmtp reads the standard input up to an end-of-file anddelivers it to an LMTP server for each user's address.The usermust be a valid user name or email address.The options are as follows:-d destinationSpecify the destination LMTP address.-f fromSpecify the sender's name or email address.-l lhloSpecify the LHLO argument used in the LMTP session.Bydefault, mail.lmtp will default to \"localhost\".",
        "name": "mail.lmtp \u2014 deliver mail through LMTP",
        "section": 8
    },
    {
        "command": "mail.maildir",
        "description": "mail.maildir reads the standard input up to an end-of-file and addsit to the mail directory located in pathname or to the maildirectory Maildir located in the user's home directory.The options are as follows:-jScan the message for an X-Spam header and move to the Junkfolder if the result is positive.",
        "name": "mail.maildir \u2014 store mail in a maildir",
        "section": 8
    },
    {
        "command": "mail.mboxfile",
        "description": "mail.mboxfile appends mail to a file in mbox format andacknowledges delivery success or failure with its exit status.",
        "name": "mail.mboxfile \u2014 deliver mail to a file in mbox format",
        "section": 8
    },
    {
        "command": "mail.mda",
        "description": "mail.mda executes the program and its parameters.The program mustread from the standard input up to an end-of-file and acknowledgedelivery success or failure with its exit status.",
        "name": "mail.mda \u2014 deliver mail to a program",
        "section": 8
    },
    {
        "command": "mailq",
        "description": "The smtpctl program controls smtpd(8).Commands may be abbreviatedto the minimum unambiguous prefix; for example, sh ro for showroutes.The mailq command is provided for compatibility with other MTAs andis simply a shortcut for show queue.The following commands are available:discover envelope-id | message-idSchedule a single envelope, or all envelopes with the samemessage ID that were manually moved to the queue.encrypt [string]Encrypt the password string to a representation suitablefor user credentials and print it to the standard output.If string is not provided, cleartext passwords are readfrom standard input.It is advised to avoid providing the password as aparameter as it will be visible from top(1) and ps(1)output.log briefDisable verbose debug logging.log verboseEnable verbose debug logging.monitorDisplay updates of some smtpd(8) internal counters in onesecond intervals.Each line reports the increment of allcounters since the last update, except for some counterswhich are always absolute values.The first line reportsthe current value of each counter.The fields are:\u2022Current number of active SMTP clients (absolute value).\u2022New SMTP clients.\u2022Disconnected clients.\u2022Current number of envelopes in the queue (absolutevalue).\u2022Newly enqueued envelopes.\u2022Dequeued envelopes.\u2022Successful deliveries.\u2022Temporary failures.\u2022Permanent failures.\u2022Message loops.\u2022Expired envelopes.\u2022Envelopes removed by the administrator.\u2022Generated bounces.pause envelope envelope-id | message-id | allTemporarily suspend scheduling for the envelope with thegiven ID, envelopes with the given message ID, or allenvelopes.pause mdaTemporarily stop deliveries to local users.pause mtaTemporarily stop relaying and deliveries to remote users.pause smtpTemporarily stop accepting incoming sessions.profile subsystemEnables real-time profiling of subsystem.Supportedsubsystems are:\u2022queue, to profile cost of queue IO\u2022imsg, to profile cost of event handlersremove envelope-id | message-id | allRemove a single envelope, envelopes with the given messageID, or all envelopes.resume envelope envelope-id | message-id | allResume scheduling for the envelope with the given ID,envelopes with the given message ID, or all envelopes.resume mdaResume deliveries to local users.resume mtaResume relaying and deliveries to remote users.resume route route-idResume routing on disabled route route-id.resume smtpResume accepting incoming sessions.schedule envelope-id | message-id | allMark as ready for immediate delivery a single envelope,envelopes with the given message ID, or all envelopes.show envelope envelope-idDisplay envelope content for the given ID.show hostsDisplay the list of known remote MX hosts.For each ofthem, it shows the IP address, the canonical hostname, areference count, the number of active connections to thishost, and the elapsed time since the last connection.show hoststatsDisplay status of last delivery for domains that have beenactive in the last 4 hours.It consists of the followingfields, separated by a \"|\":\u2022Domain.\u2022UNIX timestamp of last delivery.\u2022Status of last delivery.show message envelope-idDisplay message content for the given ID.show queueDisplay information concerning envelopes that are currentlyin the queue.Each line of output describes a singleenvelope.It consists of the following fields, separatedby a \"|\":\u2022Envelope ID.\u2022Address family of the client which enqueued the mail.\u2022Type of delivery: one of \"mta\", \"mda\" or \"bounce\".\u2022Various flags on the envelope.\u2022Sender address (return path).\u2022The original recipient address.\u2022The destination address.\u2022Time of creation.\u2022Time of expiration.\u2022Time of last delivery or relaying attempt.\u2022Number of delivery or relaying attempts.\u2022Current runstate: either \"pending\" or \"inflight\" ifsmtpd(8) is running, or \"offline\" otherwise.\u2022Delay in seconds before the next attempt if pending, ortime elapsed if currently running.This field is blankif smtpd(8) is not running.\u2022Error string for the last failed delivery or relayattempt.show relaysDisplay the list of currently active relays and associatedconnectors.For each relay, it shows a number of countersand information on its internal state on a single line.Then comes the list of connectors (source addresses toconnect from for this relay).show routesDisplay status of routes currently known by smtpd(8).Eachline consists of a route number, a source address, adestination address, a set of flags, the number ofconnections on this route, the current penalty level whichdetermines the amount of time the route is disabled if anerror occurs, and the delay before it gets reactivated.The following flags are defined:DThe route is currently disabled.NThe route is new.No SMTP session has been establishedyet.QThe route has a timeout registered to lower its penaltylevel and possibly reactivate or discard it.show statsDisplays runtime statistics concerning smtpd(8).show statusShows if MTA, MDA and SMTP systems are currently running orpaused.spf walkRecursively look up SPF records for the domains read fromstdin.For example:$ smtpctl spf walk < domains.txtSPF records may contain macros which cannot be included ina static list and must be resolved dynamically atconnection time.spf walk cannot provide full results inthese cases.trace subsystemEnables real-time tracing of subsystem.Supportedsubsystems are:\u2022imsg\u2022io\u2022smtp (incoming sessions)\u2022filters\u2022mta (outgoing sessions)\u2022bounce\u2022scheduler\u2022expand (aliases/virtual/forward expansion)\u2022lookup (user/credentials lookups)\u2022stat\u2022rules (matched by incoming sessions)\u2022mproc\u2022allunprofile subsystemDisables real-time profiling of subsystem.untrace subsystemDisables real-time tracing of subsystem.update table nameUpdates the contents of table name, for tables using the\u201cfile\u201d backend.When smtpd receives a message, it generates a message-id for themessage, and one envelope-id per recipient.The message-id is a32-bit random identifier that is guaranteed to be unique on thehost system.The envelope-id is a 64-bit unique identifier thatencodes the message-id in the 32 upper bits and a random envelopeidentifier in the 32 lower bits.A command which specifies a message-id applies to all recipients ofa message; a command which specifies an envelope-id applies to aspecific recipient of a message.",
        "name": "smtpctl, mailq \u2014 control the SMTP daemon",
        "section": 8
    },
    {
        "command": "makemap",
        "description": "Maps provide a generic interface for associating a textual key to avalue.Such associations may be accessed through a plaintext file,database, or DNS.The format of these file types is describedbelow.makemap itself creates the database maps used by keyed maplookups specified in smtpd.conf(5).makemap reads input from file and writes data to a file which isnamed by adding a \u201c.db\u201d suffix to file.The current line can beextended over multiple lines using a backslash (\u2018\\\u2019).Comments canbe put anywhere in the file using a hash mark (\u2018#\u2019), and extend tothe end of the current line.Care should be taken when commentingout multi-line text: the comment is effective until the end of theentire block.In all cases, makemap reads lines consisting ofwords separated by whitespace.The first word of a line is thedatabase key; the remainder represents the mapped value.Thedatabase key and value may optionally be separated by the coloncharacter.The options are as follows:-d dbtypeSpecify the format of the database.Available formats arehash and btree.The default value is hash.-o dbfileWrite the generated database to dbfile.-t typeSpecify the format of the resulting map file.The defaultmap format is suitable for storing simple, unstructured,key-to-value string associations.However, if the mappedvalue has special meaning, as in the case of a virtualdomains file, a suitable type must be provided.Theavailable output types are:aliasesThe mapped value is a comma-separated list of maildestinations.This format can be used forbuilding user aliases and user mappings forvirtual domain files.setThere is no mapped value \u2013 a map of this type willonly allow for the lookup of keys.This formatcan be used for building primary domain maps.-UInstead of generating a database map from text input, dumpthe contents of a database map as text with the key andvalue separated with a tab.",
        "name": "makemap \u2014 create database maps for smtpd",
        "section": 8
    },
    {
        "command": "mandb",
        "description": "mandb is used to initialise or manually update index databasecaches.The caches contain information relevant to the currentstate of the manual page system and the information stored withinthem is used by the man-db utilities to enhance their speed andfunctionality.When creating or updating an index, mandb will warn of bad ROFF.so requests, bogus manual page filenames and manual pages fromwhich the whatis cannot be parsed.Supplying mandb with an optional colon-delimited path willoverride the internal system manual page hierarchy search path,determined from information found within the man-db configurationfile.",
        "name": "mandb - create or update the manual page index caches",
        "section": 8
    },
    {
        "command": "mapscrn",
        "description": "The mapscrn command is obsolete - its function is now built-ininto setfont.However, for backwards compatibility it is stillavailable as a separate command.The mapscrn command loads a user defined output character mappingtable into the console driver. The console driver may be laterput into use user-defined mapping table mode by outputting aspecial escape sequence to the console device.This sequence is<esc>(K for the G0 character set and <esc>)K for the G1 characterset.When the -o option is given, the old map is saved inmap.orig.",
        "name": "mapscrn - load screen output mapping table",
        "section": 8
    },
    {
        "command": "mariadbd",
        "description": "mysqld, also known as MariaDB Server, is the main program thatdoes most of the work in a MariaDB installation. MariaDB Servermanages access to the MariaDB data directory that containsdatabases and tables. The data directory is also the defaultlocation for other information such as log files and statusfiles.When MariaDB server starts, it listens for network connectionsfrom client programs and manages access to databases on behalf ofthose clients.The mysqld program has many options that can be specified atstartup. For a complete list of options, run this command:shell> mysqld --verbose --helpMariaDB Server also has a set of system variables that affect itsoperation as it runs. System variables can be set at serverstartup, and many of them can be changed at runtime to effectdynamic server reconfiguration. MariaDB Server also has a set ofstatus variables that provide information about its operation.You can monitor these status variables to access runtimeperformance characteristics.For a full description of MariaDB Server command options, systemvariables, and status variables, see the MariaDB Knowledge Base.",
        "name": "mariadbd - the MariaDB server (mysqld is now a symlink tomariadbd)",
        "section": 8
    },
    {
        "command": "matchall",
        "description": "The matchall filter allows one to classify every packet thatflows on the port and run a action on it.",
        "name": "matchall - traffic control filter that matches every packet",
        "section": 8
    },
    {
        "command": "matchpathcon",
        "description": "matchpathcon queries the system policy and outputs the defaultsecurity context associated with the filepath.Note: Identical paths can have different security contexts,depending on the file type (regular file, directory, link file,char file ...).matchpathcon will also take the file type into consideration indetermining the default security context if the file exists.Ifthe file does not exist, no file type matching will occur.",
        "name": "matchpathcon - get the default SELinux security context for thespecified path from the file contexts configuration",
        "section": 8
    },
    {
        "command": "mausezahn",
        "description": "mausezahn is a fast traffic generator which allows you to sendnearly every possible and impossible packet. In contrast totrafgen(8), mausezahn's packet configuration is on a protocol-level instead of byte-level and mausezahn also comes with abuilt-in Cisco-like command-line interface, making it suitable asa network traffic generator box in your network lab.Next to network labs, it can also be used as a didactical tooland for security audits including penetration and DoS testing. Asa traffic generator, mausezahn is also able to test IP multicastor VoIP networks. Packet rates close to the physical limit arereachable, depending on the hardware platform.mausezahn supports two modes, ''direct mode'' and a multi-threaded ''interactive mode''.The ''direct mode'' allows you to create a packet directly on thecommand line and every packet parameter is specified in theargument list when calling mausezahn.The ''interactive mode'' is an advanced multi-threadedconfiguration mode with its own command line interface (CLI).This mode allows you to create an arbitrary number of packettypes and streams in parallel, each with different parameters.The interactive mode utilizes a completely redesigned and moreflexible protocol framework called ''mops'' (mausezahn's ownpacket system). The look and feel of the CLI is very close to theCisco IOS^tm command line interface.You can start the interactive mode by executing mausezahn withthe ''-x'' argument (an optional port number may follow,otherwise it is 25542). Then use telnet(1) to connect to thismausezahn instance. If not otherwise specified, the default loginand password combination is mz:mz and the enable password is:mops.This can be changed in /etc/netsniff-ng/mausezahn.conf.The direct mode supports two specification schemes: The ''raw-layer-2'' scheme, where every single byte to be sent can bespecified, and ''higher-layer'' scheme, where packet builderinterfaces are used (using the ''-t'' option).To use the ''raw-layer-2'' scheme, simply specify the desiredframe as a hexadecimal sequence (the ''hex-string''), such as:mausezahn eth0 \"00:ab:cd:ef:00 00:00:00:00:00:01 08:00ca:fe:ba:be\"In this example, whitespaces within the byte string are optionaland separate the Ethernet fields (destination and source address,type field, and a short payload). The only additional optionssupported are ''-a'', ''-b'', ''-c'', and ''-p''. The framelength must be greater than or equal to 15 bytes.The ''higher-layer'' scheme is enabled using the ''-t <packet-type>'' option.This option activates a packet builder, andbesides the ''packet-type'', an optional ''arg-string'' can bespecified. The ''arg-string'' contains packet- specificparameters, such as TCP flags, port numbers, etc. (see examplesection).",
        "name": "mausezahn - a fast versatile packet generator with Cisco-cli",
        "section": 8
    },
    {
        "command": "mcs",
        "description": "MCS (Multiple Category System) allows users to label files ontheir system within administrator defined categories.It thenuses SELinux Mandatory Access Control to protect those files.MCS is a discretionary model to allow users to mark their datawith additional tags that further restrict access.The onlymandatory aspect is authorizing users for categories by definingtheir clearance in policy.However, MCS is similar to MLS andexercises the same code paths and share the same supportinfrastructure.They just differ in their specificconfiguration.The /etc/selinux/{SELINUXTYPE}/setrans.conf configuration filetranslates the labels on disk to human readable form.Administrators can define any labels they want in this file.Certain applications like printing and auditing will use theselabels to identify the files.By setting a category on a fileyou will prevent other applications/services from having accessto the files.Examples of file labels would be PatientRecord,CompanyConfidential etc.",
        "name": "mcs - Multi-Category System",
        "section": 8
    },
    {
        "command": "mcstransd",
        "description": "This manual page describes the mcstransd program.This daemon reads /etc/selinux/{SELINUXTYPE}/setrans.confconfiguration file, and communicates with libselinux via a socketin /var/run/setrans.",
        "name": "mcstransd - MCS (Multiple Category System) daemon.TranslatesSELinux MCS/MLS labels to human readable form.",
        "section": 8
    },
    {
        "command": "mdadm",
        "description": "RAID devices are virtual devices created from two or more realblock devices.This allows multiple devices (typically diskdrives or partitions thereof) to be combined into a single deviceto hold (for example) a single filesystem.Some RAID levelsinclude redundancy and so can survive some degree of devicefailure.Linux Software RAID devices are implemented through the md(Multiple Devices) device driver.Currently, Linux supports LINEAR md devices, RAID0 (striping),RAID1 (mirroring), RAID4, RAID5, RAID6, RAID10, MULTIPATH,FAULTY, and CONTAINER.MULTIPATH is not a Software RAID mechanism, but does involvemultiple devices: each device is a path to one common physicalstorage device.New installations should not use md/multipath asit is not well supported and has no ongoing development.Use theDevice Mapper based multipath-tools instead.FAULTY is also not true RAID, and it only involves one device.It provides a layer over a true device that can be used to injectfaults.CONTAINER is different again.A CONTAINER is a collection ofdevices that are managed as a set.This is similar to the set ofdevices connected to a hardware RAID controller.The set ofdevices may contain a number of different RAID arrays eachutilising some (or all) of the blocks from a number of thedevices in the set.For example, two devices in a 5-device setmight form a RAID1 using the whole devices.The remaining threemight have a RAID5 over the first half of each device, and aRAID0 over the second half.With a CONTAINER, there is one set of metadata that describes allof the arrays in the container.So when mdadm creates aCONTAINER device, the device just represents the metadata.Othernormal arrays (RAID1 etc) can be created inside the container.",
        "name": "mdadm - manage MD devices aka Linux Software RAID",
        "section": 8
    },
    {
        "command": "mdmon",
        "description": "Metadata updates:To service metadata update requests a daemon, mdmon, isintroduced.Mdmon is tasked with polling the sysfs namespacelooking for changes in array_state, sync_action, and per diskstate attributes.When a change is detected it calls a permetadata type handler to make modifications to the metadata.Thefollowing actions are taken:array_state - inactiveClear the dirty bit for the volume and let thearray be stoppedarray_state - write pendingSet the dirty bit for the array and then setarray_state to active.Writes are blocked untiluserspace writes active.array_state - active-idleThe safe mode timer has expired so set array stateto clean to block writes to the arrayarray_state - cleanClear the dirty bit for the volumearray_state - read-onlyThis is the initial state that all arrays start at.mdmon takes one of the three actions:1/Transition the array to read-auto keepingthe dirty bit clear if the metadata handlerdetermines that the array does not needresyncing or other modification2/Transition the array to active if themetadata handler determines a resync or someother manipulation is necessary3/Leave the array read-only if the volume ismarked to not be monitored; for example, themetadata version has been set to\"external:-dev/md127\" instead of\"external:/dev/md127\"sync_action - resync-to-idleNotify the metadata handler that a resync may havecompleted.If a resync process is idled before itcompletes this event allows the metadata handler tocheckpoint resync.sync_action - recover-to-idleA spare may have completed rebuilding so tell themetadata handler about the state of each disk.This is the metadata handler's opportunity to clearany \"out-of-sync\" bits and clear the volume'sdegraded status.If a recovery process is idledbefore it completes this event allows the metadatahandler to checkpoint recovery.<disk>/state - faultyA disk failure kicks off a series of events.First, notify the metadata handler that a disk hasfailed, and then notify the kernel that it canunblock writes that were dependent on this disk.After unblocking the kernel this disk is set to beremoved+ from the member array.Finally the diskis marked failed in all other member arrays in thecontainer.+ Note This behavior differs slightly from nativeMD arrays where removal is reserved for a mdadm--remove event.In the external metadata case thecontainer holds the final reference on a blockdevice and a mdadm --remove <container> <victim>call is still required.Containers:External metadata formats, like DDF, differ from the native MDmetadata formats in that they define a set of disks and a seriesof sub-arrays within those disks.MD metadata in comparisondefines a 1:1 relationship between a set of block devices and aRAID array.For example to create 2 arrays at different RAIDlevels on a single set of disks, MD metadata requires the disksbe partitioned and then each array can be created with a subsetof those partitions.The supported external formats perform thisdisk carving internally.Container devices simply hold references to all member disks andallow tools like mdmon to determine which active arrays belong towhich container.Some array management commands like diskremoval and disk add are now only valid at the container level.Attempts to perform these actions on member arrays are blockedwith error messages like:\"mdadm: Cannot remove disks from a \u00b4member\u00b4 array, performthis operation on the parent container\"Containers are identified in /proc/mdstat with a metadata versionstring \"external:<metadata name>\". Member devices are identifiedby \"external:/<container device>/<member index>\", or\"external:-<container device>/<member index>\" if the array is toremain readonly.",
        "name": "mdmon - monitor MD external metadata arrays",
        "section": 8
    },
    {
        "command": "memhog",
        "description": "memhog mmaps a memory region for a given size and sets the numapolicy (if specified).It then updates the memory region for thegiven number of iterations using memset.-r<num>Repeat memset NUM times-f<file>Open file for mmap backing-HDisable transparent hugepages-sizeAllocation size in bytes, may have case-insensitive ordersuffix (G=gigabyte, M=megabyte, K=kilobyte)Supported numa-policies:interleaveMemory will be allocated using round robin on nodes. Whenmemory cannot be allocated on the current interleave,target fall back to other nodes.Multiple nodes may bespecified.membindOnlyallocatememoryfromnodes. Allocation will failwhen there is not enough memory available on these nodes.Multiple nodesmay be specified.preferredPreferably allocate memory on node, but if memory cannotbe allocatedtherefallbackto other nodes.Thisoption takes only a single node number.defaultMemory will be allocated on the local node (the node thethread is running on)",
        "name": "memhog - Allocates memory with policy for testing",
        "section": 8
    },
    {
        "command": "migratepages",
        "description": "migratepages moves the physical location of a processes pageswithout any changes of the virtual address space of the process.Moving the pages allows one to change the distances of a processto its memory. Performance may be optimized by moving a processespages to the node where it is executing.If multiple nodes are specified for from-nodes or to-nodes thenan attempt is made to preserve the relative location of each pagein each nodeset.For example if we move from nodes 2-5 to 7,9,12-13 then thepreferred mode of operation is to move pages from 2->7, 3->9,4->12 and 5->13. However, this is only posssible if enough memoryis available.Valid node specifiersallAll nodesnumberNode numbernumber1{,number2}Node number1 and Node number2number1-number2Nodes from number1 to number2! nodesInvert selection of the following specification.",
        "name": "migratepages - Migrate the physical location a processes pages",
        "section": 8
    },
    {
        "command": "migspeed",
        "description": "migspeed attempts to move a sample of pages from the indicatednode to the target node and measures the time it takes to performthe move.-p pagesThe default sample is 1000 pages. Override that with anothernumber.",
        "name": "migspeed - Test the speed of page migration",
        "section": 8
    },
    {
        "command": "mii-tool",
        "description": "This utility checks or sets the status of a network interface'sMedia Independent Interface (MII) unit.Most fast ethernetadapters use an MII to autonegotiate link speed and duplexsetting.Most intelligent network devices use an autonegotiation protocolto communicate what media technologies they support, and thenselect the fastest mutually supported media technology.The -Aor --advertise options can be used to tell the MII to onlyadvertise a subset of its capabilities.Some passive devices,such as single-speed hubs, are unable to autonegotiate.Tohandle such devices, the MII protocol also allows forestablishing a link by simply detecting either a 10baseT or100baseT link beat.The -F or --force options can be used toforce the MII to operate in one mode, instead of autonegotiating.The -A and -F options are mutually exclusive.The default short output reports the negotiated link speed andlink status for each interface.",
        "name": "mii-tool - view, manipulate media-independent interface status",
        "section": 8
    },
    {
        "command": "mirred",
        "description": "The mirred action allows packet mirroring (copying) orredirecting (stealing) the packet it receives. Mirroring is whatis sometimes referred to as Switch Port Analyzer (SPAN) and iscommonly used to analyze and/or debug flows.",
        "name": "mirred - mirror/redirect action",
        "section": 8
    },
    {
        "command": "mkdosfs",
        "description": "mkfs.fat is used to create a FAT filesystem on a device or in animage file.DEVICE is the special file corresponding to thedevice (e.g. /dev/sdXX) or the image file (which does not need toexist when the option -C is given).BLOCK-COUNT is the number ofblocks on the device and size of one block is always 1024 bytes,independently of the sector size or the cluster size.ThereforeBLOCK-COUNT specifies size of filesystem in KiB unit and not inthe number of sectors (like for all other mkfs.fat options).Ifomitted, mkfs.fat automatically chooses a filesystem size to fillthe available space.Two different variants of the FAT filesystem are supported.Standard is the FAT12, FAT16 and FAT32 filesystems as defined byMicrosoft and widely used on hard disks and removable media likeUSB sticks and SD cards.The other is the legacy Atari variantused on Atari ST.In Atari mode, if not directed otherwise by the user, mkfs.fatwill always use 2 sectors per cluster, since GEMDOS doesn't likeother values very much.It will also obey the maximum number ofsectors GEMDOS can handle.Larger filesystems are managed byraising the logical sector size.An Atari-compatible serialnumber for the filesystem is generated, and a 12 bit FAT is usedonly for filesystems that have one of the usual floppy sizes(720k, 1.2M, 1.44M, 2.88M), a 16 bit FAT otherwise.This can beoverridden with the -F option.Some PC-specific boot sectorfields aren't written, and a boot message (option -m) is ignored.",
        "name": "mkfs.fat - create an MS-DOS FAT filesystem",
        "section": 8
    },
    {
        "command": "mke2fs",
        "description": "mke2fs is used to create an ext2, ext3, or ext4 file system,usually in a disk partition (or file) named by device.The file system size is specified by fs-size.If fs-size doesnot have a suffix, it is interpreted as power-of-two kilobytes,unless the -b blocksize option is specified, in which case fs-size is interpreted as the number of blocksize blocks.If thefs-size is suffixed by 'k', 'm', 'g', 't' (either upper-case orlower-case), then it is interpreted in power-of-two kilobytes,megabytes, gigabytes, terabytes, etc.If fs-size is omitted,mke2fs will create the file system based on the device size.If mke2fs is run as mkfs.XXX (i.e., mkfs.ext2, mkfs.ext3, ormkfs.ext4) the option -t XXX is implied; so mkfs.ext3 will createa file system for use with ext3, mkfs.ext4 will create a filesystem for use with ext4, and so on.The defaults of the parameters for the newly created file system,if not overridden by the options listed below, are controlled bythe /etc/mke2fs.conf configuration file.See the mke2fs.conf(5)manual page for more details.",
        "name": "mke2fs - create an ext2/ext3/ext4 file system",
        "section": 8
    },
    {
        "command": "mkfs",
        "description": "This mkfs frontend is deprecated in favour of filesystem specificmkfs.<type> utils.mkfs is used to build a Linux filesystem on a device, usually ahard disk partition. The device argument is either the devicename (e.g., /dev/hda1, /dev/sdb2), or a regular file that shallcontain the filesystem. The size argument is the number of blocksto be used for the filesystem.The exit status returned by mkfs is 0 on success and 1 onfailure.In actuality, mkfs is simply a front-end for the variousfilesystem builders (mkfs.fstype) available under Linux. Thefilesystem-specific builder is searched for via your PATHenvironment setting only. Please see the filesystem-specificbuilder manual pages for further details.",
        "name": "mkfs - build a Linux filesystem",
        "section": 8
    },
    {
        "command": "mkfs.bfs",
        "description": "mkfs.bfs creates an SCO bfs filesystem on a block device (usuallya disk partition or a file accessed via the loop device).The block-count parameter is the desired size of the filesystem,in blocks. If nothing is specified, the entire partition will beused.",
        "name": "mkfs.bfs - make an SCO bfs filesystem",
        "section": 8
    },
    {
        "command": "mkfs.btrfs",
        "description": "mkfs.btrfs is used to create the btrfs filesystem on a single ormultiple devices. <device> is typically a block device but can bea file-backed image as well. Multiple devices are grouped by UUIDof the filesystem.Before mounting such filesystem, the kernel module must know allthe devices either via preceding execution of btrfs device scanor using the device mount option. See section MULTIPLE DEVICESfor more details.The default block group profiles for data and metadata depend onnumber of devices and possibly other factors. It\u2019s recommended touse specific profiles but the defaults should be OK and allowingfuture conversions to other profiles. Please see options -d and-m for further detals and btrfs-balance(8) for the profileconversion post mkfs.",
        "name": "mkfs.btrfs - create a btrfs filesystem",
        "section": 8
    },
    {
        "command": "mkfs.cramfs",
        "description": "Files on cramfs file systems are zlib-compressed one page at atime to allow random read access. The metadata is not compressed,but is expressed in a terse representation that is morespace-efficient than conventional file systems.The file system is intentionally read-only to simplify itsdesign; random write access for compressed files is difficult toimplement. cramfs ships with a utility (mkcramfs(8)) to packfiles into new cramfs images.File sizes are limited to less than 16 MB.Maximum file system size is a little under 272 MB. (The last fileon the file system must begin before the 256 MB block, but canextend past it.)",
        "name": "mkfs.cramfs - make compressed ROM file system",
        "section": 8
    },
    {
        "command": "mkfs.fat",
        "description": "mkfs.fat is used to create a FAT filesystem on a device or in animage file.DEVICE is the special file corresponding to thedevice (e.g. /dev/sdXX) or the image file (which does not need toexist when the option -C is given).BLOCK-COUNT is the number ofblocks on the device and size of one block is always 1024 bytes,independently of the sector size or the cluster size.ThereforeBLOCK-COUNT specifies size of filesystem in KiB unit and not inthe number of sectors (like for all other mkfs.fat options).Ifomitted, mkfs.fat automatically chooses a filesystem size to fillthe available space.Two different variants of the FAT filesystem are supported.Standard is the FAT12, FAT16 and FAT32 filesystems as defined byMicrosoft and widely used on hard disks and removable media likeUSB sticks and SD cards.The other is the legacy Atari variantused on Atari ST.In Atari mode, if not directed otherwise by the user, mkfs.fatwill always use 2 sectors per cluster, since GEMDOS doesn't likeother values very much.It will also obey the maximum number ofsectors GEMDOS can handle.Larger filesystems are managed byraising the logical sector size.An Atari-compatible serialnumber for the filesystem is generated, and a 12 bit FAT is usedonly for filesystems that have one of the usual floppy sizes(720k, 1.2M, 1.44M, 2.88M), a 16 bit FAT otherwise.This can beoverridden with the -F option.Some PC-specific boot sectorfields aren't written, and a boot message (option -m) is ignored.",
        "name": "mkfs.fat - create an MS-DOS FAT filesystem",
        "section": 8
    },
    {
        "command": "mkfs.minix",
        "description": "mkfs.minix creates a Linux MINIX filesystem on a device (usuallya disk partition).The device is usually of the following form:/dev/hda[1-8] (IDE disk 1)/dev/hdb[1-8] (IDE disk 2)/dev/sda[1-8] (SCSI disk 1)/dev/sdb[1-8] (SCSI disk 2)The device may be a block device or an image file of one, butthis is not enforced. Expect not much fun on a character device:-).The size-in-blocks parameter is the desired size of the filesystem, in blocks. It is present only for backwardscompatibility. If omitted the size will be determinedautomatically. Only block counts strictly greater than 10 andstrictly less than 65536 are allowed.",
        "name": "mkfs.minix - make a Minix filesystem",
        "section": 8
    },
    {
        "command": "mkfs.msdos",
        "description": "mkfs.fat is used to create a FAT filesystem on a device or in animage file.DEVICE is the special file corresponding to thedevice (e.g. /dev/sdXX) or the image file (which does not need toexist when the option -C is given).BLOCK-COUNT is the number ofblocks on the device and size of one block is always 1024 bytes,independently of the sector size or the cluster size.ThereforeBLOCK-COUNT specifies size of filesystem in KiB unit and not inthe number of sectors (like for all other mkfs.fat options).Ifomitted, mkfs.fat automatically chooses a filesystem size to fillthe available space.Two different variants of the FAT filesystem are supported.Standard is the FAT12, FAT16 and FAT32 filesystems as defined byMicrosoft and widely used on hard disks and removable media likeUSB sticks and SD cards.The other is the legacy Atari variantused on Atari ST.In Atari mode, if not directed otherwise by the user, mkfs.fatwill always use 2 sectors per cluster, since GEMDOS doesn't likeother values very much.It will also obey the maximum number ofsectors GEMDOS can handle.Larger filesystems are managed byraising the logical sector size.An Atari-compatible serialnumber for the filesystem is generated, and a 12 bit FAT is usedonly for filesystems that have one of the usual floppy sizes(720k, 1.2M, 1.44M, 2.88M), a 16 bit FAT otherwise.This can beoverridden with the -F option.Some PC-specific boot sectorfields aren't written, and a boot message (option -m) is ignored.",
        "name": "mkfs.fat - create an MS-DOS FAT filesystem",
        "section": 8
    },
    {
        "command": "mkfs.udf",
        "description": "mkudffs is used to create a UDF filesystem on a device (usually adisk). device is the special file corresponding to the device(e.g. /dev/hdX) or file image. blocks-count is the number ofblocks on the device. If omitted, mkudffs automagically figuresthe filesystem size. The order of options matters. Encodingoption must be first and options to override default settingsimplied by the media type or UDF revision should be after theoption they are overriding.",
        "name": "mkudffs \u2014 create a UDF filesystem",
        "section": 8
    },
    {
        "command": "mkfs.vfat",
        "description": "mkfs.fat is used to create a FAT filesystem on a device or in animage file.DEVICE is the special file corresponding to thedevice (e.g. /dev/sdXX) or the image file (which does not need toexist when the option -C is given).BLOCK-COUNT is the number ofblocks on the device and size of one block is always 1024 bytes,independently of the sector size or the cluster size.ThereforeBLOCK-COUNT specifies size of filesystem in KiB unit and not inthe number of sectors (like for all other mkfs.fat options).Ifomitted, mkfs.fat automatically chooses a filesystem size to fillthe available space.Two different variants of the FAT filesystem are supported.Standard is the FAT12, FAT16 and FAT32 filesystems as defined byMicrosoft and widely used on hard disks and removable media likeUSB sticks and SD cards.The other is the legacy Atari variantused on Atari ST.In Atari mode, if not directed otherwise by the user, mkfs.fatwill always use 2 sectors per cluster, since GEMDOS doesn't likeother values very much.It will also obey the maximum number ofsectors GEMDOS can handle.Larger filesystems are managed byraising the logical sector size.An Atari-compatible serialnumber for the filesystem is generated, and a 12 bit FAT is usedonly for filesystems that have one of the usual floppy sizes(720k, 1.2M, 1.44M, 2.88M), a 16 bit FAT otherwise.This can beoverridden with the -F option.Some PC-specific boot sectorfields aren't written, and a boot message (option -m) is ignored.",
        "name": "mkfs.fat - create an MS-DOS FAT filesystem",
        "section": 8
    },
    {
        "command": "mkhomedir_helper",
        "description": "mkhomedir_helper is a helper program for the pam_mkhomedir modulethat creates home directories and populates them with contents ofthe specified skel directory.The default value of umask is 0022 and the default value ofpath-to-skel is /etc/skel. The default value of home_mode iscomputed from the value of umask.The helper is separated from the module to not require directaccess from login SELinux domains to the contents of user homedirectories. The SELinux domain transition happens when themodule is executing the mkhomedir_helper.The helper never touches home directories if they already exist.",
        "name": "mkhomedir_helper - Helper binary that creates home directories",
        "section": 8
    },
    {
        "command": "mklost+found",
        "description": "mklost+found is used to create a lost+found directory in thecurrent working directory on a Linux second extended file system.There is normally a lost+found directory in the root directory ofeach file system.mklost+found pre-allocates disk blocks to the lost+founddirectory so that when e2fsck(8) is being run to recover a filesystem, it does not need to allocate blocks in the file system tostore a large number of unlinked files.This ensures that e2fsckwill not have to allocate data blocks in the file system duringrecovery.",
        "name": "mklost+found - create a lost+found directory on a mounted Linuxsecond extended file system",
        "section": 8
    },
    {
        "command": "mkswap",
        "description": "mkswap sets up a Linux swap area on a device or in a file.The device argument will usually be a disk partition (somethinglike /dev/sdb7) but can also be a file. The Linux kernel does notlook at partition IDs, but many installation scripts will assumethat partitions of hex type 82 (LINUX_SWAP) are meant to be swappartitions. (Warning: Solaris also uses this type. Be careful notto kill your Solaris partitions.)The size parameter is superfluous but retained for backwardscompatibility. (It specifies the desired size of the swap area in1024-byte blocks. mkswap will use the entire partition or file ifit is omitted. Specifying it is unwise - a typo may destroy yourdisk.)After creating the swap area, you need the swapon(8) command tostart using it. Usually swap areas are listed in /etc/fstab sothat they can be taken into use at boot time by a swapon -acommand in some boot script.",
        "name": "mkswap - set up a Linux swap area",
        "section": 8
    },
    {
        "command": "mkudffs",
        "description": "mkudffs is used to create a UDF filesystem on a device (usually adisk). device is the special file corresponding to the device(e.g. /dev/hdX) or file image. blocks-count is the number ofblocks on the device. If omitted, mkudffs automagically figuresthe filesystem size. The order of options matters. Encodingoption must be first and options to override default settingsimplied by the media type or UDF revision should be after theoption they are overriding.",
        "name": "mkudffs \u2014 create a UDF filesystem",
        "section": 8
    },
    {
        "command": "modinfo",
        "description": "modinfo extracts information from the Linux Kernel modules givenon the command line. If the module name is not a filename, thenthe /lib/modules/version directory is searched, as is also doneby modprobe(8) when loading kernel modules.modinfo by default lists each attribute of the module in formfieldname : value, for easy reading. The filename is listed thesame way (although it's not really an attribute).This version of modinfo can understand modules of any LinuxKernel architecture.",
        "name": "modinfo - Show information about a Linux Kernel module",
        "section": 8
    },
    {
        "command": "modprobe",
        "description": "modprobe intelligently adds or removes a module from the Linuxkernel: note that for convenience, there is no difference between_ and - in module names (automatic underscore conversion isperformed).modprobe looks in the module directory/lib/modules/`uname -r` for all the modules and other files,except for the optional configuration files in the/etc/modprobe.d directory (see modprobe.d(5)).modprobe willalso use module options specified on the kernel command line inthe form of <module>.<option> and blacklists in the form ofmodprobe.blacklist=<module>.Note that unlike in 2.4 series Linux kernels (which are notsupported by this tool) this version of modprobe does not doanything to the module itself: the work of resolving symbols andunderstanding parameters is done inside the kernel. So modulefailure is sometimes accompanied by a kernel message: seedmesg(8).modprobe expects an up-to-date modules.dep.bin file as generatedby the corresponding depmod utility shipped along with modprobe(see depmod(8)). This file lists what other modules each moduleneeds (if any), and modprobe uses this to add or remove thesedependencies automatically.If any arguments are given after the modulename, they are passedto the kernel (in addition to any options listed in theconfiguration file).When loading modules, modulename can also be a path to themodule. If the path is relative, it must explicitly start with\"./\". Note that this may fail when using a path to a module withdependencies not matching the installed depmod database.",
        "name": "modprobe - Add and remove modules from the Linux Kernel",
        "section": 8
    },
    {
        "command": "mount",
        "description": "All files accessible in a Unix system are arranged in one bigtree, the file hierarchy, rooted at /. These files can be spreadout over several devices. The mount command serves to attach thefilesystem found on some device to the big file tree. Conversely,the umount(8) command will detach it again. The filesystem isused to control how data is stored on the device or provided in avirtual way by network or other services.The standard form of the mount command is:mount -t type device dirThis tells the kernel to attach the filesystem found on device(which is of type type) at the directory dir. The option -t typeis optional. The mount command is usually able to detect afilesystem. The root permissions are necessary to mount afilesystem by default. See section \"Non-superuser mounts\" belowfor more details. The previous contents (if any) and owner andmode of dir become invisible, and as long as this filesystemremains mounted, the pathname dir refers to the root of thefilesystem on device.If only the directory or the device is given, for example:mount /dirthen mount looks for a mountpoint (and if not found then for adevice) in the /etc/fstab file. It\u2019s possible to use the --targetor --source options to avoid ambiguous interpretation of thegiven argument. For example:mount --target /mountpointThe same filesystem may be mounted more than once, and in somecases (e.g., network filesystems) the same filesystem may bemounted on the same mountpoint multiple times. The mount commanddoes not implement any policy to control this behavior. Allbehavior is controlled by the kernel and it is usually specificto the filesystem driver. The exception is --all, in this casealready mounted filesystems are ignored (see --all below for moredetails).Listing the mountsThe listing mode is maintained for backward compatibility only.For more robust and customizable output use findmnt(8),especially in your scripts. Note that control characters in themountpoint name are replaced with '?'.The following command lists all mounted filesystems (of typetype):mount [-l] [-t type]The option -l adds labels to this listing. See below.Indicating the device and filesystemMost devices are indicated by a filename (of a block specialdevice), like /dev/sda1, but there are other possibilities. Forexample, in the case of an NFS mount, device may look likeknuth.cwi.nl:/dir.The device names of disk partitions are unstable; hardwarereconfiguration, and adding or removing a device can causechanges in names. This is the reason why it\u2019s stronglyrecommended to use filesystem or partition identifiers like UUIDor LABEL. Currently supported identifiers (tags):LABEL=labelHuman readable filesystem identifier. See also -L.UUID=uuidFilesystem universally unique identifier. The format of theUUID is usually a series of hex digits separated by hyphens.See also -U.Note that mount uses UUIDs as strings. The UUIDs from thecommand line or from fstab(5) are not converted to internalbinary representation. The string representation of the UUIDshould be based on lower case characters.PARTLABEL=labelHuman readable partition identifier. This identifier isindependent on filesystem and does not change by mkfs ormkswap operations. It\u2019s supported for example for GUIDPartition Tables (GPT).PARTUUID=uuidPartition universally unique identifier. This identifier isindependent on filesystem and does not change by mkfs ormkswap operations. It\u2019s supported for example for GUIDPartition Tables (GPT).ID=idHardware block device ID as generated by udevd. Thisidentifier is usually based on WWN (unique storageidentifier) and assigned by the hardware manufacturer. See ls/dev/disk/by-id for more details, this directory and runningudevd is required. This identifier is not recommended forgeneric use as the identifier is not strictly defined and itdepends on udev, udev rules and hardware.The command lsblk --fs provides an overview of filesystems,LABELs and UUIDs on available block devices. The command blkid -p<device> provides details about a filesystem on the specifieddevice.Don\u2019t forget that there is no guarantee that UUIDs and labels arereally unique, especially if you move, share or copy the device.Use lsblk -o +UUID,PARTUUID to verify that the UUIDs are reallyunique in your system.The recommended setup is to use tags (e.g. UUID=uuid) rather than/dev/disk/by-{label,uuid,id,partuuid,partlabel} udev symlinks inthe /etc/fstab file. Tags are more readable, robust and portable.The mount(8) command internally uses udev symlinks, so the use ofsymlinks in /etc/fstab has no advantage over tags. For moredetails see libblkid(3).The proc filesystem is not associated with a special device, andwhen mounting it, an arbitrary keyword - for example, proc - canbe used instead of a device specification. (The customary choicenone is less fortunate: the error message 'none already mounted'from mount can be confusing.)The files /etc/fstab, /etc/mtab and /proc/mountsThe file /etc/fstab (see fstab(5)), may contain lines describingwhat devices are usually mounted where, using which options. Thedefault location of the fstab(5) file can be overridden with the--fstab path command-line option (see below for more details).The commandmount -a [-t type] [-O optlist](usually given in a bootscript) causes all filesystems mentionedin fstab (of the proper type and/or having or not having theproper options) to be mounted as indicated, except for thosewhose line contains the noauto keyword. Adding the -F option willmake mount fork, so that the filesystems are mounted in parallel.When mounting a filesystem mentioned in fstab or mtab, itsuffices to specify on the command line only the device, or onlythe mount point.The programs mount and umount(8) traditionally maintained a listof currently mounted filesystems in the file /etc/mtab. Thesupport for regular classic /etc/mtab is completely disabled atcompile time by default, because on current Linux systems it isbetter to make /etc/mtab a symlink to /proc/mounts instead. Theregular mtab file maintained in userspace cannot reliably workwith namespaces, containers and other advanced Linux features. Ifthe regular mtab support is enabled, then it\u2019s possible to usethe file as well as the symlink.If no arguments are given to mount, the list of mountedfilesystems is printed.If you want to override mount options from /etc/fstab, you haveto use the -o option:mount device|dir -o optionsand then the mount options from the command line will be appendedto the list of options from /etc/fstab. This default behaviourcan be changed using the --options-mode command-line option. Theusual behavior is that the last option wins if there areconflicting ones.The mount program does not read the /etc/fstab file if bothdevice (or LABEL, UUID, ID, PARTUUID or PARTLABEL) and dir arespecified. For example, to mount device foo at /dir:mount /dev/foo /dirThis default behaviour can be changed by using the--options-source-force command-line option to always readconfiguration from fstab. For non-root users mount always readsthe fstab configuration.Non-superuser mountsNormally, only the superuser can mount filesystems. However, whenfstab contains the user option on a line, anybody can mount thecorresponding filesystem.Thus, given a line/dev/cdrom /cd iso9660 ro,user,noauto,unhideany user can mount the iso9660 filesystem found on an insertedCDROM using the command:mount /cdNote that mount is very strict about non-root users and all pathsspecified on command line are verified before fstab is parsed ora helper program is executed. It\u2019s strongly recommended to use avalid mountpoint to specify filesystem, otherwise mount may fail.For example it\u2019s a bad idea to use NFS or CIFS source on commandline.Since util-linux 2.35, mount does not exit when user permissionsare inadequate according to libmount\u2019s internal security rules.Instead, it drops suid permissions and continues as regularnon-root user. This behavior supports use-cases where rootpermissions are not necessary (e.g., fuse filesystems, usernamespaces, etc).For more details, see fstab(5). Only the user that mounted afilesystem can unmount it again. If any user should be able tounmount it, then use users instead of user in the fstab line. Theowner option is similar to the user option, with the restrictionthat the user must be the owner of the special file. This may beuseful e.g. for /dev/fd if a login script makes the console userowner of this device. The group option is similar, with therestriction that the user must be a member of the group of thespecial file.The user mount option is accepted if no username is specified. Ifused in the format user=someone, the option is silently ignoredand visible only for external mount helpers (/sbin/mount.<type>)for compatibility with some network filesystems.Bind mount operationRemount part of the file hierarchy somewhere else. The call is:mount --bind olddir newdiror by using this fstab entry:/olddir /newdir none bindAfter this call the same contents are accessible in two places.It is important to understand that \"bind\" does not create anysecond-class or special node in the kernel VFS. The \"bind\" isjust another operation to attach a filesystem. There is nowherestored information that the filesystem has been attached by a\"bind\" operation. The olddir and newdir are independent and theolddir may be unmounted.One can also remount a single file (on a single file). It\u2019s alsopossible to use a bind mount to create a mountpoint from aregular directory, for example:mount --bind foo fooThe bind mount call attaches only (part of) a single filesystem,not possible submounts. The entire file hierarchy includingsubmounts can be attached a second place by using:mount --rbind olddir newdirNote that the filesystem mount options maintained by the kernelwill remain the same as those on the original mount point. Theuserspace mount options (e.g., _netdev) will not be copied bymount and it\u2019s necessary to explicitly specify the options on themount command line.Since util-linux 2.27 mount permits changing the mount options bypassing the relevant options along with --bind. For example:mount -o bind,ro foo fooThis feature is not supported by the Linux kernel; it isimplemented in userspace by an additional mount(2) remountingsystem call. This solution is not atomic.The alternative (classic) way to create a read-only bind mount isto use the remount operation, for example:mount --bind olddir newdirmount -o remount,bind,ro olddir newdirNote that a read-only bind will create a read-only mountpoint(VFS entry), but the original filesystem superblock will still bewritable, meaning that the olddir will be writable, but thenewdir will be read-only.It\u2019s also possible to change nosuid, nodev, noexec, noatime,nodiratime, relatime and nosymfollow VFS entry flags via a\"remount,bind\" operation. The other flags (for examplefilesystem-specific flags) are silently ignored. The classicmount(2) system call does not allow to change mount optionsrecursively (for example with -o rbind,ro). The recursivesemantic is possible with a new mount_setattr(2) kernel systemcall and it\u2019s supported since libmount from util-linux v2.39 by anew experimental \"recursive\" option argument (e.g. -orbind,ro=recursive). For more details see theFILESYSTEM-INDEPENDENT MOUNT OPTIONS section.Since util-linux 2.31, mount ignores the bind flag from/etc/fstab on a remount operation (if -o remount is specified oncommand line). This is necessary to fully control mount optionson remount by command line. In previous versions the bind flaghas been always applied and it was impossible to re-define mountoptions without interaction with the bind semantic. This mountbehavior does not affect situations when \"remount,bind\" isspecified in the /etc/fstab file.The move operationMove a mounted tree to another place (atomically). The call is:mount --move olddir newdirThis will cause the contents which previously appeared underolddir to now be accessible under newdir. The physical locationof the files is not changed. Note that olddir has to be amountpoint.Note also that moving a mount residing under a shared mount isinvalid and unsupported. Use findmnt -o TARGET,PROPAGATION to seethe current propagation flags.Shared subtree operationsSince Linux 2.6.15 it is possible to mark a mount and itssubmounts as shared, private, slave or unbindable. A shared mountprovides the ability to create mirrors of that mount such thatmounts and unmounts within any of the mirrors propagate to theother mirror. A slave mount receives propagation from its master,but not vice versa. A private mount carries no propagationabilities. An unbindable mount is a private mount which cannot becloned through a bind operation. The detailed semantics aredocumented in Documentation/filesystems/sharedsubtree.txt file inthe kernel source tree; see also mount_namespaces(7).Supported operations are:mount --make-shared mountpointmount --make-slave mountpointmount --make-private mountpointmount --make-unbindable mountpointThe following commands allow one to recursively change the typeof all the mounts under a given mountpoint.mount --make-rshared mountpointmount --make-rslave mountpointmount --make-rprivate mountpointmount --make-runbindable mountpointmount does not read fstab(5) when a --make-* operation isrequested. All necessary information has to be specified on thecommand line.Note that the Linux kernel does not allow changing multiplepropagation flags with a single mount(2) system call, and theflags cannot be mixed with other mount options and operations.Since util-linux 2.23 the mount command can be used to do morepropagation (topology) changes by one mount(8) call and do italso together with other mount operations. The propagation flagsare applied by additional mount(2) system calls when thepreceding mount operations were successful. Note that this usecase is not atomic. It is possible to specify the propagationflags in fstab(5) as mount options (private, slave, shared,unbindable, rprivate, rslave, rshared, runbindable).For example:mount --make-private --make-unbindable /dev/sda1 /foois the same as:mount /dev/sda1 /foomount --make-private /foomount --make-unbindable /foo",
        "name": "mount - mount a filesystem",
        "section": 8
    },
    {
        "command": "mount.fuse3",
        "description": "FUSE (Filesystem in Userspace) is a simple interface foruserspace programs to export a virtual filesystem to the Linuxkernel. FUSE also aims to provide a secure method for nonprivileged users to create and mount their own filesystemimplementations.",
        "name": "fuse - configuration and mount options for FUSE file systems",
        "section": 8
    },
    {
        "command": "mount.nfs",
        "description": "mount.nfs is a part of nfs(5) utilities package, which providesNFS client functionality.mount.nfs is meant to be used by the mount(8) command formounting NFS shares. This subcommand, however, can also be usedas a standalone command with limited functionality.remotetarget is a server share usually in the form ofservername:/path/to/share.dir is the directory on which thefile system is to be mounted.Under Linux 2.6.32 and later kernel versions, mount.nfs can mountall NFS file system versions.Under earlier Linux kernelversions, mount.nfs4 must be used for mounting NFSv4 file systemswhile mount.nfs must be used for NFSv3.",
        "name": "mount.nfs, mount.nfs4 - mount a Network File System",
        "section": 8
    },
    {
        "command": "mount.nfs4",
        "description": "mount.nfs is a part of nfs(5) utilities package, which providesNFS client functionality.mount.nfs is meant to be used by the mount(8) command formounting NFS shares. This subcommand, however, can also be usedas a standalone command with limited functionality.remotetarget is a server share usually in the form ofservername:/path/to/share.dir is the directory on which thefile system is to be mounted.Under Linux 2.6.32 and later kernel versions, mount.nfs can mountall NFS file system versions.Under earlier Linux kernelversions, mount.nfs4 must be used for mounting NFSv4 file systemswhile mount.nfs must be used for NFSv3.",
        "name": "mount.nfs, mount.nfs4 - mount a Network File System",
        "section": 8
    },
    {
        "command": "mountd",
        "description": "The rpc.mountd daemon implements the server side of the NFS MOUNTprotocol, an NFS side protocol used by NFS version 2 [RFC1094]and NFS version 3 [RFC1813].It also responds to requests fromthe Linux kernel to authenticate clients and provides details ofaccess permissions.The NFS server (nfsd) maintains a cache of authentication andauthorization information which is used to identify the source ofeach request, and then what access permissions that source has toany local filesystem.When required information is not found inthe cache, the server sends a request to mountd to fill in themissing information.Mountd uses a table of information storedin /var/lib/nfs/etab and maintained by exportfs(8), possiblybased on the contents of exports(5), to respond to each request.Mounting exported NFS File SystemsThe NFS MOUNT protocol has several procedures.The mostimportant of these are MNT (mount an export) and UMNT (unmount anexport).A MNT request has two arguments: an explicit argument thatcontains the pathname of the root directory of the export to bemounted, and an implicit argument that is the sender's IPaddress.When receiving a MNT request from an NFS client, rpc.mountdchecks both the pathname and the sender's IP address against itsexport table.If the sender is permitted to access the requestedexport, rpc.mountd returns an NFS file handle for the export'sroot directory to the client.The client can then use the rootfile handle and NFS LOOKUP requests to navigate the directorystructure of the export.The rmtab FileThe rpc.mountd daemon registers every successful MNT request byadding an entry to the /var/lib/nfs/rmtab file.When receivng aUMNT request from an NFS client, rpc.mountd simply removes thematching entry from /var/lib/nfs/rmtab, as long as the accesscontrol list for that export allows that sender to access theexport.Clients can discover the list of file systems an NFS server iscurrently exporting, or the list of other clients that havemounted its exports, by using the showmount(8) command.showmount(8) uses other procedures in the NFS MOUNT protocol toreport information about the server's exported file systems.Note, however, that there is little to guarantee that thecontents of /var/lib/nfs/rmtab are accurate.A client maycontinue accessing an export even after invoking UMNT.If theclient reboots without sending a UMNT request, stale entriesremain for that client in /var/lib/nfs/rmtab.Mounting File Systems with NFSv4Version 4 (and later) of NFS does not use a separate NFS MOUNTprotocol.Instead mounting is performed using regular NFSrequests handled by the NFS server in the Linux kernel (nfsd).Consequently /var/lib/nfs/rmtab is not updated to reflect anyNFSv4 activity.",
        "name": "rpc.mountd - NFS mount daemon",
        "section": 8
    },
    {
        "command": "mountstats",
        "description": "The mountstats command displays various NFS client statisiticsfor each given mountpoint.If no mountpoint is given, statistics will be displayed for allNFS mountpoints on the client.Sub-commandsValid mountstats(8) subcommands are:mountstatsDisplay a combination of per-op RPC statistics, NFS eventcounts, and NFS byte counts.This is the default sub-command that will be executed if no sub-command is given.iostat Display iostat-like statistics.nfsstatDisplay nfsstat-like statistics.",
        "name": "mountstats - Displays various NFS client per-mount statistics",
        "section": 8
    },
    {
        "command": "mpls",
        "description": "The mpls action performs mpls encapsulation or decapsulation on apacket, reflected by the operation modes POP, PUSH, MODIFY andDEC_TTL.The POP mode requires the ethertype of the header thatfollows the MPLS header (e.g.IPv4 or another MPLS). It willremove the outer MPLS header and replace the ethertype in the MACheader with that passed. The PUSH and MODIFY modes update thecurrent MPLS header information or add a new header.PUSHrequires at least an MPLS_LABEL.DEC_TTL requires no argumentsand simply subtracts 1 from the MPLS header TTL field.",
        "name": "mpls - mpls manipulation module",
        "section": 8
    },
    {
        "command": "mysqld",
        "description": "mysqld, also known as MariaDB Server, is the main program thatdoes most of the work in a MariaDB installation. MariaDB Servermanages access to the MariaDB data directory that containsdatabases and tables. The data directory is also the defaultlocation for other information such as log files and statusfiles.When MariaDB server starts, it listens for network connectionsfrom client programs and manages access to databases on behalf ofthose clients.The mysqld program has many options that can be specified atstartup. For a complete list of options, run this command:shell> mysqld --verbose --helpMariaDB Server also has a set of system variables that affect itsoperation as it runs. System variables can be set at serverstartup, and many of them can be changed at runtime to effectdynamic server reconfiguration. MariaDB Server also has a set ofstatus variables that provide information about its operation.You can monitor these status variables to access runtimeperformance characteristics.For a full description of MariaDB Server command options, systemvariables, and status variables, see the MariaDB Knowledge Base.",
        "name": "mariadbd - the MariaDB server (mysqld is now a symlink tomariadbd)",
        "section": 8
    },
    {
        "command": "nameif",
        "description": "nameif renames network interfaces based on mac addresses. When noarguments are given /etc/mactab is read. Each lineof itcontains an interface name and a Ethernet MAC address. Commentsare allowed starting with #.Otherwise the interfaces specifiedon the command line are processed.nameif looks for theinterface with the given MAC address and renames it to the namegiven.When the -s argument is given all error messages go to thesyslog.When the -c argument is given with a file name that file is readinstead of /etc/mactab.",
        "name": "nameif - name network interfaces based on MAC addresses",
        "section": 8
    },
    {
        "command": "nat",
        "description": "The nat action allows one to perform NAT without the overhead ofconntrack, which is desirable if the number of flows or addressesto perform NAT on is large. This action is best used incombination with the u32 filter to allow for efficient lookups ofa large number of stateless NAT rules in constant time.",
        "name": "nat - stateless native address translation action",
        "section": 8
    },
    {
        "command": "netcap",
        "description": "netcap is a program that prints out a report of processcapabilities. If the application is using tcp, udp, raw, orpacket family of sockets AND has any capabilities, it will be inthe report. If the process has partial capabilities, it isfurther examined to see if it has an open-ended bounding set. Ifthis is found to be true, a '+' symbol is added.If the processhas ambient capabilities, a '@' symbols is added.Some directories in the /proc file system are readonly by root.The program will try to access and report what it can. But ifnothing comes out, try changing to the root user and re-run thisprogram.",
        "name": "netcap - a program to see capabilities",
        "section": 8
    },
    {
        "command": "netem",
        "description": "The netem queue discipline provides Network Emulationfunctionality for testing protocols by emulating the propertiesof real-world networks.The queue discipline provides one or more network impairments topackets such as: delay, loss, duplication, and packet corruption.",
        "name": "netem - Network Emulator",
        "section": 8
    },
    {
        "command": "netsniff-ng",
        "description": "netsniff-ng is a fast, minimal tool to analyze network packets,capture pcap files, replay pcap files, and redirect trafficbetween interfaces with the help of zero-copy packet(7) sockets.netsniff-ng uses both Linux specific RX_RING and TX_RINGinterfaces to perform zero-copy. This is to avoid copy and systemcall overhead between kernel and user address space. When westarted working on netsniff-ng, the pcap(3) library did not usethis zero-copy facility.netsniff-ng is Linux specific, meaning there is no support forother operating systems. Therefore we can keep the code footprintquite minimal and to the point. Linux packet(7) sockets and itsRX_RING and TX_RING interfaces bypass the normal packetprocessing path through the networking stack.This is thefastest capturing or transmission performance one can get fromuser space out of the box, without having to load unsupported ornon-mainline third-party kernel modules. We explicitly refuse tobuild netsniff-ng on top of ntop/PF_RING. Not because we do notlike it (we do find it interesting), but because of the fact thatit is not part of the mainline kernel. Therefore, the ntopproject has to maintain and sync out-of-tree drivers to adaptthem to their DNA. Eventually, we went for untainted Linuxkernel, since its code has a higher rate of review, maintenance,security and bug fixes.netsniff-ng also supports early packet filtering in the kernel.It has support for low-level and high-level packet filters thatare translated into Berkeley Packet Filter instructions.netsniff-ng can capture pcap files in several different pcapformats that are interoperable with other tools. The followingpcap I/O methods are supported for efficient to-disc capturing:scatter-gather, mmap(2), read(2), and write(2).netsniff-ng isalso able to rotate pcap files based on data size or timeintervals, thus, making it a useful backend tool for subsequenttraffic analysis.netsniff-ng itself also supports analysis, replaying, and dumpingof raw 802.11 frames. For online or offline analysis, netsniff-nghas a built-in packet dissector for the current 802.3 (Ethernet),802.11* (WLAN), ARP, MPLS, 802.1Q (VLAN), 802.1QinQ, LLDP, IPv4,IPv6, ICMPv4, ICMPv6, IGMP, TCP and UDP, including GeoIP locationanalysis. Since netsniff-ng does not establish any state orperform reassembly during packet dissection, its memory footprintis quite low, thus, making netsniff-ng quite efficient foroffline analysis of large pcap files as well.Note that netsniff-ng is currently not multithreaded. However,this does not prevent you from starting multiple netsniff-nginstances that are pinned to different, non-overlapping CPUs andf.e. have different BPF filters attached.Likely that at somepoint in time your harddisc might become a bottleneck assumingyou do not rotate such pcaps in ram (and from there periodicallyscheduled move to slower medias). You can then use mergecap(1) totransform all pcap files into a single large pcap file. Thus,netsniff-ng then works multithreaded eventually.netsniff-ng can also be used to debug netlink traffic.",
        "name": "netsniff-ng - the packet sniffing beast",
        "section": 8
    },
    {
        "command": "netstat",
        "description": "Netstat prints information about the Linux networking subsystem.The type of information printed is controlled by the firstargument, as follows:(none)By default, netstat displays a list of open sockets.If youdon't specify any address families, then the active sockets ofall configured address families will be printed.--route, -rDisplay the kernel routing tables. See the description inroute(8) for details.netstat -r and route -e produce the sameoutput.--groups, -gDisplay multicast group membership information for IPv4 and IPv6.--interfaces, -iDisplay a table of all network interfaces and their respectivereception and transmission errors counters.--masquerade, -MDisplay a list of masqueraded connections.--statistics, -sDisplay summary statistics for each protocol.",
        "name": "netstat - Print network connections, routing tables, interfacestatistics, masquerade connections, and multicast memberships",
        "section": 8
    },
    {
        "command": "newaliases",
        "description": "The newaliases utility makes changes to the mail aliases filevisible to smtpd(8).It should be run every time the aliases(5)file is changed.The location of the alias file is defined insmtpd.conf(5), and defaults to /etc/mail/aliases.The options are as follows:-f fileUse file as the configuration file, instead of the default/etc/mail/smtpd.conf.If using database (db) files, newaliases is equivalent to runningmakemap(8) as follows:# makemap -t aliases /etc/mail/aliasesIf using plain text files, newaliases is equivalent to runningsmtpctl(8) as follows:# smtpctl update table aliases",
        "name": "newaliases \u2014 rebuild mail aliases",
        "section": 8
    },
    {
        "command": "newusers",
        "description": "The newusers command reads a file (or the standard input bydefault) and uses this information to update a set of existingusers or to create new users. Each line is in the same format asthe standard password file (see passwd(5)) with the exceptionsexplained below:pw_name:pw_passwd:pw_uid:pw_gid:pw_gecos:pw_dir:pw_shellpw_nameThis is the name of the user.It can be the name of a new user or the name of an existinguser (or a user created before by newusers). In case of anexisting user, the user's information will be changed,otherwise a new user will be created.pw_passwdThis field will be encrypted and used as the new value of theencrypted password.pw_uidThis field is used to define the UID of the user.If the field is empty, a new (unused) UID will be definedautomatically by newusers.If this field contains a number, this number will be used asthe UID.If this field contains the name of an existing user (or thename of a user created before by newusers), the UID of thespecified user will be used.If the UID of an existing user is changed, the filesownership of the user's file should be fixed manually.pw_gidThis field is used to define the primary group ID for theuser.If this field contains the name of an existing group (or agroup created before by newusers), the GID of this group willbe used as the primary group ID for the user.If this field is a number, this number will be used as theprimary group ID of the user. If no groups exist with thisGID, a new group will be created with this GID, and the nameof the user.If this field is empty, a new group will be created with thename of the user and a GID will be automatically defined bynewusers to be used as the primary group ID for the user andas the GID for the new group.If this field contains the name of a group which does notexist (and was not created before by newusers), a new groupwill be created with the specified name and a GID will beautomatically defined by newusers to be used as the primarygroup ID for the user and GID for the new group.pw_gecosThis field is copied in the GECOS field of the user.pw_dirThis field is used to define the home directory of the user.If this field does not specify an existing directory, thespecified directory is created, with ownership set to theuser being created or updated and its primary group. Notethat newusers does not create parent directories of the newuser's home directory. The newusers command will fail tocreate the home directory if the parent directories do notexist, and will send a message to stderr informing the userof the failure. The newusers command will not halt or returna failure to the calling shell if it fails to create the homedirectory, it will continue to process the batch of new usersspecified.If the home directory of an existing user is changed,newusers does not move or copy the content of the olddirectory to the new location. This should be done manually.pw_shellThis field defines the shell of the user. No checks areperformed on this field.newusers first tries to create or change all the specified users,and then write these changes to the user or group databases. Ifan error occurs (except in the final writes to the databases), nochanges are committed to the databases.During this first pass, users are created with a locked password(and passwords are not changed for the users which are notcreated). A second pass is used to update the passwords usingPAM. Failures to update a password are reported, but will notstop the other password updates.This command is intended to be used in a large system environmentwhere many accounts are updated at a single time.",
        "name": "newusers - update and create new users in batch",
        "section": 8
    },
    {
        "command": "nfsconf",
        "description": "The nfsconf command can be used to test for and retrieveconfiguration settings from a range of nfs-utils configurationfiles.ModesThe following modes are available:-d, --dumpOutput an alphabetically sorted dump of the currentconfiguration in conf file format. Accepts an optionalfilename in which to write the output.-e, --entryretrieve the config entry rather than its current expandedvalue-i, --issetTest if a specific tag has a value set.-g, --getOutput the current value of the specified tag.-s, --setUpdate or Add a tag and value to the config file in aspecified section, creating the tag, section, and file ifnecessary. If the section is defined as '#' then a commentis appended to the file. If a comment is set with a tagname then any exiting tagged comment with a matching nameis replaced.-u, --unsetRemove the specified tag and its value from the configfile.",
        "name": "nfsconf - Query various NFS configuration settings",
        "section": 8
    },
    {
        "command": "nfsd",
        "description": "The rpc.nfsd program implements the user level part of the NFSservice. The main functionality is handled by the nfsd kernelmodule. The user space program merely specifies what sort ofsockets the kernel service should listen on, what NFS versions itshould support, and how many kernel threads it should use.The rpc.mountd server provides an ancillary service needed tosatisfy mount requests by NFS clients.",
        "name": "rpc.nfsd - NFS server process",
        "section": 8
    },
    {
        "command": "nfsdcld",
        "description": "nfsdcld is the NFSv4 client tracking daemon. It is not necessaryto run this daemon on machines that are not acting as NFSv4servers.When a network partition is combined with a server reboot, thereare edge conditions that can cause the server to grant lockreclaims when other clients have taken conflicting locks in theinterim. A more detailed explanation of this issue is describedin RFC 3530, section 8.6.3.In order to prevent these problems, the server must track a smallamount of per-client information on stable storage. This daemonprovides the userspace piece of that functionality.",
        "name": "nfsdcld - NFSv4 Client Tracking Daemon",
        "section": 8
    },
    {
        "command": "nfsdclddb",
        "description": "The nfsdclddb command is provided to perform some manipulation ofthe nfsdcld sqlite database schema and to print the contents ofthe database.Sub-commandsValid nfsdclddb subcommands are:fix-table-namesA previous version of nfsdcld(8) contained a bug thatcorrupted the reboot epoch table names.This sub-commandwill fix those table names.downgrade-schemaDowngrade the database schema.Currently the schema canonly to downgraded from version 4 to version 3.printDisplay the contents of the database.Prints the schemaversion and the values of the current and recovery epochs.If the -s|--summary option is not given, also prints theclients in the reboot epoch tables.",
        "name": "nfsdclddb - Tool for manipulating the nfsdcld sqlite database",
        "section": 8
    },
    {
        "command": "nfsdclnts",
        "description": "The nfsdclnts(8) command parses the content present in/proc/fs/nfsd/clients/ directories. nfsdclnts(8) displays fileswhich are open, locked, delegated by the nfs-client. It alsoprints useful client information such as hostname, clientID, NFSversion mounted by the nfs-client.",
        "name": "nfsdclnts - print various nfs client information for knfsdserver.",
        "section": 8
    },
    {
        "command": "nfsdcltrack",
        "description": "nfsdcltrack is the NFSv4 client tracking callout program. It isnot necessary to install this program on machines that are notacting as NFSv4 servers.When a network partition is combined with a server reboot, thereare edge conditions that can cause the server to grant lockreclaims when other clients have taken conflicting locks in theinterim. A more detailed explanation of this issue is describedin RFC 3530, section 8.6.3 and in RFC 5661, section 8.4.3.In order to prevent these problems, the server must track a smallamount of per-client information on stable storage. This programprovides the userspace piece of that functionality. When thekernel needs to manipulate the database that stores this info, itwill execute this program to handle it.",
        "name": "nfsdcltrack - NFSv4 Client Tracking Callout Program",
        "section": 8
    },
    {
        "command": "nfsidmap",
        "description": "The NFSv4 protocol represents the local system's UID and GIDvalues on the wire as strings of the form user@domain.Theprocess of translating from UID to string and string to UID isreferred to as \"ID mapping.\"The system derives the user part of the string by performing apassword or group lookup.The lookup mechanism is configured in/etc/idmapd.conf.By default, the domain part of the string is the system's DNSdomain name.It can also be specified in /etc/idmapd.conf if thesystem is multi-homed, or if the system's DNS domain name doesnot match the name of the system's Kerberos realm.When the domain is not specified in /etc/idmapd.conf the localDNS server will be queried for the _nfsv4idmapdomain text record.If the record exists that will be used as the domain. When therecord does not exist, the domain part of the DNS domain willused.The /usr/sbin/nfsidmap program performs translations on behalf ofthe kernel.The kernel uses the request-key mechanism to performan upcall./usr/sbin/nfsidmap is invoked by /sbin/request-key,performs the translation, and initializes a key with theresulting information.The kernel then caches the translationresults in the key.nfsidmap can also clear cached ID map results in the kernel, orrevoke one particular key.An incorrect cached key can result infile and directory ownership reverting to \"nobody\" on NFSv4 mountpoints.In addition, the -d and -l options are available to help diagnosemisconfigurations.They have no effect on the keyring containingID mapping results.",
        "name": "nfsidmap - The NFS idmapper upcall program",
        "section": 8
    },
    {
        "command": "nfsiostat",
        "description": "The nfsiostat command displays NFS client per-mount statisitics.<interval>specifies the amount of time in seconds between eachreport.The first report contains statistics for the timesince each file system was mounted.Each subsequentreport contains statistics collected during the intervalsince the previous report.<count>If the <count> parameter is specified, the value of<count> determines the number of reports generated at<interval> seconds apart. if the interval parameter isspecified without the <count> parameter, the commandgenerates reports continuously.<options>Define below<mount_point>If one or more <mount point> names are specified,statistics for only these mount points will be displayed.Otherwise, all NFS mount points on the client are listed.The meaning of each column of nfsiostat's output is thefollowing:- op/sThis is the number of operations per second.- rpc bklogThis is the length of the backlog queue.- kB/sThis is the number of kB written/read per second.- kB/opThis is the number of kB written/read per eachoperation.- retransThis is the number of retransmissions.- avg RTT (ms)This is the duration from the time that client'skernel sends the RPC request until the time itreceives the reply.- avg exe (ms)This is the duration from the time that NFS clientdoes the RPC request to its kernel until the RPCrequest is completed, this includes the RTT timeabove.- avg queue (ms)This is the duration from the time the NFS clientcreated the RPC request task to the time therequest is transmitted.- errorsThis is the number of operations that completedwith an error status (status < 0).This count isonly available on kernels with RPC iostats version1.1 or above.Note that if an interval is used as argument to nfsiostat, thenthe diffrence from previous interval will be displayed, otherwisethe results will be from the time that the share was mounted.",
        "name": "nfsiostat - Emulate iostat for NFS mount points using/proc/self/mountstats",
        "section": 8
    },
    {
        "command": "nfsref",
        "description": "The nfsref(8) command is a simple way to get started managingjunction metadata.Other administrative commands provide richeraccess to junction information.SubcommandsValid nfsref(8) subcommands are:addAdds junction information to the directory named bypathname.The named directory must already exist, andmust not already contain junction information.Regulardirectory contents are obscured to NFS clients by thisoperation.A list of one or more file server and export path pairs isalso specified on the command line.When creating an NFSbasic junction, this list is stored in an extendedattribute of the directory.If junction creation is successful, the nfsref(8) commandflushes the kernel's export cache to remove previouslycached junction information.remove Removes junction information from the directory named bypathname.The named directory must exist, and mustcontain junction information.Regular directory contentsare made visible to NFS clients again by this operation.If junction deletion is successful, the nfsref(8) commandflushes the kernel's export cache to remove previouslycached junction information.lookup Displays junction information stored in the directorynamed by pathname.The named directory must exist, andmust contain junction information.When looking up an NFS basic junction, the junctioninformation in the directory is listed on stdout.Command line options-d, --debugEnables debugging messages during operation.-t, --type=junction-typeSpecifies the junction type for the operation.Validvalues for junction-type are nfs-basic or nfs-fedfs.For the add subcommand, the default value if this optionis not specified is nfs-basic.For the remove and lookupsubcommands, the --type option is not required.Thenfsref(8) command operates on whatever junction contentsare available.",
        "name": "nfsref - manage NFS referrals",
        "section": 8
    },
    {
        "command": "nfsstat",
        "description": "The nfsstat displays statistics kept about NFS client and serveractivity.",
        "name": "nfsstat - list NFS statistics",
        "section": 8
    },
    {
        "command": "nfsv4.exportd",
        "description": "The nfsv4.exportd is used to manage NFSv4 exports.The NFSserver (nfsd) maintains a cache of authentication andauthorization information which is used to identify the source ofeach request, and then what access permissions that source has toany local filesystem.When required information is not found inthe cache, the server sends a request to nfsv4.exportd to fill inthe missing information.nfsv4.exportd uses a table ofinformation stored in /var/lib/nfs/etab and maintained byexportfs(8), possibly based on the contents of exports(5), torespond to each request.",
        "name": "nfsv4.exportd - NFSv4 Server Mount Daemon",
        "section": 8
    },
    {
        "command": "nologin",
        "description": "nologin displays a message that an account is not available andexits non-zero. It is intended as a replacement shell field todeny login access to an account.If the file /etc/nologin.txt exists, nologin displays itscontents to the user instead of the default message.The exit status returned by nologin is always 1.",
        "name": "nologin - politely refuse a login",
        "section": 8
    },
    {
        "command": "nscd",
        "description": "nscd is a daemon that provides a cache for the most common nameservice requests.The default configuration file,/etc/nscd.conf, determines the behavior of the cache daemon.Seenscd.conf(5).nscd provides caching for accesses of the passwd(5), group(5),hosts(5) services(5) and netgroup databases through standard libcinterfaces, such as getpwnam(3), getpwuid(3), getgrnam(3),getgrgid(3), gethostbyname(3), and others.There are two caches for each database: a positive one for itemsfound, and a negative one for items not found.Each cache has aseparate TTL (time-to-live) period for its data.Note that theshadow file is specifically not cached.getspnam(3) calls remainuncached as a result.",
        "name": "nscd - name service cache daemon",
        "section": 8
    },
    {
        "command": "nss-myhostname",
        "description": "nss-myhostname is a plug-in module for the GNU Name ServiceSwitch (NSS) functionality of the GNU C Library (glibc),primarily providing hostname resolution for the locallyconfigured system hostname as returned by gethostname(2). Theprecise hostnames resolved by this module are:\u2022The local, configured hostname is resolved to all locallyconfigured IP addresses ordered by their scope, or \u2014 if noneare configured \u2014 the IPv4 address 127.0.0.2 (which is on thelocal loopback) and the IPv6 address ::1 (which is the localhost).\u2022The hostnames \"localhost\" and \"localhost.localdomain\" (aswell as any hostname ending in \".localhost\" or\".localhost.localdomain\") are resolved to the IP addresses127.0.0.1 and ::1.\u2022The hostname \"_gateway\" is resolved to all current defaultrouting gateway addresses, ordered by their metric. Thisassigns a stable hostname to the current gateway, useful forreferencing it independently of the current networkconfiguration state.\u2022The hostname \"_outbound\" is resolved to the local IPv4 andIPv6 addresses that are most likely used for communicationwith other hosts. This is determined by requesting a routingdecision to the configured default gateways from the kerneland then using the local IP addresses selected by thisdecision. This hostname is only available if there is atleast one local default gateway configured. This assigns astable hostname to the local outbound IP addresses, usefulfor referencing them independently of the current networkconfiguration state.Various software relies on an always-resolvable local hostname.When using dynamic hostnames, this is traditionally achieved bypatching /etc/hosts at the same time as changing the hostname.This is problematic since it requires a writable /etc/ filesystem and is fragile because the file might be edited by theadministrator at the same time. With nss-myhostname enabled,changing /etc/hosts is unnecessary, and on many systems, the filebecomes entirely optional.To activate the NSS modules, add \"myhostname\" to the linestarting with \"hosts:\" in /etc/nsswitch.conf.It is recommended to place \"myhostname\" after \"file\" and before\"dns\". This resolves well-known hostnames like \"localhost\" andthe machine hostnames locally. It is consistent with thebehaviour of nss-resolve, and still allows overriding via/etc/hosts.Please keep in mind that nss-myhostname (and nss-resolve) alsoresolve in the other direction \u2014 from locally attached IPaddresses to hostnames. If you rely on that lookup being providedby DNS, you might want to order things differently.",
        "name": "nss-myhostname, libnss_myhostname.so.2 - Hostname resolution forthe locally configured system hostname",
        "section": 8
    },
    {
        "command": "nss-mymachines",
        "description": "nss-mymachines is a plug-in module for the GNU Name ServiceSwitch (NSS) functionality of the GNU C Library (glibc),providing hostname resolution for the names of containers runninglocally that are registered with systemd-machined.service(8). Thecontainer names are resolved to the IP addresses of the specificcontainer, ordered by their scope. This functionality onlyapplies to containers using network namespacing (see thedescription of --private-network in systemd-nspawn(1)). Note thatthe name that is resolved is the one registered withsystemd-machined, which may be different than the hostnameconfigured inside of the container.Note that this NSS module only makes available names of thecontainers running immediately below the current system context.It does not provide host name resolution for containers runningside-by-side with the invoking system context, or containersfurther up or down the container hierarchy. Or in other words, onthe host system it provides host name resolution for thecontainers running immediately below the host environment. Whenused inside a container environment however, it will not be ableto provide name resolution for containers running on the host (asthose are siblings and not children of the current containerenvironment), but instead only for nested containers runningimmediately below its own container environment.To activate the NSS module, add \"mymachines\" to the line startingwith \"hosts:\" in /etc/nsswitch.conf.It is recommended to place \"mymachines\" before the \"resolve\" or\"dns\" entry of the \"hosts:\" line of /etc/nsswitch.conf in orderto make sure that its mappings are preferred over other resolverssuch as DNS.",
        "name": "nss-mymachines, libnss_mymachines.so.2 - Hostname resolution forlocal container instances",
        "section": 8
    },
    {
        "command": "nss-resolve",
        "description": "nss-resolve is a plug-in module for the GNU Name Service Switch(NSS) functionality of the GNU C Library (glibc) enabling it toresolve hostnames via the systemd-resolved(8) local network nameresolution service. It replaces the nss-dns plug-in module thattraditionally resolves hostnames via DNS.To activate the NSS module, add \"resolve [!UNAVAIL=return]\" tothe line starting with \"hosts:\" in /etc/nsswitch.conf.Specifically, it is recommended to place \"resolve\" early in/etc/nsswitch.conf's \"hosts:\" line. It should be before the\"files\" entry, since systemd-resolved supports /etc/hostsinternally, but with caching. To the contrary, it should be after\"mymachines\", to give hostnames given to local VMs and containersprecedence over names received over DNS. Finally, we recommendplacing \"dns\" somewhere after \"resolve\", to fall back to nss-dnsif systemd-resolved.service is not available.Note that systemd-resolved will synthesize DNS resource recordsin a few cases, for example for \"localhost\" and the current localhostname, see systemd-resolved(8) for the full list. Thisduplicates the functionality of nss-myhostname(8), but it isstill recommended (see examples below) to keep nss-myhostnameconfigured in /etc/nsswitch.conf, to keep those names resolveableif systemd-resolved is not running.Please keep in mind that nss-myhostname (and nss-resolve) alsoresolve in the other direction \u2014 from locally attached IPaddresses to hostnames. If you rely on that lookup being providedby DNS, you might want to order things differently.Communication between nss-resolve and systemd-resolved.servicetakes place via the /run/systemd/resolve/io.systemd.ResolveAF_UNIX socket.",
        "name": "nss-resolve, libnss_resolve.so.2 - Hostname resolution viasystemd-resolved.service",
        "section": 8
    },
    {
        "command": "nss-systemd",
        "description": "nss-systemd is a plug-in module for the GNU Name Service Switch(NSS) functionality of the GNU C Library (glibc), providing UNIXuser and group name resolution for services implementing theUser/Group Record Lookup API via Varlink[1], such as the systemand service manager systemd(1) (for its DynamicUser= feature, seesystemd.exec(5) for details), systemd-homed.service(8), orsystemd-machined.service(8).This module also ensures that the root and nobody users andgroups (i.e. the users/groups with the UIDs/GIDs 0 and 65534)remain resolvable at all times, even if they aren't listed in/etc/passwd or /etc/group, or if these files are missing.This module preferably utilizes systemd-userdbd.service(8) forresolving users and groups, but also works without the servicerunning.To activate the NSS module, add \"systemd\" to the lines startingwith \"passwd:\", \"group:\", \"shadow:\" and \"gshadow:\" in/etc/nsswitch.conf.It is recommended to place \"systemd\" after the \"files\" or\"compat\" entry of the /etc/nsswitch.conf lines so that/etc/passwd, /etc/group, /etc/shadow and /etc/gshadow basedmappings take precedence.",
        "name": "nss-systemd, libnss_systemd.so.2 - UNIX user and group nameresolution for user/group lookup via Varlink",
        "section": 8
    },
    {
        "command": "nstat",
        "description": "nstat and rtacct are simple tools to monitor kernel snmp countersand network interface statistics.nstat can filter kernel snmp counters by name with one or severalspecified wildcards. Wildcards are case-insensitive and caninclude special symbols ?and *",
        "name": "nstat, rtacct - network statistics tools.",
        "section": 8
    },
    {
        "command": "numactl",
        "description": "numactl runs processes with a specific NUMA scheduling or memoryplacement policy.The policy is set for command and inherited byall of its children.In addition it can set persistent policyfor shared memory segments or files.Use -- before command if using command options that could beconfused with numactl options.nodes may be specified as N,N,N orN-N or N,N-N orN-N,N-N andso forth.Relative nodes may be specified as +N,N,N or+N-N or+N,N-N and so forth. The + indicates that the node numbers arerelative to the process' set of allowed nodes in its currentcpuset.A !N-N notation indicates the inverse of N-N, in otherwords all nodes except N-N.If used with + notation, specify!+N-N. When same is specified the previous nodemask specified onthe command line is used.all means all nodes in the currentcpuset.Instead of a number a node can also be:netdev:DEVThe node connected to network device DEV.file:PATHThe node the block device of PATH.ip:HOSTThe node of the network device of HOSTblock:PATHThe node of block device PATHpci:[seg:]bus:dev[:func]The node of a PCI device.Note that block resolves the kernel block device names only forudev names in /dev use file:Policy settings are:--all, -aUnset default cpuset awareness, so user can use allpossible CPUs/nodes for following policy settings.--interleave=nodes, -i nodesSet a memory interleave policy. Memory will be allocatedusing round robin on nodes.When memory cannot beallocated on the current interleave target fall back toother nodes.Multiple nodes may be specified on--interleave, --membind and --cpunodebind.--membind=nodes, -m nodesOnly allocate memory from nodes.Allocation will failwhen there is not enough memory available on these nodes.nodes may be specified as noted above.--cpunodebind=nodes, -N nodesOnly execute command on the CPUs of nodes.Note thatnodes may consist of multiple CPUs.nodes may bespecified as noted above.--physcpubind=cpus, -C cpusOnly execute process on cpus.This accepts cpu numbers asshown in the processor fields of /proc/cpuinfo, orrelative cpus as in relative to the current cpuset.Youmay specify \"all\", which means all cpus in the currentcpuset.Physical cpus may be specified as N,N,N orN-Nor N,N-N orN-N,N-N and so forth.Relative cpus may bespecified as +N,N,N or+N-N or +N,N-N and so forth. The +indicates that the cpu numbers are relative to theprocess' set of allowed cpus in its current cpuset.A !N-N notation indicates the inverse of N-N, in other wordsall cpus except N-N.If used with + notation, specify!+N-N.--localalloc, -lTry to allocate on the current node of the process, but ifmemory cannot be allocated there fall back to other nodes.--preferred=nodePreferably allocate memory on node, but if memory cannotbe allocated there fall back to other nodes.This optiontakes only a single node number.Relative notation may beused.--balancing, -bEnable Linux kernel NUMA balancing for the process if itis supported by kernel.This should only be used with--membind, -m only, otherwise ignored.--preferred-many=nodePreferably allocate memory on nodes, but if memory cannotbe allocated there fall back to other nodes.This optiontakes a mask of preferred nodes where the closest node tolocal is considered most preferred.Relative notation maybe used.--show, -sShow NUMA policy settings of the current process.--hardware, -HShow inventory of available nodes on the system.Numactl can set up policy for a SYSV shared memory segment or afile in shmfs/hugetlbfs.This policy is persistent and will be used by all mappings fromthat shared memory. The order of options matters here.Thespecification must at least include either of --shm, --shmid,--file to specify the shared memory segment or file and a memorypolicy like described above ( --interleave, --localalloc,--preferred, --preferred-many, --membind ).--hugeWhen creating a SYSV shared memory segment use huge pages.Onlyvalid before --shmid or --shm--offsetSpecify offset into the shared memory segment. Default 0.Validunits are m (for MB), g (for GB), k (for KB), otherwise itspecifies bytes.--strictGive an error when a page in the policied area in the sharedmemory segment already was faulted in with a conflicting policy.Default is to silently ignore this.--shmmode shmmodeOnly valid before --shmid or --shm When creating a shared memorysegment set it to numeric mode shmmode.--length lengthApply policy to length range in the shared memory segment or makethe segment length long Default is to use the remaining lengthRequired when a shared memory segment is created and specifiesthe length of the new segment then. Valid units are m (for MB), g(for GB), k (for KB), otherwise it specifies bytes.--shmid idCreate or use a shared memory segment with numeric ID id--shm shmkeyfileCreate or use a shared memory segment, with the ID generatedusing ftok(3) from shmkeyfile--file tmpfsfileSet policy for a file in tmpfs or hugetlbfs--touchTouch pages to enforce policy early. Default is to not touchthem, the policy is applied when an applications maps andaccesses a page.--dumpDump policy in the specified range.--dump-nodesDump all nodes of the specific range (very verbose!)Valid node specifiersallAll nodesnumberNode numbernumber1{,number2}Node number1 and Node number2number1-number2Nodes from number1 to number2! nodesInvert selection of the following specification.",
        "name": "numactl - Control NUMA policy for processes or shared memory",
        "section": 8
    },
    {
        "command": "numastat",
        "description": "numastat with no command options or arguments at all, displaysper-node NUMA hit and miss system statistics from the kernelmemory allocator.This default numastat behavior is strictlycompatible with the previous long-standing numastat perl script,written by Andi Kleen.The default numastat statistics showsper-node numbers (in units of pages of memory) in thesecategories:numa_hit is memory successfully allocated on this node asintended.numa_miss is memory allocated on this node despite the processpreferring some different node. Each numa_miss has a numa_foreignon another node.numa_foreign is memory intended for this node, but actuallyallocated on some different node.Each numa_foreign has anuma_miss on another node.interleave_hit is interleaved memory successfully allocated onthis node as intended.local_node is memory allocated on this node while a process wasrunning on it.other_node is memory allocated on this node while a process wasrunning on some other node.Any supplied options or arguments with the numastat command willsignificantly change both the content and the format of thedisplay.Specified options will cause display units to change tomegabytes of memory, and will change other specific behaviors ofnumastat as described below.Memory usage information reflects the resident pages on thesystem.",
        "name": "numastat - Show per-NUMA-node memory statistics for processes andthe operating system",
        "section": 8
    },
    {
        "command": "open_init_pty",
        "description": "Run a program under a pseudo terminal. This is used by run_initto run actually run the program after setting up the propercontext. This program acquires a new Pseudo terminal, forks achild process that binds to the pseudo terminal, and then sitsaround and connects the physical terminal it was invoked uponwith the pseudo terminal, passing keyboard input into to thechild process, and passing the output of the child process to thephysical terminal.It sets up the pseudo terminal properly based on the physicalterminal attributes, and then sets the user's terminal to RAWmode, taking care to reset it on exit.",
        "name": "open_init_pty - run an program under a pseudo terminal",
        "section": 8
    },
    {
        "command": "ovn-appctl",
        "description": "OVN daemons accept certain commands at runtime to control theirbehavior and query their settings. Every daemon accepts a commonset of commands documented under COMMON COMMANDS below. Somedaemons support additional commands documented in their ownmanpages.The ovn-appctl program provides a simple way to invoke thesecommands. The command to be sent is specified on ovn-appctl\u2019scommand line as non-option arguments. ovn-appctl sends thecommand and prints the daemon\u2019s response on standard output.ovn-ctl is exactly similar to Open vSwitch ovs-appctl utility.",
        "name": "ovn-appctl - utility for configuring running OVN daemons",
        "section": 8
    },
    {
        "command": "ovn-controller",
        "description": "ovn-controller is the local controller daemon for OVN, the OpenVirtual Network. It connects up to the OVN Southbound database(see ovn-sb(5)) over the OVSDB protocol, and down to the OpenvSwitch database (see ovs-vswitchd.conf.db(5)) over the OVSDBprotocol and to ovs-vswitchd(8) via OpenFlow. Each hypervisor andsoftware gateway in an OVN deployment runs its own independentcopy of ovn-controller; thus, ovn-controller\u2019s downwardconnections are machine-local and do not run over a physicalnetwork.",
        "name": "ovn-controller - Open Virtual Network local controller",
        "section": 8
    },
    {
        "command": "ovn-controller-vtep",
        "description": "ovn-controller-vtep is the local controller daemon in OVN, theOpen Virtual Network, for VTEP enabled physical switches. Itconnects up to the OVN Southbound database (see ovn-sb(5)) overthe OVSDB protocol, and down to the VTEP database (see vtep(5))over the OVSDB protocol.PKI OptionsPKI configuration is required in order to use SSL for theconnections to the VTEP and Southbound databases.-p privkey.pem--private-key=privkey.pemSpecifies a PEM file containing the private key usedas identity for outgoing SSL connections.-c cert.pem--certificate=cert.pemSpecifies a PEM file containing a certificate thatcertifies the private key specified on -p or--private-key to be trustworthy. The certificate mustbe signed by the certificate authority (CA) that thepeer in SSL connections will use to verify it.-C cacert.pem--ca-cert=cacert.pemSpecifies a PEM file containing the CA certificatefor verifying certificates presented to this programby SSL peers. (This may be the same certificate thatSSL peers use to verify the certificate specified on-c or --certificate, or it may be a different one,depending on the PKI design in use.)-C none--ca-cert=noneDisables verification of certificates presented bySSL peers. This introduces a security risk, becauseit means that certificates cannot be verified to bethose of known trusted hosts.--bootstrap-ca-cert=cacert.pemWhen cacert.pem exists, this option has the sameeffect as -C or --ca-cert. If it does not exist,then the executable will attempt to obtain the CAcertificate from the SSL peer on its first SSLconnection and save it to the named PEM file. If itis successful, it will immediately drop theconnection and reconnect, and from then on all SSLconnections must be authenticated by a certificatesigned by the CA certificate thus obtained.This option exposes the SSL connection to a man-in-the-middle attack obtaining the initial CAcertificate, but it may be useful forbootstrapping.This option is only useful if the SSL peer sendsits CA certificate as part of the SSL certificatechain. The SSL protocol does not require the serverto send the CA certificate.This option is mutually exclusive with -C and--ca-cert.--peer-ca-cert=peer-cacert.pemSpecifies a PEM file that contains one or moreadditional certificates to send to SSL peers. peer-cacert.pem should be the CA certificate used tosign the program\u2019s own certificate, that is, thecertificate specified on -c or --certificate. Ifthe program\u2019s certificate is self-signed, then--certificate and --peer-ca-cert should specify thesame file.This option is not useful in normal operation,because the SSL peer must already have the CAcertificate for the peer to have any confidence inthe program\u2019s identity. However, this offers a wayfor a new installation to bootstrap the CAcertificate on its first SSL connection.",
        "name": "ovn-controller-vtep - Open Virtual Network local controller forvtep enabled physical switches.",
        "section": 8
    },
    {
        "command": "ovn-ctl",
        "description": "This program is intended to be invoked internally by Open VirtualNetwork startup scripts. System administrators should notnormally invoke it directly.",
        "name": "ovn-ctl - Open Virtual Network northbound daemon lifecycleutility",
        "section": 8
    },
    {
        "command": "ovn-ic",
        "description": "ovn-ic, OVN interconnection controller, is a centralized daemonwhich communicates with global interconnection databasesIC_NB/IC_SB to configure and exchange data with local NB/SB forinterconnecting with other OVN deployments.",
        "name": "ovn-ic - Open Virtual Network interconnection controller",
        "section": 8
    },
    {
        "command": "ovn-ic-nbctl",
        "description": "This utility can be used to manage the OVN interconnectionnorthbound database.",
        "name": "ovn-ic-nbctl - Open Virtual Network interconnection northbound dbmanagement utility",
        "section": 8
    },
    {
        "command": "ovn-ic-sbctl",
        "description": "This utility can be used to manage the OVN interconnectionsouthbound database.",
        "name": "ovn-ic-sbctl - Open Virtual Network interconnection southbound dbmanagement utility",
        "section": 8
    },
    {
        "command": "ovn-nbctl",
        "description": "The ovn-nbctl program configures the OVN_Northbound database byproviding a high-level interface to its configuration database.See ovn-nb(5) for comprehensive documentation of the databaseschema.ovn-nbctl connects to an ovsdb-server process that maintains anOVN_Northbound configuration database. Using this connection, itqueries and possibly applies changes to the database, dependingon the supplied commands.ovn-nbctl can perform any number of commands in a single run,implemented as a single atomic transaction against the database.The ovn-nbctl command line begins with global options (seeOPTIONS below for details). The global options are followed byone or more commands. Each command should begin with -- by itselfas a command-line argument, to separate it from the followingcommands. (The -- before the first command is optional.) Thecommand itself starts with command-specific options, if any,followed by the command name and any arguments.",
        "name": "ovn-nbctl - Open Virtual Network northbound db management utility",
        "section": 8
    },
    {
        "command": "ovn-northd",
        "description": "ovn-northd is a centralized daemon responsible for translatingthe high-level OVN configuration into logical configurationconsumable by daemons such as ovn-controller. It translates thelogical network configuration in terms of conventional networkconcepts, taken from the OVN Northbound Database (see ovn-nb(5)),into logical datapath flows in the OVN Southbound Database (seeovn-sb(5)) below it.ovn-northd is implemented in C. ovn-northd-ddlog is a compatibleimplementation written in DDlog, a language for incrementaldatabase processing. This documentation applies to bothimplementations, with differences indicated where relevant.",
        "name": "ovn-northd and ovn-northd-ddlog - Open Virtual Network centralcontrol daemon",
        "section": 8
    },
    {
        "command": "ovn-northd-ddlog",
        "description": "ovn-northd is a centralized daemon responsible for translatingthe high-level OVN configuration into logical configurationconsumable by daemons such as ovn-controller. It translates thelogical network configuration in terms of conventional networkconcepts, taken from the OVN Northbound Database (see ovn-nb(5)),into logical datapath flows in the OVN Southbound Database (seeovn-sb(5)) below it.ovn-northd is implemented in C. ovn-northd-ddlog is a compatibleimplementation written in DDlog, a language for incrementaldatabase processing. This documentation applies to bothimplementations, with differences indicated where relevant.",
        "name": "ovn-northd and ovn-northd-ddlog - Open Virtual Network centralcontrol daemon",
        "section": 8
    },
    {
        "command": "ovn-sbctl",
        "description": "The ovn-sbctl program configures the OVN_Southbound database byproviding a high-level interface to its configuration database.See ovn-sb(5) for comprehensive documentation of the databaseschema.ovn-sbctl connects to an ovsdb-server process that maintains anOVN_Southbound configuration database. Using this connection, itqueries and possibly applies changes to the database, dependingon the supplied commands.ovn-sbctl can perform any number of commands in a single run,implemented as a single atomic transaction against the database.The ovn-sbctl command line begins with global options (seeOPTIONS below for details). The global options are followed byone or more commands. Each command should begin with -- by itselfas a command-line argument, to separate it from the followingcommands. (The -- before the first command is optional.) Thecommand itself starts with command-specific options, if any,followed by the command name and any arguments.",
        "name": "ovn-sbctl - Open Virtual Network southbound db management utility",
        "section": 8
    },
    {
        "command": "ovn-trace",
        "description": "This utility simulates packet forwarding within an OVN logicalnetwork. It can be used to run through ``what-if\u2019\u2019 scenarios: ifa packet originates at a logical port, what will happen to it andwhere will it ultimately end up? Users already familiar with theOpen vSwitch ofproto/trace command described in ovs-vswitch(8)will find ovn-trace to be a similar tool for logical networks.ovn-trace works by reading the Logical_Flow and other tables fromthe OVN southbound database (see ovn-sb(5)). It simulates apacket\u2019s path through logical networks by repeatedly looking itup in the logical flow table, following the entire tree ofpossibilities.ovn-trace simulates only the OVN logical network. It does notsimulate the physical elements on which the logical network islayered. This means that, for example, it is unimportant how VMsare distributed among hypervisors, or whether their hypervisorsare functioning and reachable, so ovn-trace will yield the sameresults regardless. There is one important exception: ovn-northd,the daemon that generates the logical flows that ovn-tracesimulates, treats logical ports differently based on whether theyare up or down. Thus, if you see surprising results, ensure thatthe ports involved in a simulation are up.The simplest way to use ovn-trace is to provide the microflow(and optional datapath) arguments on the command line. In thiscase, it simulates the behavior of a single packet and exits. Foran alternate usage model, see Daemon Mode below.The optional datapath argument specifies the name of a logicaldatapath. Acceptable names are the name from the northboundLogical_Switch or Logical_Router table, the UUID of a record fromone of those tables, or the UUID of a record from the southboundDatapath_Binding table. (The datapath is optional becauseovn-trace can figure it out from the inport that the microflowmatches.)The microflow argument describes the packet whose forwarding isto be simulated, in the syntax of an OVN logical expression, asdescribed in ovn-sb(5), to express constraints. The parserunderstands prerequisites; for example, if the expression refersto ip4.src, there is no need to explicitly state ip4 or eth.type== 0x800.For reasonable L2 behavior, the microflow should include at leastinport and eth.dst, plus eth.src if port security is enabled. Forexample:inport == \"lp11\" && eth.src == 00:01:02:03:04:05 && eth.dst == ff:ff:ff:ff:ff:ffFor reasonable L3 behavior, microflow should also include ip4.srcand ip4.dst (or ip6.src and ip6.dst) and ip.ttl. For example:inport == \"lp111\" && eth.src == f0:00:00:00:01:11 && eth.dst == 00:00:00:00:ff:11&& ip4.src == 192.168.11.1 && ip4.dst == 192.168.22.2 && ip.ttl == 64Here\u2019s an ARP microflow example:inport == \"lp123\"&& eth.dst == ff:ff:ff:ff:ff:ff && eth.src == f0:00:00:00:01:11&& arp.op == 1 && arp.sha == f0:00:00:00:01:11 && arp.spa == 192.168.1.11&& arp.tha == ff:ff:ff:ff:ff:ff && arp.tpa == 192.168.2.22ovn-trace will reject erroneous microflow expressions, whichbeyond syntax errors fall into two categories. First, they can beambiguous. For example, tcp.src == 80 is ambiguous because itdoes not state IPv4 or IPv6 as the Ethernet type. ip4 && tcp.src> 1024 is also ambiguous because it does not constrain bits oftcp.src to particular values. Second, they can be contradictory,e.g. ip4 && ip6.",
        "name": "ovn-trace - Open Virtual Network logical network tracing utility",
        "section": 8
    },
    {
        "command": "ovs-appctl",
        "description": "Open vSwitch daemons accept certain commands at runtime tocontrol their behavior and query their settings.Every daemonaccepts a common set of commands documented under Common Commandsbelow.Some daemons support additional commands documented intheir own manpages.ovs-vswitchd in particular accepts a numberof additional commands documented in ovs-vswitchd(8).The ovs-appctl program provides a simple way to invoke thesecommands.The command to be sent is specified on ovs-appctl\u2019scommand line as non-option arguments.ovs-appctl sends thecommand and prints the daemon\u2019s response on standard output.In normal use only a single option is accepted:\u2022 -t <target> or --target <target>Tells ovs-appctl which daemon to contact.If <target> begins with / it must name a Unix domain socket onwhich an Open vSwitch daemon is listening for control channelconnections.By default, each daemon listens on a Unix domainsocket in the rundir (e.g. /run) named <program>.<pid>.ctl,where <program> is the program\u2019s name and <pid> is its processID.For example, if ovs-vswitchd has PID 123, it would listenon ovs-vswitchd.123.ctl.Otherwise, ovs-appctl looks in the rundir for a pidfile, thatis, a file whose contents are the process ID of a runningprocess as a decimal number, named <target>.pid.(The--pidfile option makes an Open vSwitch daemon create apidfile.)ovs-appctl reads the pidfile, then looks in therundir for a Unix socket named <target>.<pid>.ctl, where <pid>is replaced by the process ID read from the pidfile, and usesthat file as if it had been specified directly as the target.On Windows, <target> can be an absolute path to a file thatcontains a localhost TCP port on which an Open vSwitch daemonis listening for control channel connections. By default, eachdaemon writes the TCP port on which it is listening for controlconnection into the file <program>.ctl located inside therundir. If <target> is not an absolute path, ovs-appctl looksin the rundir for a file named <target>.ctl.The defaulttarget is ovs-vswitchd.\u2022 -T <secs> or --timeout=<secs>By default, or with a <secs> of 0, ovs-appctl waits forever toconnect to the daemon and receive a response.This optionlimits runtime to approximately <secs> seconds.If the timeoutexpires, ovs-appctl exits with a SIGALRM signal.",
        "name": "ovs-appctl - utility for configuring running Open vSwitch daemons",
        "section": 8
    },
    {
        "command": "ovs-bugtool",
        "description": "Generates a debug bundle with useful information about OpenvSwitch on this system and places it in /var/log/ovs-bugtool.",
        "name": "ovs-bugtool - Open vSwitch bug reporting utility",
        "section": 8
    },
    {
        "command": "ovs-ctl",
        "description": "The ovs-ctl program starts, stops, and checks the status of OpenvSwitch daemons.It is not meant to be invoked directly bysystem administrators but to be called internally by systemstartup scripts.Each ovs-ctl command is described separately below.The start commandThe start command starts Open vSwitch.It performs the followingtasks:1. Loads the Open vSwitch kernel module.If this fails, and theLinux bridge module is loaded but no bridges exist, it triesto unload the bridge module and tries loading the Open vSwitchkernel module again.(This is because the Open vSwitch kernelmodule cannot coexist with the Linux bridge module before2.6.37.)The start command skips the following steps if ovsdb-server isalready running:2. If the Open vSwitch database file does not exist, it createsit.If the database does exist, but it has an obsoleteversion, it upgrades it to the latest schema.3. Starts ovsdb-server, unless the --no-ovsdb-server commandoption is given.4. Initializes a few values inside the database.5. If the --delete-bridges option was used, deletes all of thebridges from the database.6. If the --delete-transient-ports option was used, deletes allports that have other_config:transient set to true.The start command skips the following step if ovs-vswitchd isalready running, or if the --no-ovs-vswitchd command option isgiven:7. Starts ovs-vswitchd.OptionsSeveral command-line options influence the start command\u2019sbehavior.Some form of the following option should ordinarily bespecified:\u2022 --system-id=<uuid> or --system-id=randomThis specifies a unique system identifier to store intoexternal-ids:system-id in the database\u2019s Open_vSwitch table.Remote managers that talk to the Open vSwitch database serverover network protocols use this value to identify anddistinguish Open vSwitch instances, so it should be unique (atleast) within OVS instances that will connect to a singlecontroller.When random is specified, ovs-ctl will generate a random IDthat persists from one run to another (stored in a file).Whenanother string is specified ovs-ctl uses it literally.The following options should be specified if the defaults are notsuitable:\u2022 --system-type=<type> or --system-version=<version>Sets the value to store in the system-type and system-versioncolumns, respectively, in the database\u2019s Open_vSwitch table.Remote managers may use these values too determine the kind ofsystem to which they are connected (primarily for display tohuman administrators).When not specified, ovs-ctl uses values from the optionalsystem-type.conf and system-version.conf files (see Files) orit uses the lsb_release program, if present, to providereasonable defaults.The following options are also likely to be useful:\u2022 --external-id=\"<name>=<value>\"Sets external-ids:<name> to <value> in the database\u2019sOpen_vSwitch table.Specifying this option multiple times addsmultiple key-value pairs.\u2022 --delete-bridgesOrdinarily Open vSwitch bridges persist from one system boot tothe next, as long as the database is preserved.Someenvironments instead expect to re-create all of the bridges andother configuration state on every boot.This option supportsthat, by deleting all Open vSwitch bridges after startingovsdb-server but before starting ovs-vswitchd.\u2022 --delete-transient-portsDeletes all ports that have other_config:transient set to true.This is important on certain environments where some ports aregoing to be recreated after reboot, but other ports need to bepersisted in the database.\u2022 --ovs-user=user[:group]Ordinarily Open vSwitch daemons are started as the userinvoking the ovs-ctl command.Some system administrators wouldprefer to have the various daemons spawn as different users intheir environments.This option allows passing the --useroption to the ovsdb-server and ovs-vswitchd daemons, allowingthem to change their privilege levels.The following options are less important:\u2022 --no-monitorBy default ovs-ctl passes --monitor to ovs-vswitchd andovsdb-server, requesting that it spawn a process monitor whichwill restart the daemon if it crashes.This option suppressesthat behavior.\u2022 --daemon-cwd=<directory>Specifies the current working directory that the OVS daemonsshould run from.The default is / (the root directory) if thisoption is not specified.(This option is useful because mostsystems create core files in a process\u2019s current workingdirectory and because a file system that is in use as aprocess\u2019s current working directory cannot be unmounted.)\u2022 --no-force-corefilesBy default, ovs-ctl enables core dumps for the OVS daemons.This option disables that behavior.\u2022 --no-mlockallBy default ovs-ctl passes --mlockall to ovs-vswitchd,requesting that it lock all of its virtual memory, preventingit from being paged to disk.This option suppresses thatbehavior.\u2022 --no-self-confinementDisable self-confinement for ovs-vswitchd and ovsdb-serverdaemons.This flag may be used when, for example, OpenFlowcontroller creates its Unix Domain Socket outside OVS rundirectory and OVS needs to connect to it.It is better tostick with the default behavior and not to use this flag,unless:\u2022 You have Open vSwitch running under SELinux or AppArmorMandatory Access Control that would prevent OVS from messingwith sockets outside ordinary OVS directories.\u2022 You believe that relying on protocol handshakes (e.g.OpenFlow) is enough to prevent OVS to adversely interact withother daemons running on your system.\u2022 You don\u2019t have much worries of remote OVSDB exploits in thefirst place, because, perhaps, OVSDB manager is running onthe same host as OVS and share similar attack vectors.\u2022 --ovsdb-server-priority=<niceness> or--ovs-vswitchd-priority=<niceness>Sets the nice(1) level used for each daemon.All of themdefault to -10.\u2022 --ovsdb-server-wrapper=<wrapper> or--ovs-vswitchd-wrapper=<wrapper>Configures the specified daemon to run under <wrapper>, whichis one of the following:\u2022 valgrind: Run the daemon under valgrind(1), if it isinstalled, logging to <daemon>.valgrind.log.<pid> in the logdirectory.\u2022 strace: Run the daemon under strace(1), if it is installed,logging to <daemon>.strace.log.<pid> in the log directory.\u2022 glibc: Enable GNU C library features designed to find memoryerrors.By default, no wrapper is used.Each of the wrappers can expose bugs in Open vSwitch that leadto incorrect operation, including crashes.The valgrind andstrace wrappers greatly slow daemon operations so they shouldnot be used in production.They also produce voluminous logsthat can quickly fill small disk partitions.The glibc wrapperis less resource-intensive but still somewhat slows thedaemons.The following options control file locations.They should onlybe used if the default locations cannot be used.See FILES,below, for more information.\u2022 --db-file=<file>Overrides the file name for the OVS database.\u2022 --db-sock=<socket>Overrides the file name for the Unix domain socket used toconnect to ovsdb-server.\u2022 --db-schema=<schema>Overrides the file name for the OVS database schema.\u2022 --extra-dbs=<file>Adds <file> as an extra database for ovsdb-server to serve out.Multiple space-separated file names may also be specified.<file> should begin with /; if it does not, then it will betaken as relative to <dbdir>.The stop commandThe stop command stops the ovs-vswitchd and ovsdb-server daemons.It does not unload the Open vSwitch kernel modules. It can takethe same --no-ovsdb-server and --no-ovs-vswitchd options as thatof the start command.This command does nothing and finishes successfully if the OVSdaemons aren\u2019t running.The restart commandThe restart command performs a stop followed by a start command.The command can take the same options as that of the startcommand. In addition, it saves and restores OpenFlow flows foreach individual bridge.The status commandThe status command checks whether the OVS daemons ovs-vswitchdand ovsdb-server are running and prints messages with thatinformation.It exits with status 0 if the daemons are running,1 otherwise.The version commandThe version command runs ovsdb-server --version and ovs-vswitchd--version.The force-reload-kmod commandThe force-reload-kmod command allows upgrading the Open vSwitchkernel module without rebooting.It performs the followingtasks:1. Gets a list of OVS \u201cinternal\u201d interfaces, that is, networkdevices implemented by Open vSwitch.The most common examplesof these are bridge \u201clocal ports\u201d.2. Saves the OpenFlow flows of each bridge.3. Stops the Open vSwitch daemons, as if by a call to ovs-ctlstop.4. Saves the kernel configuration state of the OVS internalinterfaces listed in step 1, including IP and IPv6 addressesand routing table entries.5. Unloads the Open vSwitch kernel module (including the bridgecompatibility module if it is loaded).6. Starts OVS back up, as if by a call to ovs-ctl start.Thisreloads the kernel module, restarts the OVS daemons andfinally restores the saved OpenFlow flows.7. Restores the kernel configuration state that was saved in step4.8. Checks for daemons that may need to be restarted because theyhave packet sockets that are listening on old instances ofOpen vSwitch kernel interfaces and, if it finds any, prints awarning on stdout.DHCP is a common example: if the ISC DHCPclient is running on an OVS internal interface, then it willhave to be restarted after completing the above procedure.(It would be nice if ovs-ctl could restart daemonsautomatically, but the details are far too specific to aparticular distribution and installation.)force-kmod-reload internally stops and starts OVS, so it acceptsall of the options accepted by the start command except for the--no-ovs-vswitchd option.The load-kmod commandThe load-kmod command loads the openvswitch kernel modules ifthey are not already loaded.This operation also occurs as partof the start command.The motivation for providing the load-kmodcommand is to allow errors when loading modules to be handledseparately from other errors that may occur when running thestart command.By default the load-kmod command attempts to load the openvswitchkernel module.The enable-protocol commandThe enable-protocol command checks for rules related to aspecified protocol in the system\u2019s iptables(8) configuration.Ifthere are no rules specifically related to that protocol, then itinserts a rule to accept the specified protocol.More specifically:\u2022 If iptables is not installed or not enabled, this command doesnothing, assuming that lack of filtering means that theprotocol is enabled.\u2022 If the INPUT chain has a rule that matches the specifiedprotocol, then this command does nothing, assuming thatwhatever rule is installed reflects the system administrator\u2019sdecisions.\u2022 Otherwise, this command installs a rule that accepts traffic ofthe specified protocol.This command normally completes successfully, even if it doesnothing.Only the failure of an attempt to insert a rulenormally causes it to return an exit code other than 0.The following options control the protocol to be enabled:\u2022 --protocol=<protocol>The name of the IP protocol to be enabled, such as gre or tcp.The default is gre.\u2022 --sport=<sport> or --dport=<dport>TCP or UDP source or destination port to match.These areoptional and allowed only with --protocol=tcp or--protocol=udp.The delete-transient-ports commandDeletes all ports that have the other_config:transient value setto true.The help commandPrints a usage message and exits successfully.",
        "name": "ovs-ctl - OVS startup helper script",
        "section": 8
    },
    {
        "command": "ovs-dpctl",
        "description": "The ovs-dpctl program can create, modify, and delete Open vSwitchdatapaths.A single machine may host any number of datapaths.This program works only with datapaths that are implementedoutside of ovs-vswitchd itself, such as the Linux and Windowskernel-based datapaths.To manage datapaths that are integratedinto ovs-vswitchd, such as the userspace (netdev) datapath, useovs-appctl(8) to invoke the dpctl/* commands, which aredocumented in ovs-vswitchd(8).A newly created datapath is associated with only one networkdevice, a virtual network device sometimes called the datapath's``local port''.A newly created datapath is not, however,associated with any of the host's other network devices.Tointercept and process traffic on a given network device, use theadd-if command to explicitly add that network device to thedatapath.If ovs-vswitchd(8) is in use, use ovs-vsctl(8) instead ofovs-dpctl.Most ovs-dpctl commands that work with datapaths take an argumentthat specifies the name of the datapath.Datapath names take theform [type@]name, where name is the network device associatedwith the datapath's local port.If type is given, it specifiesthe datapath provider of name, otherwise the default providersystem is assumed.The following commands manage datapaths.Do not use commands toadd or remove or modify datapaths if ovs-vswitchd is runningbecause this interferes with ovs-vswitchd's own datapathmanagement.add-dp dp [netdev[,option]...]Creates datapath dp, with a local port also named dp.This will fail if a network device dp already exists.If netdevs are specified, ovs-dpctl adds them to the newdatapath, just as if add-if was specified.del-dp dpDeletes datapath dp.If dp is associated with any networkdevices, they are automatically removed.add-if dp netdev[,option]...Adds each netdev to the set of network devices datapath dpmonitors, where dp is the name of an existing datapath,and netdev is the name of one of the host's networkdevices, e.g. eth0.Once a network device has been addedto a datapath, the datapath has complete ownership of thenetwork device's traffic and the network device appearssilent to the rest of the system.A netdev may be followed by a comma-separated list ofoptions.The following options are currently supported:type=typeSpecifies the type of port to add.The defaulttype is system.port_no=portRequests a specific port number within thedatapath.If this option is not specified then onewill be automatically assigned.key=valueAdds an arbitrary key-value option to the port'sconfiguration.ovs-vswitchd.conf.db(5) documents the available port typesand options.set-if dp port[,option]...Reconfigures each port in dp as specified.An option ofthe form key=value adds the specified key-value option tothe port or overrides an existing key's value.An optionof the form key=, that is, without a value, deletes thekey-value named key.The type and port number of a portcannot be changed, so type and port_no are only allowed ifthey match the existing configuration.del-if dp netdev...Removes each netdev from the list of network devicesdatapath dp monitors.dump-dpsPrints the name of each configured datapath on a separateline.[-s | --statistics] show [dp...]Prints a summary of configured datapaths, including theirdatapath numbers and a list of ports connected to eachdatapath.(The local port is identified as port 0.)If-s or --statistics is specified, then packet and bytecounters are also printed for each port.The datapath numbers consists of flow stats and mega flowmask stats.The \"lookups\" row displays three stats related to flowlookup triggered by processing incoming packets in thedatapath. \"hit\" displays number of packets matchesexisting flows. \"missed\" displays the number of packetsnot matching any existing flow and require user spaceprocessing.\"lost\" displays number of packets destinedfor user space process but subsequently dropped beforereaching userspace. The sum of \"hit\" and \"miss\" equals tothe total number of packets datapath processed.The \"flows\" row displays the number of flows in datapath.The \"masks\" row displays the mega flow mask stats. Thisrow is omitted for datapath not implementing mega flow.\"hit\" displays the total number of masks visited formatching incoming packets. \"total\" displays number ofmasks in the datapath. \"hit/pkt\" displays the averagenumber of masks visited per packet; the ratio between\"hit\" and total number of packets processed by thedatapath.If one or more datapaths are specified, information ononly those datapaths are displayed.Otherwise, ovs-dpctldisplays information about all configured datapaths.DATAPATH FLOW TABLE DEBUGGING COMMANDSThe following commands are primarily useful for debugging OpenvSwitch.The flow table entries (both matches and actions) thatthey work with are not OpenFlow flow entries.Instead, they aredifferent and considerably simpler flows maintained by the OpenvSwitch kernel module.Do not use commands to add or remove ormodify datapath flows if ovs-vswitchd is running because itinterferes with ovs-vswitchd's own datapath flow management.Useovs-ofctl(8), instead, to work with OpenFlow flow entries.The dp argument to each of these commands is optional whenexactly one datapath exists, in which case that datapath is thedefault.When multiple datapaths exist, then a datapath name isrequired.[-m | --more] [--names | --no-names] dump-flows [dp][filter=filter] [type=type] [pmd=pmd]Prints to the console all flow entries in datapath dp'sflow table.Without -m or --more, output omits matchfields that a flow wildcards entirely; with -m or --more,output includes all wildcarded fields.If filter=filter is specified, only displays the flowsthat match the filter. filter is a flow in the formsimilar to that accepted by ovs-ofctl(8)'s add-flowcommand. (This is not an OpenFlow flow: besides otherdifferences, it never contains wildcards.)The filter isalso useful to match wildcarded fields in the datapathflow. As an example, filter='tcp,tp_src=100' will matchthe datapath flow containing'tcp(src=80/0xff00,dst=8080/0xff)'.If pmd=pmd is specified, only displays flows of thespecified pmd.Using pmd=-1 will restrict the dump toflows from the main thread.This option is only supportedby the userspace datapath.If type=type is specified, only displays flows of thespecified types.This option supported only forovs-appctl dpctl/dump-flows.type is a comma separatedlist, which can contain any of the following:ovs - displays flows handled in the ovs dptc - displays flows handled in the tc dpdpdk - displays flows fully offloaded by dpdkoffloaded - displays flows offloaded to the HWnon-offloaded - displays flows not offloaded to the HWpartially-offloaded - displays flows where only part oftheir proccessing is done in HWall - displays all the types of flowsBy default all the types of flows are displayed.ovs-dpctl always acts as if the type was ovs.add-flow [dp] flow actions[--clear] [--may-create] [-s | --statistics] mod-flow [dp] flowactionsAdds or modifies a flow in dp's flow table that, when apacket matching flow arrives, causes actions to beexecuted.The add-flow command succeeds only if flow does notalready exist in dp.Contrariwise, mod-flow without--may-create only modifies the actions for an existingflow.With --may-create, mod-flow will add a new flow ormodify an existing one.If -s or --statistics is specified, then mod-flow printsthe modified flow's statistics.A flow's statistics arethe number of packets and bytes that have passed throughthe flow, the elapsed time since the flow last processed apacket (if ever), and (for TCP flows) the union of the TCPflags processed through the flow.With --clear, mod-flow zeros out the flow's statistics.The statistics printed if -s or --statistics is alsospecified are those from just before clearing thestatistics.NOTE: flow and actions do not match the syntax used withovs-ofctl(8)'s add-flow command.Usage ExamplesForward ARP between ports 1 and 2 on datapath myDP:ovs-dpctl add-flow myDP \\\"in_port(1),eth(),eth_type(0x0806),arp()\" 2ovs-dpctl add-flow myDP \\\"in_port(2),eth(),eth_type(0x0806),arp()\" 1Forward all IPv4 traffic between two addresses on ports 1and 2:ovs-dpctl add-flow myDP \\\"in_port(1),eth(),eth_type(0x800),\\ipv4(src=172.31.110.4,dst=172.31.110.5)\" 2ovs-dpctl add-flow myDP \\\"in_port(2),eth(),eth_type(0x800),\\ipv4(src=172.31.110.5,dst=172.31.110.4)\" 1add-flows [dp] filemod-flows [dp] filedel-flows [dp] fileReads flow entries from file (or stdin if file is -) andadds, modifies, or deletes each entry to the datapath.Each flow specification (e.g., each line in file) maystart with add, modify, or delete keyword to specifywhether a flow is to be added, modified, or deleted. Aflow specification without one of these keywords istreated based on the used command.All flow modificationsare executed as individual transactions in the orderspecified.[-s | --statistics] del-flow [dp] flowDeletes the flow from dp's flow table that matches flow.If -s or --statistics is specified, then del-flow printsthe deleted flow's statistics.[-m | --more] [--names | --no-names] get-flow [dp] ufid:ufidFetches the flow from dp's flow table with uniqueidentifier ufid.ufid must be specified as a string of 32hexadecimal characters.del-flows [dp]Deletes all flow entries from datapath dp's flow table.DATAPATH FLOW CACHE COMMANDSThe following commands are useful for debugging and configuringthe datapath flow cache settings.cache-get-size [dp]Prints the current cache sizes to the console.cache-set-size dp cache sizeSet the dp's specific cache to the given size.The cachename can be found by using the cache-get-size command.CONNECTION TRACKING TABLE COMMANDSThe following commands are useful for debugging and configuringthe connection tracking table in the datapath.The dp argument to each of these commands is optional whenexactly one datapath exists, in which case that datapath is thedefault.When multiple datapaths exist, then a datapath name isrequired.N.B.(Linux specific): the system datapaths (i.e. the Linux kernelmodule Open vSwitch datapaths) share a single connection trackingtable (which is also used by other kernel subsystems, such asiptables, nftables and the regular host stack).Therefore, thefollowing commands do not apply specifically to one datapath.ipf-set-enabled [dp] v4|v6ipf-set-disabled [dp] v4|v6Enables or disables IP fragmentation handling for theuserspace connection tracker.Either v4 or v6 must bespecified.Both IPv4 and IPv6 fragment reassembly areenabled by default.Only supported for the userspacedatapath.ipf-set-min-frag [dp] v4|v6 minfragSets the minimum fragment size (L3 header and data) fornon-final fragments to minfrag.Either v4 or v6 must bespecified.For enhanced DOS security, higher minimumfragment sizes can usually be used.The default IPv4value is 1200 and the clamped minimum is 400.The defaultIPv6 value is 1280, with a clamped minimum of 400, fortesting flexibility.The maximum fragment size is notclamped, however, setting this value too high might resultin valid fragments being dropped.Only supported foruserspace datapath.ipf-set-max-nfrags [dp] maxfragsSets the maximum number of fragments tracked by theuserspace datapath connection tracker to maxfrags.Thedefault value is 1000 and the clamped maximum is 5000.Note that packet buffers can be held by the fragmentationmodule while fragments are incomplete, but will timeoutafter 15 seconds.Memory pool sizing should be setaccordingly when fragmentation is enabled.Only supportedfor userspace datapath.[-m | --more] ipf-get-status [dp]Gets the configuration settings and fragment countersassociated with the fragmentation handling of theuserspace datapath connection tracker.With -m or --more,also dumps the IP fragment lists.Only supported foruserspace datapath.[-m | --more] [-s | --statistics] dump-conntrack [dp] [zone=zone]Prints to the console all the connection entries in thetracker used by dp.If zone=zone is specified, only showsthe connections in zone.With --more, some implementationspecific details are included. With --statistics timeoutsand timestamps are added to the output.flush-conntrack [dp] [zone=zone] [ct-origin-tuple [ct-reply-tuple]]Flushes the connection entries in the tracker used by dpbased on zone and connection tracking tuple ct-origin-tuple.If ct-tuple is not provided, flushes all theconnection entries.If zone=zone is specified, onlyflushes the connections in zone.If ct-[orig|reply]-tuple is provided, flushes theconnection entry specified by ct-[orig|reply]-tuple inzone.The zone defaults to 0 if it is not provided.Theuserspace connection tracker requires flushing with theoriginal pre-NATed tuple and a warning log will beotherwise generated.The tuple can be partial and willremove all connections that are matching on the specifiedfields.In order to specify only ct-reply-tuple, provideempty string as ct-origin-tuple.Note: Currently there is a limitation for matching onICMP, in order to partially match on ICMP parameters thect-[orig|reply]-tuple has to include either source ordestination IP.An example of an IPv4 ICMP ct-[orig|reply]-tuple:\"ct_nw_src=10.1.1.1,ct_nw_dst=10.1.1.2,ct_nw_proto=1,icmp_type=8,icmp_code=0,icmp_id=10\"An example of an IPv6 TCP ct-[orig|reply]-tuple:\"ct_ipv6_src=fc00::1,ct_ipv6_dst=fc00::2,ct_nw_proto=6,ct_tp_src=1,ct_tp_dst=2\"[-m | --more] ct-stats-show [dp] [zone=zone]Displays the number of connections grouped by protocolused by dp.If zone=zone is specified, numbers refer tothe connections in zone.With --more, groups byconnection state for each protocol.ct-bkts [dp] [gt=threshold]For each conntrack bucket, displays the number ofconnections used by dp.If gt=threshold is specified,bucket numbers are displayed when the number ofconnections in a bucket is greater than threshold.ct-set-maxconns [dp] maxconnsSets the maximum limit of connection tracker entries tomaxconns on dp.This can be used to reduce the processingload on the system due to connection tracking or simplylimiting connection tracking.If the number ofconnections is already over the new maximum limit requestthen the new maximum limit will be enforced when thenumber of connections decreases to that limit, whichnormally happens due to connection expiry.Only supportedfor userspace datapath.ct-get-maxconns [dp]Prints the maximum limit of connection tracker entries ondp.Only supported for userspace datapath.ct-get-nconns [dp]Prints the current number of connection tracker entries ondp.Only supported for userspace datapath.ct-enable-tcp-seq-chk [dp]ct-disable-tcp-seq-chk [dp]Enables or disables TCP sequence checking.When set todisabled, all sequence number verification is disabled,including for TCP resets.This is similar, but not thesame as 'be_liberal' mode, as in Netfilter.Disablingsequence number verification is not an optimization initself, but is needed for some hardware offload supportwhich might offer some performance advantage. Sequencenumber checking is enabled by default to enforce bettersecurity and should only be disabled if required forhardware offload support.This command is only supportedfor the userspace datapath.ct-get-tcp-seq-chk [dp]Prints whether TCP sequence checking is enabled ordisabled on dp.Only supported for the userspacedatapath.ct-set-sweep-interval [dp] msSets the sweep interval. Only supported for the userspacedatapath.ct-get-sweep-interval [dp]Prints the current sweep interval in ms. Only supportedfor the userspace datapath.ct-set-limits [dp] [default=default_limit][zone=zone,limit=limit]...Sets the maximum allowed number of connections in aconnection tracking zone.A specific zone may be set tolimit, and multiple zones may be specified with a comma-separated list.If a per-zone limit for a particular zoneis not specified in the datapath, it defaults to thedefault per-zone limit.A default zone may be specifiedwith the default=default_limit argument.Initially, thedefault per-zone limit is unlimited.An unlimited numberof entries may be set with 0 limit.ct-del-limits [dp] zone=zone[,zone]...Deletes the connection tracking limit for zone.Multiplezones may be specified with a comma-separated list.ct-get-limits [dp] [zone=zone[,zone]...]Retrieves the maximum allowed number of connections andcurrent counts per-zone.If zone is given, only thespecified zone(s) are printed.If no zones are specified,all the zone limits and counts are provided.The commandalways displays the default zone limit.",
        "name": "ovs-dpctl - administer Open vSwitch datapaths",
        "section": 8
    },
    {
        "command": "ovs-dpctl-top",
        "description": "This program summarizes ovs-dpctl flow content by aggregating thenumber of packets, total bytes and occurrence of the followingfields:- Datapath in_port- Ethernet type- Source and destination MAC addresses- IP protocol- Source and destination IPv4 addresses- Source and destination IPv6 addresses- UDP and TCP destination port- Tunnel source and destination addressesOutput shows four values:- FIELDS: the flow fields for example in_port(1).- COUNT: the number of lines in the dump-flow outputcontain the flow field.- PACKETS: the total number of packets containing the flowfield.- BYTES: the total number of bytes containing the flowfield.If units are not present then values are in bytes.- AVERAGE: the average packets size (BYTES/PACKET).Top BehaviorWhile in top mode, the default behavior, the following singlecharacter commands are supported:a - toggles top in accumulate and live mode.Accumulatemode is described below.s - toggles which column is used to sort content indecreasing order.A DESC title is placed over the column._ - a space indicating to collect dump-flow content againh - halt output.Any character will restart samplingf - cycle through flow fieldsq - q for quit.Accumulate ModeThere are two supported modes: live and accumulate.The defaultis live.The parameter --accumulateor the 'a' character in topmode enables the latter.In live mode, recent dump-flow contentis presented.Where as accumulate mode keeps track of the priorhistorical information until the flow is reset not when the flowis purged.Reset flows are determined when the packet count fora flow has decreased from its previous sample.There is onecaveat, eventually the system will run out of memory if, afterthe accumulate-decay period any flows that have not beenrefreshed are purged.The goal here is to free memory of flowsthat are not active.Statistics are not decremented.Theirpurpose is to reflect the overall history of the flow fields.Debugging ErrorsParsing errors are counted and displayed in the status line atthe beginning of the output.Use the --verbose option with--script to see what output was not parsed, like this:$ ovs-dpctl dump-flows | ovs-dpctl-top --script --verboseError messages will identify content that failed to parse.Access Remote HostsThe --host must follow the format user@hostname.This scriptsimply calls 'ssh user@Hostname' without checking for logincredentials therefore public keys should be installed on thesystem identified by hostname, such as:$ ssh-copy-id user@hostnameConsult ssh-copy-id man pages for more details.Expected usage$ ovs-dpctl-topor to run as a script:$ ovs-dpctl dump-flows > dump-flows.log$ ovs-dpctl-top --script --flow-file dump-flows.logOPTIONS-h, --helpshow this help message and exit.-v, --versionshow program's version number and exit.-f FLOWFILES, --flow-file FLOWFILESfile containing flows from ovs-dpctl dump-flow.-V, --verboseenable debug level verbosity.-s, --scriptRun from a script (no user interface).--host HOSTSpecify a user@host for retrieving flows see AccessingRemote Hosts for more information.-a, --accumulateAccumulate dump-flow content.--accumulate-decay ACCUMULATEDECAYDecay old accumulated flows.The default is 5 minutes. Avalue of 0 disables decay.-d DELAY, --delay DELAYDelay in milliseconds to collect dump-flow content (samplerate).",
        "name": "ovs-dpctl-top - Top like behavior for ovs-dpctl dump-flows",
        "section": 8
    },
    {
        "command": "ovs-kmod-ctl",
        "description": "The ovs-kmod-ctl program is responsible for inserting andremoving Open vSwitch kernel modules.It is not meant to beinvoked directly by system administrators but to be calledinternally by system startup scripts.The script is used as partof an SELinux transition domain.Each of ovs-kmod-ctl's commands is described separately below.",
        "name": "ovs-kmod-ctl - OVS startup helper script for loading kernelmodules",
        "section": 8
    },
    {
        "command": "ovs-l3ping",
        "description": "The ovs-l3ping program may be used to check for problems thatcould be caused by invalid routing policy, misconfigured firewallin the tunnel path or a bad NIC driver.On one of the nodes, runovs-l3ping in server mode and on the other node run it in clientmode.The client and server will establish L3 tunnel, over whichclient will give further testing instructions. The ovs-l3pingclient will perform UDP and TCP tests.This tool is differentfrom ovs-test that it encapsulates XML/RPC control connectionover the tunnel, so there is no need to open special holes infirewall.UDP tests can report packet loss and achieved bandwidth forvarious datagram sizes. By default target bandwidth for UDP testsis 1Mbit/s.TCP tests report only achieved bandwidth, because kernel TCPstack takes care of flow control and packet loss.Client ModeAn ovs-l3ping client will create a L3 tunnel and connect over itto the ovs-l3ping server to schedule the tests.<TunnelRemoteIP>is the peer\u2019s IP address, where tunnel will be terminated.<InnerIP> is the address that will be temporarily assigned duringtesting.All test traffic originating from this IP address tothe <RemoteInnerIP> will be tunneled.It is possible to overridedefault <ControlPort> and <DataPort>, if there is any otherapplication that already listens on those two ports.Server ModeTo conduct tests, ovs-l3ping server must be running.It isrequired that both client and server <InnerIP> addresses are inthe same subnet.It is possible to specify <InnerIP> withnetmask in CIDR format.",
        "name": "ovs-l3ping - check network deployment for L3 tunneling problems",
        "section": 8
    },
    {
        "command": "ovs-ofctl",
        "description": "The ovs-ofctl program is a command line tool for monitoring andadministering OpenFlow switches.It can also show the currentstate of an OpenFlow switch, including features, configuration,and table entries.It should work with any OpenFlow switch, notjust Open vSwitch.OpenFlow Switch Management CommandsThese commands allow ovs-ofctl to monitor and administer anOpenFlow switch.It is able to show the current state of aswitch, including features, configuration, and table entries.Most of these commands take an argument that specifies the methodfor connecting to an OpenFlow switch.The following connectionmethods are supported:ssl:host[:port]tcp:host[:port]The specified port on the given host, which can beexpressed either as a DNS name (if built withunbound library) or an IP address in IPv4 or IPv6address format.Wrap IPv6 addresses in squarebrackets, e.g. tcp:[::1]:6653.On Linux, use%device to designate a scope for IPv6 link-leveladdresses, e.g. tcp:[fe80::1234%eth0]:6653.Forssl, the --private-key, --certificate, and--ca-cert options are mandatory.If port is not specified, it defaults to 6653.unix:fileOn POSIX, a Unix domain server socket named file.On Windows, connect to a local named pipe that isrepresented by a file created in the path file tomimic the behavior of a Unix domain socket.fileThis is short for unix:file, as long as file doesnot contain a colon.bridge This is short forunix:/usr/local/var/run/openvswitch/bridge.mgmt, aslong as bridge does not contain a colon.[type@]dpAttempts to look up the bridge associated with dpand open as above.If type is given, it specifiesthe datapath provider of dp, otherwise the defaultprovider system is assumed.show switchPrints to the console information on switch, includinginformation on its flow tables and ports.dump-tables switchPrints to the console statistics for each of the flowtables used by switch.dump-table-features switchPrints to the console features for each of the flow tablesused by switch.dump-table-desc switchPrints to the console configuration for each of the flowtables used by switch for OpenFlow 1.4+.mod-table switch table settingThis command configures flow table settings in switch forOpenFlow table table, which may be expressed as a numberor (unless --no-names is specified) a name.The available settings depend on the OpenFlow version inuse.In OpenFlow 1.1 and 1.2 (which must be enabled withthe -O option) only, mod-table configures behavior when noflow is found when a packet is looked up in a flow table.The following setting values are available:dropDrop the packet.continueContinue to the next table in the pipeline.(Thisis how an OpenFlow 1.0 switch always handlespackets that do not match any flow, in tables otherthan the last one.)controllerSend to controller.(This is how an OpenFlow 1.0switch always handles packets that do not match anyflow in the last table.)In OpenFlow 1.3 and later (which must be enabled with the-O option) and Open vSwitch 2.11 and later only, mod-tablecan change the name of a table:name:new-nameChanges the name of the table to new-name.Use anempty new-name to clear the name.(This will beineffective if the name is set via the name columnin the Flow_Table table in the Open_vSwitchdatabase as described in ovs-vswitchd.conf.db(5).)In OpenFlow 1.4 and later (which must be enabled with the-O option) only, mod-table configures the behavior when acontroller attempts to add a flow to a flow table that isfull.The following setting values are available:evictDelete some existing flow from the flow table,according to the algorithm described for theFlow_Table table in ovs-vswitchd.conf.db(5).noevictRefuse to add the new flow.(Eviction might stillbe enabled through the overflow_policy column inthe Flow_Table table documented inovs-vswitchd.conf.db(5).)vacancy:low,highEnables sending vacancy events to controllers usingTABLE_STATUS messages, based on percentagethresholds low and high.novacancyDisables vacancy events.dump-ports switch [netdev]Prints to the console statistics for network devicesassociated with switch.If netdev is specified, only thestatistics associated with that device will be printed.netdev can be an OpenFlow assigned port number or devicename, e.g. eth0.dump-ports-desc switch [port]Prints to the console detailed information about networkdevices associated with switch.To dump only a specificport, specify its number as port.Otherwise, if port isomitted, or if it is specified as ANY, then all ports areprinted.This is a subset of the information provided bythe show command.If the connection to switch negotiates OpenFlow 1.0, 1.2,or 1.2, this command uses an OpenFlow extension onlyimplemented in Open vSwitch (version 1.7 and later).Only OpenFlow 1.5 and later support dumping a specificport.Earlier versions of OpenFlow always dump all ports.mod-port switch port actionModify characteristics of port port in switch.port maybe an OpenFlow port number or name (unless --no-names isspecified) or the keyword LOCAL (the preferred way torefer to the OpenFlow local port).The action may be anyone of the following:updownEnable or disable the interface.This isequivalent to ip link set up or ip link set down ona Unix system.stpno-stp Enable or disable 802.1D spanning tree protocol(STP) on the interface.OpenFlow implementationsthat don't support STP will refuse to enable it.receiveno-receivereceive-stpno-receive-stpEnable or disable OpenFlow processing of packetsreceived on this interface.When packet processingis disabled, packets will be dropped instead ofbeing processed through the OpenFlow table.Thereceive or no-receive setting applies to allpackets except 802.1D spanning tree packets, whichare separately controlled by receive-stp orno-receive-stp.forwardno-forwardAllow or disallow forwarding of traffic to thisinterface.By default, forwarding is enabled.floodno-floodControls whether an OpenFlow flood action will sendtraffic out this interface.By default, floodingis enabled.Disabling flooding is primarily usefulto prevent loops when a spanning tree protocol isnot in use.packet-inno-packet-inControls whether packets received on this interfacethat do not match a flow table entry generate a``packet in'' message to the OpenFlow controller.By default, ``packet in'' messages are enabled.The show command displays (among other information) theconfiguration that mod-port changes.get-frags switchPrints switch's fragment handling mode.See set-frags,below, for a description of each fragment handling mode.The show command also prints the fragment handling modeamong its other output.set-frags switch frag_modeConfigures switch's treatment of IPv4 and IPv6 fragments.The choices for frag_mode are:normal Fragments pass through the flow table like non-fragmented packets.The TCP ports, UDP ports, andICMP type and code fields are always set to 0, evenfor fragments where that information wouldotherwise be available (fragments with offset 0).This is the default fragment handling mode for anOpenFlow switch.dropFragments are dropped without passing through theflow table.reassembleThe switch reassembles fragments into full IPpackets before passing them through the flow table.Open vSwitch does not implement this fragmenthandling mode.nx-matchFragments pass through the flow table like non-fragmented packets.The TCP ports, UDP ports, andICMP type and code fields are available formatching for fragments with offset 0, and set to 0in fragments with nonzero offset.This mode is aNicira extension.See the description of ip_frag, in ovs-fields(7), for away to match on whether a packet is a fragment and on itsfragment offset.dump-flows switch [flows]Prints to the console all flow entries in switch's tablesthat match flows.If flows is omitted, all flows in theswitch are retrieved.See Flow Syntax, below, for thesyntax of flows.The output format is described in TableEntry Output.By default, ovs-ofctl prints flow entries in the sameorder that the switch sends them, which is unlikely to beintuitive or consistent.Use --sort and --rsort tocontrol display order.The --names/--no-names and--stats/--no-stats options also affect output formatting.See the descriptions of these options, under OPTIONSbelow, for more informationdump-aggregate switch [flows]Prints to the console aggregate statistics for flows inswitch's tables that match flows.If flows is omitted,the statistics are aggregated across all flows in theswitch's flow tables.See Flow Syntax, below, for thesyntax of flows.The output format is described in TableEntry Output.queue-stats switch [port [queue]]Prints to the console statistics for the specified queueon port within switch.port can be an OpenFlow portnumber or name, the keyword LOCAL (the preferred way torefer to the OpenFlow local port), or the keyword ALL.Either of port or queue or both may be omitted (orequivalently the keyword ALL).If both are omitted,statistics are printed for all queues on all ports.Ifonly queue is omitted, then statistics are printed for allqueues on port; if only port is omitted, then statisticsare printed for queue on every port where it exists.queue-get-config switch [port [queue]]Prints to the console the configuration of queue on portin switch.If port is omitted or ANY, reports queues forall port.If queue is omitted or ANY, reports all queues.For OpenFlow 1.3 and earlier, the output always includesall queues, ignoring queue if specified.This command has limited usefulness, because ports oftenhave no configured queues and because the OpenFlowprotocol provides only very limited information about theconfiguration of a queue.dump-ipfix-bridge switchPrints to the console the statistics of bridge IPFIX forswitch.If bridge IPFIX is configured on the switch,IPFIX statistics can be retrieved.Otherwise, errormessage will be printed.This command uses an Open vSwitch extension that is onlyin Open vSwitch 2.6 and later.dump-ipfix-flow switchPrints to the console the statistics of flow-based IPFIXfor switch.If flow-based IPFIX is configured on theswitch, statistics of all the collector set ids on theswitch will be printed.Otherwise, print error message.Refer to ovs-vswitchd.conf.db(5) for more details onconfiguring flow based IPFIX and collector set ids.This command uses an Open vSwitch extension that is onlyin Open vSwitch 2.6 and later.ct-flush-zone switch zoneFlushes the connection tracking entries in zone on switch.This command uses an Open vSwitch extension that is onlyin Open vSwitch 2.6 and later.ct-flush switch [zone=N] [ct-orig-tuple [ct-reply-tuple]]Flushes the connection entries on switch based on zone andconnection tracking tuples ct-[orig|reply]-tuple.If ct-[orig|reply]-tuple is not provided, flushes all theconnection entries.If zone is specified, only flushesthe connections in zone.If ct-[orig|reply]-tuple is provided, flushes theconnection entry specified by ct-[orig|reply]-tuple inzone.The zone defaults to 0 if it is not provided.Theuserspace connection tracker requires flushing with theoriginal pre-NATed tuple and a warning log will beotherwise generated.The tuple can be partial and willremove all connections that are matching on the specifiedfields.In order to specify only ct-reply-tuple, provideempty string as ct-orig-tuple.Note: Currently there is limitation for matching on ICMP,in order to partially match on ICMP parameters thect-[orig|reply]-tuple has to include either source ordestination IP.An example of an IPv4 ICMP ct-[orig|reply]-tuple:\"ct_nw_src=10.1.1.1,ct_nw_dst=10.1.1.2,ct_nw_proto=1,icmp_type=8,icmp_code=0,icmp_id=10\"An example of an IPv6 TCP ct-[orig|reply]-tuple:\"ct_ipv6_src=fc00::1,ct_ipv6_dst=fc00::2,ct_nw_proto=6,ct_tp_src=1,ct_tp_dst=2\"This command uses an Open vSwitch extension that is onlyin Open vSwitch 3.1 and later.OpenFlow Switch Flow Table CommandsThese commands manage the flow table in an OpenFlow switch.Ineach case, flow specifies a flow entry in the format described inFlow Syntax, below, file is a text file that contains zero ormore flows in the same syntax, one per line, and the optional--bundle option operates the command as a single atomictransaction, see option --bundle, below.[--bundle] add-flow switch flow[--bundle] add-flow switch - < file[--bundle] add-flows switch fileAdd each flow entry to switch's tables.Each flowspecification (e.g., each line in file) may start withadd, modify, delete, modify_strict, or delete_strictkeyword to specify whether a flow is to be added,modified, or deleted, and whether the modify or delete isstrict or not.For backwards compatibility a flowspecification without one of these keywords is treated asa flow add.All flow mods are executed in the orderspecified.[--bundle] [--strict] mod-flows switch flow[--bundle] [--strict] mod-flows switch - < fileModify the actions in entries from switch's tables thatmatch the specified flows.With --strict, wildcards arenot treated as active for matching purposes.[--bundle] del-flows switch[--bundle] [--strict] del-flows switch [flow][--bundle] [--strict] del-flows switch - < fileDeletes entries from switch's flow table.With only aswitch argument, deletes all flows.Otherwise, deletesflow entries that match the specified flows.With--strict, wildcards are not treated as active for matchingpurposes.[--bundle] [--readd] replace-flows switch fileReads flow entries from file (or stdin if file is -) andqueries the flow table from switch.Then it fixes up anydifferences, adding flows from flow that are missing onswitch, deleting flows from switch that are not in file,and updating flows in switch whose actions, cookie, ortimeouts differ in file.With --readd, ovs-ofctl adds all the flows from file, eventhose that exist with the same actions, cookie, andtimeout in switch.In OpenFlow 1.0 and 1.1, re-adding aflow always resets the flow's packet and byte counters to0, and in OpenFlow 1.2 and later, it does so only if thereset_counts flag is set.diff-flows source1 source2Reads flow entries from source1 and source2 and prints thedifferences.A flow that is in source1 but not in source2is printed preceded by a -, and a flow that is in source2but not in source1 is printed preceded by a +.If a flowexists in both source1 and source2 with different actions,cookie, or timeouts, then both versions are printedpreceded by - and +, respectively.source1 and source2 may each name a file or a switch.Ifa name begins with / or ., then it is considered to be afile name.A name that contains : is considered to be aswitch.Otherwise, it is a file if a file by that nameexists, a switch if not.For this command, an exit status of 0 means that nodifferences were found, 1 means that an error occurred,and 2 means that some differences were found.packet-out switch packet-outConnects to switch and instructs it to execute the packet-out OpenFlow message, specified as defined in Packet-OutSyntax section.Group Table CommandsThese commands manage the group table in an OpenFlow switch.Ineach case, group specifies a group entry in the format describedin Group Syntax, below, and file is a text file that containszero or more groups in the same syntax, one per line, and theoptional --bundle option operates the command as a single atomictransaction, see option --bundle, below.The group commands work only with switches that support OpenFlow1.1 or later or the Open vSwitch group extensions to OpenFlow 1.0(added in Open vSwitch 2.9.90).For OpenFlow 1.1 or later, it isnecessary to explicitly enable these protocol versions inovs-ofctl (using -O).For more information, see ``Q: Whatversions of OpenFlow does Open vSwitch support?'' in the OpenvSwitch FAQ.[--bundle] add-group switch group[--bundle] add-group switch - < file[--bundle] add-groups switch fileAdd each group entry to switch's tables.Each groupspecification (e.g., each line in file) may start withadd, modify, add_or_mod, delete, insert_bucket, orremove_bucket keyword to specify whether a flow is to beadded, modified, or deleted, or whether a group bucket isto be added or removed.For backwards compatibility agroup specification without one of these keywords istreated as a group add.All group mods are executed inthe order specified.[--bundle] [--may-create] mod-group switch group[--bundle] [--may-create] mod-group switch - < fileModify the action buckets in entries from switch's tablesfor each group entry.If a specified group does notalready exist, then without --may-create, this command hasno effect; with --may-create, it creates a new group.The--may-create option uses an Open vSwitch extension toOpenFlow only implemented in Open vSwitch 2.6 and later.[--bundle] del-groups switch[--bundle] del-groups switch [group][--bundle] del-groups switch - < fileDeletes entries from switch's group table.With only aswitch argument, deletes all groups.Otherwise, deletesthe group for each group entry.[--bundle] insert-buckets switch group[--bundle] insert-buckets switch - < fileAdd buckets to an existing group present in the switch'sgroup table.If no command_bucket_id is present in thegroup specification then all buckets of the group areremoved.[--bundle] remove-buckets switch group[--bundle] remove-buckets switch - < fileRemove buckets to an existing group present in theswitch's group table.If no command_bucket_id is presentin the group specification then all buckets of the groupare removed.dump-groups switch [group]Prints group entries in switch's tables to console.Todump only a specific group, specify its number as group.Otherwise, if group is omitted, or if it is specified asALL, then all groups are printed.Only OpenFlow 1.5 and later support dumping a specificgroup.Earlier versions of OpenFlow always dump allgroups.dump-group-features switchPrints to the console the group features of the switch.dump-group-stats switch [group]Prints to the console statistics for the specified groupin switch's tables.If group is omitted then statisticsfor all groups are printed.OpenFlow 1.3+ Switch Meter Table CommandsThese commands manage the meter table in an OpenFlow switch.Ineach case, meter specifies a meter entry in the format describedin Meter Syntax, below.OpenFlow 1.3 introduced support for meters, so these commandsonly work with switches that support OpenFlow 1.3 or later.Itis necessary to explicitly enable these protocol versions inovs-ofctl (using -O) and in the switch itself (with the protocolscolumn in the Bridge table).For more information, see ``Q: Whatversions of OpenFlow does Open vSwitch support?'' in the OpenvSwitch FAQ.add-meter switch meterAdd a meter entry to switch's tables. The meter syntax isdescribed in section Meter Syntax, below.mod-meter switch meterModify an existing meter.del-meters switch [meter]Delete entries from switch's meter table.To delete onlya specific meter, specify its number as meter.Otherwise,if meter is omitted, or if it is specified as all, thenall meters are deleted.dump-meters switch [meter]Print entries from switch's meter table.To print only aspecific meter, specify its number as meter.Otherwise,if meter is omitted, or if it is specified as all, thenall meters are printed.meter-stats switch [meter]Print meter statistics.meter can specify a single meterwith syntax meter=id, or all meters with syntax meter=all.meter-features switchPrint meter features.OpenFlow Switch Bundle CommandTransactional updates to both flow and group tables can be madewith the bundle command.file is a text file that contains zeroor more flow mods, group mods, or packet-outs in Flow Syntax,Group Syntax, or Packet-Out Syntax, each line preceded by flow,group, or packet-out keyword, correspondingly.The flow keywordmay be optionally followed by one of the keywords add, modify,modify_strict, delete, or delete_strict, of which the add isassumed if a bare flow is given.Similarly, the group keywordmay be optionally followed by one of the keywords add, modify,add_or_mod, delete, insert_bucket, or remove_bucket, of which theadd is assumed if a bare group is given.bundle switch fileExecute all flow and group mods in file as a single atomictransaction against switch's tables.All bundled mods areexecuted in the order specified.OpenFlow Switch Tunnel TLV Table CommandsOpen vSwitch maintains a mapping table between tunnel option TLVs(defined by <class, type, length>) and NXM fields tun_metadatan,where n ranges from 0 to 63, that can be operated on for thepurposes of matches, actions, etc. This TLV table can be used forGeneve option TLVs or other protocols with options in same TLVformat as Geneve options. This mapping must be explicitlyspecified by the user through the following commands.A TLV mapping is specified with the syntax{class=class,type=type,len=length}->tun_metadatan.When anoption mapping exists for a given tun_metadatan, matching on thedefined field becomes possible, e.g.:ovs-ofctl add-tlv-map br0\"{class=0xffff,type=0,len=4}->tun_metadata0\"ovs-ofctl add-flow br0tun_metadata0=1234,actions=controllerA mapping should not be changed while it is in active use by aflow. The result of doing so is undefined.These commands are Nicira extensions to OpenFlow and require OpenvSwitch 2.5 or later.add-tlv-map switch option[,option]...Add each option to switch's tables. Duplicate fields arerejected.del-tlv-map switch [option[,option]]...Delete each option from switch's table, or all option TLVmapping if no option is specified.Fields that aren'tmapped are ignored.dump-tlv-map switchShow the currently mapped fields in the switch's optiontable as well as switch capabilities.OpenFlow Switch Monitoring Commandssnoop switchConnects to switch and prints to the console all OpenFlowmessages received.Unlike other ovs-ofctl commands, ifswitch is the name of a bridge, then the snoop commandconnects to a Unix domain socket named/usr/local/var/run/openvswitch/switch.snoop.ovs-vswitchdlistens on such a socket for each bridge and sends to itall of the OpenFlow messages sent to or received from itsconfigured OpenFlow controller.Thus, this command can beused to view OpenFlow protocol activity between a switchand its controller.When a switch has more than one controller configured,only the traffic to and from a single controller isoutput.If none of the controllers is configured as aprimary or a secondary (using a Nicira extension toOpenFlow 1.0 or 1.1, or a standard request in OpenFlow 1.2or later), then a controller is chosen arbitrarily amongthem.If there is a primary controller, it is chosen;otherwise, if there are any controllers that are notprimaries or secondaries, one is chosen arbitrarily;otherwise, a secondary controller is chosen arbitrarily.This choice is made once at connection time and does notchange as controllers reconfigure their roles.If a switch has no controller configured, or if theconfigured controller is disconnected, no traffic is sent,so monitoring will not show any traffic.monitor switch [miss-len] [invalid_ttl] [watch:[spec...]]Connects to switch and prints to the console all OpenFlowmessages received.Usually, switch should specify thename of a bridge in the ovs-vswitchd database. This isavailable only in OpenFlow 1.0 as Nicira extension.If miss-len is provided, ovs-ofctl sends an OpenFlow ``setconfiguration'' message at connection setup time thatrequests miss-len bytes of each packet that misses theflow table.Open vSwitch does not send these and otherasynchronous messages to an ovs-ofctl monitor clientconnection unless a nonzero value is specified on thisargument.(Thus, if miss-len is not specified, verylittle traffic will ordinarily be printed.)If invalid_ttl is passed, ovs-ofctl sends an OpenFlow``set configuration'' message at connection setup timethat requests INVALID_TTL_TO_CONTROLLER, so that ovs-ofctlmonitor can receive ``packet-in'' messages when TTLreaches zero on dec_ttl action.Only OpenFlow 1.1 and 1.2support invalid_ttl; Open vSwitch also implements it forOpenFlow 1.0 as an extension.watch:[spec...] causes ovs-ofctl to send a ``monitorrequest'' Nicira extension message to the switch atconnection setup time.This message causes the switch tosend information about flow table changes as they occur.The following comma-separated spec syntax is available:!initialDo not report the switch's initial flow tablecontents.!addDo not report newly added flows.!deleteDo not report deleted flows.!modifyDo not report modifications to existing flows.!ownAbbreviate changes made to the flow table byovs-ofctl's own connection to the switch.(Thesecould only occur using the ofctl/send commanddescribed below under RUNTIME MANAGEMENT COMMANDS.)!actionsDo not report actions as part of flow updates.table=tableLimits the monitoring to the table with the giventable, which may be expressed as a number between 0and 254 or (unless --no-names is specified) a name.By default, all tables are monitored.out_port=portIf set, only flows that output to port aremonitored.The port may be an OpenFlow port numberor keyword (e.g. LOCAL).out_group=groupIf set, only flows that output to group number aremonitored.This field requires OpenFlow 1.4(-OOpenFlow14) or later.field=valueMonitors only flows that have field specified asthe given value.Any syntax valid for matching ondump-flows may be used.This command may be useful for debugging switch orcontroller implementations.With watch:, it isparticularly useful for observing how a controller updatesflow tables.OpenFlow Switch and Controller CommandsThe following commands, like those in the previous section, maybe applied to OpenFlow switches, using any of the connectionmethods described in that section.Unlike those commands, thesemay also be applied to OpenFlow controllers.probe targetSends a single OpenFlow echo-request message to target andwaits for the response.With the -t or --timeout option,this command can test whether an OpenFlow switch orcontroller is up and running.ping target [n]Sends a series of 10 echo request packets to target andtimes each reply.The echo request packets consist of anOpenFlow header plus n bytes (default: 64) of randomlygenerated payload.This measures the latency ofindividual requests.benchmark target n countSends count echo request packets that each consist of anOpenFlow header plus n bytes of payload and waits for eachresponse.Reports the total time required.This is ameasure of the maximum bandwidth to target for round-tripsof n-byte messages.Other Commandsofp-parse fileReads file (or stdin if file is -) as a series of OpenFlowmessages in the binary format used on an OpenFlowconnection, and prints them to the console.This can beuseful for printing OpenFlow messages captured from a TCPstream.ofp-parse-pcap file [port...]Reads file, which must be in the PCAP format used bynetwork capture tools such as tcpdump or wireshark,extracts all the TCP streams for OpenFlow connections, andprints the OpenFlow messages in those connections inhuman-readable format on stdout.OpenFlow connections are distinguished by TCP port number.Non-OpenFlow packets are ignored.By default, data on TCPports 6633 and 6653 are considered to be OpenFlow.Specify one or more port arguments to override thedefault.This command cannot usefully print SSL encrypted traffic.It does not understand IPv6.Flow SyntaxSome ovs-ofctl commands accept an argument that describes a flowor flows.Such flow descriptions comprise a series offield=value assignments, separated by commas or white space.(Embedding spaces into a flow description normally requiresquoting to prevent the shell from breaking the description intomultiple arguments.)Flow descriptions should be in normal form.This means that aflow may only specify a value for an L3 field if it alsospecifies a particular L2 protocol, and that a flow may onlyspecify an L4 field if it also specifies particular L2 and L3protocol types.For example, if the L2 protocol type dl_type iswildcarded, then L3 fields nw_src, nw_dst, and nw_proto must alsobe wildcarded.Similarly, if dl_type or nw_proto (the L3protocol type) is wildcarded, so must be the L4 fields tcp_dstand tcp_src.ovs-ofctl will warn about flows not in normal form.ovs-fields(7) describes the supported fields and how to matchthem.In addition to match fields, commands that operate onflows accept a few additional key-value pairs:table=tableFor flow dump commands, limits the flows dumped to thosein table, which may be expressed as a number between 0 and255 or (unless --no-names is specified) a name.If notspecified (or if 255 is specified as table), then flows inall tables are dumped.For flow table modification commands, behavior variesbased on the OpenFlow version used to connect to theswitch:OpenFlow 1.0OpenFlow 1.0 does not support table for modifyingflows.ovs-ofctl will exit with an error if table(other than table=255) is specified for a switchthat only supports OpenFlow 1.0.In OpenFlow 1.0, the switch chooses the table intowhich to insert a new flow.The Open vSwitchsoftware switch always chooses table 0.Other OpenvSwitch datapaths and other OpenFlowimplementations may choose different tables.The OpenFlow 1.0 behavior in Open vSwitch formodifying or removing flows depends on whether--strict is used.Without --strict, the commandapplies to matching flows in all tables.With--strict, the command will operate on any singlematching flow in any table; it will do nothing ifthere are matches in more than one table.(Thedistinction between these behaviors only matters ifnon-OpenFlow 1.0 commands were also used, becauseOpenFlow 1.0 alone cannot add flows with the samematching criteria to multiple tables.)OpenFlow 1.0 with table_id extensionOpen vSwitch implements an OpenFlow extension thatallows the controller to specify the table on whichto operate.ovs-ofctl automatically enables theextension when table is specified and OpenFlow 1.0is used.ovs-ofctl automatically detects whetherthe switch supports the extension.As of thiswriting, this extension is only known to beimplemented by Open vSwitch.With this extension, ovs-ofctl operates on therequested table when table is specified, and actsas described for OpenFlow 1.0 above when no tableis specified (or for table=255).OpenFlow 1.1OpenFlow 1.1 requires flow table modificationcommands to specify a table.When table is notspecified (or table=255 is specified), ovs-ofctldefaults to table 0.OpenFlow 1.2 and laterOpenFlow 1.2 and later allow flow deletioncommands, but not other flow table modificationcommands, to operate on all flow tables, with thebehavior described above for OpenFlow 1.0.duration=...n_packet=...n_bytes=...ovs-ofctl ignores assignments to these ``fields'' to allowoutput from the dump-flows command to be used as input forother commands that parse flows.The add-flow, add-flows, and mod-flows commands require anadditional field, which must be the final field specified:actions=[action][,action...]Specifies a comma-separated list of actions to take on apacket when the flow entry matches.If no action isspecified, then packets matching the flow are dropped.See ovs-actions(7) for details on the syntax and semanticsof actions.KAn opaque identifier called a cookie can be used as a handle toidentify a set of flows:cookie=valueA cookie can be associated with a flow using the add-flow,add-flows, and mod-flows commands.value can be any64-bit number and need not be unique among flows.If thisfield is omitted, a default cookie value of 0 is used.cookie=value/maskWhen using NXM, the cookie can be used as a handle forquerying, modifying, and deleting flows.value and maskmay be supplied for the del-flows, mod-flows, dump-flows,and dump-aggregate commands to limit matching cookies.A1-bit in mask indicates that the corresponding bit incookie must match exactly, and a 0-bit wildcards that bit.A mask of -1 may be used to exactly match a cookie.The mod-flows command can update the cookies of flows thatmatch a cookie by specifying the cookie field twice (oncewith a mask for matching and once without to indicate thenew value):ovs-ofctl mod-flows br0 cookie=1,actions=normalChange all flows' cookies to 1 and change theiractions to normal.ovs-ofctl mod-flows br0cookie=1/-1,cookie=2,actions=normalUpdate cookies with a value of 1 to 2 and changetheir actions to normal.The ability to match on cookies was added in Open vSwitch1.5.0.The following additional field sets the priority for flows addedby the add-flow and add-flows commands.For mod-flows anddel-flows when --strict is specified, priority must match alongwith the rest of the flow specification.For mod-flows without--strict, priority is only significant if the command creates anew flow, that is, non-strict mod-flows does not match onpriority and will not change the priority of existing flows.Other commands do not allow priority to be specified.priority=valueThe priority at which a wildcarded entry will match incomparison to others.value is a number between 0 and65535, inclusive.A higher value will match before alower one.An exact-match entry will always have priorityover an entry containing wildcards, so it has an implicitpriority value of 65535.When adding a flow, if the fieldis not specified, the flow's priority will default to32768.OpenFlow leaves behavior undefined when two or more flowswith the same priority can match a single packet.Someusers expect ``sensible'' behavior, such as more specificflows taking precedence over less specific flows, butOpenFlow does not specify this and Open vSwitch does notimplement it.Users should therefore take care to usepriorities to ensure the behavior that they expect.The add-flow, add-flows, and mod-flows commands support thefollowing additional options.These options affect only newflows.Thus, for add-flow and add-flows, these options arealways significant, but for mod-flows they are significant onlyif the command creates a new flow, that is, their values do notupdate or affect existing flows.idle_timeout=secondsCauses the flow to expire after the given number ofseconds of inactivity.A value of 0 (the default)prevents a flow from expiring due to inactivity.hard_timeout=secondsCauses the flow to expire after the given number ofseconds, regardless of activity.A value of 0 (thedefault) gives the flow no hard expiration deadline.importance=valueSets the importance of a flow.The flow entry evictionmechanism can use importance as a factor in deciding whichflow to evict.A value of 0 (the default) makes the flownon-evictable on the basis of importance.Specify a valuebetween 0 and 65535.Only OpenFlow 1.4 and later support importance.send_flow_remMarks the flow with a flag that causes the switch togenerate a ``flow removed'' message and send it tointerested controllers when the flow later expires or isremoved.check_overlapForces the switch to check that the flow match does notoverlap that of any different flow with the same priorityin the same table.(This check is expensive so it is bestto avoid it.)reset_countsWhen this flag is specified on a flow being added to aswitch, and the switch already has a flow with anidentical match, an OpenFlow 1.2 (or later) switch resetsthe flow's packet and byte counters to 0.Without theflag, the packet and byte counters are preserved.OpenFlow 1.0 and 1.1 switches always reset counters inthis situation, as if reset_counts were always specified.Open vSwitch 1.10 added support for reset_counts.no_packet_countsno_byte_countsAdding these flags to a flow advises an OpenFlow 1.3 (orlater) switch that the controller does not need packet orbyte counters, respectively, for the flow.Some switchimplementations might achieve higher performance or reduceresource consumption when these flags are used.Theseflags provide no benefit to the Open vSwitch softwareswitch implementation.OpenFlow 1.2 and earlier do not support these flags.Open vSwitch 1.10 added support for no_packet_counts andno_byte_counts.The dump-flows, dump-aggregate, del-flow and del-flows commandssupport these additional optional fields:out_port=portIf set, a matching flow must include an output action toport, which must be an OpenFlow port number or name (e.g.local).out_group=groupIf set, a matching flow must include an group actionnaming group, which must be an OpenFlow group number.This field is supported in Open vSwitch 2.5 and later andrequires OpenFlow 1.1 or later.Table Entry OutputThe dump-tables and dump-aggregate commands print informationabout the entries in a datapath's tables.Each line of output isa flow entry as described in Flow Syntax, above, plus someadditional fields:duration=secsThe time, in seconds, that the entry has been in thetable.secs includes as much precision as the switchprovides, possibly to nanosecond resolution.n_packetsThe number of packets that have matched the entry.n_bytesThe total number of bytes from packets that have matchedthe entry.The following additional fields are included only if the switchis Open vSwitch 1.6 or later and the NXM flow format is used todump the flow (see the description of the --flow-format optionbelow).The values of these additional fields are approximationsonly and in particular idle_age will sometimes become nonzeroeven for busy flows.hard_age=secsThe integer number of seconds since the flow was added ormodified.hard_age is displayed only if it differs fromthe integer part of duration.(This is separate fromduration because mod-flows restarts the hard_timeout timerwithout zeroing duration.)idle_age=secsThe integer number of seconds that have passed without anypackets passing through the flow.Packet-Out Syntaxovs-ofctl bundle command accepts packet-outs to be specified inthe bundle file.Each packet-out comprises of a series offield=value assignments, separated by commas or white space.(Embedding spaces into a packet-out description normally requiresquoting to prevent the shell from breaking the description intomultiple arguments.).Unless noted otherwise only the lastinstance of each field is honoured.This same syntax is alsosupported by the ovs-ofctl packet-out command.in_port=portThe port number to be considered the in_port whenprocessing actions.This can be any valid OpenFlow portnumber, or any of the LOCAL, CONTROLLER, or NONE.Thisfield is required.pipeline_field=valueOptionally, user can specify a list of pipeline fields fora packet-out message. The supported pipeline fieldsincludes tunnel fields and register fields as defined inovs-fields(7).packet=hex-stringThe actual packet to send, expressed as a string ofhexadecimal bytes.This field is required.actions=[action][,action...]The syntax of actions are identical to the actions= fielddescribed in Flow Syntax above.Specifying actions= isoptional, but omitting actions is interpreted as a drop,so the packet will not be sent anywhere from the switch.actions must be specified at the end of each line, likefor flow mods.Group SyntaxSome ovs-ofctl commands accept an argument that describes a groupor groups.Such flow descriptions comprise a series field=valueassignments, separated by commas or white space.(Embeddingspaces into a group description normally requires quoting toprevent the shell from breaking the description into multiplearguments.). Unless noted otherwise only the last instance ofeach field is honoured.group_id=idThe integer group id of group.When this field isspecified in del-groups or dump-groups, the keyword \"all\"may be used to designate all groups.This field isrequired.type=typeThe type of the group.The add-group, add-groups and mod-groups commands require this field.It is prohibited forother commands. The following keywords designated theallowed types:allExecute all buckets in the group.select Execute one bucket in the group, balancing acrossthe buckets according to their weights.To selecta bucket, for each live bucket, Open vSwitch hashesflow data with the bucket ID and multiplies by thebucket weight to obtain a ``score,'' and thenselects the bucket with the highest score.Useselection_method to control the flow data used forselection.indirectExecutes the one bucket in the group.fffast_failoverExecutes the first live bucket in the group whichis associated with a live port or group.command_bucket_id=idThe bucket to operate on.The insert-buckets and remove-buckets commands require this field.It is prohibited forother commands.id may be an integer or one of thefollowing keywords:allOperate on all buckets in the group.Only validwhen used with the remove-buckets command in whichcase the effect is to remove all buckets from thegroup.firstOperate on the first bucket present in the group.In the case of the insert-buckets command theeffect is to insert new bucets just before thefirst bucket already present in the group; or toreplace the buckets of the group if there are nobuckets already present in the group.In the caseof the remove-buckets command the effect is toremove the first bucket of the group; or do nothingif there are no buckets present in the group.lastOperate on the last bucket present in the group.In the case of the insert-buckets command theeffect is to insert new bucets just after the lastbucket already present in the group; or to replacethe buckets of the group if there are no bucketsalready present in the group.In the case of theremove-buckets command the effect is to remove thelast bucket of the group; or do nothing if thereare no buckets present in the group.If id is an integer then it should correspond to thebucket_id of a bucket present in the group.In case ofthe insert-buckets command the effect is to insert bucketsjust before the bucket in the group whose bucket_id is id.In case of the iremove-buckets command the effect is toremove the in the group whose bucket_id is id.It is anerror if there is no bucket persent group in whosebucket_id is id.selection_method=methodThe selection method used to select a bucket for a selectgroup.This is a string of 1 to 15 bytes in length knownto lower layers.This field is optional for add-group,add-groups and mod-group commands on groups of typeselect. Prohibited otherwise.If no selection method isspecified, Open vSwitch up to release 2.9 applies the hashmethod with default fields. From 2.10 onwards Open vSwitchdefaults to the dp_hash method with symmetric L3/L4 hashalgorithm, as long as the weighted group buckets can bemapped to dp_hash values with sufficient accuracy.In2.10 this was restricted to a maximum of 64 buckets, andin 2.17 the limit was raised to 256 buckets.In thoserare cases Open vSwitch 2.10 and later fall back to thehash method with the default set of hash fields.dp_hashUse a datapath computed hash value.The hashalgorithm varies across different datapathimplementations.dp_hash uses the upper 32 bits ofthe selection_method_param as the datapath hashalgorithm selector.The supported values are 0(corresponding to hash computation over the IP5-tuple) and 1 (corresponding to a symmetric hashcomputation over the IP 5-tuple).Selectingspecific fields with the fields option is notsupported with dp_hash).The lower 32 bits areused as the hash basis.Using dp_hash has the advantage that it does notrequire the generated datapath flows to exact matchany additional packet header fields.For example,even if multiple TCP connections thus hashed todifferent select group buckets have differentsource port numbers, generally all of them would behandled with a small set of already establisheddatapath flows, resulting in less latency for TCPSYN packets.The downside is that the shareddatapath flows must match each packet twice, as thedatapath hash value calculation happens only whenneeded, and a second match is required to matchsome bits of its value.This double-matchingincurs a small additional latency cost for eachpacket, but this latency is orders of magnitudeless than the latency of creating new datapathflows for new TCP connections.hashUse a hash computed over the fields specified withthe fields option, see below.If no hash fieldsare specified, hash defaults to a symmetric hashover the combination of MAC addresses, VLAN tags,Ether type, IP addresses and L4 port numbers.hashuses the selection_method_param as the hash basis.Note that the hashed fields become exact matched bythe datapath flows.For example, if the TCP sourceport is hashed, the created datapath flows willmatch the specific TCP source port value present inthe packet received.Since each TCP connectiongenerally has a different source port value, aseparate datapath flow will be need to be insertedfor each TCP connection thus hashed to a selectgroup bucket.This option uses a Netronome OpenFlow extension which isonly supported when using Open vSwitch 2.4 and later withOpenFlow 1.5 and later.selection_method_param=param64-bit integer parameter to the selection method selectedby the selection_method field.The parameter's use isdefined by the lower-layer that implements theselection_method.It is optional if the selection_methodfield is specified as a non-empty string.Prohibitedotherwise. The default value is zero.This option uses a Netronome OpenFlow extension which isonly supported when using Open vSwitch 2.4 and later withOpenFlow 1.5 and later.fields=fieldfields(field[=mask]...)The field parameters to selection method selected by theselection_method field.The syntax is described in FlowSyntax with the additional restrictions that if a value isprovided it is treated as a wildcard mask and wildcardmasks following a slash are prohibited. The pre-requisitesof fields must be provided by any flows that output to thegroup.The use of the fields is defined by the lower-layer that implements the selection_method.They areoptional if the selection_method field is specified as``hash', prohibited otherwise.The default is no fields.This option will use a Netronome OpenFlow extension whichis only supported when using Open vSwitch 2.4 and laterwith OpenFlow 1.5 and later.bucket=bucket_parametersThe add-group, add-groups and mod-group commands requireat least one bucket field. Bucket fields must appear afterall other fields.Multiple bucket fields to specifymultiple buckets.The order in which buckets arespecified corresponds to their order in the group. If thetype of the group is \"indirect\" then only one group may bespecified.bucket_parameters consists of a list offield=value assignments, separated by commas or whitespace followed by a comma-separated list of actions.Thefields for bucket_parameters are:bucket_id=idThe 32-bit integer group id of the bucket.Valuesgreater than 0xffffff00 are reserved.This fieldwas added in Open vSwitch 2.4 to conform with theOpenFlow 1.5 specification. It is not supportedwhen earlier versions of OpenFlow are used.OpenvSwitch will automatically allocate bucket ids whenthey are not specified.actions=[action][,action...]The syntax of actions are identical to the actions=field described in Flow Syntax above. Specifyingactions= is optional, any unknown bucket parameterwill be interpreted as an action.weight=valueThe relative weight of the bucket as an integer.This may be used by the switch during bucket selectfor groups whose type is select.watch_port=portPort used to determine liveness of group.This orthe watch_group field is required for groups whosetype is ff or fast_failover.This or thewatch_group field can also be used for groups whosetype is select.watch_group=group_idGroup identifier of group used to determineliveness of group.This or the watch_port field isrequired for groups whose type is ff orfast_failover.This or the watch_port field canalso be used for groups whose type is select.Meter SyntaxThe meter table commands accept an argument that describes ameter.Such meter descriptions comprise a series field=valueassignments, separated by commas or white space.(Embeddingspaces into a group description normally requires quoting toprevent the shell from breaking the description into multiplearguments.). Unless noted otherwise only the last instance ofeach field is honoured.meter=idThe identifier for the meter.An integer is used tospecify a user-defined meter.In addition, the keywords\"all\", \"controller\", and \"slowpath\", are also supported asvirtual meters.The \"controller\" and \"slowpath\" virtualmeters apply to packets sent to the controller and to theOVS userspace, respectively.When this field is specified in del-meter, dump-meter, ormeter-stats, the keyword \"all\" may be used to designateall meters.This field is required, except for meter-stats, which dumps all stats when this field is notspecified.kbpspktpsThe unit for the rate and burst_size band parameters.kbps specifies kilobits per second, and pktps specifiespackets per second.A unit is required for the add-meterand mod-meter commands.burstIf set, enables burst support for meter bands through theburst_size parameter.statsIf set, enables the collection of meter and bandstatistics.bands=band_parametersThe add-meter and mod-meter commands require at least oneband specification. Bands must appear after all otherfields.type=typeThe type of the meter band.This keyword starts anew band specification.Each band specifies a rateabove which the band is to take some action. Theaction depends on the band type.If multiplebands' rate is exceeded, then the band with thehighest rate among the exceeded bands is selected.The following keywords designate the allowed meterband types:dropDrop packets exceeding the band's ratelimit.The other band_parameters are:rate=valueThe relative rate limit for this band, in kilobitsper second or packets per second, depending onwhether kbps or pktps was specified.burst_size=sizeIf burst is specified for the meter entry,configures the maximum burst allowed for the bandin kilobits or packets, depending on whether kbpsor pktps was specified.If unspecified, the switchis free to select some reasonable value dependingon its configuration.",
        "name": "ovs-ofctl - administer OpenFlow switches",
        "section": 8
    },
    {
        "command": "ovs-parse-backtrace",
        "description": "In some configurations, many Open vSwitch daemons can produce aseries of backtraces using the ovs-appctl backtrace command.Users can analyze these backtraces to figure out what the givenOpen vSwitch daemon may be spending most of its time doing.ovs-parse-backtrace makes this output easier to interpret.The ovs-appctl backtrace output must be supplied on standardinput.The binary that produced the output should be supplied asthe sole non-option argument.For best results, the binaryshould have debug symbols.",
        "name": "ovs-parse-backtrace - parses ovs-appctl backtrace output",
        "section": 8
    },
    {
        "command": "ovs-pki",
        "description": "The ovs-pki program sets up and manages a public keyinfrastructure for use with OpenFlow.It is intended to be asimple interface for organizations that do not have anestablished public key infrastructure.Other PKI tools cansubstitute for or supplement the use of ovs-pki.ovs-pki uses openssl(1) for certificate management and keygeneration.",
        "name": "ovs-pki - OpenFlow public key infrastructure management utility",
        "section": 8
    },
    {
        "command": "ovs-tcpdump",
        "description": "ovs-tcpdump creates switch mirror ports in the ovs-vswitchddaemon and executes tcpdump to listen against those ports. Whenthe tcpdump instance exits, it then cleans up the mirror port itcreated.ovs-tcpdump will not allow multiple mirrors for the same port. Ithas some logic to parse the current configuration and preventduplicate mirrors.The -i option may not appear multiple times.It is important to note that under Linux-based kernels, tapdevices do not receive packets unless the specific tuntap devicehas been opened by an application.This requires CAP_NET_ADMINprivileges, so the ovs-tcpdump command must be run as a user withsuch permissions (this is usually a super-user).",
        "name": "ovs-tcpdump - Dump traffic from an Open vSwitch port usingtcpdump",
        "section": 8
    },
    {
        "command": "ovs-test",
        "description": "The ovs-test program may be used to check for problems sending802.1Q or GRE traffic that Open vSwitch may uncover. Theseproblems, for example, can occur when Open vSwitch is used tosend 802.1Q traffic through physical interfaces running certaindrivers of certain Linux kernel versions.To run a test,configure IP addresses on server1 and server2 for interfaces youintended to test. These interfaces could also be alreadyconfigured OVS bridges that have a physical interface attached tothem. Then, on one of the nodes, run ovs-test in server mode andon the other node run it in client mode. The client will connectto ovs-test server and schedule tests between both of them. Theovs-test client will perform UDP and TCP tests.UDP tests can report packet loss and achieved bandwidth forvarious datagram sizes. By default target bandwidth for UDP testsis 1Mbit/s.TCP tests report only achieved bandwidth, because kernel TCPstack takes care of flow control and packet loss. TCP tests areessential to detect potential TSO related issues.To determine whether Open vSwitch is encountering any problems,the user must compare packet loss and achieved bandwidth in asetup where traffic is being directly sent and in one where it isnot. If in the 802.1Q or L3 tunneled tests both ovs-testprocesses are unable to communicate or the achieved bandwidth ismuch lower compared to direct setup, then, most likely, OpenvSwitch has encountered a pre-existing kernel or driver bug.Some examples of the types of problems that may be encounteredare:\u2022 When NICs use VLAN stripping on receive they must pass apointer to a vlan_group when reporting the stripped tag to thenetworking core. If no vlan_group is in use then some driversjust drop the extracted tag.Drivers are supposed to onlyenable stripping if a vlan_group is registered but not all ofthem do that.\u2022 On receive, some drivers handle priority tagged packetsspecially and don\u2019t pass the tag onto the network stack at all,so Open vSwitch never has a chance to see it.\u2022 Some drivers size their receive buffers based on whether avlan_group is enabled, meaning that a maximum size packet witha VLAN tag will not fit if no vlan_group is configured.\u2022 On transmit, some drivers expect that VLAN acceleration will beused if it is available, which can only be done if a vlan_groupis configured. In these cases, the driver may fail to parse thepacket and correctly setup checksum offloading or TSO.Client ModeAn ovs-test client will connect to two ovs-test serversand will ask them to exchange test traffic. It is alsopossible to spawn an ovs-test server automatically fromthe client.Server ModeTo conduct tests, two ovs-test servers must be running ontwo different hosts where the client can connect. Theactual test traffic is exchanged only between bothovs-test servers. It is recommended that both servers havetheir IP addresses in the same subnet, otherwise one wouldhave to make sure that routing is set up correctly.",
        "name": "ovs-test - Check Linux drivers for performance, vlan and L3tunneling problems",
        "section": 8
    },
    {
        "command": "ovs-testcontroller",
        "description": "ovs-testcontroller is a simple OpenFlow controller that managesany number of switches over the OpenFlow protocol, causing themto function as L2 MAC-learning switches or hubs.It is suitablefor initial testing of OpenFlow networks.It is not a necessaryor desirable part of a production OpenFlow deployment.ovs-testcontroller controls one or more OpenFlow switches,specified as one or more of the following OpenFlow connectionmethods:pssl:[port][:host]ptcp:[port][:host]Listens for OpenFlow connections on port.Thedefault port is 6653.By default, connections areallowed from any IPv4 address.Specify host as anIPv4 address or a bracketed IPv6 address (e.g.ptcp:6653:[::1]).On Linux, use %device todesignate a scope for IPv6 link-level addresses,e.g. ptcp:6653:[fe80::1234%eth0].DNS names can beused if built with unbound library.For pssl, the--private-key,--certificate, and --ca-cert optionsare mandatory.punix:fileListens for OpenFlow connections on the Unix domainserver socket named file.ssl:host[:port]tcp:host[:port]The specified port on the given host, which can beexpressed either as a DNS name (if built withunbound library) or an IP address in IPv4 or IPv6address format.Wrap IPv6 addresses in squarebrackets, e.g. tcp:[::1]:6653.On Linux, use%device to designate a scope for IPv6 link-leveladdresses, e.g. tcp:[fe80::1234%eth0]:6653.Forssl, the --private-key, --certificate, and--ca-cert options are mandatory.If port is not specified, it defaults to 6653.unix:fileOn POSIX, a Unix domain server socket named file.On Windows, connect to a local named pipe that isrepresented by a file created in the path file tomimic the behavior of a Unix domain socket.",
        "name": "ovs-testcontroller - simple OpenFlow controller for testing",
        "section": 8
    },
    {
        "command": "ovs-vlan-test",
        "description": "The ovs-vlan-test utility has some limitations, for example, itdoes not use TCP in its tests. Also it does not take into accountMTU to detect potential edge cases. To overcome those limitationsa new tool was developed - ovs-test. ovs-test is currentlysupported only on Debian so, if possible, try to use that oninstead of ovs-vlan-test.The ovs-vlan-test program may be used to check for problemssending 802.1Q traffic which may occur when running Open vSwitch.These problems can occur when Open vSwitch is used to send 802.1Qtraffic through physical interfaces running certain drivers ofcertain Linux kernel versions. To run a test, configure OpenvSwitch to tag traffic originating from vlan_ip and forward itout the target interface. Then run the ovs-vlan-test in clientmode connecting to an ovs-vlan-test server.ovs-vlan-test willdisplay \u201cOK\u201d if it did not detect problems.Some examples of the types of problems that may be encounteredare:\u2022 When NICs use VLAN stripping on receive they must pass apointer to a vlan_group when reporting the stripped tag to thenetworking core. If no vlan_group is in use then some driversjust drop the extracted tag.Drivers are supposed to onlyenable stripping if a vlan_group is registered but not all ofthem do that.\u2022 On receive, some drivers handle priority tagged packetsspecially and don\u2019t pass the tag onto the network stack at all,so Open vSwitch never has a chance to see it.\u2022 Some drivers size their receive buffers based on whether avlan_group is enabled, meaning that a maximum size packet witha VLAN tag will not fit if no vlan_group is configured.\u2022 On transmit, some drivers expect that VLAN acceleration will beused if it is available, which can only be done if a vlan_groupis configured. In these cases, the driver may fail to parse thepacket and correctly setup checksum offloading or TSO.Client ModeAn ovs-vlan-test client may be run on a host to check forVLAN connectivity problems. The client must be able toestablish HTTP connections with an ovs-vlan-test serverlocated at the specified control_ip address. UDP trafficsourced at vlan_ip should be tagged and directed out theinterface whose connectivity is being tested.Server ModeTo conduct tests, an ovs-vlan-test server must be runningon a host known not to have VLAN connectivity problems.The server must have a control_ip on a non-VLAN networkwhich clients can establish connectivity with. It mustalso have a vlan_ip address on a VLAN network whichclients will use to test their VLAN connectivity. Multipleclients may test against a single ovs-vlan-test serverconcurrently.",
        "name": "ovs-vlan-test - Check Linux drivers for problems with vlantraffic",
        "section": 8
    },
    {
        "command": "ovs-vsctl",
        "description": "The ovs-vsctl program configures ovs-vswitchd(8) by providing ahigh-level interface to its configuration database.Seeovs-vswitchd.conf.db(5) for comprehensive documentation of thedatabase schema.ovs-vsctl connects to an ovsdb-server process that maintains anOpen vSwitch configuration database.Using this connection, itqueries and possibly applies changes to the database, dependingon the supplied commands.Then, if it applied any changes, bydefault it waits until ovs-vswitchd has finished reconfiguringitself before it exits.(If you use ovs-vsctl when ovs-vswitchdis not running, use --no-wait.)ovs-vsctl can perform any number of commands in a single run,implemented as a single atomic transaction against the database.The ovs-vsctl command line begins with global options (seeOPTIONS below for details).The global options are followed byone or more commands.Each command should begin with -- byitself as a command-line argument, to separate it from thefollowing commands.(The -- before the first command isoptional.)The command itself starts with command-specificoptions, if any, followed by the command name and any arguments.See EXAMPLES below for syntax examples.Linux VLAN Bridging CompatibilityThe ovs-vsctl program supports the model of a bridge implementedby Open vSwitch, in which a single bridge supports ports onmultiple VLANs.In this model, each port on a bridge is either atrunk port that potentially passes packets tagged with 802.1Qheaders that designate VLANs or it is assigned a single implicitVLAN that is never tagged with an 802.1Q header.For compatibility with software designed for the Linux bridge,ovs-vsctl also supports a model in which traffic associated witha given 802.1Q VLAN is segregated into a separate bridge.Aspecial form of the add-br command (see below) creates a ``fakebridge'' within an Open vSwitch bridge to simulate this behavior.When such a ``fake bridge'' is active, ovs-vsctl will treat itmuch like a bridge separate from its ``parent bridge,'' but theactual implementation in Open vSwitch uses only a single bridge,with ports on the fake bridge assigned the implicit VLAN of thefake bridge of which they are members.(A fake bridge for VLAN 0receives packets that have no 802.1Q tag or a tag with VLAN 0.)",
        "name": "ovs-vsctl - utility for querying and configuring ovs-vswitchd",
        "section": 8
    },
    {
        "command": "ovs-vswitchd",
        "description": "A daemon that manages and controls any number of Open vSwitchswitches on the local machine.The database argument specifies how ovs-vswitchd connects toovsdb-server.database may be an OVSDB active or passiveconnection method, as described in ovsdb(7).The default isunix:/usr/local/var/run/openvswitch/db.sock.ovs-vswitchd retrieves its configuration from database atstartup.It sets up Open vSwitch datapaths and then operatesswitching across each bridge described in its configurationfiles.As the database changes, ovs-vswitchd automaticallyupdates its configuration to match.ovs-vswitchd switches may be configured with any of the followingfeatures:\u2022L2 switching with MAC learning.\u2022NIC bonding with automatic fail-over and source MAC-basedTX load balancing (\"SLB\").\u2022802.1Q VLAN support.\u2022Port mirroring, with optional VLAN tagging.\u2022NetFlow v5 flow logging.\u2022sFlow(R) monitoring.\u2022Connectivity to an external OpenFlow controller, such asNOX.Only a single instance of ovs-vswitchd is intended to run at atime.A single ovs-vswitchd can manage any number of switchinstances, up to the maximum number of supported Open vSwitchdatapaths.ovs-vswitchd does all the necessary management of Open vSwitchdatapaths itself.Thus, ovs-dpctl(8) (and its userspace datapathcounterparts accessible via ovs-appctl dpctl/command) are notneeded with ovs-vswitchd and should not be used because they caninterfere with its operation.These tools are still useful fordiagnostics.An Open vSwitch datapath kernel module must be loaded forovs-vswitchd to be useful.Refer to the documentation forinstructions on how to build and load the Open vSwitch kernelmodule.",
        "name": "ovs-vswitchd - Open vSwitch daemon",
        "section": 8
    },
    {
        "command": "pam",
        "description": "This manual is intended to offer a quick introduction toLinux-PAM. For more information the reader is directed to theLinux-PAM system administrators' guide.Linux-PAM is a system of libraries that handle the authenticationtasks of applications (services) on the system. The libraryprovides a stable general interface (Application ProgrammingInterface - API) that privilege granting programs (such aslogin(1) and su(1)) defer to to perform standard authenticationtasks.The principal feature of the PAM approach is that the nature ofthe authentication is dynamically configurable. In other words,the system administrator is free to choose how individualservice-providing applications will authenticate users. Thisdynamic configuration is set by the contents of the singleLinux-PAM configuration file /etc/pam.conf. Alternatively, theconfiguration can be set by individual configuration fileslocated in the /etc/pam.d/ directory. The presence of thisdirectory will cause Linux-PAM to ignore /etc/pam.conf.Vendor-supplied PAM configuration files might be installed in thesystem directory /usr/lib/pam.d/ or a configurable vendorspecific directory instead of the machine configuration directory/etc/pam.d/. If no machine configuration file is found, thevendor-supplied file is used. All files in /etc/pam.d/ overridefiles with the same name in other directories.From the point of view of the system administrator, for whom thismanual is provided, it is not of primary importance to understandthe internal behavior of the Linux-PAM library. The importantpoint to recognize is that the configuration file(s) define theconnection between applications (services) and the pluggableauthentication modules (PAMs) that perform the actualauthentication tasks.Linux-PAM separates the tasks of authentication into fourindependent management groups: account management; authenticationmanagement; password management; and session management. (Wehighlight the abbreviations used for these groups in theconfiguration file.)Simply put, these groups take care of different aspects of atypical user's request for a restricted service:account - provide account verification types of service: has theuser's password expired?; is this user permitted access to therequested service?authentication - authenticate a user and set up user credentials.Typically this is via some challenge-response request that theuser must satisfy: if you are who you claim to be please enteryour password. Not all authentications are of this type, thereexist hardware based authentication schemes (such as the use ofsmart-cards and biometric devices), with suitable modules, thesemay be substituted seamlessly for more standard approaches toauthentication - such is the flexibility of Linux-PAM.password - this group's responsibility is the task of updatingauthentication mechanisms. Typically, such services are stronglycoupled to those of the auth group. Some authenticationmechanisms lend themselves well to being updated with such afunction. Standard UN*X password-based access is the obviousexample: please enter a replacement password.session - this group of tasks cover things that should be doneprior to a service being given and after it is withdrawn. Suchtasks include the maintenance of audit trails and the mounting ofthe user's home directory. The session management group isimportant as it provides both an opening and closing hook formodules to affect the services available to a user.",
        "name": "PAM, pam - Pluggable Authentication Modules for Linux",
        "section": 8
    },
    {
        "command": "pam_access",
        "description": "The pam_access PAM module is mainly for access management. Itprovides logdaemon style login access control based on loginnames, host or domain names, internet addresses or networknumbers, or on terminal line names, X $DISPLAY values, or PAMservice names in case of non-networked logins.By default rules for access management are taken from config file/etc/security/access.conf if you don't specify another file. Thenindividual *.conf files from the /etc/security/access.d/directory are read. The files are parsed one after another in theorder of the system locale. The effect of the individual files isthe same as if all the files were concatenated together in theorder of parsing. This means that once a pattern is matched insome file no further files are parsed. If a config file isexplicitly specified with the accessfile option the files in theabove directory are not parsed.If Linux PAM is compiled with audit support the module willreport when it denies access based on origin (host, tty, etc.).",
        "name": "pam_access - PAM module for logdaemon style login access control",
        "section": 8
    },
    {
        "command": "pam_debug",
        "description": "The pam_debug PAM module is intended as a debugging aide fordetermining how the PAM stack is operating. This module returnswhat its module arguments tell it to return.",
        "name": "pam_debug - PAM module to debug the PAM stack",
        "section": 8
    },
    {
        "command": "pam_deny",
        "description": "This module can be used to deny access. It always indicates afailure to the application through the PAM framework. It might besuitable for using for default (the OTHER) entries.",
        "name": "pam_deny - The locking-out PAM module",
        "section": 8
    },
    {
        "command": "pam_echo",
        "description": "The pam_echo PAM module is for printing text messages to informuser about special things. Sequences starting with the %character are interpreted in the following way:%HThe name of the remote host (PAM_RHOST).%hThe name of the local host.%sThe service name (PAM_SERVICE).%tThe name of the controlling terminal (PAM_TTY).%UThe remote user name (PAM_RUSER).%uThe local user name (PAM_USER).All other sequences beginning with % expands to the charactersfollowing the % character.",
        "name": "pam_echo - PAM module for printing text messages",
        "section": 8
    },
    {
        "command": "pam_env",
        "description": "The pam_env PAM module allows the (un)setting of environmentvariables. Supported is the use of previously set environmentvariables as well as PAM_ITEMs such as PAM_RHOST.By default rules for (un)setting of variables are taken from theconfig file /etc/security/pam_env.conf. An alternate file can bespecified with the conffile option.Second a file (/etc/environment by default) with simple KEY=VALpairs on separate lines will be read. With the envfile option analternate file can be specified. And with the readenv option thiscan be completely disabled.Third it will read a user configuration file($HOME/.pam_environment by default). The default file can bechanged with the user_envfile option and it can be turned on andoff with the user_readenv option.Since setting of PAM environment variables can have side effectsto other modules, this module should be the last one on thestack.",
        "name": "pam_env - PAM module to set/unset environment variables",
        "section": 8
    },
    {
        "command": "pam_exec",
        "description": "pam_exec is a PAM module that can be used to run an externalcommand.The child's environment is set to the current PAM environmentlist, as returned by pam_getenvlist(3) In addition, the followingPAM items are exported as environment variables: PAM_RHOST,PAM_RUSER, PAM_SERVICE, PAM_TTY, PAM_USER and PAM_TYPE, whichcontains one of the module types: account, auth, password,open_session and close_session.Commands called by pam_exec need to be aware of that the user canhave control over the environment.",
        "name": "pam_exec - PAM module which calls an external command",
        "section": 8
    },
    {
        "command": "pam_faildelay",
        "description": "pam_faildelay is a PAM module that can be used to set the delayon failure per-application.If no delay is given, pam_faildelay will use the value ofFAIL_DELAY from /etc/login.defs.",
        "name": "pam_faildelay - Change the delay on failure per-application",
        "section": 8
    },
    {
        "command": "pam_faillock",
        "description": "This module maintains a list of failed authentication attemptsper user during a specified interval and locks the account incase there were more than deny consecutive failedauthentications.Normally, failed attempts to authenticate root will not cause theroot account to become blocked, to prevent denial-of-service: ifyour users aren't given shell accounts and root may only loginvia su or at the machine console (not telnet/rsh, etc), this issafe.",
        "name": "pam_faillock - Module counting authentication failures during aspecified interval",
        "section": 8
    },
    {
        "command": "pam_filter",
        "description": "This module is intended to be a platform for providing access toall of the input/output that passes between the user and theapplication. It is only suitable for tty-based and (stdin/stdout)applications.To function this module requires filters to be installed on thesystem. The single filter provided with the module simplytransposes upper and lower case letters in the input and outputstreams. (This can be very annoying and is not kind to termcapbased editors).Each component of the module has the potential to invoke thedesired filter. The filter is always execv(2) with the privilegeof the calling application and not that of the user. For thisreason it cannot usually be killed by the user without closingtheir session.",
        "name": "pam_filter - PAM filter module",
        "section": 8
    },
    {
        "command": "pam_ftp",
        "description": "pam_ftp is a PAM module which provides a pluggable anonymous ftpmode of access.This module intercepts the user's name and password. If the nameis ftp or anonymous, the user's password is broken up at the @delimiter into a PAM_RUSER and a PAM_RHOST part; these pam-itemsbeing set accordingly. The username (PAM_USER) is set to ftp. Inthis case the module succeeds. Alternatively, the module sets thePAM_AUTHTOK item with the entered password and fails.This module is not safe and easily spoofable.",
        "name": "pam_ftp - PAM module for anonymous access module",
        "section": 8
    },
    {
        "command": "pam_group",
        "description": "The pam_group PAM module does not authenticate the user, butinstead it grants group memberships (in the credential settingphase of the authentication module) to the user. Such membershipsare based on the service they are applying for.By default rules for group memberships are taken from config file/etc/security/group.conf.This module's usefulness relies on the file-systems accessible tothe user. The point being that once granted the membership of agroup, the user may attempt to create a setgid binary with arestricted group ownership. Later, when the user is not givenmembership to this group, they can recover group membership withthe precompiled binary. The reason that the file-systems that theuser has access to are so significant, is the fact that when asystem is mounted nosuid the user is unable to create or executesuch a binary file. For this module to provide any level ofsecurity, all file-systems that the user has write access toshould be mounted nosuid.The pam_group module functions in parallel with the /etc/groupfile. If the user is granted any groups based on the behavior ofthis module, they are granted in addition to those entries/etc/group (or equivalent).",
        "name": "pam_group - PAM module for group access",
        "section": 8
    },
    {
        "command": "pam_issue",
        "description": "pam_issue is a PAM module to prepend an issue file to theusername prompt. It also by default parses escape codes in theissue file similar to some common getty's (using \\x format).Recognized escapes:\\dcurrent day\\lname of this tty\\mmachine architecture (uname -m)\\nmachine's network node hostname (uname -n)\\odomain name of this system\\rrelease number of operating system (uname -r)\\tcurrent time\\soperating system name (uname -s)\\unumber of users currently logged in\\Usame as \\u except it is suffixed with \"user\" or \"users\" (eg.\"1 user\" or \"10 users\")\\voperating system version and build date (uname -v)",
        "name": "pam_issue - PAM module to add issue file to user prompt",
        "section": 8
    },
    {
        "command": "pam_keyinit",
        "description": "The pam_keyinit PAM module ensures that the invoking process hasa session keyring other than the user default session keyring.The module checks to see if the process's session keyring is theuser-session-keyring(7), and, if it is, creates a newsession-keyring(7) with which to replace it. If a new sessionkeyring is created, it will install a link to the user-keyring(7)in the session keyring so that keys common to the user will beautomatically accessible through it. The session keyring of theinvoking process will thenceforth be inherited by all itschildren unless they override it.In order to allow other PAM modules to attach tokens to thekeyring, this module provides both an auth (limited topam_setcred(3) and a session component. The session keyring iscreated in the module called. Moreover this module should beincluded as early as possible in a PAM configuration.This module is intended primarily for use by login processes. Beaware that after the session keyring has been replaced, the oldsession keyring and the keys it contains will no longer beaccessible.This module should not, generally, be invoked by programs likesu, since it is usually desirable for the key set to percolatethrough to the alternate context. The keys have their ownpermissions system to manage this.The keyutils package is used to manipulate keys more directly.This can be obtained from:Keyutils[1]",
        "name": "pam_keyinit - Kernel session keyring initialiser module",
        "section": 8
    },
    {
        "command": "pam_lastlog",
        "description": "pam_lastlog is a PAM module to display a line of informationabout the last login of the user. In addition, the modulemaintains the /var/log/lastlog file.Some applications may perform this function themselves. In suchcases, this module is not necessary.The module checks LASTLOG_UID_MAX option in /etc/login.defs anddoes not update or display last login records for users with UIDhigher than its value. If the option is not present or its valueis invalid, no user ID limit is applied.If the module is called in the auth or account phase, theaccounts that were not used recently enough will be disallowed tolog in. The check is not performed for the root account so theroot is never locked out. It is also not performed for users withUID higher than the LASTLOG_UID_MAX value.",
        "name": "pam_lastlog - PAM module to display date of last login andperform inactive account lock out",
        "section": 8
    },
    {
        "command": "pam_limits",
        "description": "The pam_limits PAM module sets limits on the system resourcesthat can be obtained in a user-session. Users of uid=0 areaffected by this limits, too.By default limits are taken from the /etc/security/limits.confconfig file. Then individual *.conf files from the/etc/security/limits.d/ directory are read. The files are parsedone after another in the order of \"C\" locale. The effect of theindividual files is the same as if all the files wereconcatenated together in the order of parsing. If a config fileis explicitly specified with a module option then the files inthe above directory are not parsed.The module must not be called by a multithreaded application.If Linux PAM is compiled with audit support the module willreport when it denies access based on limit of maximum number ofconcurrent login sessions.",
        "name": "pam_limits - PAM module to limit resources",
        "section": 8
    },
    {
        "command": "pam_listfile",
        "description": "pam_listfile is a PAM module which provides a way to deny orallow services based on an arbitrary file.The module gets the item of the type specified -- user specifiesthe username, PAM_USER; tty specifies the name of the terminalover which the request has been made, PAM_TTY; rhost specifiesthe name of the remote host (if any) from which the request wasmade, PAM_RHOST; and ruser specifies the name of the remote user(if available) who made the request, PAM_RUSER -- and looks foran instance of that item in the file=filename.filename containsone line per item listed. If the item is found, then ifsense=allow, PAM_SUCCESS is returned, causing the authorizationrequest to succeed; else if sense=deny, PAM_AUTH_ERR is returned,causing the authorization request to fail.If an error is encountered (for instance, if filename does notexist, or a poorly-constructed argument is encountered), then ifonerr=succeed, PAM_SUCCESS is returned, otherwise if onerr=fail,PAM_AUTH_ERR or PAM_SERVICE_ERR (as appropriate) will bereturned.An additional argument, apply=, can be used to restrict theapplication of the above to a specific user (apply=username) or agiven group (apply=@groupname). This added restriction is onlymeaningful when used with the tty, rhost and shell items.Besides this last one, all arguments should be specified; do notcount on any default behavior.No credentials are awarded by this module.",
        "name": "pam_listfile - deny or allow services based on an arbitrary file",
        "section": 8
    },
    {
        "command": "pam_localuser",
        "description": "pam_localuser is a PAM module to help implementing site-widelogin policies, where they typically include a subset of thenetwork's users and a few accounts that are local to a particularworkstation. Using pam_localuser and pam_wheel or pam_listfile isan effective way to restrict access to either local users and/ora subset of the network's users.This could also be implemented using pam_listfile.so and a veryshort awk script invoked by cron, but it's common enough to havebeen separated out.",
        "name": "pam_localuser - require users to be listed in /etc/passwd",
        "section": 8
    },
    {
        "command": "pam_loginuid",
        "description": "The pam_loginuid module sets the loginuid process attribute forthe process that was authenticated. This is necessary forapplications to be correctly audited. This PAM module should onlybe used for entry point applications like: login, sshd, gdm,vsftpd, crond and atd. There are probably other entry pointapplications besides these. You should not use it forapplications like sudo or su as that defeats the purpose bychanging the loginuid to the account they just switched to.",
        "name": "pam_loginuid - Record user's login uid to the process attribute",
        "section": 8
    },
    {
        "command": "pam_mail",
        "description": "The pam_mail PAM module provides the \"you have new mail\" serviceto the user. It can be plugged into any application that hascredential or session hooks. It gives a single message indicatingthe newness of any mail it finds in the user's mail folder. Thismodule also sets the PAM environment variable, MAIL, to theuser's mail directory.If the mail spool file (be it /var/mail/$USER or a pathname givenwith the dir= parameter) is a directory then pam_mail assumes itis in the Maildir format.",
        "name": "pam_mail - Inform about available mail",
        "section": 8
    },
    {
        "command": "pam_mkhomedir",
        "description": "The pam_mkhomedir PAM module will create a users home directoryif it does not exist when the session begins. This allows usersto be present in central database (such as NIS, kerberos or LDAP)without using a distributed file system or pre-creating a largenumber of directories. The skeleton directory (usually/etc/skel/) is used to copy default files and also sets a umaskfor the creation.The new users home directory will not be removed after logout ofthe user.",
        "name": "pam_mkhomedir - PAM module to create users home directory",
        "section": 8
    },
    {
        "command": "pam_motd",
        "description": "pam_motd is a PAM module that can be used to display arbitrarymotd (message of the day) files after a successful login. Bydefault, pam_motd shows files in the following locations:/etc/motd/run/motd/usr/lib/motd/etc/motd.d//run/motd.d//usr/lib/motd.d/Each message size is limited to 64KB.If /etc/motd does not exist, then /run/motd is shown. If/run/motd does not exist, then /usr/lib/motd is shown.Similar overriding behavior applies to the directories. Files in/etc/motd.d/ override files with the same name in /run/motd.d/and /usr/lib/motd.d/. Files in /run/motd.d/ override files withthe same name in /usr/lib/motd.d/.Files in the directories listed above are displayed inlexicographic order by name. Moreover, the files are filtered byreading them with the credentials of the target userauthenticating on the system.To silence a message, a symbolic link with target /dev/null maybe placed in /etc/motd.d with the same filename as the message tobe silenced. Example: Creating a symbolic link as followssilences /usr/lib/motd.d/my_motd.ln -s /dev/null /etc/motd.d/my_motdThe MOTD_SHOWN=pam environment variable is set after showing themotd files, even when all of them were silenced using symboliclinks.",
        "name": "pam_motd - Display the motd file",
        "section": 8
    },
    {
        "command": "pam_namespace",
        "description": "The pam_namespace PAM module sets up a private namespace for asession with polyinstantiated directories. A polyinstantiateddirectory provides a different instance of itself based on username, or when using SELinux, user name, security context or both.If an executable script /etc/security/namespace.init exists, itis used to initialize the instance directory after it is set upand mounted on the polyinstantiated directory. The scriptreceives the polyinstantiated directory path, the instancedirectory path, flag whether the instance directory was newlycreated (0 for no, 1 for yes), and the user name as itsarguments.The pam_namespace module disassociates the session namespace fromthe parent namespace. Any mounts/unmounts performed in the parentnamespace, such as mounting of devices, are not reflected in thesession namespace. To propagate selected mount/unmount eventsfrom the parent namespace into the disassociated sessionnamespace, an administrator may use the special shared-subtreefeature. For additional information on shared-subtree feature,please refer to the mount(8) man page and the shared-subtreedescription at http://lwn.net/Articles/159077 andhttp://lwn.net/Articles/159092.",
        "name": "pam_namespace - PAM module for configuring namespace for asession",
        "section": 8
    },
    {
        "command": "pam_namespace_helper",
        "description": "pam_namespace_helper is a helper program for the pam_namespacemodule that sets up a private namespace for a session withpolyinstantiated directories. The helper ensures that thenamespace mount points exist before they are started to be usedfor the polyinstantiated directories. Mount points for homedirectories (lines with $HOME) are not created.pam_namespace_helper should be run by systemd at system startup.It should also be run by the administrator after defining thepolyinstantiated directories but before enabling them.",
        "name": "pam_namespace_helper - Helper binary that creates homedirectories",
        "section": 8
    },
    {
        "command": "pam_nologin",
        "description": "pam_nologin is a PAM module that prevents users from logging intothe system when /var/run/nologin or /etc/nologin exists. Thecontents of the file are displayed to the user. The pam_nologinmodule has no effect on the root user's ability to log in.",
        "name": "pam_nologin - Prevent non-root users from login",
        "section": 8
    },
    {
        "command": "pam_permit",
        "description": "pam_permit is a PAM module that always permit access. It doesnothing else.In the case of authentication, the user's name will be set tonobody if the application didn't set one. Many applications andPAM modules become confused if this name is unknown.This module is very dangerous. It should be used with extremecaution.",
        "name": "pam_permit - The promiscuous module",
        "section": 8
    },
    {
        "command": "pam_pwhistory",
        "description": "This module saves the last passwords for each user in order toforce password change history and keep the user from alternatingbetween the same password too frequently.This module does not work together with kerberos. In general, itdoes not make much sense to use this module in conjunction withNIS or LDAP, since the old passwords are stored on the localmachine and are not available on another machine for passwordhistory checking.",
        "name": "pam_pwhistory - PAM module to remember last passwords",
        "section": 8
    },
    {
        "command": "pam_rhosts",
        "description": "This module performs the standard network authentication forservices, as used by traditional implementations of rlogin andrsh etc.The authentication mechanism of this module is based on thecontents of two files; /etc/hosts.equiv (or and ~/.rhosts.Firstly, hosts listed in the former file are treated asequivalent to the localhost. Secondly, entries in the user's owncopy of the latter file is used to map \"remote-host remote-user\"pairs to that user's account on the current host. Access isgranted to the user if their host is present in /etc/hosts.equivand their remote account is identical to their local one, or iftheir remote account has an entry in their personal configurationfile.The module authenticates a remote user (internally specified bythe item PAM_RUSER connecting from the remote host (internallyspecified by the item PAM_RHOST). Accordingly, for applicationsto be compatible this authentication module they must set theseitems prior to calling pam_authenticate(). The module is notcapable of independently probing the network connection for suchinformation.",
        "name": "pam_rhosts - The rhosts PAM module",
        "section": 8
    },
    {
        "command": "pam_rootok",
        "description": "pam_rootok is a PAM module that authenticates the user if theirUID is 0. Applications that are created setuid-root generallyretain the UID of the user but run with the authority of anenhanced effective-UID. It is the real UID that is checked.",
        "name": "pam_rootok - Gain only root access",
        "section": 8
    },
    {
        "command": "pam_securetty",
        "description": "pam_securetty is a PAM module that allows root logins only if theuser is logging in on a \"secure\" tty, as defined by the listingin the securetty file. pam_securetty checks at first, if/etc/securetty exists. If not and it was built with vendordirsupport, it will use /securetty. pam_securetty also checks thatthe securetty files are plain files and not world writable. Itwill also allow root logins on the tty specified with console=switch on the kernel command line and on ttys from the/sys/class/tty/console/active.This module has no effect on non-root users and requires that theapplication fills in the PAM_TTY item correctly.For canonical usage, should be listed as a requiredauthentication method before any sufficient authenticationmethods.",
        "name": "pam_securetty - Limit root login to special devices",
        "section": 8
    },
    {
        "command": "pam_selinux",
        "description": "pam_selinux is a PAM module that sets up the default SELinuxsecurity context for the next executed process.When a new session is started, the open_session part of themodule computes and sets up the execution security context usedfor the next execve(2) call, the file security context for thecontrolling terminal, and the security context used for creatinga new kernel keyring.When the session is ended, the close_session part of the modulerestores old security contexts that were in effect before thechange made by the open_session part of the module.Adding pam_selinux into the PAM stack might disrupt behavior ofother PAM modules which execute applications. To avoid that,pam_selinux.so open should be placed after such modules in thePAM stack, and pam_selinux.so close should be placed before them.When such a placement is not feasible, pam_selinux.so restorecould be used to temporary restore original security contexts.",
        "name": "pam_selinux - PAM module to set the default security context",
        "section": 8
    },
    {
        "command": "pam_selinux_check",
        "description": "With no arguments, pam_selinux_check will prompt for user",
        "name": "pam_selinux_check - login program to test pam_selinux.so",
        "section": 8
    },
    {
        "command": "pam_sepermit",
        "description": "The pam_sepermit module allows or denies login depending onSELinux enforcement state.When the user which is logging in matches an entry in the configfile he is allowed access only when the SELinux is in enforcingmode. Otherwise he is denied access. For users not matching anyentry in the config file the pam_sepermit module returnsPAM_IGNORE return value.The config file contains a list of user names one per line withoptional arguments. If the name is prefixed with @ character itmeans that all users in the group name match. If it is prefixedwith a % character the SELinux user is used to match against thename instead of the account name. Note that when SELinux isdisabled the SELinux user assigned to the account cannot bedetermined. This means that such entries are never matched whenSELinux is disabled and pam_sepermit will return PAM_IGNORE.See sepermit.conf(5) for details.",
        "name": "pam_sepermit - PAM module to allow/deny login depending onSELinux enforcement state",
        "section": 8
    },
    {
        "command": "pam_setquota",
        "description": "pam_setquota is a PAM module to set or modify a disk quota atsession startThis makes quotas usable with central user databases, such asMySQL or LDAP.",
        "name": "pam_setquota - PAM module to set or modify disk quotas on sessionstart",
        "section": 8
    },
    {
        "command": "pam_shells",
        "description": "pam_shells is a PAM module that only allows access to the systemif the user's shell is listed in /etc/shells.It also checks if needed files (e.g./etc/shells) are plainfiles and not world writable.",
        "name": "pam_shells - PAM module to check for valid login shell",
        "section": 8
    },
    {
        "command": "pam_stress",
        "description": "The pam_stress PAM module is mainly intended to give theimpression of failing as a fully functioning module might.",
        "name": "pam_stress - The stress-testing PAM module",
        "section": 8
    },
    {
        "command": "pam_succeed_if",
        "description": "pam_succeed_if.so is designed to succeed or fail authenticationbased on characteristics of the account belonging to the userbeing authenticated or values of other PAM items. One use is toselect whether to load other modules based on this test.The module should be given one or more conditions as modulearguments, and authentication will succeed only if all of theconditions are met.",
        "name": "pam_succeed_if - test account characteristics",
        "section": 8
    },
    {
        "command": "pam_systemd",
        "description": "pam_systemd registers user sessions with the systemd loginmanager systemd-logind.service(8), and hence the systemd controlgroup hierarchy.The module also applies various resource management and runtimeparameters to the new session, as configured in the JSON UserRecords[1] of the user, when one is defined.On login, this module \u2014 in conjunction withsystemd-logind.service \u2014 ensures the following:1. If it does not exist yet, the user runtime directory/run/user/$UID is either created or mounted as new \"tmpfs\"file system with quota applied, and its ownership changed tothe user that is logging in.2. The $XDG_SESSION_ID environment variable is initialized. Ifauditing is available and pam_loginuid.so was run before thismodule (which is highly recommended), the variable isinitialized from the auditing session id(/proc/self/sessionid). Otherwise, an independent sessioncounter is used.3. A new systemd scope unit is created for the session. If thisis the first concurrent session of the user, an implicitper-user slice unit below user.slice is automatically createdand the scope placed into it. An instance of the systemservice user@.service, which runs the systemd user managerinstance, is started.4. The \"$TZ\", \"$EMAIL\" and \"$LANG\" environment variables areconfigured for the user, based on the respective data fromthe user's JSON record (if it is defined). Moreover, anyenvironment variables explicitly configured in the userrecord are imported, and the umask, nice level, and resourcelimits initialized.On logout, this module ensures the following:1. If enabled in logind.conf(5) (KillUserProcesses=), allprocesses of the session are terminated. If the lastconcurrent session of a user ends, the user's systemdinstance will be terminated too, and so will the user's sliceunit.2. If the last concurrent session of a user ends, the userruntime directory /run/user/$UID and all its contents areremoved, too.If the system was not booted up with systemd as init system, thismodule does nothing and immediately returns PAM_SUCCESS.",
        "name": "pam_systemd - Register user sessions in the systemd login manager",
        "section": 8
    },
    {
        "command": "pam_time",
        "description": "The pam_time PAM module does not authenticate the user, butinstead it restricts access to a system and or specificapplications at various times of the day and on specific days orover various terminal lines. This module can be configured todeny access to (individual) users based on their name, the timeof day, the day of week, the service they are applying for andtheir terminal from which they are making their request.By default rules for time/port access are taken from config file/etc/security/time.conf. An alternative file can be specifiedwith the conffile option.If Linux PAM is compiled with audit support the module willreport when it denies access.",
        "name": "pam_time - PAM module for time control access",
        "section": 8
    },
    {
        "command": "pam_timestamp",
        "description": "In a nutshell, pam_timestamp caches successful authenticationattempts, and allows you to use a recent successful attempt asthe basis for authentication. This is similar mechanism which isused in sudo.When an application opens a session using pam_timestamp, atimestamp file is created in the timestampdir directory for theuser. When an application attempts to authenticate the user, apam_timestamp will treat a sufficiently recent timestamp file asgrounds for succeeding.",
        "name": "pam_timestamp - Authenticate using cached successfulauthentication attempts",
        "section": 8
    },
    {
        "command": "pam_timestamp_check",
        "description": "With no arguments pam_timestamp_check will check to see if thedefault timestamp is valid, or optionally remove it.",
        "name": "pam_timestamp_check - Check to see if the default timestamp isvalid",
        "section": 8
    },
    {
        "command": "pam_tty_audit",
        "description": "The pam_tty_audit PAM module is used to enable or disable TTYauditing. By default, the kernel does not audit input on any TTY.",
        "name": "pam_tty_audit - Enable or disable TTY auditing for specifiedusers",
        "section": 8
    },
    {
        "command": "pam_umask",
        "description": "pam_umask is a PAM module to set the file mode creation mask ofthe current environment. The umask affects the defaultpermissions assigned to newly created files.The PAM module tries to get the umask value from the followingplaces in the following order:\u2022umask= entry in the user's GECOS field\u2022umask= argument\u2022UMASK entry from /etc/login.defs\u2022UMASK= entry from /etc/default/loginThe GECOS field is split on comma ',' characters. The module alsoin addition to the umask= entry recognizes pri= entry, which setsthe nice priority value for the session, and ulimit= entry, whichsets the maximum size of files the processes in the session cancreate.",
        "name": "pam_umask - PAM module to set the file mode creation mask",
        "section": 8
    },
    {
        "command": "pam_unix",
        "description": "This is the standard Unix authentication module. It uses standardcalls from the system's libraries to retrieve and set accountinformation as well as authentication. Usually this is obtainedfrom the /etc/passwd and the /etc/shadow file as well if shadowis enabled.The account component performs the task of establishing thestatus of the user's account and password based on the followingshadow elements: expire, last_change, max_change, min_change,warn_change. In the case of the latter, it may offer advice tothe user on changing their password or, through thePAM_AUTHTOKEN_REQD return, delay giving service to the user untilthey have established a new password. The entries listed aboveare documented in the shadow(5) manual page. Should the user'srecord not contain one or more of these entries, thecorresponding shadow check is not performed.The authentication component performs the task of checking theusers credentials (password). The default action of this moduleis to not permit the user access to a service if their officialpassword is blank.A helper binary, unix_chkpwd(8), is provided to check the user'spassword when it is stored in a read protected database. Thisbinary is very simple and will only check the password of theuser invoking it. It is called transparently on behalf of theuser by the authenticating component of this module. In this wayit is possible for applications like xlock(1) to work withoutbeing setuid-root. The module, by default, will temporarily turnoff SIGCHLD handling for the duration of execution of the helperbinary. This is generally the right thing to do, as manyapplications are not prepared to handle this signal from a childthey didn't know was fork()d. The noreap module argument can beused to suppress this temporary shielding and may be needed foruse with certain applications.The maximum length of a password supported by the pam_unix modulevia the helper binary is PAM_MAX_RESP_SIZE - currently 512 bytes.The rest of the password provided by the conversation function tothe module will be ignored.The password component of this module performs the task ofupdating the user's password. The default encryption hash istaken from the ENCRYPT_METHOD variable from /etc/login.defsThe session component of this module logs when a user logins orleave the system.Remaining arguments, supported by others functions of thismodule, are silently ignored. Other arguments are logged aserrors through syslog(3).",
        "name": "pam_unix - Module for traditional password authentication",
        "section": 8
    },
    {
        "command": "pam_userdb",
        "description": "The pam_userdb module is used to verify a username/password pairagainst values stored in a Berkeley DB database. The database isindexed by the username, and the data fields corresponding to theusername keys are the passwords.",
        "name": "pam_userdb - PAM module to authenticate against a db database",
        "section": 8
    },
    {
        "command": "pam_usertype",
        "description": "pam_usertype.so is designed to succeed or fail authenticationbased on type of the account of the authenticated user. The typeof the account is decided with help of SYS_UID_MAX settings in/etc/login.defs. One use is to select whether to load othermodules based on this test.The module should be given only one condition as module argument.Authentication will succeed only if the condition is met.",
        "name": "pam_usertype - check if the authenticated user is a system orregular account",
        "section": 8
    },
    {
        "command": "pam_warn",
        "description": "pam_warn is a PAM module that logs the service, terminal, user,remote user and remote host to syslog(3). The items are notprobed for, but instead obtained from the standard PAM items. Themodule always returns PAM_IGNORE, indicating that it does notwant to affect the authentication process.",
        "name": "pam_warn - PAM module which logs all PAM items if called",
        "section": 8
    },
    {
        "command": "pam_wheel",
        "description": "The pam_wheel PAM module is used to enforce the so-called wheelgroup. By default it permits access to the target user if theapplicant user is a member of the wheel group. If no group withthis name exist, the module is using the group with the group-ID0.",
        "name": "pam_wheel - Only permit root access to members of group wheel",
        "section": 8
    },
    {
        "command": "pam_xauth",
        "description": "The pam_xauth PAM module is designed to forward xauth keys(sometimes referred to as \"cookies\") between users.Without pam_xauth, when xauth is enabled and a user uses thesu(1) command to assume another user's privileges, that user isno longer able to access the original user's X display becausethe new user does not have the key needed to access the display.pam_xauth solves the problem by forwarding the key from the userrunning su (the source user) to the user whose identity thesource user is assuming (the target user) when the session iscreated, and destroying the key when the session is torn down.This means, for example, that when you run su(1) from an xtermsession, you will be able to run X programs without explicitlydealing with the xauth(1) xauth command or ~/.Xauthority files.pam_xauth will only forward keys if xauth can list a keyconnected to the $DISPLAY environment variable.Primitive access control is provided by ~/.xauth/export in theinvoking user's home directory and ~/.xauth/import in the targetuser's home directory.If a user has a ~/.xauth/import file, the user will only receivecookies from users listed in the file. If there is no~/.xauth/import file, the user will accept cookies from any otheruser.If a user has a .xauth/export file, the user will only forwardcookies to users listed in the file. If there is no~/.xauth/export file, and the invoking user is not root, the userwill forward cookies to any other user. If there is no~/.xauth/export file, and the invoking user is root, the userwill not forward cookies to other users.Both the import and export files support wildcards (such as *).Both the import and export files can be empty, signifying that nousers are allowed.",
        "name": "pam_xauth - PAM module to forward xauth keys between users",
        "section": 8
    },
    {
        "command": "parted",
        "description": "parted is a program to manipulate disk partitions.It supportsmultiple partition table formats, including MS-DOS and GPT.Itis useful for creating space for new operating systems,reorganising disk usage, and copying data to new hard disks.This manual page documents parted briefly.Completedocumentation is distributed with the package in GNU Info format.",
        "name": "parted - a partition manipulation program",
        "section": 8
    },
    {
        "command": "partprobe",
        "description": "This manual page documents briefly the partprobe command.partprobe is a program that informs the operating system kernelof partition table changes.",
        "name": "partprobe - inform the OS of partition table changes",
        "section": 8
    },
    {
        "command": "partx",
        "description": "Given a device or disk-image, partx tries to parse the partitiontable and list its contents. It can also tell the kernel to addor remove partitions from its bookkeeping.The disk argument is optional when a partition argument isprovided. To force scanning a partition as if it were a wholedisk (for example to list nested subpartitions), use the argument\"-\" (hyphen-minus). For example:partx --show - /dev/sda3This will see sda3 as a whole-disk rather than as a partition.partx is not an fdisk program - adding and removing partitionsdoes not change the disk, it just tells the kernel about thepresence and numbering of on-disk partitions.",
        "name": "partx - tell the kernel about the presence and numbering ofon-disk partitions",
        "section": 8
    },
    {
        "command": "pedit",
        "description": "The pedit action can be used to change arbitrary packet data. Thelocation of data to change can either be specified by giving anoffset and size as in RAW_OP, or for header values by naming theheader and field to edit the size is then chosen automaticallybased on the header field size.",
        "name": "pedit - generic packet editor action",
        "section": 8
    },
    {
        "command": "pfifo",
        "description": "The pfifo and bfifo qdiscs are unadorned First In, First Outqueues. They are the simplest queues possible and therefore haveno overhead.pfifo constrains the queue size as measured inpackets.bfifo does so as measured in bytes.Like all non-default qdiscs, they maintain statistics. This mightbe a reason to prefer pfifo or bfifo over the default.",
        "name": "pfifo - Packet limited First In, First Out queuebfifo - Byte limited First In, First Out queue",
        "section": 8
    },
    {
        "command": "pfifo_fast",
        "description": "pfifo_fast is the default qdisc of each interface.Whenever an interface is created, the pfifo_fast qdisc isautomatically used as a queue. If another qdisc is attached, itpreempts the default pfifo_fast, which automatically returns tofunction when an existing qdisc is detached.In this sense this qdisc is magic, and unlike other qdiscs.",
        "name": "pfifo_fast - three-band first in, first out queue",
        "section": 8
    },
    {
        "command": "ping",
        "description": "ping uses the ICMP protocol's mandatory ECHO_REQUEST datagram toelicit an ICMP ECHO_RESPONSE from a host or gateway. ECHO_REQUESTdatagrams (\u201cpings\u201d) have an IP and ICMP header, followed by astruct timeval and then an arbitrary number of \u201cpad\u201d bytes usedto fill out the packet.ping works with both IPv4 and IPv6. Using only one of themexplicitly can be enforced by specifying -4 or -6.ping can also send IPv6 Node Information Queries (RFC4620).Intermediate hops may not be allowed, because IPv6 source routingwas deprecated (RFC5095).",
        "name": "ping - send ICMP ECHO_REQUEST to network hosts",
        "section": 8
    },
    {
        "command": "pivot_root",
        "description": "pivot_root moves the root file system of the current process tothe directory put_old and makes new_root the new root filesystem. Since pivot_root(8) simply calls pivot_root(2), we referto the man page of the latter for further details.Note that, depending on the implementation of pivot_root, rootand current working directory of the caller may or may notchange. The following is a sequence for invoking pivot_root thatworks in either case, assuming that pivot_root and chroot are inthe current PATH:cd new_rootpivot_root . put_oldexec chroot . commandNote that chroot must be available under the old root and underthe new root, because pivot_root may or may not have implicitlychanged the root directory of the shell.Note that exec chroot changes the running executable, which isnecessary if the old root directory should be unmountedafterwards. Also note that standard input, output, and error maystill point to a device on the old root file system, keeping itbusy. They can easily be changed when invoking chroot (see below;note the absence of leading slashes to make it work whetherpivot_root has changed the shell\u2019s root or not).",
        "name": "pivot_root - change the root filesystem",
        "section": 8
    },
    {
        "command": "pkgtorrent-service",
        "description": null,
        "name": "pkgtorrent-service(8) Is a python WSGI web application that cangenerate BitTorrent files for binary files available from the webserver.",
        "section": 8
    },
    {
        "command": "pktsetup",
        "description": "Pktsetup is used to associate packet devices with CD or DVD blockdevices, so that the packet device can then be mounted andpotentially used as a read/write filesystem. This requires kernelsupport for the packet device, and the UDF filesystem.See: HOWTO.udf (in the udftools documents directory)",
        "name": "pktsetup - set up and tear down packet device associations",
        "section": 8
    },
    {
        "command": "plipconfig",
        "description": "Plipconfig is used to (hopefully) improve PLIP performance bychanging the default timing parameters used by the PLIP protocol.Results are dependent on the parallel port hardware, cable, andthe CPU speed of each machine on each end of the PLIP link.If the single interface argument is given, plipconfig displaysthe status of the given interface only.Otherwise, it will tryto set the options.",
        "name": "plipconfig - fine tune PLIP device parameters",
        "section": 8
    },
    {
        "command": "police",
        "description": "The police action allows limiting of the byte or packet rate oftraffic matched by the filter it is attached to.There are two different algorithms available to measure the byterate: The first one uses an internal dual token bucket and isconfigured using the rate, burst, mtu, peakrate, overhead andlinklayer parameters. The second one uses an in-kernel samplingmechanism. It can be fine-tuned using the estimator filterparameter.There is one algorithm available to measure packet rate and it issimilar to the first algorithm described for byte rate. It isconfigured using the pkt_rate and pkt_burst parameters.At least one of the rate and pkt_rate parameters must beconfigured.",
        "name": "police - policing action",
        "section": 8
    },
    {
        "command": "poweroff",
        "description": "poweroff, reboot, and halt may be used to power off, reboot, orhalt the machine. All three commands take the same options.",
        "name": "poweroff, reboot, halt - Power off, reboot, or halt the machine",
        "section": 8
    },
    {
        "command": "pppd",
        "description": "PPP is the protocol used for establishing internet links overdial-up modems, DSL connections, and many other types of point-to-point links.The pppd daemon works together with the kernelPPP driver to establish and maintain a PPP link with anothersystem (called the peer) and to negotiate Internet Protocol (IP)addresses for each end of the link.Pppd can also authenticatethe peer and/or supply authentication information to the peer.PPP can be used with other network protocols besides IP, but suchuse is becoming increasingly rare.",
        "name": "pppd - Point-to-Point Protocol Daemon",
        "section": 8
    },
    {
        "command": "pppd-radattr",
        "description": "The radattr plugin for pppd causes all radius attributes returnedby the RADIUS server at authentication time to be stored in thefile /var/run/radattr.pppN where pppN is the name of the PPPinterface.The RADIUS attributes are stored one per line in theformat \"Attribute-Name Attribute-Value\".This format isconvenient for use in /etc/ppp/ip-up and /etc/ppp/ip-downscripts.Note that you must load the radius.so plugin before loading theradattr.so plugin; radattr.so depends on symbols defined inradius.so.",
        "name": "radattr.so - RADIUS utility plugin for pppd(8)",
        "section": 8
    },
    {
        "command": "pppd-radius",
        "description": "The RADIUS plugin for pppd permits pppd to perform PAP, CHAP, MS-CHAP and MS-CHAPv2 authentication against a RADIUS server insteadof the usual /etc/ppp/pap-secrets and /etc/ppp/chap-secretsfiles.The RADIUS plugin is built on a library called radiusclient whichhas its own configuration files (usually in /etc/radiusclient),consult those files for more information on configuring theRADIUS plugin",
        "name": "radius.so - RADIUS authentication plugin for pppd(8)",
        "section": 8
    },
    {
        "command": "pppdump",
        "description": "The pppdump utility converts the files written using the recordoption of pppd into a human-readable format.If one or morefilenames are specified, pppdump will read each in turn;otherwise it will read its standard input.In each case theresult is written to standard output.The options are as follows:-hPrints the bytes sent and received in hexadecimal.Ifneither this option nor the -p option is specified, thebytes are printed as the characters themselves, with non-printing and non-ASCII characters printed as escapesequences.-pCollects the bytes sent and received into PPP packets,interpreting the async HDLC framing and escape charactersand checking the FCS (frame check sequence) of eachpacket.The packets are printed as hex values and ascharacters (non-printable characters are printed as `.').-dWith the -p option, this option causes pppdump todecompress packets which have been compressed with theBSD-Compress or Deflate methods.-rReverses the direction indicators, so that `sent' isprinted for bytes or packets received, and `rcvd' isprinted for bytes or packets sent.-aPrints absolute times.-m mru Use mru as the MRU (maximum receive unit) for bothdirections of the link when checking for over-length PPPpackets (with the -p option).",
        "name": "pppdump - convert PPP record file to readable format",
        "section": 8
    },
    {
        "command": "pppgetpass",
        "description": "pppgetpass the outer half of a plugin for PAP password promptingin pppd.If the peer requires PAP, and the passprompt.so pluginis loaded into pppd, it will run /usr/sbin/pppgetpass (or anotherprogram specified by the promptprog option) to prompt the userfor the password.",
        "name": "pppgetpass - prompt for PAP password",
        "section": 8
    },
    {
        "command": "pppoe-discovery",
        "description": "pppoe-discovery performs the same discovery process as pppoe, butdoes not initiate a session.It sends a PADI packet and thenprints the names of access concentrators in each PADO packet itreceives.",
        "name": "pppoe-discovery - perform PPPoE discovery",
        "section": 8
    },
    {
        "command": "pppstats",
        "description": "The pppstats utility reports PPP-related statistics at regularintervals for the specified PPP interface.If the interface isunspecified, it will default to ppp0.The display is splithorizontally into input and output sections containing columns ofstatistics describing the properties and volume of packetsreceived and transmitted by the interface.The options are as follows:-aDisplay absolute values rather than deltas.With thisoption, all reports show statistics for the time since thelink was initiated.Without this option, the second andsubsequent reports show statistics for the time since thelast report.-dShow data rate (kB/s) instead of bytes.-c countRepeat the display count times.If this option is notspecified, the default repeat count is 1 if the -w optionis not specified, otherwise infinity.-rDisplay additional statistics summarizing the compressionratio achieved by the packet compression algorithm in use.-vDisplay additional statistics relating to the performanceof the Van Jacobson TCP header compression algorithm.-w waitPause wait seconds between each display.If this optionis not specified, the default interval is 5 seconds.-zInstead of the standard display, show statisticsindicating the performance of the packet compressionalgorithm in use.The following fields are printed on the input side when the -zoption is not used:INThe total number of bytes received by this interface.PACKThe total number of packets received by this interface.VJCOMP The number of header-compressed TCP packets received bythis interface.VJUNCThe number of header-uncompressed TCP packets received bythis interface.Not reported when the -r option isspecified.VJERRThe number of corrupted or bogus header-compressed TCPpackets received by this interface.Not reported when the-r option is specified.VJTOSS The number of VJ header-compressed TCP packets dropped onreception by this interface because of preceding errors.Only reported when the -v option is specified.NON-VJ The total number of non-TCP packets received by thisinterface. Only reported when the -v option is specified.RATIOThe compression ratio achieved for received packets by thepacket compression scheme in use, defined as theuncompressed size divided by the compressed size.Onlyreported when the -r option is specified.UBYTEThe total number of bytes received, after decompression ofcompressed packets.Only reported when the -r option isspecified.The following fields are printed on the output side:OUTThe total number of bytes transmitted from this interface.PACKThe total number of packets transmitted from thisinterface.VJCOMP The number of TCP packets transmitted from this interfacewith VJ-compressed TCP headers.VJUNCThe number of TCP packets transmitted from this interfacewith VJ-uncompressed TCP headers.Not reported when the-r option is specified.NON-VJ The total number of non-TCP packets transmitted from thisinterface.Not reported when the -r option is specified.VJSRCH The number of searches for the cached header entry for aVJ header compressed TCP packet.Only reported when the-v option is specified.VJMISS The number of failed searches for the cached header entryfor a VJ header compressed TCP packet.Only reported whenthe -v option is specified.RATIOThe compression ratio achieved for transmitted packets bythe packet compression scheme in use, defined as the sizebefore compression divided by the compressed size.Onlyreported when the -r option is specified.UBYTEThe total number of bytes to be transmitted, before packetcompression is applied.Only reported when the -r optionis specified.When the -z option is specified, pppstats instead displays thefollowing fields, relating to the packet compression algorithmcurrently in use.If packet compression is not in use, thesefields will all display zeroes.The fields displayed on theinput side are:COMPRESSED BYTEThe number of bytes of compressed packets received.COMPRESSED PACKThe number of compressed packets received.INCOMPRESSIBLE BYTEThe number of bytes of incompressible packets (that is,those which were transmitted in uncompressed form)received.INCOMPRESSIBLE PACKThe number of incompressible packets received.COMP RATIOThe recent compression ratio for incoming packets, definedas the uncompressed size divided by the compressed size(including both compressible and incompressible packets).The fields displayed on the output side are:COMPRESSED BYTEThe number of bytes of compressed packets transmitted.COMPRESSED PACKThe number of compressed packets transmitted.INCOMPRESSIBLE BYTEThe number of bytes of incompressible packets transmitted(that is, those which were transmitted in uncompressedform).INCOMPRESSIBLE PACKThe number of incompressible packets transmitted.COMP RATIOThe recent compression ratio for outgoing packets.",
        "name": "pppstats - print PPP statistics",
        "section": 8
    },
    {
        "command": "prelink",
        "description": "prelink is a program that modifies ELF shared libraries and ELFdynamically linked binaries in such a way that the time neededfor the dynamic linker to perform relocations at startupsignificantly decreases.Due to fewer relocations, the run-timememory consumption decreases as well (especially the number ofunshareable pages).The prelinking information is only used atstartup time if none of the dependent libraries have changedsince prelinking; otherwise programs are relocated normally.prelink first collects ELF binaries to be prelinked and all theELF shared libraries they depend on. Then it assigns a uniquevirtual address space slot to each library and relinks the sharedlibrary to that base address.When the dynamic linker attemptsto load such a library, unless that virtual address space slot isalready occupied, it maps the library into the given slot.Afterthis is done, prelink, with the help of dynamic linker, resolvesall relocations in the binary or library against its dependentlibraries and stores the relocations into the ELF object.Italso stores a list of all dependent libraries together with theirchecksums into the binary or library.For binaries, it alsocomputes a list of conflicts (relocations that resolvedifferently in the binary's symbol search scope than in thesmaller search scope in which the dependent library was resolved)and stores it into a special ELF section.At runtime, the dynamic linker first checks whether all dependentlibraries were successfully mapped into their designated addressspace slots, and whether they have not changed since theprelinking was done.If all checks are successful, the dynamiclinker just replays the list of conflicts (which is usuallysignificantly shorter than total number of relocations) insteadof relocating each library.",
        "name": "prelink - prelink ELF shared libraries and binaries to speed upstartup time",
        "section": 8
    },
    {
        "command": "pscap",
        "description": "pscap is a program that prints out a report of processcapabilities. If the application has any capabilities, it will bein the report with the exception of init. By giving the -acommand line option, init will be included, too. If a process isnot in the report, it has dropped all capabilities. If theprocess has partial capabilities, it is further examined to seeif it has an open-ended bounding set. If this is found to betrue, a '+' symbol is added. If the process has ambientcapabilities, a '@' symbols is added.The command name in the output may be followed by an asteriskmark (*). This mark denotes processes which run in child usernamespaces (relative to the user namespace of pscap itself).",
        "name": "pscap - a program to see capabilities",
        "section": 8
    },
    {
        "command": "pvchange",
        "description": "pvchange changes PV attributes in the VG.For options listed in parentheses, any one is required, afterwhich the others are optional.",
        "name": "pvchange \u2014 Change attributes of physical volume(s)",
        "section": 8
    },
    {
        "command": "pvck",
        "description": "pvck checks and repairs LVM metadata on PVs.Dump optionsheadersPrint LVM on-disk headers and structures: label_header,pv_header, mda_header(s), and metadata text.Warnings areprinted if any values are incorrect.The label_header andpv_header both exist in a 512 byte sector, usually the secondsector of the device.An mda_header exists in a 512 byte sectorat offset 4096 bytes.A second mda_header can optionally existnear the end of the device.The metadata text exists in an area(about 1MiB by default) immediately following the mda_headersector.The metadata text is checked but not printed (see otheroptions).metadataPrint the current LVM VG metadata text (or save to a file), usingheaders to locate the latest copy of metadata.If headers aredamaged, metadata may not be found (see metadata_search).Use--settings \"mda_num=2\" to look in mda2 (the second mda at the endof the device, if used).The metadata text is printed to stdoutor saved to a file with --file.metadata_allList all versions of VG metadata found in the metadata area,using headers to locate metadata.Full copies of all metadataare saved to a file with the --file option.If headers aredamaged, metadata may not be found (see metadata_search).Use--settings \"mda_num=2\" as above.Use -v to include descriptionsand dates when listing metadata versions.metadata_searchList all versions of VG metadata found in the metadata area,searching common locations so metadata can be found if headersare damaged.Full copies of all metadata are saved to a filewith the --file option.To save one specific version ofmetadata, use --settings \"metadata_offset=<offset>\", where theoffset is taken from the list of versions found.Use -v toinclude descriptions and dates when listing metadata versions.metadata_areaSave the entire text metadata area to a file without processing.Repair options--repairRepair headers and metadata on a PV.This uses a metadata inputfile that was extracted by --dump, or a backup file (from/etc/lvm/backup).When possible, use metadata saved by --dumpfrom another PV in the same VG (or from a second metadata area onthe PV).There are cases where the PV UUID needs to be specified for thePV being repaired.It is specified using --settings\"pv_uuid=<UUID>\".In particular, if the device name for the PVbeing repaired does not match the previous device name of the PV,then LVM may not be able to determine the correct PV UUID.Whenheaders are damaged on more than one PV in a VG, it is importantfor the user to determine the correct PV UUID and specify it in--settings.Otherwise, the wrong PV UUID could be used if devicenames have been swapped since the metadata was last written.If a PV has no metadata areas and the pv_header is damaged, thenthe repair will not know to create no metadata areas duringrepair.It will by default repair metadata in mda1.To repairwith no metadata areas, use --settings \"mda_offset=0 mda_size=0\".There are cases where repair should be run on all PVs in the VG(using the same metadata file):if all PVs in the VG aredamaged, if using an old metadata version, or if a backup file isused instead of raw metadata (taken from pvck dump.)Using --repair is equivalent to running --repairtype pv_headerfollowed by --repairtype metadata.--repairtype pv_headerRepairs the header sector, containing the pv_header andlabel_header.--repairtype metadataRepairs the mda_header and metadata text.It requires theheaders to be correct (having been undamaged or alreadyrepaired).--repairtype label_headerRepairs label_header fields, leaving the pv_header (in the samesector) unchanged.(repairtype pv_header should usually be usedinstead.)SettingsThe --settings option controls or overrides certain dump orrepair behaviors.All offset and size values in settings are inbytes (units are not recognized.)These settings are subject tochange.mda_num=1|2Select which metadata area should be used.By default the firstmetadata area (1) is used.mda1 is always located at offset4096.mda2, at the end of the device, often does not exist (it'snot created by default.) If mda1 is erased, mda2, if it exists,will often still have metadata.metadata_offset=bytesSelect metadata text at this offset.Use with metadata_search toprint/save one instance of metadata text.mda_offset=bytes mda_size=bytesRefers to a metadata area (mda) location and size.An mdaincludes an mda_header and circular metadata text buffer.Setting this forces metadata_search look for metadata in thegiven area instead of the standard locations.When set to zerowith repair, it indicates no metadata areas should exist.mda2_offset=bytes mda2_size=bytesWhen repairing a pv_header, this forces a specific offset andsize for mda2 that should be recorded in the pv_header.pv_uuid=uuidSpecify the PV UUID of the device being repaired.When notspecified, repair will attempt to determine the correct PV UUIDby matching a device name in the metadata.device_size=bytesdata_offset=bytesWhen repairing a pv_header, the device_size, data_offset, andpvid can all be specified directly, in which case these valuesare not taken from a metadata file (where they usually comefrom), and the metadata file can be omitted.data_offset is thestarting location of the first physical extent (data), whichfollows the first metadata area.",
        "name": "pvck \u2014 Check metadata on physical volumes",
        "section": 8
    },
    {
        "command": "pvcreate",
        "description": "pvcreate initializes a Physical Volume (PV) on a device so thedevice is recognized as belonging to LVM.This allows the PV tobe used in a Volume Group (VG).An LVM disk label is written tothe device, and LVM metadata areas are initialized.A PV can beplaced on a whole device or partition.Use vgcreate(8) to create a new VG on the PV, or vgextend(8) toadd the PV to an existing VG.Use pvremove(8) to remove the LVMdisk label from the device.The force option will create a PV without confirmation.Repeating the force option (-ff) will forcibly create a PV,overriding checks that normally prevent it, e.g. if the PV isalready in a VG.Metadata location, size, and alignmentThe LVM disk label begins 512 bytes from the start of the device,and is 512 bytes in size.The LVM metadata area begins at an offset (from the start of thedevice) equal to the page size of the machine creating the PV(often 4 KiB.) The metadata area contains a 512 byte header and amulti-KiB circular buffer that holds text copies of the VGmetadata.With default settings, the first physical extent (PE), whichcontains LV data, is 1 MiB from the start of the device.Thislocation is controlled by default_data_alignment in lvm.conf,which is set to 1 (MiB) by default.The pe_start will be amultiple of this many MiB.This location can be checked with:pvs -o pe_start PVThe size of the LVM metadata area is the space between the thestart of the metadata area and the first PE.When metadatabegins at 4 KiB and the first PE is at 1024 KiB, the metadataarea size is 1020 KiB.This can be checked with:pvs -o mda_size PVThe mda_size cannot be increased after pvcreate, so if largermetadata is needed, it must be set during pvcreate.Two copiesof the VG metadata must always fit within the metadata area, sothe maximum VG metadata size is around half the mda_size.Thiscan be checked with:vgs -o mda_free VGA larger metadata area can be set with --metadatasize.Theresulting mda_size may be larger than specified due todefault_data_alignment placing pe_start on a MiB boundary, andthe fact that the metadata area extends to the first PE.Withmetadata starting at 4 KiB and default_data_alignment 1 (MiB),setting --metadatasize 2048k results in pe_start of 3 MiB andmda_size of 3068 KiB.Alternatively, --metadatasize 2044kresults in pe_start at 2 MiB and mda_size of 2044 KiB.The alignment of pe_start described above may be automaticallyoverridden based on md device properties or device i/o propertiesreported in sysfs.These automatic adjustments can beenabled/disabled using lvm.conf settings md_chunk_alignment anddata_alignment_offset_detection.To use a different pe_start alignment, use the --dataalignmentoption.The --metadatasize option would also typically be usedin this case because the metadata area size also determines thelocation of pe_start.When using these two options together,pe_start is calculated as: metadata area start (page size), plusthe specified --metadatasize, rounded up to the next multiple of--dataalignment.With metadata starting at 4 KiB, --metadatasize2048k, and --dataalignment 128k, pe_start is 2176 KiB andmda_size is 2172 KiB.The pe_start of 2176 KiB is the nearesteven multiple of 128 KiB that provides at least 2048 KiB ofmetadata space.Always check the resulting alignment andmetadata size when using these options.To shift an aligned pe_start value, use the --dataalignmentoffsetoption.The pe_start alignment is calculated as described above,and then the value specified with --dataalignmentoffset is addedto produce the final pe_start value.",
        "name": "pvcreate \u2014 Initialize physical volume(s) for use by LVM",
        "section": 8
    },
    {
        "command": "pvdisplay",
        "description": "pvdisplay shows the attributes of PVs, like size, physical extentsize, space used for the VG descriptor area, etc.pvs(8) is a preferred alternative that shows the same informationand more, using a more compact and configurable output format.",
        "name": "pvdisplay \u2014 Display various attributes of physical volume(s)",
        "section": 8
    },
    {
        "command": "pvmove",
        "description": "pvmove moves the allocated physical extents (PEs) on a source PVto one or more destination PVs.You can optionally specify asource LV in which case only extents used by that LV will bemoved to free (or specified) extents on the destination PV. If nodestination PV is specified, the normal allocation rules for theVG are used.If pvmove is interrupted for any reason (e.g. the machinecrashes) then run pvmove again without any PV arguments torestart any operations that were in progress from the lastcheckpoint. Alternatively, use the abort option at any time toabort the operation. The resulting location of LVs after an abortdepends on whether the atomic option was used.More than one pvmove can run concurrently if they are moving datafrom different source PVs, but additional pvmoves will ignore anyLVs already in the process of being changed, so some data mightnot get moved.",
        "name": "pvmove \u2014 Move extents from one physical volume to another",
        "section": 8
    },
    {
        "command": "pvremove",
        "description": "pvremove wipes the label on a device so that LVM will no longerrecognise it as a PV.A PV cannot be removed from a VG while it is used by an activeLV.Repeat the force option (-ff) to forcibly remove a PV belongingto an existing VG. Normally, vgreduce(8) should be used instead.",
        "name": "pvremove \u2014 Remove LVM label(s) from physical volume(s)",
        "section": 8
    },
    {
        "command": "pvresize",
        "description": "pvresize resizes a PV. The PV may already be in a VG and may haveactive LVs allocated on it.",
        "name": "pvresize \u2014 Resize physical volume(s)",
        "section": 8
    },
    {
        "command": "pvs",
        "description": "pvs produces formatted output about PVs.",
        "name": "pvs \u2014 Display information about physical volumes",
        "section": 8
    },
    {
        "command": "pvscan",
        "description": "When called without the --cache option, pvscan lists PVs on thesystem, like pvs(8) or pvdisplay(8).When --cache is used, pvscan updates runtime lvm state on thesystem, or with -aay performs autoactivation.pvscan --cache deviceIf device is present, lvm records that the PV on device isonline.If device is not present, lvm removes the online recordfor the PV.pvscan only reads the named device.pvscan --cacheUpdates the runtime state for all lvm devices.pvscan --cache -aay devicePerforms the --cache steps for the device, then checks if the VGusing the device is complete.If so, LVs in the VG areautoactivated, the same as vgchange -aay vgname would do.(Adevice name may be replaced with major and minor numbers.)pvscan --cache -aayPerforms the --cache steps for all devices, then autoactivatesany complete VGs.pvscan --cache --listvg|--listlvs devicePerforms the --cache steps for the device, then prints the nameof the VG using the device, or the names of LVs using the device.--checkcomplete is usually included to check if all PVs for theVG or LVs are online.When this command is called by a udevrule, the output must conform to udev rule specifications (see--udevoutput.)The udev rule will use the results to performautoactivation.Autoactivation of VGs or LVs can be enabled/disabled usingvgchange or lvchange with --setautoactivation y|n, or by addingnames to lvm.conf(5) activation/auto_activation_volume_listSee lvmautoactivation(7) for more information about how pvscan isused for autoactivation.",
        "name": "pvscan \u2014 List all physical volumes",
        "section": 8
    },
    {
        "command": "pwck",
        "description": "The pwck command verifies the integrity of the users andauthentication information. It checks that all entries in/etc/passwd and /etc/shadow have the proper format and containvalid data. The user is prompted to delete entries that areimproperly formatted or which have other uncorrectable errors.Checks are made to verify that each entry has:\u2022the correct number of fields\u2022a unique and valid user name\u2022a valid user and group identifier\u2022a valid primary group\u2022a valid home directory\u2022a valid login shellChecks for shadowed password information are enabled when thesecond file parameter SHADOWFILE is specified or when /etc/shadowexists on the system.These checks are the following:\u2022every passwd entry has a matching shadow entry, and everyshadow entry has a matching passwd entry\u2022passwords are specified in the shadowed file\u2022shadow entries have the correct number of fields\u2022shadow entries are unique in shadow\u2022the last password changes are not in the futureThe checks for correct number of fields and unique user name arefatal. If the entry has the wrong number of fields, the user willbe prompted to delete the entire line. If the user does notanswer affirmatively, all further checks are bypassed. An entrywith a duplicated user name is prompted for deletion, but theremaining checks will still be made. All other errors arewarnings and the user is encouraged to run the usermod command tocorrect the error.The commands which operate on the /etc/passwd file are not ableto alter corrupted or duplicated entries.pwck should be used inthose circumstances to remove the offending entry.",
        "name": "pwck - verify the integrity of password files",
        "section": 8
    },
    {
        "command": "pwconv",
        "description": "The pwconv command creates shadow from passwd and an optionallyexisting shadow.The pwunconv command creates passwd from passwd and shadow andthen removes shadow.The grpconv command creates gshadow from group and an optionallyexisting gshadow.The grpunconv command creates group from group and gshadow andthen removes gshadow.These four programs all operate on the normal and shadow passwordand group files: /etc/passwd, /etc/group, /etc/shadow, and/etc/gshadow.Each program acquires the necessary locks before conversion.pwconv and grpconv are similar. First, entries in the shadowedfile which don't exist in the main file are removed. Then,shadowed entries which don't have `x' as the password in the mainfile are updated. Any missing shadowed entries are added.Finally, passwords in the main file are replaced with `x'. Theseprograms can be used for initial conversion as well to update theshadowed file if the main file is edited by hand.pwconv will use the values of PASS_MIN_DAYS, PASS_MAX_DAYS, andPASS_WARN_AGE from /etc/login.defs when adding new entries to/etc/shadow.Likewise pwunconv and grpunconv are similar. Passwords in themain file are updated from the shadowed file. Entries which existin the main file but not in the shadowed file are left alone.Finally, the shadowed file is removed. Some password aginginformation is lost by pwunconv. It will convert what it can.",
        "name": "pwconv, pwunconv, grpconv, grpunconv - convert to and from shadowpasswords and groups",
        "section": 8
    },
    {
        "command": "pwhistory_helper",
        "description": "pwhistory_helper is a helper program for the pam_pwhistory modulethat transfers password hashes from passwd or shadow file to theopasswd file and checks a password supplied by user against theexisting hashes in the opasswd file.The purpose of the helper is to enable tighter confinement oflogin and password changing services. The helper is thus calledonly when SELinux is enabled on the system.The interface of the helper - command line options, andinput/output data format are internal to the pam_pwhistory moduleand it should not be called directly from applications.",
        "name": "pwhistory_helper - Helper binary that transfers password hashesfrom passwd or shadow to opasswd",
        "section": 8
    },
    {
        "command": "pwunconv",
        "description": "The pwconv command creates shadow from passwd and an optionallyexisting shadow.The pwunconv command creates passwd from passwd and shadow andthen removes shadow.The grpconv command creates gshadow from group and an optionallyexisting gshadow.The grpunconv command creates group from group and gshadow andthen removes gshadow.These four programs all operate on the normal and shadow passwordand group files: /etc/passwd, /etc/group, /etc/shadow, and/etc/gshadow.Each program acquires the necessary locks before conversion.pwconv and grpconv are similar. First, entries in the shadowedfile which don't exist in the main file are removed. Then,shadowed entries which don't have `x' as the password in the mainfile are updated. Any missing shadowed entries are added.Finally, passwords in the main file are replaced with `x'. Theseprograms can be used for initial conversion as well to update theshadowed file if the main file is edited by hand.pwconv will use the values of PASS_MIN_DAYS, PASS_MAX_DAYS, andPASS_WARN_AGE from /etc/login.defs when adding new entries to/etc/shadow.Likewise pwunconv and grpunconv are similar. Passwords in themain file are updated from the shadowed file. Entries which existin the main file but not in the shadowed file are left alone.Finally, the shadowed file is removed. Some password aginginformation is lost by pwunconv. It will convert what it can.",
        "name": "pwconv, pwunconv, grpconv, grpunconv - convert to and from shadowpasswords and groups",
        "section": 8
    },
    {
        "command": "quota_nld",
        "description": "quota_nld listens on netlink socket and processes received quotawarnings. By default quota_nld forwards warning messages to boththe system's DBUS (so that the desktop manager can display adialog) and the last-accessed terminal of the user to whom eachwarning is directed. Either of these destinations can be disabledwith the -D and -C options, respectively.In the case of the user's terminal, quota messages about fallingbelow the hard and soft limits are not sent unless the -b optionis specified. In the case of the DBUS, all quota messages aresent.Note, that you have to enable the kernel support for sendingquota messages over netlink (in Filesystems->Quota menu).",
        "name": "quota_nld - quota netlink message daemon",
        "section": 8
    },
    {
        "command": "quotacheck",
        "description": "quotacheck examines each filesystem, builds a table of currentdisk usage, and compares this table against that recorded in thedisk quota file for the filesystem (this step is omitted ifoption -c is specified). If any inconsistencies are detected,both the quota file and the current system copy of the incorrectquotas are updated (the latter only occurs if an activefilesystem is checked which is not advised).By default, onlyuser quotas are checked.quotacheck expects each filesystem tobe checked to have quota files named [a]quota.user and[a]quota.group located at the root of the associated filesystem.If a file is not present, quotacheck will create it.If the quota file is corrupted, quotacheck tries to save as muchdata as possible.Rescuing data may need user intervention. Withno additional options quotacheck will simply exit in such asituation. When in interactive mode (option -i) , the user isasked for advice. Advice can also be provided from command line(see option -n) , which is useful when quotacheck is runautomatically (ie. from script) and failure is unacceptable.quotacheck should be run each time the system boots and mountsnon-valid filesystems.This is most likely to happen after asystem crash.It is strongly recommended to run quotacheck with quotas turnedoff for the filesystem. Otherwise, possible damage or loss todata in the quota files can result.It is also unwise to runquotacheck on a live filesystem as actual usage may change duringthe scan.To prevent this, quotacheck tries to remount thefilesystem read-only before starting the scan.After the scan isdone it remounts the filesystem read-write. You can disable thiswith option -m.You can also make quotacheck ignore the failureto remount the filesystem read-only with option -M.",
        "name": "quotacheck - scan a filesystem for disk usage, create, check andrepair quota files",
        "section": 8
    },
    {
        "command": "quotaoff",
        "description": "quotaonquotaon announces to the system that disk quotas should beenabled on one or more filesystems. The filesystem quota filesmust be present in the root directory of the specified filesystemand be named either aquota.user (for version 2 user quota),quota.user (for version 1 user quota), aquota.group (for version2 group quota), or quota.group (for version 1 group quota).XFS filesystems are a special case - XFS considers quotainformation as filesystem metadata and uses journaling to providea higher level guarantee of consistency.There are twocomponents to the XFS disk quota system: accounting and limitenforcement.XFS filesystems require that quota accounting beturned on at mount time.It is possible to enable and disablelimit enforcement on an XFS filesystem after quota accounting isalready turned on.The default is to turn on both accounting andenforcement.The XFS quota implementation does not maintain quota informationin user-visible files, but rather stores this informationinternally.quotaoffquotaoff announces to the system that the specified filesystemsshould have any disk quotas turned off.",
        "name": "quotaon, quotaoff - turn filesystem quotas on and off",
        "section": 8
    },
    {
        "command": "quotaon",
        "description": "quotaonquotaon announces to the system that disk quotas should beenabled on one or more filesystems. The filesystem quota filesmust be present in the root directory of the specified filesystemand be named either aquota.user (for version 2 user quota),quota.user (for version 1 user quota), aquota.group (for version2 group quota), or quota.group (for version 1 group quota).XFS filesystems are a special case - XFS considers quotainformation as filesystem metadata and uses journaling to providea higher level guarantee of consistency.There are twocomponents to the XFS disk quota system: accounting and limitenforcement.XFS filesystems require that quota accounting beturned on at mount time.It is possible to enable and disablelimit enforcement on an XFS filesystem after quota accounting isalready turned on.The default is to turn on both accounting andenforcement.The XFS quota implementation does not maintain quota informationin user-visible files, but rather stores this informationinternally.quotaoffquotaoff announces to the system that the specified filesystemsshould have any disk quotas turned off.",
        "name": "quotaon, quotaoff - turn filesystem quotas on and off",
        "section": 8
    },
    {
        "command": "quotastats",
        "description": "quotastats queries the kernel for quota statistics.It displays:\u2022Supported kernel quota version\u2022Number of dquot lookups\u2022Number of dquot drops\u2022Number of dquot reads\u2022Number of dquot writes\u2022Number of quotafile syncs\u2022Number of dquot cache hits\u2022Number of allocated dquots\u2022Number of free dquots\u2022Number of in use dquot entries (user/group)",
        "name": "quotastats - Program to query quota statistics",
        "section": 8
    },
    {
        "command": "radattr.so",
        "description": "The radattr plugin for pppd causes all radius attributes returnedby the RADIUS server at authentication time to be stored in thefile /var/run/radattr.pppN where pppN is the name of the PPPinterface.The RADIUS attributes are stored one per line in theformat \"Attribute-Name Attribute-Value\".This format isconvenient for use in /etc/ppp/ip-up and /etc/ppp/ip-downscripts.Note that you must load the radius.so plugin before loading theradattr.so plugin; radattr.so depends on symbols defined inradius.so.",
        "name": "radattr.so - RADIUS utility plugin for pppd(8)",
        "section": 8
    },
    {
        "command": "radius.so",
        "description": "The RADIUS plugin for pppd permits pppd to perform PAP, CHAP, MS-CHAP and MS-CHAPv2 authentication against a RADIUS server insteadof the usual /etc/ppp/pap-secrets and /etc/ppp/chap-secretsfiles.The RADIUS plugin is built on a library called radiusclient whichhas its own configuration files (usually in /etc/radiusclient),consult those files for more information on configuring theRADIUS plugin",
        "name": "radius.so - RADIUS authentication plugin for pppd(8)",
        "section": 8
    },
    {
        "command": "raid6check",
        "description": "RAID6 devices in which one single component drive has errors canuse the double parity in order to find out which component drive.The \"raid6check\" tool checks, for each stripe, the double parityconsistency, reports mismatches and, if possible, which componentdrive has the mismatch.Since it works at stripe level, it canreport different drives with mismatches at different stripes.\"raid6check\" requires a non-degraded RAID6 MD device as firstparameter, a starting stripe (usually 0) and the number ofstripes to be checked.If this third parameter is also 0, itwill check the array up to the end.\"raid6check\" will start printing information about the RAID6,then for each stripe, it will report the parity rotation status.In case of parity mismatches, \"raid6check\" reports, if possible,which component drive could be responsible. Otherwise it reportsthat it is not possible to find the component drive.If the given MD device is not a RAID6, \"raid6check\" will, ofcourse, not continue.If the RAID6 MD device is degraded, \"raid6check\" will report anerror and it will not proceed further.No write operations are performed on the array or the components.Furthermore, the checked array can be online and in use duringthe operation of \"raid6check\".",
        "name": "raid6check - check MD RAID6 device for errors aka Linux SoftwareRAID",
        "section": 8
    },
    {
        "command": "rarp",
        "description": "Rarp manipulates the kernel's RARP table in various ways.Theprimary options are clearing an address mapping entry andmanually setting up one.For debugging purposes, the rarpprogram also allows a complete dump of the RARP table.",
        "name": "rarp - manipulate the system RARP table",
        "section": 8
    },
    {
        "command": "rc-local.service",
        "description": "systemd-rc-local-generator is a generator that checks whether/etc/rc.local exists and is executable, and if it is, pulls therc-local.service unit into the boot process. This unit isresponsible for running this script during late boot. The scriptis run after network.target, but in parallel with most otherregular system services.Note that rc-local.service runs with slightly different semanticsthan the original System V version, which was executed \"last\" inthe boot process, which is a concept that does not translate tosystemd.Also note that rc-local.service is ordered after network.target,which does not mean that the network is functional, seesystemd.special(7). If the script requires a configured networkconnection, it may be desirable to pull in and order it afternetwork-online.target with a drop-in:# /etc/systemd/system/rc-local.service.d/network.conf[Unit]Wants=network-online.targetAfter=network-online.targetSupport for /etc/rc.local is provided for compatibility withspecific System V systems only. However, it is stronglyrecommended to avoid making use of this script today, and insteadprovide proper unit files with appropriate dependencies for anyscripts to run during the boot process. Note that the path to thescript is set at compile time and varies between distributions.systemd-rc-local-generator implements systemd.generator(7).",
        "name": "systemd-rc-local-generator, rc-local.service - Compatibilitygenerator and service to start /etc/rc.local during boot",
        "section": 8
    },
    {
        "command": "rdma",
        "description": null,
        "name": "rdma - RDMA tool",
        "section": 8
    },
    {
        "command": "rdma-dev",
        "description": "rdma dev set - rename RDMA device or set network namespace or setRDMA device adaptive-moderationrdma dev show - display RDMA device attributesDEV - specifies the RDMA device to show.If this argument isomitted all devices are listed.",
        "name": "rdma-dev - RDMA device configuration",
        "section": 8
    },
    {
        "command": "rdma-link",
        "description": "rdma link show - display rdma link attributesDEV/PORT_INDEX - specifies the RDMA link to show.If thisargument is omitted all links are listed.rdma link add NAME type TYPE netdev NETDEV - add an rdma link for thespecified type to the network deviceNAME - specifies the new name of the rdma link to addTYPE - specifies which rdma type to use.Link types:rxe - Soft RoCE driversiw - Soft iWARP driverNETDEV - specifies the network device to which the link is boundrdma link delete NAME - delete an rdma linkNAME - specifies the name of the rdma link to delete",
        "name": "rdma-link - rdma link configuration",
        "section": 8
    },
    {
        "command": "rdma-resource",
        "description": "rdma resource show - display rdma resource tracking informationDEV/PORT_INDEX - specifies the RDMA link to show.If thisargument is omitted all links are listed.",
        "name": "rdma-resource - rdma resource configuration",
        "section": 8
    },
    {
        "command": "rdma-statistic",
        "description": "rdma statistic [object] show - Queries the specified RDMA device forRDMA and driver-specific statistics. Show the default hw countersif object is not specifiedDEV - specifies counters on this RDMA device to show.PORT_INDEX - specifies counters on this RDMA port to show.FILTER_NAME - specifies a filter to show only the resultsmatching it.rdma statistic <object> set - configure counter statistic auto-modefor a specific device/portIn auto mode all objects belong to one category are bindautomatically to a single counter set. The \"off\" is global forall auto modes together. Not applicable for MR's.rdma statistic <object> bind - manually bind an object (e.g., a qp)with a counterWhen bound the statistics of this object are available in thiscounter. Not applicable for MR's.rdma statistic <object> unbind - manually unbind an object (e.g., aqp) from the counter previously boundWhen unbound the statistics of this object are no longeravailable in this counter; And if object id is not specified thenall objects on this counter will be unbound. Not applicable forMR's.COUNTER-ID - specifies the id of the counter to be bound.Ifthis argument is omitted then a new counter will be allocated.rdma statistic mode - Display the enabled optional counters for eachlink.rdma statistic mode supported - Display the supported optionalcounters for each link.rdma statistic set - Enable a set of optional counters for a specificdevice/port.OPTIONAL-COUNTERS - specifies the name of the optional countersto enable. Optional counters that are not specified will bedisabled. Note that optional counters are driver-specific.rdma statistic unset - Disable all optional counters for a specificdevice/port.",
        "name": "rdma-statistic - RDMA statistic counter configuration",
        "section": 8
    },
    {
        "command": "rdma-system",
        "description": "rdma system set - set RDMA subsystem network namespace moderdma system show - display RDMA subsystem network namespace modeNEWMODE - specifies the RDMA subsystem mode. Either exclusive orshared.When user wants to assign dedicated RDMA device to aparticular network namespace, exclusive mode should be set beforecreating any network namespace. If there are active networknamespaces and if one or more RDMA devices exist, changing modefrom shared to exclusive returns error code EBUSY.When RDMA subsystem is in shared mode, RDMA device is accessiblein all network namespace. When RDMA device isolation amongmultiple network namespaces is not needed, shared mode can beused.It is preferred to not change the subsystem mode when there isactive RDMA traffic running, even though it is supported.",
        "name": "rdma-system - RDMA subsystem configuration",
        "section": 8
    },
    {
        "command": "readprofile",
        "description": "The readprofile command uses the /proc/profile information toprint ascii data on standard output. The output is organized inthree columns: the first is the number of clock ticks, the secondis the name of the C function in the kernel where those manyticks occurred, and the third is the normalized `load' of theprocedure, calculated as a ratio between the number of ticks andthe length of the procedure. The output is filled with blanks toease readability.",
        "name": "readprofile - read kernel profiling information",
        "section": 8
    },
    {
        "command": "reboot",
        "description": "poweroff, reboot, and halt may be used to power off, reboot, orhalt the machine. All three commands take the same options.",
        "name": "poweroff, reboot, halt - Power off, reboot, or halt the machine",
        "section": 8
    },
    {
        "command": "red",
        "description": "Random Early Detection is a classless qdisc which manages itsqueue size smartly. Regular queues simply drop packets from thetail when they are full, which may not be the optimal behaviour.RED also performs tail drop, but does so in a more gradual way.Once the queue hits a certain average length, packets enqueuedhave a configurable chance of being marked (which may meandropped). This chance increases linearly up to a point called themax average queue length, although the queue might get bigger.This has a host of benefits over simple taildrop, while not beingprocessor intensive. It prevents synchronous retransmits after aburst in traffic, which cause further retransmits, etc.The goal is to have a small queue size, which is good forinteractivity while not disturbing TCP/IP traffic with too manysudden drops after a burst of traffic.Depending on if ECN is configured, marking either means droppingor purely marking a packet as overlimit.",
        "name": "red - Random Early Detection",
        "section": 8
    },
    {
        "command": "repquota",
        "description": "repquota prints a summary of the disc usage and quotas for thespecified file systems.For each user the current number offiles and amount of space (in kilobytes) is printed, along withany quota limits set with edquota(8) or setquota(8).In thesecond column repquota prints two characters marking which limitsare exceeded. If user is over his space softlimit or reaches hisspace hardlimit in case softlimit is unset, the first characteris '+'. Otherwise the character printed is '-'. The secondcharacter denotes the state of inode usage analogously.repquota has to translate ids of all users/groups/projects tonames (unless option -n was specified) so it may take a while toprint all the information. To make translating as fast aspossible repquota tries to detect (by reading /etc/nsswitch.conf)whether entries are stored in standard plain text file or in adatabase and either translates chunks of 1024 names or each nameindividually. You can override this autodetection by -c or -Coptions.",
        "name": "repquota - summarize quotas for a filesystem",
        "section": 8
    },
    {
        "command": "request-key",
        "description": "This program is invoked by the kernel when the kernel is askedfor a key that it doesn't have immediately available. The kernelcreates a partially set up key and then calls out to this programto instantiate it. It is not intended to be called directly.However, for debugging purposes, it can be given some options onthe command line:-dTurn on debugging mode.In this mode, no attempts aremade to access any keys and, if a handler program isselected, it won't be executed; instead, this program willprint a message and exit 0.-DIn debugging mode, use the proposed key descriptionspecified with this rather than the sample(\"user;0;0;1f0000;debug:1234\") built into the program.-lUse configuration from the current directory.The programwill use request-key.d/* and request-key.conf from thecurrent directory rather than from /etc.-nDon't log to the system log.Ordinarily, error messagesand debugging messages will be copied to the system log -this will prevent that.-vTurn on debugging output.This may be specified multipletimes to produce increasing levels of verbosity.--versionPrint the program version and exit.",
        "name": "request-key - handle key instantiation callback requests from thekernel",
        "section": 8
    },
    {
        "command": "resize2fs",
        "description": "The resize2fs program will resize ext2, ext3, or ext4 filesystems.It can be used to enlarge or shrink an unmounted filesystem located on device.If the file system is mounted, it canbe used to expand the size of the mounted file system, assumingthe kernel and the file system supports on-line resizing.(Modern Linux 2.6 kernels will support on-line resize for filesystems mounted using ext3 and ext4; ext3 file systems willrequire the use of file systems with the resize_inode featureenabled.)The size parameter specifies the requested new size of the filesystem.If no units are specified, the units of the sizeparameter shall be the file system blocksize of the file system.Optionally, the size parameter may be suffixed by one of thefollowing units designators: 'K', 'M', 'G', 'T' (either upper-case or lower-case) or 's' for power-of-two kilobytes, megabytes,gigabytes, terabytes or 512 byte sectors respectively. The sizeof the file system may never be larger than the size of thepartition.If size parameter is not specified, it will defaultto the size of the partition.The resize2fs program does not manipulate the size of partitions.If you wish to enlarge a file system, you must make sure you canexpand the size of the underlying partition first.This can bedone using fdisk(8) by deleting the partition and recreating itwith a larger size or using lvextend(8), if you're using thelogical volume manager lvm(8).When recreating the partition,make sure you create it with the same starting disk cylinder asbefore!Otherwise, the resize operation will certainly not work,and you may lose your entire file system.After runningfdisk(8), run resize2fs to resize the ext2 file system to use allof the space in the newly enlarged partition.If you wish to shrink an ext2 partition, first use resize2fs toshrink the size of file system.Then you may use fdisk(8) toshrink the size of the partition.When shrinking the size of thepartition, make sure you do not make it smaller than the new sizeof the ext2 file system!The -b and -s options enable and disable the 64bit feature,respectively.The resize2fs program will, of course, take careof resizing the block group descriptors and moving other datablocks out of the way, as needed.It is not possible to resizethe file system concurrent with changing the 64bit status.",
        "name": "resize2fs - ext2/ext3/ext4 file system resizer",
        "section": 8
    },
    {
        "command": "resizecons",
        "description": "The resizecons command tries to change the videomode of theconsole.There are several aspects to this: (a) the kernel mustknow about it, (b) the hardware must know about it, (c) userprograms must know about it, (d) the console font may have to beadapted.(a) The kernel is told about the change using the ioctlVT_RESIZE.This causes the kernel to reallocate console screenmemory for all virtual consoles, and might fail if there is notenough memory.(In that case, try to disallocate some virtualconsoles first.)If this ioctl succeeds, but a later step fails(e.g., because you do not have root permissions), you may be leftwith a very messy screen.The most difficult part of this is (b), since it requiresdetailed knowledge of the video card hardware, and the setting ofnumerous registers. Only changing the number of rows is slightlyeasier, and resizecons will try to do that itself, when given the-lines option. (Probably, root permission will be required.)Thecommand resizecons COLSxROWS will execute restoretextmode -rCOLSxROWS (and hence requires that you have svgalib installed).Here COLSxROWS is a file that was created earlier byrestoretextmode -w COLSxROWS.Again, either root permissions arerequired, or restoretextmode has to be suid root.In order to deal with (c), resizecons does a `stty rows ROWS colsCOLS' for each active console (in the range tty0..tty15), andsends a SIGWINCH signal to selection if it finds the file/tmp/selection.pid.Finally, (d) is dealt with by executing a setfont command. Mostlikely, the wrong font is loaded, and you may want to do anothersetfont yourself afterwards.",
        "name": "resizecons - change kernel idea of the console size",
        "section": 8
    },
    {
        "command": "resizepart",
        "description": "resizepart tells the Linux kernel about the new size of thespecified partition. The command is a simple wrapper around the\"resize partition\" ioctl.This command doesn\u2019t manipulate partitions on a block device.",
        "name": "resizepart - tell the kernel about the new size of a partition",
        "section": 8
    },
    {
        "command": "restorecon",
        "description": "This manual page describes the restorecon program.This program is primarily used to set the security context(extended attributes) on one or more files.It can also be run at any other time to correct inconsistentlabels, to add support for newly-installed policy or, by usingthe -n option, to passively check whether the file contexts areall set as specified by the active policy (default behavior).If a file object does not have a context, restorecon will writethe default context to the file object's extended attributes. Ifa file object has a context, restorecon will only modify the typeportion of the security context.The -F option will force areplacement of the entire context.If a file is labeled with customizable SELinux type (for list ofcustomizable types see/etc/selinux/{SELINUXTYPE}/contexts/customizable_types),restorecon won't reset the label unless the -F option is used.It is the same executable as setfiles but operates in a slightlydifferent manner depending on its argv[0].",
        "name": "restorecon - restore file(s) default SELinux security contexts.",
        "section": 8
    },
    {
        "command": "restorecon_xattr",
        "description": "restorecon_xattr will display the SHA1 digests added to extendedattributes security.sehash or delete the attribute completely.These attributes are set by restorecon(8) or setfiles(8) tospecified directories when relabeling recursively.restorecon_xattr is useful for managing the extended attributeentries particularly when users forget what directories they ranrestorecon(8) or setfiles(8) from.RAMFS and TMPFS filesystems do not support the security.sehashextended attribute and are automatically excluded from searches.By default restorecon_xattr will display the SHA1 digests with\"Match\" appended if they match the default specfile set or thespecfile set used with the -f option. Non-matching SHA1 digestswill be displayed with \"No Match\" appended.This feature can bedisabled by the -n option.",
        "name": "restorecon_xattr - manage security.sehash extended attributeentries added by setfiles(8) or restorecon(8).",
        "section": 8
    },
    {
        "command": "restorecond",
        "description": "This manual page describes the restorecond program.This daemon uses inotify to watch files listed in the/etc/selinux/restorecond.conf, when they are created, this daemonwill make sure they have the correct file context associated withthe policy.",
        "name": "restorecond - daemon that watches for file creation and then setsthe default SELinux file context",
        "section": 8
    },
    {
        "command": "rfkill",
        "description": "rfkill lists, enabling and disabling wireless devices.The command \"list\" output format is deprecated and maintained forbackward compatibility only. The new output format is the defaultwhen no command is specified or when the option --output is used.The default output is subject to change. So whenever possible,you should avoid using default outputs in your scripts. Alwaysexplicitly define expected columns by using the --output optiontogether with a columns list in environments where a stableoutput is required.",
        "name": "rfkill - tool for enabling and disabling wireless devices",
        "section": 8
    },
    {
        "command": "rmmod",
        "description": "rmmod is a trivial program to remove a module (when moduleunloading support is provided) from the kernel. Most users willwant to use modprobe(8) with the -r option instead since itremoves unused dependent modules as well.",
        "name": "rmmod - Simple program to remove a module from the Linux Kernel",
        "section": 8
    },
    {
        "command": "route",
        "description": "Route manipulates the kernel's IP routing tables.Its primaryuse is to set up static routes to specific hosts or networks viaan interface after it has been configured with the ifconfig(8)program.When the add or del options are used, route modifies the routingtables.Without these options, route displays the currentcontents of the routing tables.",
        "name": "route - show / manipulate the IP routing table",
        "section": 8
    },
    {
        "command": "routel",
        "description": "The routel script will list routes in a format that some mightconsider easier to interpret then the ip route list equivalent.",
        "name": "routel - list routes with pretty output format",
        "section": 8
    },
    {
        "command": "rpc.gssd",
        "description": "To establish GSS security contexts using these credential files,the Linux kernel RPC client depends on a userspace daemon calledrpc.gssd.The rpc.gssd daemon uses the rpc_pipefs filesystem tocommunicate with the kernel.User CredentialsWhen a user authenticates using a command such as kinit(1), theresulting credential is stored in a file with a well-known nameconstructed using the user's UID.To interact with an NFS server on behalf of a particularKerberos-authenticated user, the Linux kernel RPC client requeststhat rpc.gssd initialize a security context with the credentialin that user's credential file.Typically, credential files are placed in /tmp.However,rpc.gssd can search for credential files in more than onedirectory.See the description of the -d option for details.Machine Credentialsrpc.gssd searches the default keytab, /etc/krb5.keytab, in thefollowing order for a principal and password to use whenestablishing the machine credential.For the search, rpc.gssdreplaces <hostname> and <REALM> with the local system's hostnameand Kerberos realm.<HOSTNAME>$@<REALM>root/<hostname>@<REALM>nfs/<hostname>@<REALM>host/<hostname>@<REALM>root/<anyname>@<REALM>nfs/<anyname>@<REALM>host/<anyname>@<REALM>rpc.gssd selects one of the <anyname> entries if it does not finda service principal matching the local hostname, e.g. if DHCPassigns the local hostname dynamically.The <anyname> facilityenables the use of the same keytab on multiple systems.However,using the same service principal to establish a machinecredential on multiple hosts can create unwanted securityexposures and is therefore not recommended.Note that <HOSTNAME>$@<REALM> is a user principal that enablesKerberized NFS when the local system is joined to an ActiveDirectory domain using Samba.The keytab provides the passwordfor this principal.You can specify a different keytab by using the -k option if/etc/krb5.keytab does not exist or does not provide one of theseprincipals.Credentials for UID 0UID 0 is a special case.By default rpc.gssd uses the system'smachine credentials for UID 0 accesses that require GSSauthentication.This limits the privileges of the root user whenaccessing network resources that require authentication.Specify the -n option when starting rpc.gssd if you'd like toforce the root user to obtain a user credential rather than usethe local system's machine credential.When -n is specified, the kernel continues to request a GSScontext established with a machine credential for NFSv4operations, such as SETCLIENTID or RENEW, that manage state.Ifrpc.gssd cannot obtain a machine credential (say, the localsystem has no keytab), NFSv4 operations that require machinecredentials will fail.Encryption typesA realm administrator can choose to add keys encoded in a numberof different encryption types to the local system's keytab.Forinstance, a host/ principal might have keys for the aes256-cts-hmac-sha1-96, aes128-cts-hmac-sha1-96, des3-cbc-sha1, andarcfour-hmac encryption types.This permits rpc.gssd to choosean appropriate encryption type that the target NFS serversupports.These encryption types are stronger than legacy single-DESencryption types.To interoperate in environments where serverssupport only weak encryption types, you can restrict your clientto use only single-DES encryption types by specifying the -loption when starting rpc.gssd.",
        "name": "rpc.gssd - RPCSEC_GSS daemon",
        "section": 8
    },
    {
        "command": "rpc.idmapd",
        "description": "rpc.idmapd is the NFSv4 ID <-> name mapping daemon.It providesfunctionality to the NFSv4 kernel client and server, to which itcommunicates via upcalls, by translating user and group IDs tonames, and vice versa.The system derives the user part of the string by performing apassword or group lookup.The lookup mechanism is configured in/etc/idmapd.confBy default, the domain part of the string is the system's DNSdomain name.It can also be specified in /etc/idmapd.conf if thesystem is multi-homed, or if the system's DNS domain name does notmatch the name of the system's Kerberos realm.When the domain is not specified in /etc/idmapd.conf the local DNSserver will be queried for the _nfsv4idmapdomain text record. Ifthe record exists that will be used as the domain. When the recorddoes not exist, the domain part of the DNS domain will used.Note that on more recent kernels only the NFSv4 server usesrpc.idmapd.The NFSv4 client instead uses nfsidmap(8), and onlyfalls back to rpc.idmapd if there was a problem running thenfsidmap(8) program.The options are as follows:-hDisplay usage message.-vIncreases the verbosity level (can be specifiedmultiple times).-fRuns rpc.idmapd in the foreground and prints alloutput to the terminal.-p pathSpecifies the location of the RPC pipefs to be path.The default value is \"/var/lib/nfs/rpc_pipefs\".-c pathUse configuration file path.This option isdeprecated.-CClient-only: perform no idmapping for any NFS server,even if one is detected.-SServer-only: perform no idmapping for any NFS client,even if one is detected.",
        "name": "rpc.idmapd \u2014 NFSv4 ID <-> Name Mapper",
        "section": 8
    },
    {
        "command": "rpc.mountd",
        "description": "The rpc.mountd daemon implements the server side of the NFS MOUNTprotocol, an NFS side protocol used by NFS version 2 [RFC1094]and NFS version 3 [RFC1813].It also responds to requests fromthe Linux kernel to authenticate clients and provides details ofaccess permissions.The NFS server (nfsd) maintains a cache of authentication andauthorization information which is used to identify the source ofeach request, and then what access permissions that source has toany local filesystem.When required information is not found inthe cache, the server sends a request to mountd to fill in themissing information.Mountd uses a table of information storedin /var/lib/nfs/etab and maintained by exportfs(8), possiblybased on the contents of exports(5), to respond to each request.Mounting exported NFS File SystemsThe NFS MOUNT protocol has several procedures.The mostimportant of these are MNT (mount an export) and UMNT (unmount anexport).A MNT request has two arguments: an explicit argument thatcontains the pathname of the root directory of the export to bemounted, and an implicit argument that is the sender's IPaddress.When receiving a MNT request from an NFS client, rpc.mountdchecks both the pathname and the sender's IP address against itsexport table.If the sender is permitted to access the requestedexport, rpc.mountd returns an NFS file handle for the export'sroot directory to the client.The client can then use the rootfile handle and NFS LOOKUP requests to navigate the directorystructure of the export.The rmtab FileThe rpc.mountd daemon registers every successful MNT request byadding an entry to the /var/lib/nfs/rmtab file.When receivng aUMNT request from an NFS client, rpc.mountd simply removes thematching entry from /var/lib/nfs/rmtab, as long as the accesscontrol list for that export allows that sender to access theexport.Clients can discover the list of file systems an NFS server iscurrently exporting, or the list of other clients that havemounted its exports, by using the showmount(8) command.showmount(8) uses other procedures in the NFS MOUNT protocol toreport information about the server's exported file systems.Note, however, that there is little to guarantee that thecontents of /var/lib/nfs/rmtab are accurate.A client maycontinue accessing an export even after invoking UMNT.If theclient reboots without sending a UMNT request, stale entriesremain for that client in /var/lib/nfs/rmtab.Mounting File Systems with NFSv4Version 4 (and later) of NFS does not use a separate NFS MOUNTprotocol.Instead mounting is performed using regular NFSrequests handled by the NFS server in the Linux kernel (nfsd).Consequently /var/lib/nfs/rmtab is not updated to reflect anyNFSv4 activity.",
        "name": "rpc.mountd - NFS mount daemon",
        "section": 8
    },
    {
        "command": "rpc.nfsd",
        "description": "The rpc.nfsd program implements the user level part of the NFSservice. The main functionality is handled by the nfsd kernelmodule. The user space program merely specifies what sort ofsockets the kernel service should listen on, what NFS versions itshould support, and how many kernel threads it should use.The rpc.mountd server provides an ancillary service needed tosatisfy mount requests by NFS clients.",
        "name": "rpc.nfsd - NFS server process",
        "section": 8
    },
    {
        "command": "rpc.rquotad",
        "description": "rpc.rquotad is an rpc(3) server which returns quotas for a userof a local filesystem which is mounted by a remote machine overthe NFS.It also allows setting of quotas on NFS mountedfilesystem (if configured during compilation and allowed by acommand line option -S).The results are used by quota(1) todisplay user quotas for remote filesystems and by edquota(8) toset quotas on remote filesystems.rquotad daemon uses tcp-wrappers library (under service name rquotad) which allows you tospecify hosts allowed/disallowed to use the daemon (seehosts.allow(5) manpage for more information). The rquotad daemonis normally started at boot time from the system startup scripts.",
        "name": "rpc.rquotad - remote quota server",
        "section": 8
    },
    {
        "command": "rpc.statd",
        "description": "File locks are not part of persistent file system state.Lockstate is thus lost when a host reboots.Network file systems must also detect when lock state is lostbecause a remote host has rebooted.After an NFS client reboots,an NFS server must release all file locks held by applicationsthat were running on that client.After a server reboots, aclient must remind the server of file locks held by applicationsrunning on that client.For NFS version 2 [RFC1094] and NFS version 3 [RFC1813], theNetwork Status Monitor protocol (or NSM for short) is used tonotify NFS peers of reboots.On Linux, two separate user-spacecomponents constitute the NSM service:rpc.statdA daemon that listens for reboot notifications from otherhosts, and manages the list of hosts to be notified whenthe local system rebootssm-notifyA helper program that notifies NFS peers after the localsystem rebootsThe local NFS lock manager alerts its local rpc.statd of eachremote peer that should be monitored.When the local systemreboots, the sm-notify command notifies the NSM service onmonitored peers of the reboot.When a remote reboots, that peernotifies the local rpc.statd, which in turn passes the rebootnotification back to the local NFS lock manager.",
        "name": "rpc.statd - NSM service daemon",
        "section": 8
    },
    {
        "command": "rpc.svcgssd",
        "description": "The rpcsec_gss protocol gives a means of using the gss-apigeneric security api to provide security for protocols using rpc(in particular, nfs).Before exchanging any rpc requests usingrpcsec_gss, the rpc client must first establish a securitycontext with the rpc server.The linux kernel's implementationof rpcsec_gss depends on the userspace daemon rpc.svcgssd tohandle context establishment on the rpc server.The daemon usesfiles in the proc filesystem to communicate with the kernel.",
        "name": "rpc.svcgssd - server-side rpcsec_gss daemon",
        "section": 8
    },
    {
        "command": "rpcbind",
        "description": "The rpcbind utility is a server that converts RPC program numbersinto universal addresses.It must be running on the host to beable to make RPC calls on a server on that machine.When an RPC service is started, it tells rpcbind the address atwhich it is listening, and the RPC program numbers it is preparedto serve.When a client wishes to make an RPC call to a givenprogram number, it first contacts rpcbind on the server machine todetermine the address where RPC requests should be sent.The rpcbind utility should be started before any other RPC service.Normally, standard RPC servers are started by port monitors, sorpcbind must be started before port monitors are invoked.When rpcbind is started, it checks that certain name-to-addresstranslation-calls function correctly.If they fail, the networkconfiguration databases may be corrupt.Since RPC services cannotfunction correctly in this situation, rpcbind reports the conditionand terminates.The rpcbind utility can only be started by the super-user.",
        "name": "rpcbind \u2014 universal addresses to RPC program number mapper",
        "section": 8
    },
    {
        "command": "rpcctl",
        "description": "The rpcctl command displays information collected in the SunRPCsysfs files about the system's SunRPC objects.rpcctl client - Commands operating on RPC clientsshow [ CLIENT ] (default)Show detailed information about the RPC clients on thissystem.If CLIENT was provided, then only showinformation about a single RPC client.rpcctl switch - Commands operating on groups of transportsset SWITCH dstaddr NEWADDRChange the destination address of all transports in theSWITCH to NEWADDR.NEWADDR can be an IP address, DNSname, or anything else resolvable by gethostbyname(3).show [ SWITCH ] (default)Show detailed information about groups of transports onthis system.If SWITCH was provided, then only showinformation about a single transport group.rpcctl xprt - Commands operating on individual transportsremove XPRTRemoves the specified XPRT from the system.Note that\"main\" transports cannot be removed.set XPRT dstaddr NEWADDRChange the destination address of the specified XPRTtoNEWADDR.NEWADDR can be an IP address, DNS name, oranything else resolvable by gethostbyname(3).set XPRT offlineSets the specified XPRT's state to offline.set XPRT onlineSets the specified XPRT's state to online.show [ XPRT ] (default)Show detailed information about this system's transports.If XPRT was provided, then only show information about asingle transport.",
        "name": "rpcctl - Displays SunRPC connection information",
        "section": 8
    },
    {
        "command": "rpcdebug",
        "description": "The rpcdebug command allows an administrator to set and clear theLinux kernel's NFS client and server debug flags.Setting theseflags causes the kernel to emit messages to the system log inresponse to NFS activity; this is typically useful when debuggingNFS problems.The first form in the synopsis can be used to list all availabledebug flags.The second form shows the currently set debug flagsfor the given module.The third form sets one or more flags, andthe fourth form clears one or more flags.The value all may be used to set or clear all the flags for thegiven module.",
        "name": "rpcdebug - set and clear NFS and RPC kernel debug flags",
        "section": 8
    },
    {
        "command": "rpcinfo",
        "description": "rpcinfo makes an RPC call to an RPC server and reports what itfinds.In the first synopsis, rpcinfo lists all the registered RPCservices with rpcbind on host.If host is not specified, the localhost is the default.If -s is used, the information is displayedin a concise format.In the second synopsis, rpcinfo lists all the RPC servicesregistered with rpcbind, version 2.Also note that the format ofthe information is different in the first and the second synopsis.This is because the second synopsis is an older protocol used tocollect the information displayed (version 2 of the rpcbindprotocol).The third synopsis makes an RPC call to procedure 0 of prognum andversnum on the specified host and reports whether a response wasreceived.transport is the transport which has to be used forcontacting the given service.The remote address of the service isobtained by making a call to the remote rpcbind.The prognum argument is a number that represents an RPC programnumber.If a versnum is specified, rpcinfo attempts to call thatversion of the specified prognum.Otherwise, rpcinfo attempts tofind all the registered version numbers for the specified prognumby calling version 0, which is presumed not to exist; if it doesexist, rpcinfo attempts to obtain this information by calling anextremely high version number instead, and attempts to call eachregistered version.Note: the version number is required for -band -d options.",
        "name": "rpcinfo \u2014 report RPC information",
        "section": 8
    },
    {
        "command": "rpm",
        "description": "rpm is a powerful Package Manager, which can be used to build,install, query, verify, update, and erase individual softwarepackages.A package consists of an archive of files and meta-data used to install and erase the archive files.The meta-dataincludes helper scripts, file attributes, and descriptiveinformation about the package.Packages come in two varieties:binary packages, used to encapsulate software to be installed,and source packages, containing the source code and recipenecessary to produce binary packages.One of the following basic modes must be selected: Query, Verify,Install/Upgrade/Freshen/Reinstall, Uninstall, Set Owners/Groups,Show Querytags, and Show Configuration.GENERAL OPTIONSThese options can be used in all the different modes.-?, --helpPrint a longer usage message than normal.--versionPrint a single line containing the version number of rpmbeing used.--quietPrint as little as possible - normally only error messageswill be displayed.-v, --verbosePrint verbose information - normally routine progressmessages will be displayed.-vvPrint lots of ugly debugging information.--rcfile FILELISTReplace the list of configuration files to be read.Eachof the files in the colon separated FILELIST is readsequentially by rpm for configuration information.Onlythe first file in the list must exist, and tildes will beexpanded to the value of $HOME.The default FILELIST is/usr/lib/rpm/rpmrc:/usr/lib/rpm/redhat/rpmrc:/etc/rpmrc:~/.rpmrc.--load FILELoad an individual macro file.--macros FILELISTReplace the list of macro files to be loaded.Each of thefiles in the colon separated FILELIST is read sequentiallyby rpm for macro definitions.Only the first file in thelist must exist, and tildes will be expanded to the valueof $HOME.The default FILELIST is/usr/lib/rpm/macros:/usr/lib/rpm/macros.d/macros.*:/usr/lib/rpm/platform/%{_target}/macros:/usr/lib/rpm/fileattrs/*.attr:/usr/lib/rpm/redhat/macros:/etc/rpm/macros.*:/etc/rpm/macros:/etc/rpm/%{_target}/macros:~/.rpmmacros--pipe CMDPipes the output of rpm to the command CMD.--dbpath DIRECTORYUse the database in DIRECTORY rather than the default path/var/lib/rpm--root DIRECTORYUse the file system tree rooted at DIRECTORY for alloperations.Note that this means the database withinDIRECTORY will be used for dependency checks and anyscriptlet(s) (e.g. %post if installing, or %prep ifbuilding, a package) will be run after a chroot(2) toDIRECTORY.Note that rpm assumes the environment inside the root isset up by the caller, such as any mounts needed for theoperation inside the root directory.-D, --define='MACRO EXPR'Defines MACRO with value EXPR.--undefine='MACRO'Undefines MACRO.-E, --eval='EXPR'Prints macro expansion of EXPR.More - less often needed - options can be found on the rpm-misc(8) man page.INSTALL AND UPGRADE OPTIONSIn these options, PACKAGE_FILE can be either rpm binary file orASCII package manifest (see PACKAGE SELECTION OPTIONS), and maybe specified as an ftp or http URL, in which case the packagewill be downloaded before being installed.See FTP/HTTP OPTIONSfor information on rpm's ftp and http client support.The general form of an rpm install command isrpm {-i|--install} [install-options] PACKAGE_FILE ...This installs a new package.The general form of an rpm upgrade command isrpm {-U|--upgrade} [install-options] PACKAGE_FILE ...This upgrades or installs the package currently installed to anewer version.This is the same as install, except all otherversion(s) of the package are removed after the new package isinstalled.rpm {-F|--freshen} [install-options] PACKAGE_FILE ...This will upgrade packages, but only ones for which an earlierversion is installed.The general form of an rpm reinstall command isrpm {--reinstall} [install-options] PACKAGE_FILE ...This reinstalls a previously installed package.--allfilesInstalls or upgrades all the missingok files in thepackage, regardless if they exist.--badrelocUsed with --relocate, permit relocations on all filepaths, not just those OLDPATH's included in the binarypackage relocation hint(s).--excludepath OLDPATHDon't install files whose name begins with OLDPATH.--excludeartifactsDon't install any files which are marked as artifacts,such as build-id links.--excludedocsDon't install any files which are marked as documentation(which includes man pages and texinfo documents).--forceSame as using --replacepkgs, --replacefiles, and--oldpackage.-h, --hashPrint 50 hash marks as the package archive is unpacked.Use with -v|--verbose for a nicer display.--ignoresizeDon't check mount file systems for sufficient disk spacebefore installing this package.--ignorearchAllow installation or upgrading even if the architecturesof the binary package and host don't match.--ignoreosAllow installation or upgrading even if the operatingsystems of the binary package and host don't match.--includedocsInstall documentation files.This is the defaultbehavior.--justdbUpdate only the database, not the filesystem.--nodb Update only the filesystem, not the database.--nodigestDon't verify package or header digests when reading.--nomanifestDon't process non-package files as manifests.--nosignatureDon't verify package or header signatures when reading.--nodepsDon't do a dependency check before installing or upgradinga package.--nocapsDon't set file capabilities.--noorderDon't reorder the packages for an install.The list ofpackages would normally be reordered to satisfydependencies.--noverifyDon't perform verify package files prior to installation.--nopluginsDo not load and execute plugins.--noscripts, --nopre, --nopost, --nopreun, --nopostun,--nopretrans, --noposttrans, --nopreuntrans, --nopostuntransDon't execute the scriptlet of the same name.The--noscripts option is equivalent to--nopre --nopost --nopreun --nopostun --nopretrans --noposttrans--nopreuntrans --nopostuntransand turns off the execution of the corresponding %pre, %post,%preun, %postun %pretrans, %posttrans, %preuntrans and%postuntrans scriptlet(s).--notriggers, --notriggerin, --notriggerun, --notriggerprein,--notriggerpostunDon't execute any trigger scriptlet of the named type.The --notriggers option is equivalent to--notriggerprein --notriggerin --notriggerun --notriggerpostunand turns off execution of the corresponding %triggerprein,%triggerin, %triggerun, and %triggerpostun scriptlet(s).--nosysusersDon\u2019t create sysusers from packages--oldpackageAllow an upgrade to replace a newer package with an olderone.--percentPrint percentages as files are unpacked from the packagearchive.This is intended to make rpm easy to run fromother tools.--prefix NEWPATHFor relocatable binary packages, translate all file pathsthat start with the installation prefix in the packagerelocation hint(s) to NEWPATH.--relocate OLDPATH=NEWPATHFor relocatable binary packages, translate all file pathsthat start with OLDPATH in the package relocation hint(s)to NEWPATH.This option can be used repeatedly if severalOLDPATH's in the package are to be relocated.--replacefilesInstall the packages even if they replace files fromother, already installed, packages.--replacepkgsInstall the packages even if some of them are alreadyinstalled on this system.--test Do not install the package, simply check for and reportpotential conflicts.ERASE OPTIONSThe general form of an rpm erase command isrpm {-e|--erase} [--allmatches] [--justdb] [--nodeps][--noscripts] [--notriggers] [--test] PACKAGE_NAME ...The following options may also be used:--allmatchesRemove all versions of the package which matchPACKAGE_NAME.Normally an error is issued if PACKAGE_NAMEmatches multiple packages.--justdbUpdate only the database, not the filesystem.--nodepsDon't check dependencies before uninstalling the packages.--noscripts, --nopreun, --nopostunDon't execute the scriptlet of the same name.The--noscripts option during package erase is equivalent to--nopreun --nopostunand turns off the execution of the corresponding %preun, and%postun scriptlet(s).--notriggers, --notriggerun, --notriggerpostunDon't execute any trigger scriptlet of the named type.The --notriggers option is equivalent to--notriggerun --notriggerpostunand turns off execution of the corresponding %triggerun, and%triggerpostun scriptlet(s).--test Don't really uninstall anything, just go through themotions.Useful in conjunction with the -vv option fordebugging.QUERY OPTIONSThe general form of an rpm query command isrpm {-q|--query} [select-options] [query-options]You may specify the format that package information should beprinted in.To do this, you use the--qf|--queryformat QUERYFMToption, followed by the QUERYFMT format string.Query formatsare modified versions of the standard printf(3) formatting.Theformat is made up of static strings (which may include standard Ccharacter escapes for newlines, tabs, and other specialcharacters (not including \\0)) and printf(3) type formatters.Asrpm already knows the type to print, the type specifier must beomitted however, and replaced by the name of the header tag to beprinted, enclosed by {} characters.Tag names are caseinsensitive, and the leading RPMTAG_ portion of the tag name maybe omitted as well.Alternate output formats may be requested by following the tagwith :typetag.Currently, the following types are supported::armor Wrap a public key in ASCII armor.:arraysizeDisplay number of elements in array tags.:base64Encode binary data using base64.:dateUse strftime(3) \"%c\" format.:dayUse strftime(3) \"%a %b %d %Y\" format.:depflagsFormat dependency comparison operator.:deptypeFormat dependency type.:expandPerform macro expansion.:fflagsFormat file flags.:fstateFormat file state.:fstatusFormat file verify status.:hexFormat in hexadecimal.:octal Format in octal.:humaniecHuman readable number (in IEC 80000).The suffix K =1024, M = 1048576, ...:humansiHuman readable number (in SI).The suffix K = 1000, M =1000000, ...:perms Format file permissions.:pgpsigDisplay signature fingerprint and time.:shescapeEscape single quotes for use in a script.:stringDisplay string format.(default):tagnameDisplay tag name.:tagnumDisplay tag number.:triggertypeDisplay trigger suffix.:vflagsFile verification flags.:xmlWrap data in simple xml markup.For example, to print only the names of the packages queried, youcould use %{NAME} as the format string.To print the packagesname and distribution information in two columns, you could use%-30{NAME}%{DISTRIBUTION}.rpm will print a list of all of thetags it knows about when it is invoked with the --querytagsargument.There are three subsets of options for querying: packageselection, file selection and information selection.PACKAGE SELECTION OPTIONS:PACKAGE_NAMEQuery installed package named PACKAGE_NAME.To specifythe package more precisely the package name may befollowed by the version or version and release bothseparated by a dash or an architecture name separated by adot.See the output of rpm -qa or rpm -qp PACKAGE_FILE asan example.-a, --all [SELECTOR]Query all installed packages.An optional SELECTOR in the form of tag=pattern can be providedto narrow the selection, for example name=\"b*\" to query packageswhose name starts with \"b\".--dupesList duplicated packages.-f, --file FILEQuery package owning installed FILE.--filecapsList file names with POSIX1.e capabilities.--fileclassList file names with their classes (libmagicclassification).--filecolorList file names with their colors (0 for noarch, 1 for32bit, 2 for 64 bit).--fileprovideList file names with their provides.--filerequireList file names with their requires.-g, --group GROUPQuery packages with the group of GROUP.--hdrid SHA1Query package that contains a given header identifier,i.e. the SHA1 digest of the immutable header region.-p, --package PACKAGE_FILEQuery an (uninstalled) package PACKAGE_FILE.ThePACKAGE_FILE may be specified as an ftp or http style URL,in which case the package header will be downloaded andqueried.See FTP/HTTP OPTIONS for information on rpm'sftp and http client support.The PACKAGE_FILEargument(s), if not a binary package, will be interpretedas an ASCII package manifest unless --nomanifest option isused.In manifests, comments are permitted, starting witha '#', and each line of a package manifest file mayinclude white space separated glob expressions, includingURL's, that will be expanded to paths that are substitutedin place of the package manifest as additionalPACKAGE_FILE arguments to the query.--path PATHQuery package(s) owning PATH, whether the file isinstalled or not.Multiple packages may own a PATH, butthe file is only owned by the package installed last.--pkgid MD5Query package that contains a given package identifier,i.e. the MD5 digest of the combined header and payloadcontents.--querybynumber HDRNUMQuery the HDRNUMth database entry directly; this is usefulonly for debugging.--specfile SPECFILEParse and query SPECFILE as if it were a package.Although not all the information (e.g. file lists) isavailable, this type of query permits rpm to be used toextract information from spec files without having towrite a specfile parser.--tid TIDQuery package(s) that have a given TID transactionidentifier.A unix time stamp is currently used as atransaction identifier.All package(s) installed orerased within a single transaction have a commonidentifier.--triggeredby PACKAGE_NAMEQuery packages that are triggered by package(s)PACKAGE_NAME.--whatobsoletes CAPABILITYQuery all packages that obsolete CAPABILITY for properfunctioning.--whatprovides CAPABILITYQuery all packages that provide the CAPABILITY capability.--whatrequires CAPABILITYQuery all packages that require CAPABILITY for properfunctioning.--whatconflicts CAPABILITYQuery all packages that conflict with CAPABILITY.--whatrecommends CAPABILITYQuery all packages that recommend CAPABILITY.--whatsuggests CAPABILITYQuery all packages that suggest CAPABILITY.--whatsupplements CAPABILITYQuery all packages that supplement CAPABILITY.--whatenhances CAPABILITYQuery all packages that enhance CAPABILITY.PACKAGE QUERY OPTIONS:--changelogDisplay change information for the package.--changesDisplay change information for the package with full timestamps.--conflictsList capabilities this package conflicts with.--dump Dump file information as follows (implies -l):path size mtime digest mode owner group isconfig isdoc rdev symlink--enhancesList capabilities enhanced by package(s)--filesbypkgList all the files in each selected package.--filetriggersList filetrigger scriptlets from package(s).-i, --infoDisplay package information, including name, version, anddescription.This uses the --queryformat if one wasspecified.--last Orders the package listing by install time such that thelatest packages are at the top.-l, --listList files in package.--obsoletesList packages this package obsoletes.--providesList capabilities this package provides.--recommendsList capabilities recommended by package(s)-R, --requiresList capabilities on which this package depends.--suggestsList capabilities suggested by package(s)--supplementsList capabilities supplemented by package(s)--scriptsList the package specific scriptlet(s) that are used aspart of the installation and uninstallation processes.-s, --stateDisplay the states of files in the package (implies -l).The state of each file is one of normal, not installed, orreplaced.--triggers, --triggerscriptsDisplay the trigger scripts, if any, which are containedin the package.--xml Format package headers as XML.FILE SELECTION OPTIONS:-A, --artifactfilesOnly include artifact files (implies -l).-c, --configfilesOnly include configuration files (implies -l).-d, --docfilesOnly include documentation files (implies -l).-L, --licensefilesOnly include license files (implies -l).--noartifactExclude artifact files.--noconfigExclude config files.--noghostExclude ghost files.VERIFY OPTIONSThe general form of an rpm verify command isrpm {-V|--verify} [select-options] [verify-options]Verifying a package compares information about the installedfiles in the package with information about the files taken fromthe package metadata stored in the rpm database.Among otherthings, verifying compares the size, digest, permissions, type,owner and group of each file.Any discrepancies are displayed.Files that were not installed from the package, for example,documentation files excluded on installation using the\"--excludedocs\" option, will be silently ignored.The package and file selection options are the same as forpackage querying (including package manifest files as arguments).Other options unique to verify mode are:--nodepsDon't verify dependencies of packages.--nodigestDon't verify package or header digests when reading.--nofilesDon't verify any attributes of package files.--noscriptsDon't execute the %verifyscript scriptlet (if any).--nosignatureDon't verify package or header signatures when reading.--nolinkto--nofiledigest (formerly --nomd5)--nosize--nouser--nogroup--nomtime--nomode--nordevDon't verify the corresponding file attribute.--nocapsDon't verify file capabilities.The format of the output is a string of 9 characters, a possibleattribute marker:c %config configuration file.d %doc documentation file.g %ghost file (i.e. the file contents are not included in the package payload).l %license license file.r %readme readme file.from the package header, followed by the file name.Each of the9 characters denotes the result of a comparison of attribute(s)of the file to the value of those attribute(s) recorded in thedatabase.A single \".\" (period) means the test passed, while asingle \"?\" (question mark) indicates the test could not beperformed (e.g. file permissions prevent reading).Otherwise,the (mnemonically emBoldened) character denotes failure of thecorresponding --verify test:S file Size differsM Mode differs (includes permissions and file type)5 digest (formerly MD5 sum) differsD Device major/minor number mismatchL readLink(2) path mismatchU User ownership differsG Group ownership differsT mTime differsP caPabilities differMISCELLANEOUS COMMANDSrpm --showrcshows the values rpm will use for all of the options arecurrently set in rpmrc and macros configuration file(s).rpm --setperms | --setugids | --setcaps PACKAGE_NAMEobsolete aliases for --restorerpm --restore [select-options]The option restores file metadata such as timestamp,owner, group, permissions and capabilities of files inpackages.FTP/HTTP OPTIONSrpm can act as an FTP and/or HTTP client so that packages can bequeried or installed from the internet.Package files forinstall, upgrade, and query operations may be specified as an ftpor http style URL:http://HOST[:PORT]/path/to/package.rpmftp://[USER:PASSWORD]@HOST[:PORT]/path/to/package.rpmIf both the user and password are omitted, anonymous ftp is used.rpm allows the following options to be used with ftp URLs:rpm allows the following options to be used withhttp and ftp URLs:--httpproxy HOSTThe host HOST will be used as a proxy server for all httpand ftp transfers.This option may also be specified byconfiguring the macro %_httpproxy.--httpport PORTThe TCP PORT number to use for the http connection on theproxy http server instead of the default port.Thisoption may also be specified by configuring the macro%_httpport.",
        "name": "rpm - RPM Package Manager",
        "section": 8
    },
    {
        "command": "rpm-misc",
        "description": null,
        "name": "rpm - lesser need options for rpm(8)",
        "section": 8
    },
    {
        "command": "rpm-plugin-audit",
        "description": null,
        "name": "rpm-plugin-audit - Audit plugin for the RPM Package Manager",
        "section": 8
    },
    {
        "command": "rpm-plugin-dbus-announce",
        "description": null,
        "name": "rpm-plugin-dbus-announce - DBus plugin for the RPM PackageManager",
        "section": 8
    },
    {
        "command": "rpm-plugin-fapolicyd",
        "description": null,
        "name": "rpm-plugin-fapolicyd - Fapolicyd plugin for the RPM PackageManager",
        "section": 8
    },
    {
        "command": "rpm-plugin-prioreset",
        "description": null,
        "name": "rpm-plugin-prioreset - Plugin for the RPM Package Manager to fixissues with priorities of deamons on SysV init",
        "section": 8
    },
    {
        "command": "rpm-plugin-selinux",
        "description": null,
        "name": "rpm-plugin-selinux - SELinux plugin for the RPM Package Manager",
        "section": 8
    },
    {
        "command": "rpm-plugin-syslog",
        "description": null,
        "name": "rpm-plugin-syslog - Syslog plugin for the RPM Package Manager",
        "section": 8
    },
    {
        "command": "rpm-plugin-systemd-inhibit",
        "description": null,
        "name": "rpm-plugin-systemd-inhibit - Plugin for the RPM Package Manager",
        "section": 8
    },
    {
        "command": "rpm-plugins",
        "description": null,
        "name": "rpm-plugins - Plugins for the RPM Package Manager",
        "section": 8
    },
    {
        "command": "rpm2archive",
        "description": "rpm2archive converts the .rpm files specified as arguments to tararchives on standard out.If a '-' argument is given, an rpm stream is read from standardin.If standard out is connected to a terminal, the output is writtento tar files with a \".tgz\" suffix, gzip compressed by default.In opposite to rpm2cpio rpm2archive also works with RPM packagescontaining files greater than 4GB which are not supported bycpio.Unless rpm2cpio rpm2archive needs a working rpminstallation which limits its usefulness for some disasterrecovery scenarios.",
        "name": "rpm2archive - Create tar archive from RPM Package Manager (RPM)package.",
        "section": 8
    },
    {
        "command": "rpm2cpio",
        "description": "rpm2cpio converts the .rpm file specified as a single argument toa cpio archive on standard out.If a '-' argument is given, anrpm stream is read from standard in.rpm2cpio glint-1.0-1.i386.rpm | cpio -diumcat glint-1.0-1.i386.rpm | rpm2cpio - | cpio -tv",
        "name": "rpm2cpio - Extract cpio archive from RPM Package Manager (RPM)package.",
        "section": 8
    },
    {
        "command": "rpmbuild",
        "description": "rpmbuild is used to build both binary and source softwarepackages.A package consists of an archive of files and meta-data used to install and erase the archive files.The meta-dataincludes helper scripts, file attributes, and descriptiveinformation about the package.Packages come in two varieties:binary packages, used to encapsulate software to be installed,and source packages, containing the source code and recipenecessary to produce binary packages.One of the following basic modes must be selected: Build Package,Build Package from Tarball, Recompile Package, ShowConfiguration.GENERAL OPTIONSThese options can be used in all the different modes.-?, --helpPrint a longer usage message then normal.--versionPrint a single line containing the version number of rpmbeing used.--quietPrint as little as possible - normally only error messageswill be displayed.-vPrint verbose information - normally routine progressmessages will be displayed.-vvPrint lots of ugly debugging information.--rpmfcdebugEnables to debug dependencies generation.--rcfile FILELISTEach of the files in the colon separated FILELIST is readsequentially by rpm for configuration information.Onlythe first file in the list must exist, and tildes will beexpanded to the value of $HOME.The default FILELIST is/usr/lib/rpm/rpmrc:/usr/lib/rpm/<vendor>/rpmrc:/etc/rpmrc:~/.rpmrc.--pipe CMDPipes the output of rpm to the command CMD.--dbpath DIRECTORYUse the database in DIRECTORY rather than the default path/var/lib/rpm--root DIRECTORYUse the file system tree rooted at DIRECTORY for alloperations.Note that this means the database withinDIRECTORY will be used for dependency checks and anyscriptlet(s) (e.g. %post if installing, or %prep ifbuilding, a package) will be run after a chroot(2) toDIRECTORY.-D, --define='MACRO EXPR'Defines MACRO with value EXPR.--scm=SCMSelect the SCM to use with %autosetup, if one is not setin the spec file.Note that not all values for SCM, e.g.,patch (the default) and gendiff, git, or quilt workinterchangeably with all other patches and options statedin the %autosetup line, especially option -pN.BUILD OPTIONSThe general form of an rpm build command isrpmbuild {-bSTAGE|-rSTAGE|-tSTAGE} [rpmbuild-options] FILE ...The argument used is -b if a spec file is being used to build thepackage, -r if a source package is to be rebuilt and -t ifrpmbuild should look inside of a (possibly compressed) tar filefor the spec file to use.Packages are built in a number of stages.The first sixcorrespond to the following sections in a spec file: %prep,%generate_buildrequires, %build, %install, %check and %clean.Finally, binary and source packages are created in an assemblystage.The STAGE character specifies the stage to finish with (afterdoing all the stages preceding it), and is one of:-baPerform a full build - executes up to and including theassembly stage.In most cases, this is the option tochoose.-bbBuild just the binary packages - executes up to andincluding the assembly stage, but without creating thesource package.On success, the build directory isremoved (as in --clean).-bpUnpack the sources and apply any patches - executes the%prep stage only.-bfConfigure the sources - executes up to and including the%conf stage.This generally involves the equivalent of a\"./configure\".-bcCompile the sources - executes up to and including the%build stage.This generally involves the equivalent of a\"make\".-biInstall the binaries into the build root - executes up toand including the %check stage.This generally involvesthe equivalent of a \"make install\" and \"make check\".-blDo a \"list check\" - the %files section from the spec fileis macro expanded, and checks are made to verify that eachfile exists.-bsBuild just the source package - skips straight to theassembly stage, without executing any of the precedingstages or creating binary packages.-brBuild just the source package, but also parse and includedynamic build dependencies - executes up to and includingthe %generate_buildrequires stage and then skips straightto the assembly stage, without creating binary packages.This command can be used to fully resolve dynamic builddependencies.See the DYNAMIC BUILD DEPENDENCIES sectionfor details.-bdCheck dynamic build dependencies and build the.buildreqs.nosrc.rpm package if any are missing.Don\u2019tbuild anything else.The following options may also be used:--buildroot DIRECTORYWhen building a package, override the BuildRoot tag withdirectory DIRECTORY.--cleanRemove the build tree after the packages are made.--nobuildDo not execute any build stages.Useful for testing outspec files.--noprepDo not execute %prep build stage even if present in spec.--nocleanDo not execute %clean build stage even if present in spec.--nocheckDo not execute %check build stage even if present in spec.--nodebuginfoDo not generate debuginfo packages.--nodepsDo not verify build dependencies.--rmsourceRemove the sources after the build (may also be usedstandalone, e.g. \"rpmbuild --rmsource foo.spec\").--rmspecRemove the spec file after the build (may also be usedstandalone, eg.\"rpmbuild --rmspec foo.spec\").--short-circuitSkip straight to specified stage (i.e., skip all stagesleading up to the specified stage).Only valid with -bc,-bi, and -bb.Useful for local testing only.Packagesbuilt this way will be marked with an unsatisfiabledependency to prevent their accidental use.--build-in-placeBuild from locally checked out sources.Sets _builddir tocurrent working directory.Skips handling of -n and untarin the %setup and the deletion of the buildSubdir.--target PLATFORMWhen building the package, interpret PLATFORM as arch-vendor-os and set the macros %_target, %_target_cpu, and%_target_os accordingly.--with OPTIONEnable configure OPTION for build.--without OPTIONDisable configure OPTION for build.REBUILD AND RECOMPILE OPTIONSThere are two other ways to invoke building with rpm:rpmbuild --rebuild|--recompile SOURCEPKG ...When invoked this way, rpmbuild installs the named sourcepackage, and does a prep, compile and install.In addition,--rebuild builds a new binary package.When the build hascompleted, the build directory is removed (as in --clean) and thethe sources and spec file for the package are removed.These options are now superseded by the -r* options which allowmuch more fine control over what stages of the build to run.DYNAMIC BUILD DEPENDENCIESWhen the %generate_buildrequires stage runs and some of the newlygenerated BuildRequires are not satisfied, rpmbuild creates anintermediate source package ending in buildreqs.nosrc.rpm, whichhas the new BuildRequires, and exits with code 11.This packagecan then be used in place of the original source package toresolve and install the missing build dependencies in the usualway, such as with dnf-builddep(8).Multiple layers of dynamic build dependencies may exist in a specfile; the presence of specific BuildRequires on the system mayyield new BuildRequires next time a build is performed with thesame source package.The easiest way to ensure that all dynamicbuild dependencies are satisfied is to run the -br command,install the new dependencies of the buildreqs.nosrc.rpm packageand repeat the whole procedure until rpmbuild no longer exitswith code 11.If the -br command is coupled with --nodeps, exit code 11 isalways returned and a buildreqs.nosrc.rpm package is alwayscreated.SHOWRCThe commandrpmbuild --showrcshows the values rpmbuild will use for all of the options arecurrently set in rpmrc and macros configuration file(s).",
        "name": "rpmbuild - Build RPM Package(s)",
        "section": 8
    },
    {
        "command": "rpmdb",
        "description": "The general form of an rpm rebuild database command isrpm {--initdb|--rebuilddb} [-v] [--dbpath DIRECTORY] [--rootDIRECTORY]Use --initdb to create a new database if one doesn't alreadyexist (existing database is not overwritten), use --rebuilddb torebuild the database indices from the installed package headers.",
        "name": "rpmdb - RPM Database Tool",
        "section": 8
    },
    {
        "command": "rpmdeps",
        "description": "rpmdeps generates package dependencies for the set of FILEarguments.Each FILE argument is searched for Elf32/Elf64,script interpreter, or per-script dependencies, and thedependencies are printed to stdout.",
        "name": "rpmdeps - Generate RPM Package Dependencies",
        "section": 8
    },
    {
        "command": "rpmgraph",
        "description": "rpmgraph uses PACKAGE_FILE arguments to generate a packagedependency graph.Each PACKAGE_FILE argument is read and addedto an rpm transaction set.The elements of the transaction setare partially ordered using a topological sort.The partiallyordered elements are then printed to standard output.Nodes in the dependency graph are package names, and edges in thedirected graph point to the parent of each node.The parent nodeis defined as the last predecessor of a package when partiallyordered using the package dependencies as a relation.That meansthat the parent of a given package is the package's lastprerequisite.The output is in dot(1) directed graph format, and can bedisplayed or printed using the dotty graph editor from thegraphviz package.There are no rpmgraph specific options, onlycommon rpm options.See the rpmgraph usage message for what iscurrently implemented.",
        "name": "rpmgraph - Display RPM Package Dependency Graph",
        "section": 8
    },
    {
        "command": "rpmkeys",
        "description": "The general forms of rpm digital signature commands arerpmkeys --import PUBKEY ...rpmkeys {-K|--checksig} PACKAGE_FILE ...The --checksig option checks all the digests and signaturescontained in PACKAGE_FILE to ensure the integrity and origin ofthe package.Note that signatures are now verified whenever apackage is read, and --checksig is useful to verify all of thedigests and signatures associated with a package.Digital signatures cannot be verified without a public key.AnASCII armored public key can be added to the rpm database using--import.An imported public key is carried in a header, and keyring management is performed exactly like package management.For example, all currently imported public keys can be displayedby:rpm -qa gpg-pubkey*Details about a specific public key, when imported, can bedisplayed by querying.Here's information about the Red HatGPG/DSA key:rpm -qi gpg-pubkey-db42a60eFinally, public keys can be erased after importing just likepackages.Here's how to remove the Red Hat GPG/DSA keyrpm -e gpg-pubkey-db42a60e",
        "name": "rpmkeys - RPM Keyring",
        "section": 8
    },
    {
        "command": "rpmlua",
        "description": "Run RPM internal Lua interpreter.-i|--interactiveRun an interactive session after executing optional scriptor statement.\u2013opts=OPTSTRINGPerform getopt(3) option processing on the passedarguments according to OPTSTRING.-e|--executeExecute a Lua statement before executing optional script.",
        "name": "rpmlua - RPM Lua interpreter",
        "section": 8
    },
    {
        "command": "rpmsign",
        "description": "Both of the --addsign and --resign options generate and insertnew signatures for each package PACKAGE_FILE given, replacing anyexisting signatures.There are two options for historicalreasons, there is no difference in behavior currently.To create a signature rpm needs to verify the package's checksum.As a result packages with a MD5/SHA1 checksums cannot be signedin FIPS mode.rpm --delsign PACKAGE_FILE ...Delete all signatures from each package PACKAGE_FILE given.rpm --delfilesign PACKAGE_FILE ...Delete all IMA and fsverity file signatures from each packagePACKAGE_FILE given.SIGN OPTIONS--rpmv3Force RPM V3 header+payload signature addition.These areexpensive and redundant baggage on packages where aseparate payload digest exists (packages built with rpm >=4.14).Rpm will automatically detect the need for V3signatures, but this option can be used to force theircreation if the packages must be fully signatureverifiable with rpm < 4.14 or other interoperabilityreasons.--fskpath KEYUsed with --signfiles, use file signing key Key.--certpath CERTUsed with --signverity, use file signing certificate Cert.--verityalgo ALGUsed with --signverity, to specify the signing algorithm.sha256 and sha512 are supported, with sha256 being thedefault if this argument is not specified.This can alsobe specified with the macro %_verity_algorithm--signfilesSign package files.The macro%_binary_filedigest_algorithm must be set to a supportedalgorithm before building the package.The supportedalgorithms are SHA1, SHA256, SHA384, and SHA512, which arerepresented as 2, 8, 9, and 10 respectively.The filesigning key (RSA private key) must be set before signingthe package, it can be configured on the command line with--fskpath or the macro %_file_signing_key.--signveritySign package files with fsverity signatures.The filesigning key (RSA private key) and the signing certificatemust be set before signing the package.The key can beconfigured on the command line with --fskpath or the macro%_file_signing_key, and the cert can be configured on thecommand line with --certpath or the macro%_file_signing_cert.USING GPG TO SIGN PACKAGESIn order to sign packages using GPG, rpm must be configured torun GPG and be able to find a key ring with the appropriate keys.By default, rpm uses the same conventions as GPG to find keyrings, namely the $GNUPGHOME environment variable.If your keyrings are not located where GPG expects them to be, you will needto configure the macro %_gpg_path to be the location of the GPGkey rings to use.If you want to be able to sign packages youcreate yourself, you also need to create your own public andsecret key pair (see the GPG manual).You will also need toconfigure the rpm macros%_gpg_nameThe name of the \"user\" whose key you wish to use to signyour packages.For example, to be able to use GPG to sign packages as the user\"John Doe <jdoe@foo.com>\" from the key rings located in/etc/rpm/.gpg using the executable /usr/bin/gpg you would include%_gpg_path /etc/rpm/.gpg%_gpg_name John Doe <jdoe@foo.com>%__gpg /usr/bin/gpgin a macro configuration file.Use /etc/rpm/macros for per-system configuration and ~/.rpmmacros for per-user configuration.Typically it's sufficient to set just %_gpg_name.",
        "name": "rpmsign - RPM Package Signing",
        "section": 8
    },
    {
        "command": "rpmsort",
        "description": "rpmsort sorts the input files, and writes a sorted list tostandard out - like sort(1), but aware of RPM versioning.If '-' is given as an argument, or no arguments are given,versions are read from stdandard in and writen to standard out.",
        "name": "rpmsort - Sort input by RPM Package Manager (RPM) versioning.",
        "section": 8
    },
    {
        "command": "rpmspec",
        "description": "rpmspec is a tool for querying a spec file.More specificallyfor querying hypothetical packages which would be created fromthe given spec file.So querying a spec file with rpmspec issimilar to querying a package built from that spec file.But isis not identical.With rpmspec you can't query all fields whichyou can query from a built package.E.g.you can't queryBUILDTIME with rpmspec for obvious reasons.You also cannotquery other fields automatically generated during a build of apackage like auto generated dependencies.select-options[--rpms] [--srpm]query-options[--qf,--queryformat QUERYFMT] [--target TARGET_PLATFORM]QUERY OPTIONSThe general form of an rpm spec query command isrpm {-q|--query} [select-options] [query-options]You may specify the format that the information should be printedin.To do this, you use the--qf|--queryformat QUERYFMToption, followed by the QUERYFMT format string.See rpm(8) fordetails.SELECT OPTIONS--rpms Operate on the all binary package headers generated fromspec.--builtrpms Operate only on the binary package headers ofpackages which would be built from spec.That means ignoringpackage headers of packages that won't be built from spec i.e.ignoring package headers of packages without file section.--srpm Operate on the source package header(s) generated fromspec.",
        "name": "rpmspec - RPM Spec Tool",
        "section": 8
    },
    {
        "command": "rsyslogd",
        "description": "Rsyslogd is a system utility providing support for messagelogging.Support of both internet and unix domain socketsenables this utility to support both local and remote logging.Note that this version of rsyslog ships with extensivedocumentation in HTML format.This is provided in the ./docsubdirectory and probably in a separate package if you installedrsyslog via a packaging system.To use rsyslog's advancedfeatures, you need to look at the HTML documentation, because theman pages only covers basic aspects of operation.For detailsand configuration examples, see the rsyslog.conf (5) man page andthe online documentation at https://www.rsyslog.com/doc/Rsyslogd(8) is derived from the sysklogd package which in turn isderived from the stock BSD sources.Rsyslogd provides a kind of logging that many modern programsuse.Every logged message contains at least a time and ahostname field, normally a program name field, too, but thatdepends on how trusty the logging program is. The rsyslog packagesupports free definition of output formats via templates. It alsosupports precise timestamps and writing directly to databases. Ifthe database option is used, tools like phpLogCon can be used toview the log data.While the rsyslogd sources have been heavily modified a couple ofnotes are in order.First of all there has been a systematicattempt to ensure that rsyslogd follows its default, standard BSDbehavior. Of course, some configuration file changes arenecessary in order to support the template system. However,rsyslogd should be able to use a standard syslog.conf and actlike the original syslogd. However, an original syslogd will notwork correctly with a rsyslog-enhanced configuration file. Atbest, it will generate funny looking file names.The secondimportant concept to note is that this version of rsyslogdinteracts transparently with the version of syslog found in thestandard libraries.If a binary linked to the standard sharedlibraries fails to function correctly we would like an example ofthe anomalous behavior.The main configuration file /etc/rsyslog.conf or an alternativefile, given with the -f option, is read at startup.Any linesthat begin with the hash mark (``#'') and empty lines areignored.If an error occurs during parsing the error element isignored. It is tried to parse the rest of the line.",
        "name": "rsyslogd - reliable and extended syslogd",
        "section": 8
    },
    {
        "command": "rtacct",
        "description": "nstat and rtacct are simple tools to monitor kernel snmp countersand network interface statistics.nstat can filter kernel snmp counters by name with one or severalspecified wildcards. Wildcards are case-insensitive and caninclude special symbols ?and *",
        "name": "nstat, rtacct - network statistics tools.",
        "section": 8
    },
    {
        "command": "rtcwake",
        "description": "This program is used to enter a system sleep state and toautomatically wake from it at a specified time.This uses cross-platform Linux interfaces to enter a system sleepstate, and leave it no later than a specified time. It uses anyRTC framework driver that supports standard driver model wakeupflags.This is normally used like the old apmsleep utility, to wake froma suspend state like ACPI S1 (standby) or S3 (suspend-to-RAM).Most platforms can implement those without analogues of BIOS,APM, or ACPI.On some systems, this can also be used like nvram-wakeup, wakingfrom states like ACPI S4 (suspend to disk). Not all systems havepersistent media that are appropriate for such suspend modes.Note that alarm functionality depends on hardware; not every RTCis able to setup an alarm up to 24 hours in the future.The suspend setup may be interrupted by active hardware; forexample wireless USB input devices that continue to send eventsfor some fraction of a second after the return key is pressed.rtcwake tries to avoid this problem and it waits to the terminalto settle down before entering a system sleep.",
        "name": "rtcwake - enter a system sleep state until specified wakeup time",
        "section": 8
    },
    {
        "command": "rtmon",
        "description": "This manual page documents briefly the rtmon command.rtmon listens on netlink socket and monitors routing tablechanges.rtmon can be started before the first network configurationcommand is issued.For example if you insert:rtmon file /var/log/rtmon.login a startup script, you will be able to view the full historylater.Certainly, it is possible to start rtmon at any time. Itprepends the history with the state snapshot dumped at the momentof starting.",
        "name": "rtmon - listens to and monitors RTnetlink",
        "section": 8
    },
    {
        "command": "rtstat",
        "description": "This manual page documents briefly the lnstat command.lnstat is a generalized and more feature-complete replacement forthe old rtstat program. It is commonly used to periodically printa selection of statistical values exported by the kernel.Inaddition to routing cache statistics, it supports any kind ofstatistics the linux kernel exports via a file in/proc/net/stat/.Each file in /proc/net/stat/ contains a header line listing thecolumn names.These names are used by lnstat as keys forselecting which statistics to print. For every CPU present in thesystem, a line follows which lists the actual values for eachcolumn of the file. lnstat sums these values up (which in factare counters) before printing them. After each interval, only thedifference to the last value is printed.Files and columns may be selected by using the -f and -kparameters. By default, all columns of all files are printed.",
        "name": "lnstat - unified linux network statistics",
        "section": 8
    },
    {
        "command": "run_init",
        "description": "Run a init script under the proper context, which is specified in/etc/selinux/POLICYTYPE/contexts/initrc_context.It is generallyused interactively as it requires either shadow or PAM userauthentication (depending on compile-time options).It should bepossible to configure PAM such that interactive input is notrequired.Check your PAM documentation.",
        "name": "run_init - run an init script in the proper SELinux context",
        "section": 8
    },
    {
        "command": "runlevel",
        "description": "runlevel prints the previous and current SysV runlevel if theyare known.The two runlevel characters are separated by a single spacecharacter. If a runlevel cannot be determined, N is printedinstead. If neither can be determined, the word \"unknown\" isprinted.Unless overridden in the environment, this will check the utmpdatabase for recent runlevel changes.",
        "name": "runlevel - Print previous and current SysV runlevel",
        "section": 8
    },
    {
        "command": "sa",
        "description": "sa summarizes information about previously executed commands asrecorded in the acct file.In addition, it condenses this datainto a summary file named savacct which contains the number oftimes the command was called and the system resources used.Theinformation can also be summarized on a per-user basis; sa willsave this information into a file named usracct.If no arguments are specified, sa will print information aboutall of the commands in the acct file.If called with a file name as the last argument, sa will use thatfile instead of the system's default acct file.By default, sa will sort the output by sum of user and systemtime.If command names have unprintable characters, or are onlycalled once, sa will sort them into a group called `***other'.If more than one sorting option is specified, the list will besorted by the one specified last on the command line.The output fields are labeled as follows:cpusum of system and user time in cpu minutesre\"elapsed time\" in minuteskcpu-time averaged core usage, in 1k unitsavioaverage number of I/O operations per executiontiototal number of I/O operationsk*seccpu storage integral (kilo-core seconds)uuser cpu time in cpu secondsssystem time in cpu secondsNote that these column titles do not appear in the first row ofthe table, but after each numeric entry (as units of measurement)in every row.For example, you might see `79.29re', meaning79.29 cpu seconds of \"real time\".An asterisk will appear after the name of commands that forkedbut didn't call exec.GNU sa takes care to implement a number of features not found inother versions.For example, most versions of sa don't payattention to flags like `--print-seconds' and `--sort-num-calls'when printing out commands when combined with the `--user-summary' or `--print-users' flags.GNU sa pays attention tothese flags if they are applicable.Also, MIPS' sa stores theaverage memory use as a short rather than a double, resulting insome round-off errors.GNU sa uses double the whole way through.",
        "name": "sa -summarizes accounting information",
        "section": 8
    },
    {
        "command": "sa1",
        "description": "The sa1 command is a shell procedure variant of the sadc commandand handles all of the flags and parameters of that command. Thesa1 command collects and stores binary data in the currentstandard system activity daily data file.The standard system activity daily data file is named saDD unlesssadc's option -D is used, in which case its name is saYYYYMMDD,where YYYY stands for the current year, MM for the current monthand DD for the current day. By default it is located in the/var/log/sa directory.The interval and count parameters specify that the record shouldbe written count times at interval seconds. If no arguments aregiven to sa1 then a single record is written.The sa1 command is designed to be started automatically by thecron command.",
        "name": "sa1 - Collect and store binary data in the system activity dailydata file.",
        "section": 8
    },
    {
        "command": "sa2",
        "description": "The sa2 command is a shell procedure variant of the sar commandwhich writes a daily report in the sarDD or the sarYYYYMMDD file,where YYYY stands for the current year, MM for the current monthand DD for the current day. By default the report is saved in the/var/log/sa directory. The sa2 command will also remove reportsmore than one week old by default.You can however keep reportsfor a longer (or a shorter) period by setting the HISTORYenvironment variable. Read the sysstat(5) manual page fordetails.The sa2 command accepts most of the flags and parameters of thesar command.The sa2 command is designed to be started automatically by thecron command.",
        "name": "sa2 - Create a report from the current standard system activitydaily data file.",
        "section": 8
    },
    {
        "command": "sadc",
        "description": "The sadc command samples system data a specified number of times(count) at a specified interval measured in seconds (interval).It writes in binary format to the specified outfile or tostandard output. If outfile is set to -, then sadc uses thestandard system activity daily data file (see below).In thiscase, if the file already exists, sadc will overwrite it if it isfrom a previous month. By default sadc collects most of the dataavailable from the kernel.But there are also optional metrics,for which the relevant options must be explicitly passed to sadcto be collected (see option -S below).The standard system activity daily data file is named saDD unlessoption -D is used, in which case its name is saYYYYMMDD, whereYYYY stands for the current year, MM for the current month and DDfor the current day. By default it is located in the /var/log/sadirectory. Yet it is possible to specify an alternate locationfor it: If outfile is a directory (instead of a plain file) thenit will be considered as the directory where the standard systemactivity daily data file will be saved.When the count parameter is not specified, sadc writes its dataendlessly. When both interval and count are not specified, andoption -C is not used, a dummy record, which is used at systemstartup to mark the time when the counter restarts from 0, willbe written.For example, one of the system startup script maywrite the restart mark to the daily data file by the commandentry:/usr/local/lib64/sa/sadc -The sadc command is intended to be used as a backend to the sarcommand.Note: The sadc command only reports on local activities.",
        "name": "sadc - System activity data collector.",
        "section": 8
    },
    {
        "command": "sample",
        "description": "The sample action allows sampling packets matching classifier.The packets are chosen randomly according to the rate parameter,and are sampled using the psample generic netlink channel. Theuser can also specify packet truncation to save user-kerneltraffic. Each sample includes some informative metadata about theoriginal packet, which is sent using netlink attributes,alongside the original packet data.The user can either specify the sample action parameters aspresented in the first form above, or use an existing sampleaction using its index, as presented in the second form.",
        "name": "sample - packet sampling tc action",
        "section": 8
    },
    {
        "command": "sandbox",
        "description": "Run the cmd application within a tightly confined SELinux domain.The default sandbox domain only allows applications the abilityto read and write stdin, stdout and any other file descriptorshanded to it. It is not allowed to open any other files.The -Moption will mount an alternate homedir and tmpdir to be used bythe sandbox.If you have the policycoreutils-sandbox package installed, youcan use the -X option and the -M option.sandbox -X allows youto run X applications within a sandbox.These applications willstart up their own X Server and create a temporary home directoryand /tmp.The default SELinux policy does not allow anycapabilities or network access.It also prevents all access tothe users other processes and files.Files specified on thecommand that are in the home directory or /tmp will be copiedinto the sandbox directories.If directories are specified with -H or -T the directory willhave its context modified with chcon(1) unless a level isspecified with -l.If the MLS/MCS security level is specified,the user is responsible to set the correct labels.-h --helpdisplay usage message-H --homedirUse alternate homedir to mount over your home directory.Defaults to temporary. Requires -X or -M.-i --includeCopy this file into the appropriate temporary sandboxdirectory. Command can be repeated.-I --includefileCopy all files listed in inputfile into the appropriatetemporary sandbox directories.-l --levelSpecify the MLS/MCS Security Level to run the sandboxwith.Defaults to random.-M --mountCreate a Sandbox with temporary files for $HOME and /tmp.-s --shredShred temporary files created in $HOME and /tmp, beforedeleting.-t --typeUse alternate sandbox type, defaults to sandbox_t orsandbox_x_t for -X.Examples:sandbox_t -No X, No Network Access, No Open,read/write on passed in file descriptors.sandbox_min_t-No Network Accesssandbox_x_t-Ports for X applications to runlocallysandbox_web_t-Ports required for web browsingsandbox_net_t-Network ports (for serversoftware)sandbox_net_client_t-All network ports-T --tmpdirUse alternate temporary directory to mount on /tmp.Defaults to tmpfs. Requires -X or -M.-R --runuserdirUse alternate temporary directory to mount onXDG_RUNTIME_DIR (/run/user/$UID).-S --sessionRun a full desktop session, Requires level, and home andtmpdir.-w --windowsizeSpecifies the windowsize when creating an X based Sandbox.The default windowsize is 1000x700.-W --windowmanagerSelect alternative window manager to run within sandbox-X.Default to /usr/bin/openbox.-XCreate an X based Sandbox for gui apps, temporary filesfor $HOME and /tmp, secondary Xserver, defaults tosandbox_x_t-d --dpiSet the DPI value for the sandbox X Server. Defaults tothe current X Sever DPI.-C --capabilities Use capabilities within thesandbox. By default applications executed within thesandbox will not be allowed to use capabilities (setuidapps), with the -C flag, you can use programs requiringcapabilities.",
        "name": "sandbox - Run cmd under an SELinux sandbox",
        "section": 8
    },
    {
        "command": "sefcontext_compile",
        "description": "sefcontext_compile is used to compile file context regularexpressions into pcre(3) format.The compiled file is used by libselinux file labeling functions.By default sefcontext_compile writes the compiled pcre file withthe .bin suffix appended (e.g. inputfile.bin).",
        "name": "sefcontext_compile - compile file context regular expressionfiles",
        "section": 8
    },
    {
        "command": "selinux",
        "description": "NSA Security-Enhanced Linux (SELinux) is an implementation of aflexible mandatory access control architecture in the Linuxoperating system.The SELinux architecture provides generalsupport for the enforcement of many kinds of mandatory accesscontrol policies, including those based on the concepts of TypeEnforcement\u00ae, Role- Based Access Control, and Multi-LevelSecurity.Background information and technical documentationabout SELinux can be found at https://github.com/SELinuxProject.The /etc/selinux/config configuration file controls whetherSELinux is enabled or disabled, and if enabled, whether SELinuxoperates in permissive mode or enforcing mode.The SELINUXvariable may be set to any one of disabled, permissive, orenforcing to select one of these options.The disabled disablesmost of the SELinux kernel and application code, leaving thesystem running without any SELinux protection.The permissiveoption enables the SELinux code, but causes it to operate in amode where accesses that would be denied by policy are permittedbut audited.The enforcing option enables the SELinux code andcauses it to enforce access denials as well as auditing them.permissive mode may yield a different set of denials thanenforcing mode, both because enforcing mode will prevent anoperation from proceeding past the first denial and because someapplication code will fall back to a less privileged mode ofoperation if denied access.NOTE: Disabling SELinux by setting SELINUX=disabled in/etc/selinux/config is deprecated and depending on kernel versionand configuration it might not lead to SELinux being completelydisabled.Specifically, the SELinux hooks will still be executedinternally, but the SELinux policy will not be loaded and nooperation will be denied.In such state, the system will act asif SELinux was disabled, although some operations might behaveslightly differently.To properly disable SELinux, it isrecommended to use the selinux=0 kernel boot option instead.Inthat case SELinux will be disabled regardless of what is set inthe /etc/selinux/config file.The /etc/selinux/config configuration file also controls whatpolicy is active on the system.SELinux allows for multiplepolicies to be installed on the system, but only one policy maybe active at any given time.At present, multiple kinds ofSELinux policy exist: targeted, mls for example.The targetedpolicy is designed as a policy where most user processes operatewithout restrictions, and only specific services are placed intodistinct security domains that are confined by the policy.Forexample, the user would run in a completely unconfined domainwhile the named daemon or apache daemon would run in a specificdomain tailored to its operation.The MLS (Multi-Level Security)policy is designed as a policy where all processes arepartitioned into fine-grained security domains and confined bypolicy.MLS also supports the Bell And LaPadula model, whereprocesses are not only confined by the type but also the level ofthe data.You can define which policy you will run by setting theSELINUXTYPE environment variable within /etc/selinux/config.Youmust reboot and possibly relabel if you change the policy type tohave it take effect on the system.The corresponding policyconfiguration for each such policy must be installed in the/etc/selinux/{SELINUXTYPE}/ directories.A given SELinux policy can be customized further based on a setof compile-time tunable options and a set of runtime policybooleans.system-config-selinux allows customization of thesebooleans and tunables.Many domains that are protected by SELinux also include SELinuxman pages explaining how to customize their policy.",
        "name": "SELinux - NSA Security-Enhanced Linux (SELinux)",
        "section": 8
    },
    {
        "command": "selinux-polgengui",
        "description": "selinux-polgengui is a graphical tool, which can be used tocreate a framework for building SELinux Policy.",
        "name": "selinux-polgengui - SELinux Policy Generation Tool",
        "section": 8
    },
    {
        "command": "selinuxenabled",
        "description": "Indicates whether SELinux is enabled or disabled.",
        "name": "selinuxenabled - tool to be used within shell scripts todetermine if selinux is enabled",
        "section": 8
    },
    {
        "command": "selinuxexeccon",
        "description": "selinuxexeccon reports the SELinux process context for thespecified command from the specified context or the currentcontext.",
        "name": "selinuxexeccon - report SELinux context used for this executable",
        "section": 8
    },
    {
        "command": "semanage",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.This includes the mapping from Linux usernames toSELinux user identities (which controls the initial securitycontext assigned to Linux users when they login and bounds theirauthorized role set) as well as security context mappings forvarious kinds of objects, such as network ports, interfaces,infiniband pkeys and endports, and nodes (hosts) as well as thefile context mapping. Note that the semanage login command dealswith the mapping from Linux usernames (logins) to SELinux useridentities, while the semanage user command deals with themapping from SELinux user identities to authorized role sets.Inmost cases, only the former mapping needs to be adjusted by theadministrator; the latter is principally defined by the basepolicy and usually does not require modification.",
        "name": "semanage - SELinux Policy Management tool",
        "section": 8
    },
    {
        "command": "semanage-boolean",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage boolean command controls the settings ofbooleans in SELinux policy. Booleans are if-then-else ruleswritten in SELinux Policy. They can be used to customize the waythat SELinux Policy rules effect a confined domain.",
        "name": "semanage-boolean - SELinux Policy Management boolean tool",
        "section": 8
    },
    {
        "command": "semanage-dontaudit",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage dontaudit toggles whether or not dontauditrules will be in the policy. Policy writers use dontaudit rulesto cause confined applications to use alternative paths.Dontaudit rules are denied but not reported in the logs.Sometimes dontaudit rules can cause bugs in applications butpolicy writers will not realize it since the AVC is not audited.Turn off dontaudit rules with this command to see if the kernelis blocking an access.",
        "name": "semanage-dontaudit - SELinux Policy Management dontaudit tool",
        "section": 8
    },
    {
        "command": "semanage-export",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage import and export can be used to extract theSELinux modifications from one machine and apply them to another.Please note that this will remove all current semanagecustomizations on the second machine as the command listgenerated using semanage export start with <command> -D for allsemanage sub-commands. You can put a whole group of semanagecommands within a file and apply them to a machine in a singletransaction.",
        "name": "semanage-export - SELinux Policy Management import tool",
        "section": 8
    },
    {
        "command": "semanage-fcontext",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage fcontext is used to manage the default filesystem labeling on an SELinux system.This command maps filepaths using regular expressions to SELinux labels.FILE_SPEC may contain either a fully qualified path, or a Perlcompatible regular expression (PCRE), describing fully qualifiedpath(s). The only PCRE flag in use is PCRE2_DOTALL, which causesa wildcard '.' to match anything, including a new line.Stringsrepresenting paths are processed as bytes (as opposed toUnicode), meaning that non-ASCII characters are not matched by asingle wildcard.Note, that file context definitions specified using 'semanagefcontext' (i.e. local file context modifications stored infile_contexts.local) have higher priority than those specified inpolicy modules.This means that whenever a match for given filepath is found in file_contexts.local, no other file contextdefinitions are considered.Entries in file_contexts.local areprocessed from most recent one to the oldest, with first matchbeing used (as opposed to the most specific match, which is usedwhen matching other file context definitions).All regularexpressions should therefore be as specific as possible, to avoidunintentionally impacting other parts of the filesystem.",
        "name": "semanage-fcontext - SELinux Policy Management file context tool",
        "section": 8
    },
    {
        "command": "semanage-ibendport",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage ibendport controls the ibendport number toibendport type definitions.",
        "name": "semanage-ibendport - SELinux Policy Management ibendport mappingtool",
        "section": 8
    },
    {
        "command": "semanage-ibpkey",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage ibpkey controls the ibpkey number to ibpkeytype definitions.",
        "name": "semanage-ibpkey - SELinux Policy Management ibpkey mapping tool",
        "section": 8
    },
    {
        "command": "semanage-import",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage import and export can be used to extract theSELinux modifications from one machine and apply them to another.Please note that this will remove all current semanagecustomizations on the second machine as the command listgenerated using semanage export start with <command> -D for allsemanage sub-commands. You can put a whole group of semanagecommands within a file and apply them to a machine in a singletransaction.",
        "name": "semanage-import - SELinux Policy Management import tool",
        "section": 8
    },
    {
        "command": "semanage-interface",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage interface controls the labels assigned tonetwork interfaces.",
        "name": "semanage-interface - SELinux Policy Management network interfacetool",
        "section": 8
    },
    {
        "command": "semanage-login",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage login controls the mapping between a LinuxUser and the SELinux User. It can be used to turn on confinedusers. For example you could define that a particular user orgroup of users will login to a system as the user_u user. Prefixthe group name with a '%' sign to indicate a group name.",
        "name": "semanage-login - SELinux Policy Management linux user to SELinuxUser mapping tool",
        "section": 8
    },
    {
        "command": "semanage-module",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage module installs, removes, disables, or enablesSELinux Policy modules.",
        "name": "semanage-module - SELinux Policy Management module mapping tool",
        "section": 8
    },
    {
        "command": "semanage-node",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage node controls the IP address to node typedefinitions.",
        "name": "semanage-node - SELinux Policy Management node mapping tool",
        "section": 8
    },
    {
        "command": "semanage-permissive",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage permissive adds or removes a SELinux Policypermissive module. Please note that this command can make anydomain permissive, but can only remove the permissive propertyfrom domains where it was added by semanage permissive (\"semanagepermissive -d\" can only be used on types listed as \"CustomizedPermissive Types\" by \"semanage permissive -l\").",
        "name": "semanage-permissive - SELinux Policy Management permissivemapping tool",
        "section": 8
    },
    {
        "command": "semanage-port",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage port controls the port number to port typedefinitions.",
        "name": "semanage-port - SELinux Policy Management port mapping tool",
        "section": 8
    },
    {
        "command": "semanage-user",
        "description": "semanage is used to configure certain elements of SELinux policywithout requiring modification to or recompilation from policysources.semanage user controls the mapping between an SELinuxUser and the roles and MLS/MCS levels.",
        "name": "semanage-user - SELinux Policy Management SELinux User mappingtool",
        "section": 8
    },
    {
        "command": "semodule",
        "description": "semodule is the tool used to manage SELinux policy modules,including installing, upgrading, listing and removing modules.semodule may also be used to force a rebuild of policy from themodule store and/or to force a reload of policy withoutperforming any other transaction.semodule acts on modulepackages created by semodule_package.Conventionally, thesefiles have a .pp suffix (policy package), although this is notmandated in any way.",
        "name": "semodule - Manage SELinux policy modules.",
        "section": 8
    },
    {
        "command": "semodule_expand",
        "description": "semodule_expand is a developer tool for manually expanding a basepolicy module package into a kernel binary policy file.Thistool is not necessary for normal operation of SELinux.In normaloperation, such expanding is performed internally by libsemanagein response to semodule commands.Base policy module packagescan be created directly by semodule_package or by semodule_link(when linking together a set of packages into a single package).",
        "name": "semodule_expand - Expand a SELinux policy module package.",
        "section": 8
    },
    {
        "command": "semodule_link",
        "description": "semodule_link is a developer tool for manually linking together aset of SELinux policy module packages into a single policy modulepackage.This tool is not necessary for normal operation ofSELinux.In normal operation, such linking is performedinternally by libsemanage in response to semodule commands.Module packages are created by semodule_package.",
        "name": "semodule_link - Link SELinux policy module packages together",
        "section": 8
    },
    {
        "command": "semodule_package",
        "description": "semodule_package is the tool used to create a SELinux policymodule package from a binary policy module and optionally otherdata such as file contexts.semodule_package packages binarypolicy modules created by checkmodule.The policy packagecreated by semodule_package can then be installed via semodule.",
        "name": "semodule_package - Create a SELinux policy module package.",
        "section": 8
    },
    {
        "command": "semodule_unpackage",
        "description": "semodule_unpackage is a tool used to extract SELinux policymodule file and file context file from an SELinux Policy Package.",
        "name": "semodule_unpackage - Extract policy module and file context filefrom an SELinux policy module package.",
        "section": 8
    },
    {
        "command": "sendmail",
        "description": "The sendmail utility is a local enqueuer for the smtpd(8) daemon,compatible with mailwrapper(8).The message is read on standardinput (stdin) until sendmail encounters an end-of-file.Thesendmail enqueuer is not intended to be used directly to send mail,but rather via a frontend known as a mail user agent.Unless the optional -t flag is specified, one or more recipientsmust be specified on the command line.The options are as follows:-F nameSet the sender's full name.-f fromSet the sender's address.-tRead the message's To:, Cc:, and Bcc: fields forrecipients.The Bcc: field will be deleted before sending.-vEnable verbose output.To maintain compatibility with Sendmail, Inc.'s implementation ofsendmail, various other flags are accepted, but have no effect.",
        "name": "sendmail \u2014 a mail enqueuer for smtpd(8)",
        "section": 8
    },
    {
        "command": "sepolgen",
        "description": "Use sepolicy generate to generate an SELinux policy Module.sepolicy generate will create 5 files.When specifying a confined application you must specify a path.sepolicy generate will use the rpm payload of the applicationalong with nm -D APPLICATION to help it generate types and policyrules for your policy files.Type Enforcing File NAME.teThis file can be used to define all the types rules for aparticular domain.Note: Policy generated by sepolicy generate will automaticallyadd a permissive DOMAIN to your te file.When you are satisfiedthat your policy works, you need to remove the permissive linefrom the te file to run your domain in enforcing mode.Interface File NAME.ifThis file defines the interfaces for the types generated in thete file, which can be used by other policy domains.File Context NAME.fcThis file defines the default file context for the system, ittakes the file types created in the te file and associates filepaths to the types.Tools like restorecon and RPM will use thesepaths to put down labels.RPM Spec File NAME_selinux.specThis file is an RPM SPEC file that can be used to install theSELinux policy on to machines and setup the labeling. The specfile also installs the interface file and a man page describingthe policy.You can use sepolicy manpage -d NAME to generate theman page.Shell File NAME.shThis is a helper shell script to compile, install and fix thelabeling on your test system.It will also generate a man pagebased on the installed policy, and compile and build an RPMsuitable to be installed on other machinesIf a generate is possible, this tool will print out all generatepaths from the source domain to the target domain",
        "name": "sepolicy-generate - Generate an initial SELinux policy moduletemplate.",
        "section": 8
    },
    {
        "command": "sepolicy",
        "description": "sepolicy is a tools set that will query the installed SELinuxpolicy and generate useful reports, man pages, or even new policymodules.See the argument specific man pages for options anddescriptions.",
        "name": "sepolicy - SELinux Policy Inspection tool",
        "section": 8
    },
    {
        "command": "sepolicy-booleans",
        "description": "sepolicy booleans will show all booleans and their descriptions,or you can choose individual booleans to display",
        "name": "sepolicy-booleans - Query SELinux Policy to see description ofbooleans",
        "section": 8
    },
    {
        "command": "sepolicy-communicate",
        "description": "Use sepolicy communicate to examine SELinux Policy to if a sourceSELinux Domain can communicate with a target SELinux Domain.Thedefault command looks to see if there are any file types that thesource domain can write, which the target domain can read.",
        "name": "sepolicy-communicate - Generate a report showing if two SELinuxPolicy Domains can communicate",
        "section": 8
    },
    {
        "command": "sepolicy-generate",
        "description": "Use sepolicy generate to generate an SELinux policy Module.sepolicy generate will create 5 files.When specifying a confined application you must specify a path.sepolicy generate will use the rpm payload of the applicationalong with nm -D APPLICATION to help it generate types and policyrules for your policy files.Type Enforcing File NAME.teThis file can be used to define all the types rules for aparticular domain.Note: Policy generated by sepolicy generate will automaticallyadd a permissive DOMAIN to your te file.When you are satisfiedthat your policy works, you need to remove the permissive linefrom the te file to run your domain in enforcing mode.Interface File NAME.ifThis file defines the interfaces for the types generated in thete file, which can be used by other policy domains.File Context NAME.fcThis file defines the default file context for the system, ittakes the file types created in the te file and associates filepaths to the types.Tools like restorecon and RPM will use thesepaths to put down labels.RPM Spec File NAME_selinux.specThis file is an RPM SPEC file that can be used to install theSELinux policy on to machines and setup the labeling. The specfile also installs the interface file and a man page describingthe policy.You can use sepolicy manpage -d NAME to generate theman page.Shell File NAME.shThis is a helper shell script to compile, install and fix thelabeling on your test system.It will also generate a man pagebased on the installed policy, and compile and build an RPMsuitable to be installed on other machinesIf a generate is possible, this tool will print out all generatepaths from the source domain to the target domain",
        "name": "sepolicy-generate - Generate an initial SELinux policy moduletemplate.",
        "section": 8
    },
    {
        "command": "sepolicy-gui",
        "description": "Use sepolicy gui to run a the graphical user interface, whichallows you to explore how SELinux confines different processdomains.",
        "name": "sepolicy-gui - Graphical User Interface for SELinux policy.",
        "section": 8
    },
    {
        "command": "sepolicy-interface",
        "description": "Use sepolicy interface to print interfaces information based onSELinux Policy.",
        "name": "sepolicy-interface - Print interface information based on theinstalled SELinux Policy",
        "section": 8
    },
    {
        "command": "sepolicy-manpage",
        "description": "Use sepolicy manpage to generate manpages based on SELinuxPolicy.",
        "name": "sepolicy-manpage - Generate a man page based on the installedSELinux Policy",
        "section": 8
    },
    {
        "command": "sepolicy-network",
        "description": "Use sepolicy network to examine SELinux Policy and generatenetwork reports.",
        "name": "sepolicy-network - Examine the SELinux Policy and generate anetwork report",
        "section": 8
    },
    {
        "command": "sepolicy-transition",
        "description": "sepolicy transition will show all domains that a give SELinuxsource domain can transition to, including the entrypoint.If a target domain is given, sepolicy transition will examinepolicy for all transition paths from the source domain to thetarget domain, and will list the paths.If a transition ispossible, this tool will print out all transition paths from thesource domain to the target domain",
        "name": "sepolicy-transition - Examine the SELinux Policy and generate aprocess transition report",
        "section": 8
    },
    {
        "command": "sestatus",
        "description": "This manual page describes the sestatus program.This tool is used to get the status of a system running SELinux.It displays data about whether SELinux is enabled or disabled,location of key directories, and the loaded policy with itsstatus as shown in the example:> sestatusSELinux status:enabledSELinuxfs mount:/selinuxSELinux root directory:/etc/selinuxLoaded policy name:targetedCurrent mode:permissiveMode from config file:enforcingPolicy MLS status:enabledPolicy deny_unknown status:allowMemory protection checking:actual (secure)Max kernel policy version:26sestatus can also be used to display:-The security context of files and processes listed inthe /etc/sestatus.conf file. The format of this fileis described in sestatus.conf(5).-The status of booleans.",
        "name": "sestatus - SELinux status tool",
        "section": 8
    },
    {
        "command": "setarch",
        "description": "setarch modifies execution domains and process personality flags.The execution domains currently only affects the output of uname-m. For example, on an AMD64 system, running setarch i386 programwill cause program to see i686 instead of x86_64 as the machinetype. It can also be used to set various personality options. Thedefault program is /bin/sh.Since version 2.33 the arch command line argument is optional andsetarch may be used to change personality flags (ADDR_LIMIT_*,SHORT_INODE, etc) without modification of the execution domain.",
        "name": "setarch - change reported architecture in new program environmentand/or set personality flags",
        "section": 8
    },
    {
        "command": "setcap",
        "description": "In the absence of the -v (verify) option setcap sets thecapabilities of each specified filename to the capabilitiesspecified.The optional -n <rootuid> argument can be used to setthe file capability for use only in a user namespace with thisroot user ID owner. The -v option is used to verify that thespecified capabilities are currently associated with the file. If-v and -n are supplied, the -n <rootuid> argument is alsoverified.The capabilities are specified in the form described incap_from_text(3).The special capability string, '-', can be used to indicate thatcapabilities are read from the standard input. In such cases, thecapability set is terminated with a blank line.The special capability string, '-r', is used to remove acapability set from a file. Note, setting an empty capability setis not the same as removing it. An empty set can be used toguarantee a file is not executed with privilege in spite of thefact that the prevailing ambient+inheritable sets would otherwisebestow capabilities on executed binaries.The -q flag is used to make the program less verbose in itsoutput.",
        "name": "setcap - set file capabilities",
        "section": 8
    },
    {
        "command": "setenforce",
        "description": "Use Enforcing or 1 to put SELinux in enforcing mode.Use Permissive or 0 to put SELinux in permissive mode.If SELinux is disabled and you want to enable it, or SELinux isenabled and you want to disable it, please see selinux(8).",
        "name": "setenforce - modify the mode SELinux is running in",
        "section": 8
    },
    {
        "command": "setfiles",
        "description": "This manual page describes the setfiles program.This program is primarily used to initialize the security contextfields (extended attributes) on one or more filesystems (or partsof them).Usually it is initially run as part of the SELinuxinstallation process (a step commonly known as labeling).It can also be run at any other time to correct inconsistentlabels, to add support for newly-installed policy or, by usingthe -n option, to passively check whether the file contexts areall set as specified by the active policy (default behavior) orby some other policy (see the -c option).If a file object does not have a context, setfiles will write thedefault context to the file object's extended attributes. If afile object has a context, setfiles will only modify the typeportion of the security context.The -F option will force areplacement of the entire context.",
        "name": "setfiles - set SELinux file security contexts.",
        "section": 8
    },
    {
        "command": "setfont",
        "description": "The setfont command reads a font from the file font.new and loadsit into the EGA/VGA character generator, and optionally outputsthe previous font.It can also load various mapping tables andoutput the previous versions.If no args are given (or only the option -N for some number N),then a default (8xN) font is loaded (see below).One may giveseveral small fonts, all containing a Unicode table, and setfontwill combine them and load the union.Typical use:setfontLoad a default font.setfont drdos8x16Load a given font (here the 448-glyph drdos font).setfont cybercafe -u cybercafeLoad a given font that does not have a Unicode map andprovide one explicitly.setfont LatArCyrHeb-19 -m 8859-2Load a given font (here a 512-glyph font combining severalcharacter sets) and indicate that one's local characterset is ISO 8859-2.Note: if a font has more than 256 glyphs, only 8 out of 16 colorscan be used simultaneously. It can make console perception worse(loss of intensity and even some colors).",
        "name": "setfont - load EGA/VGA console screen font",
        "section": 8
    },
    {
        "command": "setkeycodes",
        "description": "The setkeycodes command reads its arguments two at a time, eachpair of arguments consisting of a scancode (given in hexadecimal)and a keycode (given in decimal). For each such pair, it tellsthe kernel keyboard driver to map the specified scancode to thespecified keycode.This command is useful only for people with slightly unusualkeyboards, that have a few keys which produce scancodes that thekernel does not recognize.",
        "name": "setkeycodes - load kernel scancode-to-keycode mapping tableentries",
        "section": 8
    },
    {
        "command": "setpci",
        "description": "setpci is a utility for querying and configuring PCI devices.All numbers are entered in hexadecimal notation.Root privileges are necessary for almost all operations,excluding reads of the standard header of the configuration spaceon some operating systems.Please see lspci(8) for details onaccess rights.",
        "name": "setpci - configure PCI devices",
        "section": 8
    },
    {
        "command": "setquota",
        "description": "setquota is a command line quota editor.The filesystem,user/group/project name and new quotas for this filesystem can bespecified on the command line. Note that if a number is given inthe place of a user/group/project name it is treated as anUID/GID/project ID.-r, --remoteEdit also remote quota use rpc.rquotad on remote server toset quota. This option is available only if quota toolswere compiled with enabled support for setting quotas overRPC.-m, --no-mixed-pathnamesCurrently, pathnames of NFSv4 mountpoints are sent withoutleading slash in the path.rpc.rquotad uses this torecognize NFSv4 mounts and properly prepend pseudoroot ofNFS filesystem to the path. If you specify this option,setquota will always send paths with a leading slash. Thiscan be useful for legacy reasons but be aware that quotaover RPC will stop working if you are using newrpc.rquotad.-F, --format=quotaformatPerform setting for specified format (ie. don't performformat autodetection).Possible format names are: vfsoldOriginal quota format with 16-bit UIDs / GIDs, vfsv0 Quotaformat with 32-bit UIDs / GIDs, 64-bit space usage, 32-bitinode usage and limits, vfsv1 Quota format with 64-bitquota limits and usage, rpc (quota over NFS), xfs (quotaon XFS filesystem)-u, --userSet user quotas for named user. This is the default.-g, --groupSet group quotas for named group.-P, --projectSet project quotas for named project.-p, --prototype=protonameUse quota settings of user, group or project protoname toset the quota for the named user, group or project.--always-resolveAlways try to translate user / group / project name to uid/ gid / project ID even if the name is composed of digitsonly.-b, --batchRead information to set from stdin (input format is nameblock-softlimit block-hardlimit inode-softlimit inode-hardlimit ). Empty lines and lines starting with # areignored.-c, --continue-batchIf parsing of an input line in batch mode fails, continuewith processing the next line.-t, --edit-periodSet grace times for users/groups/projects. Times block-grace and inode-grace are specified in seconds.-T, --edit-timesAlter times for individual user/group/project whensoftlimit is enforced. Times block-grace and inode-graceare specified in seconds or can be string 'unset'.-a, --allGo through all filesystems with quota in /etc/mtab andperform setting.block-softlimit and block-hardlimit are interpreted as multiplesof kibibyte (1024 bytes) blocks by default.Symbols K, M, G, andT can be appended to numeric value to express kibibytes,mebibytes, gibibytes, and tebibytes.inode-softlimit and inode-hardlimit are interpreted literally.Symbols k, m, g, and t can be appended to numeric value toexpress multiples of 10^3, 10^6, 10^9, and 10^12 inodes.To disable a quota, set the corresponding parameter to 0. Tochange quotas for several filesystems, invoke once for eachfilesystem.Only the super-user may edit quotas.",
        "name": "setquota - set disk quotas",
        "section": 8
    },
    {
        "command": "setsebool",
        "description": "setsebool sets the current state of a particular SELinux booleanor a list of booleans to a given value. The value may be 1 ortrue or on to enable the boolean, or 0 or false or off to disableit.Without the -P option, only the current boolean value isaffected; the boot-time default settings are not changed.If the -P option is given, all pending values are written to thepolicy file on disk. So they will be persistent across reboots.If the -N option is given, the policy on disk is not reloadedinto the kernel.If the -V option is given, verbose error messages will be printedfrom semanage libraries.",
        "name": "setsebool - set SELinux boolean value",
        "section": 8
    },
    {
        "command": "setvtrgb",
        "description": "The setvtrgb command takes a single argument, either the stringvga , or a path to a file containing the colors to be used by theLinux virtual terminals.You can choose to write the colors in decimal or hexadecimalformat, it will be detected on runtime.Decimal FILE format should be exactly 3 lines of 16 comma-separated decimal values for RED, GREEN, and BLUE.To seed a valid FILE :cat /sys/module/vt/parameters/default_{red,grn,blu} > FILEAnd then edit the values in FILEHexadecimal FILE format should be exactly 16 lines of hextriplets for RED, GREEN and BLUE, prefixed with a number sign(#). For example:#000000#AA0000#00AA00#AA5500#0000AA#AA00AA#00AAAA#AAAAAA#555555#FF5555#55FF55#FFFF55#5555FF#FF55FF#55FFFF#FFFFFF",
        "name": "setvtrgb - set the virtual terminal RGB colors",
        "section": 8
    },
    {
        "command": "seunshare",
        "description": "Run the executable within the specified context, using thealternate home directory and /tmp directory.The seunsharecommand unshares from the default namespace, then mounts thespecified homedir and tmpdir over the default homedir and /tmp.Finally it tells the kernel to execute the application under thespecified SELinux context.-h homedirAlternate homedir to be used by the application.Homedirmust be owned by the user.-t tmpdirUse alternate temporary directory to mount on /tmp.tmpdir must be owned by the user.-r runuserdirUse alternate temporary directory to mount onXDG_RUNTIME_DIR (/run/user/$UID). runuserdir must be ownedby the user.-C --capabilitiesAllow apps executed within the namespace to usecapabilities.Default is no capabilities.-k --killKill all processes with matching MCS level.-Z contextUse alternate SELinux context while running theexecutable.-vVerbose output",
        "name": "seunshare - Run cmd with alternate homedir, tmpdir and/or SELinuxcontext",
        "section": 8
    },
    {
        "command": "sfb",
        "description": "Stochastic Fair Blue is a classless qdisc to manage congestionbased on packet loss and link utilization history while trying toprevent non-responsive flows (i.e. flows that do not react tocongestion marking or dropped packets) from impacting performanceof responsive flows.Unlike RED, where the marking probabilityhas to be configured, BLUE tries to determine the ideal markingprobability automatically.",
        "name": "sfb - Stochastic Fair Blue",
        "section": 8
    },
    {
        "command": "sfdisk",
        "description": "sfdisk is a script-oriented tool for partitioning any blockdevice. It runs in interactive mode if executed on a terminal(stdin refers to a terminal).Since version 2.26 sfdisk supports MBR (DOS), GPT, SUN and SGIdisk labels, but no longer provides any functionality for CHS(Cylinder-Head-Sector) addressing. CHS has never been importantfor Linux, and this addressing concept does not make any sensefor new devices.sfdisk protects the first disk sector when create a new disklabel. The option --wipe always disables this protection. Notethat fdisk(8) and cfdisk(8) completely erase this area bydefault.sfdisk (since version 2.26) aligns the start and end ofpartitions to block-device I/O limits when relative sizes arespecified, when the default values are used or whenmultiplicative suffixes (e.g., MiB) are used for sizes. It ispossible that partition size will be optimized (reduced orenlarged) due to alignment if the start offset is specifiedexactly in sectors and partition size relative or bymultiplicative suffixes.The recommended way is not to specify start offsets at all andspecify partition size in MiB, GiB (or so). In this case sfdiskaligns all partitions to block-device I/O limits (or when I/Olimits are too small then to megabyte boundary to keep disklayout portable). If this default behaviour is unwanted (usuallyfor very small partitions) then specify offsets and sizes insectors. In this case sfdisk entirely follows specified numberswithout any optimization.sfdisk does not create the standard system partitions for SGI andSUN disk labels like fdisk(8) does. It is necessary to explicitlycreate all partitions including whole-disk system partitions.sfdisk uses BLKRRPART (reread partition table) ioctl to make surethat the device is not used by system or other tools (see also--no-reread). It\u2019s possible that this feature or another sfdiskactivity races with systemd-udevd(8). The recommended way how toavoid possible collisions is to use --lock option. The exclusivelock will cause systemd-udevd to skip the event handling on thedevice.The sfdisk prompt is only a hint for users and a displayedpartition number does not mean that the same partition tableentry will be created (if -N not specified), especially fortables with gaps.",
        "name": "sfdisk - display or manipulate a disk partition table",
        "section": 8
    },
    {
        "command": "sfq",
        "description": "Stochastic Fairness Queueing is a classless queueing disciplineavailable for traffic control with the tc(8) command.SFQ does not shape traffic but only schedules the transmission ofpackets, based on 'flows'.The goal is to ensure fairness sothat each flow is able to send data in turn, thus preventing anysingle flow from drowning out the rest.This may in fact have some effect in mitigating a Denial ofService attempt.SFQ is work-conserving and therefore always delivers a packet ifit has one available.",
        "name": "sfq - Stochastic Fairness Queueing",
        "section": 8
    },
    {
        "command": "sftp-server",
        "description": "sftp-server is a program that speaks the server side of SFTPprotocol to stdout and expects client requests from stdin.sftp-server is not intended to be called directly, but from sshd(8)using the Subsystem option.Command-line flags to sftp-server should be specified in theSubsystem declaration.See sshd_config(5) for more information.Valid options are:-d start_directorySpecifies an alternate starting directory for users.Thepathname may contain the following tokens that are expandedat runtime: %% is replaced by a literal '%', %d is replacedby the home directory of the user being authenticated, and%u is replaced by the username of that user.The defaultis to use the user's home directory.This option is usefulin conjunction with the sshd_config(5) ChrootDirectoryoption.-eCauses sftp-server to print logging information to stderrinstead of syslog for debugging.-f log_facilitySpecifies the facility code that is used when loggingmessages from sftp-server.The possible values are:DAEMON, USER, AUTH, LOCAL0, LOCAL1, LOCAL2, LOCAL3, LOCAL4,LOCAL5, LOCAL6, LOCAL7.The default is AUTH.-hDisplays sftp-server usage information.-l log_levelSpecifies which messages will be logged by sftp-server.The possible values are: QUIET, FATAL, ERROR, INFO,VERBOSE, DEBUG, DEBUG1, DEBUG2, and DEBUG3.INFO andVERBOSE log transactions that sftp-server performs onbehalf of the client.DEBUG and DEBUG1 are equivalent.DEBUG2 and DEBUG3 each specify higher levels of debuggingoutput.The default is ERROR.-P denied_requestsSpecifies a comma-separated list of SFTP protocol requeststhat are banned by the server.sftp-server will reply toany denied request with a failure.The -Q flag can be usedto determine the supported request types.If both deniedand allowed lists are specified, then the denied list isapplied before the allowed list.-p allowed_requestsSpecifies a comma-separated list of SFTP protocol requeststhat are permitted by the server.All request types thatare not on the allowed list will be logged and replied towith a failure message.Care must be taken when using this feature to ensure thatrequests made implicitly by SFTP clients are permitted.-Q protocol_featureQueries protocol features supported by sftp-server.Atpresent the only feature that may be queried is \u201crequests\u201d,which may be used to deny or allow specific requests (flags-P and -p respectively).-RPlaces this instance of sftp-server into a read-only mode.Attempts to open files for writing, as well as otheroperations that change the state of the filesystem, will bedenied.-u umaskSets an explicit umask(2) to be applied to newly-createdfiles and directories, instead of the user's default mask.On some systems, sftp-server must be able to access /dev/log forlogging to work, and use of sftp-server in a chroot configurationtherefore requires that syslogd(8) establish a logging socketinside the chroot directory.",
        "name": "sftp-server \u2014 OpenSSH SFTP server subsystem",
        "section": 8
    },
    {
        "command": "showconsolefont",
        "description": "The showconsolefont command outputs the current console font tostdout. The option -v prints additional information, while theoption -V prints the program version number. The option -idoesn't print out the font table, just shows ROWSxCOLSxCOUNT andexits.On Linux 2.6.1 and later, the option -C allows one toindicate the console involved. Its argument is a pathname.",
        "name": "showconsolefont - Show the current EGA/VGA console screen font",
        "section": 8
    },
    {
        "command": "showmount",
        "description": "showmount queries the mount daemon on a remote host forinformation about the state of the NFS server on that machine.With no options showmount lists the set of clients who aremounting from that host.The output from showmount is designedto appear as though it were processed through ``sort -u''.",
        "name": "showmount - show mount information for an NFS server",
        "section": 8
    },
    {
        "command": "shutdown",
        "description": "shutdown may be used to halt, power off, or reboot the machine.The first argument may be a time string (which is usually \"now\").Optionally, this may be followed by a wall message to be sent toall logged-in users before going down.The time string may either be in the format \"hh:mm\" forhour/minutes specifying the time to execute the shutdown at,specified in 24h clock format. Alternatively it may be in thesyntax \"+m\" referring to the specified number of minutes m fromnow.\"now\" is an alias for \"+0\", i.e. for triggering animmediate shutdown. If no time argument is specified, \"+1\" isimplied.Note that to specify a wall message you must specify a timeargument, too.If the time argument is used, 5 minutes before the system goesdown the /run/nologin file is created to ensure that furtherlogins shall not be allowed.",
        "name": "shutdown - Halt, power off or reboot the machine",
        "section": 8
    },
    {
        "command": "simple",
        "description": "This is a pedagogical example rather than an actually usefulaction. Upon every access, it prints the given STRING which maybe of arbitrary length.",
        "name": "simple - basic example action",
        "section": 8
    },
    {
        "command": "skbedit",
        "description": "The skbedit action allows one to change a packet's associatedmeta data. It complements the pedit action, which in turn allowsone to change parts of the packet data itself.The most unique feature of skbedit is its ability to decide overwhich queue of an interface with multiple transmit queues thepacket is to be sent out. The number of available transmit queuesis reflected by sysfs entries within/sys/class/net/<interface>/queues with name tx-N (where N is theactual queue number).",
        "name": "skbedit - SKB editing action",
        "section": 8
    },
    {
        "command": "skbmod",
        "description": "The skbmod action is intended as a usability upgrade to theexisting pedit action. Instead of having to manually edit 8-,16-, or 32-bit chunks of an ethernet header, skbmod allowscomplete substitution of supported elements.Action must be oneof set, swap and ecn.set and swap only affect Ethernet packets,while ecn only affects IP packets.",
        "name": "skbmod - user-friendly packet editor action",
        "section": 8
    },
    {
        "command": "skbprio",
        "description": "SKB Priority Queue is a queueing discipline intended toprioritize the most important packets during a denial-of-service( DoS ) attack. The priority of a packet is given byskb->priority , where a higher value places the packet closer tothe exit of the queue. When the queue is full, the lowestpriority packet in the queue is dropped to make room for thepacket to be added if it has higher priority. If the packet to beadded has lower priority than all packets in the queue, it isdropped.Without SKB priority queue, queue length limits must be imposedon individual sub-queues, and there is no straightforward way toenforce a global queue length limit across all priorities.SKBprio queue enforces a global queue length limit while notrestricting the lengths of individual sub-queues.While SKB Priority Queue is agnostic to how skb->priority isassigned. A typical use case is to copy the 6-bit DS field ofIPv4 and IPv6 packets using tc-skbedit(8).If skb->priority isgreater or equal to 64, the priority is assumed to be 63.Priorities less than 64 are taken at face value.SKB Priority Queue enables routers to locally decide whichpackets to drop under a DoS attack.Priorities should beassigned to packets such that the higher the priority, the moreexpected behavior a source shows.So sources have an incentiveto play by the rules.",
        "name": "skbprio - SKB Priority Queue",
        "section": 8
    },
    {
        "command": "slapacl",
        "description": "slapacl is used to check the behavior of slapd(8) by verifyingaccess to directory data according to the access control listdirectives defined in its configuration.It opens theslapd.conf(5) configuration file or the slapd-config(5) backend,reads in the access/olcAccess directives, and then parses theattr list given on the command-line; if none is given, access tothe entry pseudo-attribute is tested.",
        "name": "slapacl - Check access to a list of attributes.",
        "section": 8
    },
    {
        "command": "slapadd",
        "description": "Slapadd is used to add entries specified in LDAP DirectoryInterchange Format (LDIF) to a slapd(8) database.It opens thegiven database determined by the database number or suffix andadds entries corresponding to the provided LDIF to the database.Databases configured as subordinate of this one are also updated,unless -g is specified.The LDIF input is read from standardinput or the specified file.All files eventually created by slapadd will belong to theidentity slapadd is run as, so make sure you either run slapaddwith the same identity slapd(8) will be run as (see option -u inslapd(8)), or change file ownership before running slapd(8).Note: slapadd will also perform the relevant indexing whilstadding the database if any are configured. For specific details,please see slapindex(8).",
        "name": "slapadd - Add entries to a SLAPD database",
        "section": 8
    },
    {
        "command": "slapauth",
        "description": "Slapauth is used to check the behavior of the slapd in mappingidentities for authentication and authorization purposes, asspecified in slapd.conf(5).It opens the slapd.conf(5)configuration file or the slapd-config(5) backend, reads in theauthz-policy/olcAuthzPolicy and authz-regexp/olcAuthzRegexpdirectives, and then parses the ID list given on the command-line.",
        "name": "slapauth - Check a list of string-represented IDs for LDAPauthc/authz",
        "section": 8
    },
    {
        "command": "slapcat",
        "description": "Slapcat is used to generate an LDAP Directory Interchange Format(LDIF) output based upon the contents of a slapd(8) database.Itopens the given database determined by the database number orsuffix and writes the corresponding LDIF to standard output orthe specified file.Databases configured as subordinate of thisone are also output, unless -g is specified.The entry records are presented in database order, not superiorfirst order.The entry records will include all (user andoperational) attributes stored in the database.The entryrecords will not include dynamically generated attributes (suchas subschemaSubentry).The output of slapcat is intended to be used as input toslapadd(8).The output of slapcat cannot generally be used asinput to ldapadd(1) or other LDAP clients without first editingthe output.This editing would normally include reordering therecords into superior first order and removing no-user-modification operational attributes.",
        "name": "slapcat - SLAPD database to LDIF utility",
        "section": 8
    },
    {
        "command": "slapd",
        "description": "Slapd is the stand-alone LDAP daemon. It listens for LDAPconnections on any number of ports (default 389), responding tothe LDAP operations it receives over these connections.slapd istypically invoked at boot time, usually out of /etc/rc.local.Upon startup, slapd normally forks and disassociates itself fromthe invoking tty.If configured in the config file (or configdirectory), the slapd process will print its process ID (seegetpid(2)) to a .pid file, as well as the command line optionsduring invocation to an .args file (see slapd.conf(5)).If the-d flag is given, even with a zero argument, slapd will not forkand disassociate from the invoking tty.See the \"OpenLDAP Administrator's Guide\" for more details onslapd.",
        "name": "slapd - Stand-alone LDAP Daemon",
        "section": 8
    },
    {
        "command": "slapdn",
        "description": "Slapdn is used to check the conformance of a DN based on theschema defined in slapd(8) and that loaded via slapd.conf(5).Itopens the slapd.conf(5) configuration file or the slapd-config(5) backend, reads in the schema definitions, and then parses theDN list given on the command-line.",
        "name": "slapdn - Check a list of string-represented LDAP DNs based onschema syntax",
        "section": 8
    },
    {
        "command": "slapindex",
        "description": "Slapindex is used to regenerate slapd(8) indices based upon thecurrent contents of a database.It opens the given databasedetermined by the database number or suffix and updates theindices for all values of all attributes of all entries. If alist of specific attributes is provided on the command line, onlythe indices for those attributes will be regenerated.Databasesconfigured as subordinate of this one are also re-indexed, unless-g is specified.All files eventually created by slapindex will belong to theidentity slapindex is run as, so make sure you either runslapindex with the same identity slapd(8) will be run as (seeoption -u in slapd(8)), or change file ownership before runningslapd(8).",
        "name": "slapindex - Reindex entries in a SLAPD database",
        "section": 8
    },
    {
        "command": "slapmodify",
        "description": "Slapmodify is used to apply modifications specified in LDAPDirectory Interchange Format (LDIF) to a slapd(8) database.Itopens the given database determined by the database number orsuffix and performs modifications corresponding to the providedLDIF to the database.Databases configured as subordinate ofthis one are also updated, unless -g is specified.The LDIFinput is read from standard input or the specified file.All files eventually created by slapmodify will belong to theidentity slapmodify is run as, so make sure you either runslapmodify with the same identity slapd(8) will be run as (seeoption -u in slapd(8)), or change file ownership before runningslapd(8).Note: slapmodify will also perform the relevant indexing whilstmodifying the database if any are configured. For specificdetails, please see slapindex(8).",
        "name": "slapmodify - Modify entries in a SLAPD database",
        "section": 8
    },
    {
        "command": "slappasswd",
        "description": "Slappasswd is used to generate an userPassword value suitable foruse with ldapmodify(1), slapd.conf(5) rootpw configurationdirective or the slapd-config(5) olcRootPW configurationdirective.",
        "name": "slappasswd - OpenLDAP password utility",
        "section": 8
    },
    {
        "command": "slapschema",
        "description": "Slapschema is used to check schema compliance of the contents ofa slapd(8) database.It opens the given database determined bythe database number or suffix and checks the compliance of itscontents with the corresponding schema. Errors are written tostandard output or the specified file.Databases configured assubordinate of this one are also output, unless -g is specified.Administrators may need to modify existing schema items,including adding new required attributes to objectClasses,removing existing required or allowed attributes fromobjectClasses, entirely removing objectClasses, or any otherchange that may result in making perfectly valid entries nolonger compliant with the modified schema.The execution of theslapschema tool after modifying the schema can point outinconsistencies that would otherwise surface only wheninconsistent entries need to be modified.The entry records are checked in database order, not superiorfirst order.The entry records will be checked considering all(user and operational) attributes stored in the database.Dynamically generated attributes (such as subschemaSubentry) willnot be considered.",
        "name": "slapschema - SLAPD in-database schema checking utility",
        "section": 8
    },
    {
        "command": "slaptest",
        "description": "Slaptest is used to check the conformance of the slapd(8)configuration.It opens the slapd.conf(5) configuration file orthe slapd-config(5) backend, and parses it according to thegeneral and the backend-specific rules, checking its sanity.",
        "name": "slaptest - Check the suitability of the OpenLDAP slapdconfiguration",
        "section": 8
    },
    {
        "command": "slattach",
        "description": "Slattach is a tiny little program that can be used to put anormal terminal (\"serial\") line into one of several \"network\"modes, thus allowing you to use it for point-to-point links toother computers.",
        "name": "slattach - attach a network interface to a serial line",
        "section": 8
    },
    {
        "command": "sln",
        "description": "The sln program creates symbolic links.Unlike the ln(1)program, it is statically linked.This means that if for somereason the dynamic linker is not working, sln can be used to makesymbolic links to dynamic libraries.The command line has two forms.In the first form, it createsdest as a new symbolic link to source.In the second form, filelist is a list of space-separatedpathname pairs, and the effect is as if sln was executed once foreach line of the file, with the two pathnames as the arguments.The sln program supports no command-line options.",
        "name": "sln - create symbolic links",
        "section": 8
    },
    {
        "command": "sm-notify",
        "description": "File locks are not part of persistent file system state.Lockstate is thus lost when a host reboots.Network file systems must also detect when lock state is lostbecause a remote host has rebooted.After an NFS client reboots,an NFS server must release all file locks held by applicationsthat were running on that client.After a server reboots, aclient must remind the server of file locks held by applicationsrunning on that client.For NFS version 2 and version 3, the Network Status Monitorprotocol (or NSM for short) is used to notify NFS peers ofreboots.On Linux, two separate user-space components constitutethe NSM service:sm-notifyA helper program that notifies NFS peers after the localsystem rebootsrpc.statdA daemon that listens for reboot notifications from otherhosts, and manages the list of hosts to be notified whenthe local system rebootsThe local NFS lock manager alerts its local rpc.statd of eachremote peer that should be monitored.When the local systemreboots, the sm-notify command notifies the NSM service onmonitored peers of the reboot.When a remote reboots, that peernotifies the local rpc.statd, which in turn passes the rebootnotification back to the local NFS lock manager.",
        "name": "sm-notify - send reboot notifications to NFS peers",
        "section": 8
    },
    {
        "command": "smtpctl",
        "description": "The smtpctl program controls smtpd(8).Commands may be abbreviatedto the minimum unambiguous prefix; for example, sh ro for showroutes.The mailq command is provided for compatibility with other MTAs andis simply a shortcut for show queue.The following commands are available:discover envelope-id | message-idSchedule a single envelope, or all envelopes with the samemessage ID that were manually moved to the queue.encrypt [string]Encrypt the password string to a representation suitablefor user credentials and print it to the standard output.If string is not provided, cleartext passwords are readfrom standard input.It is advised to avoid providing the password as aparameter as it will be visible from top(1) and ps(1)output.log briefDisable verbose debug logging.log verboseEnable verbose debug logging.monitorDisplay updates of some smtpd(8) internal counters in onesecond intervals.Each line reports the increment of allcounters since the last update, except for some counterswhich are always absolute values.The first line reportsthe current value of each counter.The fields are:\u2022Current number of active SMTP clients (absolute value).\u2022New SMTP clients.\u2022Disconnected clients.\u2022Current number of envelopes in the queue (absolutevalue).\u2022Newly enqueued envelopes.\u2022Dequeued envelopes.\u2022Successful deliveries.\u2022Temporary failures.\u2022Permanent failures.\u2022Message loops.\u2022Expired envelopes.\u2022Envelopes removed by the administrator.\u2022Generated bounces.pause envelope envelope-id | message-id | allTemporarily suspend scheduling for the envelope with thegiven ID, envelopes with the given message ID, or allenvelopes.pause mdaTemporarily stop deliveries to local users.pause mtaTemporarily stop relaying and deliveries to remote users.pause smtpTemporarily stop accepting incoming sessions.profile subsystemEnables real-time profiling of subsystem.Supportedsubsystems are:\u2022queue, to profile cost of queue IO\u2022imsg, to profile cost of event handlersremove envelope-id | message-id | allRemove a single envelope, envelopes with the given messageID, or all envelopes.resume envelope envelope-id | message-id | allResume scheduling for the envelope with the given ID,envelopes with the given message ID, or all envelopes.resume mdaResume deliveries to local users.resume mtaResume relaying and deliveries to remote users.resume route route-idResume routing on disabled route route-id.resume smtpResume accepting incoming sessions.schedule envelope-id | message-id | allMark as ready for immediate delivery a single envelope,envelopes with the given message ID, or all envelopes.show envelope envelope-idDisplay envelope content for the given ID.show hostsDisplay the list of known remote MX hosts.For each ofthem, it shows the IP address, the canonical hostname, areference count, the number of active connections to thishost, and the elapsed time since the last connection.show hoststatsDisplay status of last delivery for domains that have beenactive in the last 4 hours.It consists of the followingfields, separated by a \"|\":\u2022Domain.\u2022UNIX timestamp of last delivery.\u2022Status of last delivery.show message envelope-idDisplay message content for the given ID.show queueDisplay information concerning envelopes that are currentlyin the queue.Each line of output describes a singleenvelope.It consists of the following fields, separatedby a \"|\":\u2022Envelope ID.\u2022Address family of the client which enqueued the mail.\u2022Type of delivery: one of \"mta\", \"mda\" or \"bounce\".\u2022Various flags on the envelope.\u2022Sender address (return path).\u2022The original recipient address.\u2022The destination address.\u2022Time of creation.\u2022Time of expiration.\u2022Time of last delivery or relaying attempt.\u2022Number of delivery or relaying attempts.\u2022Current runstate: either \"pending\" or \"inflight\" ifsmtpd(8) is running, or \"offline\" otherwise.\u2022Delay in seconds before the next attempt if pending, ortime elapsed if currently running.This field is blankif smtpd(8) is not running.\u2022Error string for the last failed delivery or relayattempt.show relaysDisplay the list of currently active relays and associatedconnectors.For each relay, it shows a number of countersand information on its internal state on a single line.Then comes the list of connectors (source addresses toconnect from for this relay).show routesDisplay status of routes currently known by smtpd(8).Eachline consists of a route number, a source address, adestination address, a set of flags, the number ofconnections on this route, the current penalty level whichdetermines the amount of time the route is disabled if anerror occurs, and the delay before it gets reactivated.The following flags are defined:DThe route is currently disabled.NThe route is new.No SMTP session has been establishedyet.QThe route has a timeout registered to lower its penaltylevel and possibly reactivate or discard it.show statsDisplays runtime statistics concerning smtpd(8).show statusShows if MTA, MDA and SMTP systems are currently running orpaused.spf walkRecursively look up SPF records for the domains read fromstdin.For example:$ smtpctl spf walk < domains.txtSPF records may contain macros which cannot be included ina static list and must be resolved dynamically atconnection time.spf walk cannot provide full results inthese cases.trace subsystemEnables real-time tracing of subsystem.Supportedsubsystems are:\u2022imsg\u2022io\u2022smtp (incoming sessions)\u2022filters\u2022mta (outgoing sessions)\u2022bounce\u2022scheduler\u2022expand (aliases/virtual/forward expansion)\u2022lookup (user/credentials lookups)\u2022stat\u2022rules (matched by incoming sessions)\u2022mproc\u2022allunprofile subsystemDisables real-time profiling of subsystem.untrace subsystemDisables real-time tracing of subsystem.update table nameUpdates the contents of table name, for tables using the\u201cfile\u201d backend.When smtpd receives a message, it generates a message-id for themessage, and one envelope-id per recipient.The message-id is a32-bit random identifier that is guaranteed to be unique on thehost system.The envelope-id is a 64-bit unique identifier thatencodes the message-id in the 32 upper bits and a random envelopeidentifier in the 32 lower bits.A command which specifies a message-id applies to all recipients ofa message; a command which specifies an envelope-id applies to aspecific recipient of a message.",
        "name": "smtpctl, mailq \u2014 control the SMTP daemon",
        "section": 8
    },
    {
        "command": "smtpd",
        "description": "smtpd is a Simple Mail Transfer Protocol (SMTP) daemon which can beused as a machine's primary mail system.smtpd can listen on anetwork interface and handle SMTP transactions; it can also be fedmessages through the standard sendmail(8) interface.It can relaymessages through remote mail transfer agents or store them locallyusing either the mbox or maildir format.This implementationsupports SMTP as defined by RFC 5321 as well as several extensions.A running smtpd can be controlled through smtpctl(8).The options are as follows:-D macro=valueDefine macro to be set to value on the command line.Overrides the definition of macro in the configurationfile.-dDo not daemonize.If this option is specified, smtpd willrun in the foreground and log to stderr.-FDo not daemonize.If this option is specified, smtpd willrun in the foreground and log to syslogd(8).-f fileSpecify an alternative configuration file.-hDisplay version and usage.-nConfigtest mode.Only check the configuration file forvalidity.-P systemPause a specific subsystem at startup.Normal operationcan be resumed using smtpctl(8).This option can be usedmultiple times.The accepted values are:mdaDo not schedule local deliveries.mtaDo not schedule remote transfers.smtpDo not listen on SMTP sockets.-T traceEnables real-time tracing at startup.Normal operation canbe resumed using smtpctl(8).This option can be usedmultiple times.The accepted values are:\u2022imsg\u2022io\u2022smtp (incoming sessions)\u2022filters\u2022transfer (outgoing sessions)\u2022bounce\u2022scheduler\u2022expand (aliases/virtual/forward expansion)\u2022lookup (user/credentials lookups)\u2022stat\u2022rules (matched by incoming sessions)\u2022mproc\u2022all-vProduce more verbose output.",
        "name": "smtpd \u2014 Simple Mail Transfer Protocol (SMTP) daemon",
        "section": 8
    },
    {
        "command": "snmp",
        "description": "The DEPRECATED CUPS SNMP backend provides legacy discovery andidentification of network printers using SNMPv1.When used fordiscovery through the scheduler, the backend will list allprinters that respond to a broadcast SNMPv1 query with the\"public\" community name.Additional queries are then sent toprinters that respond in order to determine the correct deviceURI, make and model, and other information needed for printing.In the first form, the SNMP backend is run directly by the userto look up the device URI and other information when you have anIP address or hostname.This can be used for programs that needto configure print queues where the user has supplied an addressbut nothing else.In the second form, the SNMP backend is run indirectly using thelpinfo(8) command.The output provides all printers detected viaSNMP on the configured broadcast addresses.Note: no broadcastaddresses are configured by default.",
        "name": "snmp - cups snmp backend (deprecated)",
        "section": 8
    },
    {
        "command": "srp-entry",
        "description": "This utility generates an entry suitable for use in the/etc/ppp/srp-secrets file on a PPP EAP SRP-SHA1 authenticator(\"server\").This file has the same basic layout as the otherpppd(8) authentication files, /etc/ppp/pap-secrets and/etc/ppp/chap-secrets.Thus, the entry generated has at leastfour main fields separated by spaces.The first field is theauthenticatee (\"client\") name.The second is the server name.The third is the secret.The fourth is the allowed (or assigned)IP address for the client, and defaults to \"*\".Additionalfields can contain additional IP addresses or pppd options; seepppd(8) for details.The third field has three subfields, separated by colons.Thefirst subfield is the index of the modulus and generator fromSRP's /etc/tpasswd.conf.The special value 0 is used torepresent the well-known modulus and generator specified in theEAP SRP-SHA1 draft.The second subfield is the passwordvalidator.The third is the password salt.These latter twovalues are encoded in base64 notation.",
        "name": "srp-entry - Generate a SRP-SHA1 Server Entry",
        "section": 8
    },
    {
        "command": "ss",
        "description": "ss is used to dump socket statistics. It allows showinginformation similar to netstat.It can display more TCP andstate information than other tools.",
        "name": "ss - another utility to investigate sockets",
        "section": 8
    },
    {
        "command": "ssh-keysign",
        "description": "ssh-keysign is used by ssh(1) to access the local host keys andgenerate the digital signature required during host-basedauthentication.ssh-keysign is disabled by default and can only be enabled in theglobal client configuration file /etc/ssh/ssh_config by settingEnableSSHKeysign to \u201cyes\u201d.ssh-keysign is not intended to be invoked by the user, but fromssh(1).See ssh(1) and sshd(8) for more information about host-based authentication.",
        "name": "ssh-keysign \u2014 OpenSSH helper for host-based authentication",
        "section": 8
    },
    {
        "command": "ssh-pkcs11-helper",
        "description": "ssh-pkcs11-helper is used by ssh(1), ssh-agent(1), andssh-keygen(1) to access keys provided by a PKCS#11 token.ssh-pkcs11-helper is not intended to be invoked directly by theuser.A single option is supported:-vVerbose mode.Causes ssh-pkcs11-helper to print debuggingmessages about its progress.This is helpful in debuggingproblems.Multiple -v options increase the verbosity.Themaximum is 3.Note that ssh(1), ssh-agent(1), and ssh-keygen(1) willautomatically pass the -v flag to ssh-pkcs11-helper whenthey have themselves been placed in debug mode.",
        "name": "ssh-pkcs11-helper \u2014 OpenSSH helper for PKCS#11 support",
        "section": 8
    },
    {
        "command": "ssh-sk-helper",
        "description": "ssh-sk-helper is used by ssh(1), ssh-agent(1), and ssh-keygen(1) toaccess keys provided by a FIDO authenticator.ssh-sk-helper is not intended to be invoked directly by the user.A single option is supported:-vVerbose mode.Causes ssh-sk-helper to print debuggingmessages about its progress.This is helpful in debuggingproblems.Multiple -v options increase the verbosity.Themaximum is 3.Note that ssh(1), ssh-agent(1), and ssh-keygen(1) willautomatically pass the -v flag to ssh-sk-helper when theyhave themselves been placed in debug mode.",
        "name": "ssh-sk-helper \u2014 OpenSSH helper for FIDO authenticator support",
        "section": 8
    },
    {
        "command": "sshd",
        "description": "sshd (OpenSSH Daemon) is the daemon program for ssh(1).Itprovides secure encrypted communications between two untrustedhosts over an insecure network.sshd listens for connections from clients.It is normally startedat boot from /etc/rc.It forks a new daemon for each incomingconnection.The forked daemons handle key exchange, encryption,authentication, command execution, and data exchange.sshd can be configured using command-line options or aconfiguration file (by default sshd_config(5)); command-lineoptions override values specified in the configuration file.sshdrereads its configuration file when it receives a hangup signal,SIGHUP, by executing itself with the name and options it wasstarted with, e.g. /usr/sbin/sshd.The options are as follows:-4Forces sshd to use IPv4 addresses only.-6Forces sshd to use IPv6 addresses only.-C connection_specSpecify the connection parameters to use for the -Textended test mode.If provided, any Match directives inthe configuration file that would apply are applied beforethe configuration is written to standard output.Theconnection parameters are supplied as keyword=value pairsand may be supplied in any order, either with multiple -Coptions or as a comma-separated list.The keywords are\u201caddr\u201d, \u201cuser\u201d, \u201chost\u201d, \u201claddr\u201d, \u201clport\u201d, and \u201crdomain\u201d andcorrespond to source address, user, resolved source hostname, local address, local port number and routing domainrespectively.-c host_certificate_fileSpecifies a path to a certificate file to identify sshdduring key exchange.The certificate file must match ahost key file specified using the -h option or the HostKeyconfiguration directive.-DWhen this option is specified, sshd will not detach anddoes not become a daemon.This allows easy monitoring ofsshd.-dDebug mode.The server sends verbose debug output tostandard error, and does not put itself in the background.The server also will not fork(2) and will only process oneconnection.This option is only intended for debugging forthe server.Multiple -d options increase the debugginglevel.Maximum is 3.-E log_fileAppend debug logs to log_file instead of the system log.-eWrite debug logs to standard error instead of the systemlog.-f config_fileSpecifies the name of the configuration file.The defaultis /etc/ssh/sshd_config.sshd refuses to start if there isno configuration file.-GParse and print configuration file.Check the validity ofthe configuration file, output the effective configurationto stdout and then exit.Optionally, Match rules may beapplied by specifying the connection parameters using oneor more -C options.-g login_grace_timeGives the grace time for clients to authenticate themselves(default 120 seconds).If the client fails to authenticatethe user within this many seconds, the server disconnectsand exits.A value of zero indicates no limit.-h host_key_fileSpecifies a file from which a host key is read.Thisoption must be given if sshd is not run as root (as thenormal host key files are normally not readable by anyonebut root).The default is /etc/ssh/ssh_host_ecdsa_key,/etc/ssh/ssh_host_ed25519_key and/etc/ssh/ssh_host_rsa_key.It is possible to have multiplehost key files for the different host key algorithms.-iSpecifies that sshd is being run from inetd(8).-o optionCan be used to give options in the format used in theconfiguration file.This is useful for specifying optionsfor which there is no separate command-line flag.For fulldetails of the options, and their values, seesshd_config(5).-p portSpecifies the port on which the server listens forconnections (default 22).Multiple port options arepermitted.Ports specified in the configuration file withthe Port option are ignored when a command-line port isspecified.Ports specified using the ListenAddress optionoverride command-line ports.-qQuiet mode.Nothing is sent to the system log.Normallythe beginning, authentication, and termination of eachconnection is logged.-TExtended test mode.Check the validity of theconfiguration file, output the effective configuration tostdout and then exit.Optionally, Match rules may beapplied by specifying the connection parameters using oneor more -C options.This is similar to the -G flag, but itincludes the additional testing performed by the -t flag.-tTest mode.Only check the validity of the configurationfile and sanity of the keys.This is useful for updatingsshd reliably as configuration options may change.-u lenThis option is used to specify the size of the field in theutmp structure that holds the remote host name.If theresolved host name is longer than len, the dotted decimalvalue will be used instead.This allows hosts with verylong host names that overflow this field to still beuniquely identified.Specifying -u0 indicates that onlydotted decimal addresses should be put into the utmp file.-u0 may also be used to prevent sshd from making DNSrequests unless the authentication mechanism orconfiguration requires it.Authentication mechanisms thatmay require DNS include HostbasedAuthentication and using afrom=\"pattern-list\" option in a key file.Configurationoptions that require DNS include using a USER@HOST patternin AllowUsers or DenyUsers.-VDisplay the version number and exit.",
        "name": "sshd \u2014 OpenSSH daemon",
        "section": 8
    },
    {
        "command": "stap-exporter",
        "description": "stap-exporter runs a set of systemtap scripts and relays theirprocfs outputs to remote HTTP clients on demand.This makessystemtap scripts directly usable as individual prometheusexporters.This is assisted by a set of macros provided in theprometheus.stpm tapset file.",
        "name": "stap-exporter - systemtap-prometheus interoperation mechanism",
        "section": 8
    },
    {
        "command": "stap-server",
        "description": "A systemtap compile server listens for connections from stapclients on a secure SSL network port and accepts requests to runthe stap front end. Each server advertises its presence andconfiguration on the local network using mDNS (avahi) allowingfor automatic detection by clients.The stap-server script aims to provide:\u2022management of systemtap compile servers as a service.\u2022convenient control over configured servers and individual(ad-hoc) servers.",
        "name": "stap-server - systemtap compile server management",
        "section": 8
    },
    {
        "command": "stapbpf",
        "description": "The stapbpf program is the BPF back-end of the Systemtap tool.It expects a bpf-elf file produced by the front-end stap tool,when run with --runtime=bpf.Splitting the systemtap tool into a front-end and a back-endallows a user to compile a systemtap script on a developmentmachine that has the debugging information needed to compile thescript and then transfer the resulting shared object to aproduction machine that doesn't have any development tools ordebugging information installed.Please refer to stappaths(7) for the version number, or run$ rpm -q systemtap # (for Fedora/RHEL)$ apt-get -v systemtap # (for Ubuntu)",
        "name": "stapbpf - systemtap bpf runtime",
        "section": 8
    },
    {
        "command": "stapdyn",
        "description": "The stapdyn program is the dyninst back-end of the Systemtaptool.It expects a shared library produced by the front-end staptool, when run with --dyninst.Splitting the systemtap tool into a front-end and a back-endallows a user to compile a systemtap script on a developmentmachine that has the debugging information (need to compile thescript) and then transfer the resulting shared object to aproduction machine that doesn't have any development tools ordebugging information installed.Please refer to stappaths (7) for the version number, or run rpm-q systemtap (fedora/red hat) apt-get -v systemtap (ubuntu)",
        "name": "stapdyn - systemtap dyninst runtime",
        "section": 8
    },
    {
        "command": "staprun",
        "description": "The staprun program is the back-end of the Systemtap tool.Itexpects a kernel module produced by the front-end stap tool.Splitting the systemtap tool into a front-end and a back-endallows a user to compile a systemtap script on a developmentmachine that has the kernel development tools (needed to compilethe script) and then transfer the resulting kernel module to aproduction machine that doesn't have any development toolsinstalled.Please refer to stappaths (7) for the version number, or run rpm-q systemtap (fedora/red hat) apt-get -v systemtap (ubuntu)",
        "name": "staprun - systemtap runtime",
        "section": 8
    },
    {
        "command": "stapsh",
        "description": "The stapsh executable is used by the stap --remote functionality,as a wrapper shell on the remote machines.It is not intended tobe run directly by users.",
        "name": "stapsh",
        "section": 8
    },
    {
        "command": "start-stop-daemon",
        "description": "start-stop-daemon is used to control the creation and terminationof system-level processes.Using one of the matching options,start-stop-daemon can be configured to find existing instances ofa running process.Note: Unless --pid or --pidfile are specified, start-stop-daemonbehaves similar to killall(1).start-stop-daemon will scan theprocess table looking for any processes which match the processname, parent pid, uid, and/or gid (if specified). Any matchingprocess will prevent --start from starting the daemon. Allmatching processes will be sent the TERM signal (or the onespecified via --signal or --retry) if --stop is specified. Fordaemons which have long-lived children which need to live througha --stop, you must specify a pidfile.",
        "name": "start-stop-daemon - start and stop system daemon programs",
        "section": 8
    },
    {
        "command": "statd",
        "description": "File locks are not part of persistent file system state.Lockstate is thus lost when a host reboots.Network file systems must also detect when lock state is lostbecause a remote host has rebooted.After an NFS client reboots,an NFS server must release all file locks held by applicationsthat were running on that client.After a server reboots, aclient must remind the server of file locks held by applicationsrunning on that client.For NFS version 2 [RFC1094] and NFS version 3 [RFC1813], theNetwork Status Monitor protocol (or NSM for short) is used tonotify NFS peers of reboots.On Linux, two separate user-spacecomponents constitute the NSM service:rpc.statdA daemon that listens for reboot notifications from otherhosts, and manages the list of hosts to be notified whenthe local system rebootssm-notifyA helper program that notifies NFS peers after the localsystem rebootsThe local NFS lock manager alerts its local rpc.statd of eachremote peer that should be monitored.When the local systemreboots, the sm-notify command notifies the NSM service onmonitored peers of the reboot.When a remote reboots, that peernotifies the local rpc.statd, which in turn passes the rebootnotification back to the local NFS lock manager.",
        "name": "rpc.statd - NSM service daemon",
        "section": 8
    },
    {
        "command": "sudo",
        "description": "sudo allows a permitted user to execute a command as the superuseror another user, as specified by the security policy.The invokinguser's real (not effective) user-ID is used to determine the username with which to query the security policy.sudo supports a plugin architecture for security policies,auditing, and input/output logging.Third parties can develop anddistribute their own plugins to work seamlessly with the sudofront-end.The default security policy is sudoers, which isconfigured via the file /etc/sudoers, or via LDAP.See the Pluginssection for more information.The security policy determines what privileges, if any, a user hasto run sudo.The policy may require that users authenticatethemselves with a password or another authentication mechanism.Ifauthentication is required, sudo will exit if the user's passwordis not entered within a configurable time limit.This limit ispolicy-specific; the default password prompt timeout for thesudoers security policy is 5 minutes.Security policies may support credential caching to allow the userto run sudo again for a period of time without requiringauthentication.By default, the sudoers policy caches credentialson a per-terminal basis for 5 minutes.See the timestamp_type andtimestamp_timeout options in sudoers(5) for more information.Byrunning sudo with the -v option, a user can update the cachedcredentials without running a command.On systems where sudo is the primary method of gaining superuserprivileges, it is imperative to avoid syntax errors in the securitypolicy configuration files.For the default security policy,sudoers(5), changes to the configuration files should be made usingthe visudo(8) utility which will ensure that no syntax errors areintroduced.When invoked as sudoedit, the -e option (described below), isimplied.Security policies and audit plugins may log successful and failedattempts to run sudo.If an I/O plugin is configured, the runningcommand's input and output may be logged as well.The options are as follows:-A, --askpassNormally, if sudo requires a password, it will read it fromthe user's terminal.If the -A (askpass) option isspecified, a (possibly graphical) helper program isexecuted to read the user's password and output thepassword to the standard output.If the SUDO_ASKPASSenvironment variable is set, it specifies the path to thehelper program.Otherwise, if sudo.conf(5) contains a linespecifying the askpass program, that value will be used.For example:# Path to askpass helper programPath askpass /usr/X11R6/bin/ssh-askpassIf no askpass program is available, sudo will exit with anerror.-B, --bellRing the bell as part of the password prompt when aterminal is present.This option has no effect if anaskpass program is used.-b, --backgroundRun the given command in the background.It is notpossible to use shell job control to manipulate backgroundprocesses started by sudo.Most interactive commands willfail to work properly in background mode.-C num, --close-from=numClose all file descriptors greater than or equal to numbefore executing a command.Values less than three are notpermitted.By default, sudo will close all open filedescriptors other than standard input, standard output, andstandard error when executing a command.The securitypolicy may restrict the user's ability to use this option.The sudoers policy only permits use of the -C option whenthe administrator has enabled the closefrom_overrideoption.-D directory, --chdir=directoryRun the command in the specified directory instead of thecurrent working directory.The security policy may returnan error if the user does not have permission to specifythe working directory.-E, --preserve-envIndicates to the security policy that the user wishes topreserve their existing environment variables.Thesecurity policy may return an error if the user does nothave permission to preserve the environment.--preserve-env=listIndicates to the security policy that the user wishes toadd the comma-separated list of environment variables tothose preserved from the user's environment.The securitypolicy may return an error if the user does not havepermission to preserve the environment.This option may bespecified multiple times.-e, --editEdit one or more files instead of running a command.Inlieu of a path name, the string \"sudoedit\" is used whenconsulting the security policy.If the user is authorizedby the policy, the following steps are taken:1.Temporary copies are made of the files to be editedwith the owner set to the invoking user.2.The editor specified by the policy is run to edit thetemporary files.The sudoers policy uses theSUDO_EDITOR, VISUAL and EDITOR environment variables(in that order).If none of SUDO_EDITOR, VISUAL orEDITOR are set, the first program listed in the editorsudoers(5) option is used.3.If they have been modified, the temporary files arecopied back to their original location and thetemporary versions are removed.To help prevent the editing of unauthorized files, thefollowing restrictions are enforced unless explicitlyallowed by the security policy:\u2022Symbolic links may not be edited (version 1.8.15 andhigher).\u2022Symbolic links along the path to be edited are notfollowed when the parent directory is writable by theinvoking user unless that user is root (version 1.8.16and higher).\u2022Files located in a directory that is writable by theinvoking user may not be edited unless that user isroot (version 1.8.16 and higher).Users are never allowed to edit device special files.If the specified file does not exist, it will be created.Unlike most commands run by sudo, the editor is run withthe invoking user's environment unmodified.If thetemporary file becomes empty after editing, the user willbe prompted before it is installed.If, for some reason,sudo is unable to update a file with its edited version,the user will receive a warning and the edited copy willremain in a temporary file.-g group, --group=groupRun the command with the primary group set to group insteadof the primary group specified by the target user'spassword database entry.The group may be either a groupname or a numeric group-ID (GID) prefixed with the \u2018#\u2019character (e.g., \u2018#0\u2019 for GID 0).When running a commandas a GID, many shells require that the \u2018#\u2019 be escaped witha backslash (\u2018\\\u2019).If no -u option is specified, thecommand will be run as the invoking user.In either case,the primary group will be set to group.The sudoers policypermits any of the target user's groups to be specified viathe -g option as long as the -P option is not in use.-H, --set-homeRequest that the security policy set the HOME environmentvariable to the home directory specified by the targetuser's password database entry.Depending on the policy,this may be the default behavior.-h, --helpDisplay a short help message to the standard output andexit.-h host, --host=hostRun the command on the specified host if the securitypolicy plugin supports remote commands. The sudoers plugindoes not currently support running remote commands. Thismay also be used in conjunction with the -l option to lista user's privileges for the remote host.-i, --loginRun the shell specified by the target user's passworddatabase entry as a login shell.This means that login-specific resource files such as .profile, .bash_profile, or.login will be read by the shell.If a command isspecified, it is passed to the shell as a simple commandusing the -c option.The command and any args areconcatenated, separated by spaces, after escaping eachcharacter (including white space) with a backslash (\u2018\\\u2019)except for alphanumerics, underscores, hyphens, and dollarsigns.If no command is specified, an interactive shell isexecuted.sudo attempts to change to that user's homedirectory before running the shell.The command is runwith an environment similar to the one a user would receiveat log in.Most shells behave differently when a commandis specified as compared to an interactive session; consultthe shell's manual for details.The Command environmentsection in the sudoers(5) manual documents how the -ioption affects the environment in which a command is runwhen the sudoers policy is in use.-K, --remove-timestampSimilar to the -k option, except that it removes everycached credential for the user, regardless of the terminalor parent process ID.The next time sudo is run, apassword must be entered if the security policy requiresauthentication.It is not possible to use the -K option inconjunction with a command or other option.This optiondoes not require a password.Not all security policiessupport credential caching.-k, --reset-timestampWhen used without a command, invalidates the user's cachedcredentials for the current session.The next time sudo isrun in the session, a password must be entered if thesecurity policy requires authentication.By default, thesudoers policy uses a separate record in the credentialcache for each terminal (or parent process ID if noterminal is present).This prevents the -k option frominterfering with sudo commands run in a different terminalsession.See the timestamp_type option in sudoers(5) formore information.This option does not require a password,and was added to allow a user to revoke sudo permissionsfrom a .logout file.When used in conjunction with a command or an option thatmay require a password, this option will cause sudo toignore the user's cached credentials.As a result, sudowill prompt for a password (if one is required by thesecurity policy) and will not update the user's cachedcredentials.Not all security policies support credential caching.-l, --listIf no command is specified, list the privileges for theinvoking user (or the user specified by the -U option) onthe current host.A longer list format is used if thisoption is specified multiple times and the security policysupports a verbose output format.If a command is specified and is permitted by the securitypolicy, the fully-qualified path to the command isdisplayed along with any args. If a command is specifiedbut not allowed by the policy, sudo will exit with a statusvalue of 1.-N, --no-updateDo not update the user's cached credentials, even if theuser successfully authenticates.Unlike the -k flag,existing cached credentials are used if they are valid.Todetect when the user's cached credentials are valid (orwhen no authentication is required), the following can beused:sudo -NnvNot all security policies support credential caching.-n, --non-interactiveAvoid prompting the user for input of any kind.If apassword is required for the command to run, sudo willdisplay an error message and exit.-P, --preserve-groupsPreserve the invoking user's group vector unaltered.Bydefault, the sudoers policy will initialize the groupvector to the list of groups the target user is a memberof.The real and effective group-IDs, however, are stillset to match the target user.-p prompt, --prompt=promptUse a custom password prompt with optional escapesequences.The following percent (\u2018%\u2019) escape sequencesare supported by the sudoers policy:%Hexpanded to the host name including the domain name(only if the machine's host name is fully qualified orthe fqdn option is set in sudoers(5))%hexpanded to the local host name without the domain name%pexpanded to the name of the user whose password isbeing requested (respects the rootpw, targetpw, andrunaspw flags in sudoers(5))%Uexpanded to the login name of the user the command willbe run as (defaults to root unless the -u option isalso specified)%uexpanded to the invoking user's login name%%two consecutive \u2018%\u2019 characters are collapsed into asingle \u2018%\u2019 characterThe custom prompt will override the default promptspecified by either the security policy or the SUDO_PROMPTenvironment variable.On systems that use PAM, the customprompt will also override the prompt specified by a PAMmodule unless the passprompt_override flag is disabled insudoers.-R directory, --chroot=directoryChange to the specified root directory (see chroot(8))before running the command.The security policy may returnan error if the user does not have permission to specifythe root directory.-S, --stdinWrite the prompt to the standard error and read thepassword from the standard input instead of using theterminal device.-s, --shellRun the shell specified by the SHELL environment variableif it is set or the shell specified by the invoking user'spassword database entry.If a command is specified, it ispassed to the shell as a simple command using the -coption.The command and any args are concatenated,separated by spaces, after escaping each character(including white space) with a backslash (\u2018\\\u2019) except foralphanumerics, underscores, hyphens, and dollar signs.Ifno command is specified, an interactive shell is executed.Most shells behave differently when a command is specifiedas compared to an interactive session; consult the shell'smanual for details.-U user, --other-user=userUsed in conjunction with the -l option to list theprivileges for user instead of for the invoking user.Thesecurity policy may restrict listing other users'privileges.When using the sudoers policy, the -U optionis restricted to the root user and users with either the\u201clist\u201d priviege for the specified user or the ability torun any command as root or user on the current host.-T timeout, --command-timeout=timeoutUsed to set a timeout for the command.If the timeoutexpires before the command has exited, the command will beterminated.The security policy may restrict the user'sability to set timeouts.The sudoers policy requires thatuser-specified timeouts be explicitly enabled.-u user, --user=userRun the command as a user other than the default targetuser (usually root).The user may be either a user name ora numeric user-ID (UID) prefixed with the \u2018#\u2019 character(e.g., \u2018#0\u2019 for UID 0).When running commands as a UID,many shells require that the \u2018#\u2019 be escaped with abackslash (\u2018\\\u2019).Some security policies may restrict UIDsto those listed in the password database.The sudoerspolicy allows UIDs that are not in the password database aslong as the targetpw option is not set.Other securitypolicies may not support this.-V, --versionPrint the sudo version string as well as the version stringof any configured plugins.If the invoking user is alreadyroot, the -V option will display the options passed toconfigure when sudo was built; plugins may displayadditional information such as default options.-v, --validateUpdate the user's cached credentials, authenticating theuser if necessary.For the sudoers plugin, this extendsthe sudo timeout for another 5 minutes by default, but doesnot run a command.Not all security policies supportcached credentials.--The -- is used to delimit the end of the sudo options.Subsequent options are passed to the command.Options that take a value may only be specified once unlessotherwise indicated in the description.This is to help guardagainst problems caused by poorly written scripts that invoke sudowith user-controlled input.Environment variables to be set for the command may also be passedas options to sudo in the form VAR=value, for exampleLD_LIBRARY_PATH=/usr/local/pkg/lib.Environment variables may besubject to restrictions imposed by the security policy plugin.Thesudoers policy subjects environment variables passed as options tothe same restrictions as existing environment variables with oneimportant difference.If the setenv option is set in sudoers, thecommand to be run has the SETENV tag set or the command matched isALL, the user may set variables that would otherwise be forbidden.See sudoers(5) for more information.",
        "name": "sudo, sudoedit \u2014 execute a command as another user",
        "section": 8
    },
    {
        "command": "sudo_logsrvd",
        "description": "sudo_logsrvd is a high-performance log server that accepts eventand I/O logs from sudo.It can be used to implement centralizedlogging of sudo logs.The server has two modes of operation: localand relay.By default, sudo_logsrvd stores the logs locally but itcan also be configured to relay them to another server thatsupports the sudo_logsrv.proto(5) protocol.When not relaying, event log entries may be logged either viasyslog(3) or to a local file.I/O Logs stored locally bysudo_logsrvd can be replayed via the sudoreplay(8) utility in thesame way as logs generated directly by the sudoers plugin.The server also supports restarting interrupted log transfers.Todistinguish completed I/O logs from incomplete ones, the I/O logtiming file is set to be read-only when the log is complete.Configuration parameters for sudo_logsrvd may be specified in thesudo_logsrvd.conf(5) file or the file specified via the -f option.sudo_logsrvd rereads its configuration file when it receives SIGHUPand writes server state to the debug file (if one is configured)when it receives SIGUSR1.The options are as follows:-f file, --file=fileRead configuration from file instead of the default,/etc/sudo_logsrvd.conf.-h, --helpDisplay a short help message to the standard output andexit.-n, --no-forkRun sudo_logsrvd in the foreground instead of detachingfrom the terminal and becoming a daemon.-R percentage, --random-drop=percentageFor each message, there is a percentage chance that theserver will drop the connection.This is only intended fordebugging the ability of a client to restart a connection.-V, --versionPrint the sudo_logsrvd version and exit.Securing server connectionsThe I/O log data sent to sudo_logsrvd may contain sensitiveinformation such as passwords and should be secured using TransportLayer Security (TLS).Doing so requires having a signedcertificate on the server and, if tls_checkpeer is enabled insudo_logsrvd.conf(5), a signed certificate on the client as well.The certificates can either be signed by a well-known CertificateAuthority (CA), or a private CA can be used.Instructions forcreating a private CA are included below in the EXAMPLES section.Debugging sudo_logsrvdsudo_logsrvd supports a flexible debugging framework that isconfigured via Debug lines in the sudo.conf(5) file.For more information on configuring sudo.conf(5), refer to itsmanual.",
        "name": "sudo_logsrvd \u2014 sudo event and I/O log server",
        "section": 8
    },
    {
        "command": "sudo_sendlog",
        "description": "sudo_sendlog can be used to send the existing sudoers I/O log pathto a remote log server such as sudo_logsrvd(8) for central storage.The options are as follows:-A, --accept-onlyOnly send the accept event, not the I/O associated with thelog.This can be used to test the logging of accept eventswithout any associated I/O.-b, --ca-bundleThe path to a certificate authority bundle file, in PEMformat, to use instead of the system's default certificateauthority database when authenticating the log server.Thedefault is to use the system's default certificateauthority database.-c, --certThe path to the client's certificate file in PEM format.This setting is required when the connection to the remotelog server is secured with TLS.--helpDisplay a short help message to the standard output andexit.-h, --hostConnect to the specified host instead of localhost.-i, --iolog-idUse the specified iolog-id when restarting a log transfer.The iolog-id is reported by the server when it creates theremote I/O log.This option may only be used inconjunction with the -r option.-k, --keyThe path to the client's private key file in PEM format.This setting is required when the connection to the remotelog server is secured with TLS.-n, --no-verifyIf specified, the server's certificate will not be verifiedduring the TLS handshake.By default, sudo_sendlogverifies that the server's certificate is valid and that itcontains either the server's host name or its IP address.This setting is only supported when the connection to theremote log server is secured with TLS.-p, --portUse the specified network port when connecting to the logserver instead of the default, port 30344.-r, --restartRestart an interrupted connection to the log server.Thespecified restart-point is used to tell the server thepoint in time at which to continue the log.Therestart-point is specified in the form\u201cseconds,nanoseconds\u201d and is usually the last commit pointreceived from the server.The -i option must also bespecified when restarting a transfer.-R, --rejectSend a reject event for the command using the specifiedreject-reason, even though it was actually acceptedlocally.This can be used to test the logging of rejectevents; no I/O will be sent.-s, --stop-afterStop sending log records and close the connection whenstop-point is reached.This can be used for testingpurposes to send a partial I/O log to the server.Partiallogs can be restarted using the -r option.The stop-pointis an elapsed time specified in the form\u201cseconds,nanoseconds\u201d.-t, --testOpen number simultaneous connections to the log server andsend the specified I/O log file on each one.This optionis useful for performance testing.-V, --versionPrint the sudo_sendlog version and exit.Debugging sendlogsudo_sendlog supports a flexible debugging framework that isconfigured via Debug lines in the sudo.conf(5) file.For more information on configuring sudo.conf(5), refer to itsmanual.",
        "name": "sudo_sendlog \u2014 send sudo I/O log to log server",
        "section": 8
    },
    {
        "command": "sudoedit",
        "description": "sudo allows a permitted user to execute a command as the superuseror another user, as specified by the security policy.The invokinguser's real (not effective) user-ID is used to determine the username with which to query the security policy.sudo supports a plugin architecture for security policies,auditing, and input/output logging.Third parties can develop anddistribute their own plugins to work seamlessly with the sudofront-end.The default security policy is sudoers, which isconfigured via the file /etc/sudoers, or via LDAP.See the Pluginssection for more information.The security policy determines what privileges, if any, a user hasto run sudo.The policy may require that users authenticatethemselves with a password or another authentication mechanism.Ifauthentication is required, sudo will exit if the user's passwordis not entered within a configurable time limit.This limit ispolicy-specific; the default password prompt timeout for thesudoers security policy is 5 minutes.Security policies may support credential caching to allow the userto run sudo again for a period of time without requiringauthentication.By default, the sudoers policy caches credentialson a per-terminal basis for 5 minutes.See the timestamp_type andtimestamp_timeout options in sudoers(5) for more information.Byrunning sudo with the -v option, a user can update the cachedcredentials without running a command.On systems where sudo is the primary method of gaining superuserprivileges, it is imperative to avoid syntax errors in the securitypolicy configuration files.For the default security policy,sudoers(5), changes to the configuration files should be made usingthe visudo(8) utility which will ensure that no syntax errors areintroduced.When invoked as sudoedit, the -e option (described below), isimplied.Security policies and audit plugins may log successful and failedattempts to run sudo.If an I/O plugin is configured, the runningcommand's input and output may be logged as well.The options are as follows:-A, --askpassNormally, if sudo requires a password, it will read it fromthe user's terminal.If the -A (askpass) option isspecified, a (possibly graphical) helper program isexecuted to read the user's password and output thepassword to the standard output.If the SUDO_ASKPASSenvironment variable is set, it specifies the path to thehelper program.Otherwise, if sudo.conf(5) contains a linespecifying the askpass program, that value will be used.For example:# Path to askpass helper programPath askpass /usr/X11R6/bin/ssh-askpassIf no askpass program is available, sudo will exit with anerror.-B, --bellRing the bell as part of the password prompt when aterminal is present.This option has no effect if anaskpass program is used.-b, --backgroundRun the given command in the background.It is notpossible to use shell job control to manipulate backgroundprocesses started by sudo.Most interactive commands willfail to work properly in background mode.-C num, --close-from=numClose all file descriptors greater than or equal to numbefore executing a command.Values less than three are notpermitted.By default, sudo will close all open filedescriptors other than standard input, standard output, andstandard error when executing a command.The securitypolicy may restrict the user's ability to use this option.The sudoers policy only permits use of the -C option whenthe administrator has enabled the closefrom_overrideoption.-D directory, --chdir=directoryRun the command in the specified directory instead of thecurrent working directory.The security policy may returnan error if the user does not have permission to specifythe working directory.-E, --preserve-envIndicates to the security policy that the user wishes topreserve their existing environment variables.Thesecurity policy may return an error if the user does nothave permission to preserve the environment.--preserve-env=listIndicates to the security policy that the user wishes toadd the comma-separated list of environment variables tothose preserved from the user's environment.The securitypolicy may return an error if the user does not havepermission to preserve the environment.This option may bespecified multiple times.-e, --editEdit one or more files instead of running a command.Inlieu of a path name, the string \"sudoedit\" is used whenconsulting the security policy.If the user is authorizedby the policy, the following steps are taken:1.Temporary copies are made of the files to be editedwith the owner set to the invoking user.2.The editor specified by the policy is run to edit thetemporary files.The sudoers policy uses theSUDO_EDITOR, VISUAL and EDITOR environment variables(in that order).If none of SUDO_EDITOR, VISUAL orEDITOR are set, the first program listed in the editorsudoers(5) option is used.3.If they have been modified, the temporary files arecopied back to their original location and thetemporary versions are removed.To help prevent the editing of unauthorized files, thefollowing restrictions are enforced unless explicitlyallowed by the security policy:\u2022Symbolic links may not be edited (version 1.8.15 andhigher).\u2022Symbolic links along the path to be edited are notfollowed when the parent directory is writable by theinvoking user unless that user is root (version 1.8.16and higher).\u2022Files located in a directory that is writable by theinvoking user may not be edited unless that user isroot (version 1.8.16 and higher).Users are never allowed to edit device special files.If the specified file does not exist, it will be created.Unlike most commands run by sudo, the editor is run withthe invoking user's environment unmodified.If thetemporary file becomes empty after editing, the user willbe prompted before it is installed.If, for some reason,sudo is unable to update a file with its edited version,the user will receive a warning and the edited copy willremain in a temporary file.-g group, --group=groupRun the command with the primary group set to group insteadof the primary group specified by the target user'spassword database entry.The group may be either a groupname or a numeric group-ID (GID) prefixed with the \u2018#\u2019character (e.g., \u2018#0\u2019 for GID 0).When running a commandas a GID, many shells require that the \u2018#\u2019 be escaped witha backslash (\u2018\\\u2019).If no -u option is specified, thecommand will be run as the invoking user.In either case,the primary group will be set to group.The sudoers policypermits any of the target user's groups to be specified viathe -g option as long as the -P option is not in use.-H, --set-homeRequest that the security policy set the HOME environmentvariable to the home directory specified by the targetuser's password database entry.Depending on the policy,this may be the default behavior.-h, --helpDisplay a short help message to the standard output andexit.-h host, --host=hostRun the command on the specified host if the securitypolicy plugin supports remote commands. The sudoers plugindoes not currently support running remote commands. Thismay also be used in conjunction with the -l option to lista user's privileges for the remote host.-i, --loginRun the shell specified by the target user's passworddatabase entry as a login shell.This means that login-specific resource files such as .profile, .bash_profile, or.login will be read by the shell.If a command isspecified, it is passed to the shell as a simple commandusing the -c option.The command and any args areconcatenated, separated by spaces, after escaping eachcharacter (including white space) with a backslash (\u2018\\\u2019)except for alphanumerics, underscores, hyphens, and dollarsigns.If no command is specified, an interactive shell isexecuted.sudo attempts to change to that user's homedirectory before running the shell.The command is runwith an environment similar to the one a user would receiveat log in.Most shells behave differently when a commandis specified as compared to an interactive session; consultthe shell's manual for details.The Command environmentsection in the sudoers(5) manual documents how the -ioption affects the environment in which a command is runwhen the sudoers policy is in use.-K, --remove-timestampSimilar to the -k option, except that it removes everycached credential for the user, regardless of the terminalor parent process ID.The next time sudo is run, apassword must be entered if the security policy requiresauthentication.It is not possible to use the -K option inconjunction with a command or other option.This optiondoes not require a password.Not all security policiessupport credential caching.-k, --reset-timestampWhen used without a command, invalidates the user's cachedcredentials for the current session.The next time sudo isrun in the session, a password must be entered if thesecurity policy requires authentication.By default, thesudoers policy uses a separate record in the credentialcache for each terminal (or parent process ID if noterminal is present).This prevents the -k option frominterfering with sudo commands run in a different terminalsession.See the timestamp_type option in sudoers(5) formore information.This option does not require a password,and was added to allow a user to revoke sudo permissionsfrom a .logout file.When used in conjunction with a command or an option thatmay require a password, this option will cause sudo toignore the user's cached credentials.As a result, sudowill prompt for a password (if one is required by thesecurity policy) and will not update the user's cachedcredentials.Not all security policies support credential caching.-l, --listIf no command is specified, list the privileges for theinvoking user (or the user specified by the -U option) onthe current host.A longer list format is used if thisoption is specified multiple times and the security policysupports a verbose output format.If a command is specified and is permitted by the securitypolicy, the fully-qualified path to the command isdisplayed along with any args. If a command is specifiedbut not allowed by the policy, sudo will exit with a statusvalue of 1.-N, --no-updateDo not update the user's cached credentials, even if theuser successfully authenticates.Unlike the -k flag,existing cached credentials are used if they are valid.Todetect when the user's cached credentials are valid (orwhen no authentication is required), the following can beused:sudo -NnvNot all security policies support credential caching.-n, --non-interactiveAvoid prompting the user for input of any kind.If apassword is required for the command to run, sudo willdisplay an error message and exit.-P, --preserve-groupsPreserve the invoking user's group vector unaltered.Bydefault, the sudoers policy will initialize the groupvector to the list of groups the target user is a memberof.The real and effective group-IDs, however, are stillset to match the target user.-p prompt, --prompt=promptUse a custom password prompt with optional escapesequences.The following percent (\u2018%\u2019) escape sequencesare supported by the sudoers policy:%Hexpanded to the host name including the domain name(only if the machine's host name is fully qualified orthe fqdn option is set in sudoers(5))%hexpanded to the local host name without the domain name%pexpanded to the name of the user whose password isbeing requested (respects the rootpw, targetpw, andrunaspw flags in sudoers(5))%Uexpanded to the login name of the user the command willbe run as (defaults to root unless the -u option isalso specified)%uexpanded to the invoking user's login name%%two consecutive \u2018%\u2019 characters are collapsed into asingle \u2018%\u2019 characterThe custom prompt will override the default promptspecified by either the security policy or the SUDO_PROMPTenvironment variable.On systems that use PAM, the customprompt will also override the prompt specified by a PAMmodule unless the passprompt_override flag is disabled insudoers.-R directory, --chroot=directoryChange to the specified root directory (see chroot(8))before running the command.The security policy may returnan error if the user does not have permission to specifythe root directory.-S, --stdinWrite the prompt to the standard error and read thepassword from the standard input instead of using theterminal device.-s, --shellRun the shell specified by the SHELL environment variableif it is set or the shell specified by the invoking user'spassword database entry.If a command is specified, it ispassed to the shell as a simple command using the -coption.The command and any args are concatenated,separated by spaces, after escaping each character(including white space) with a backslash (\u2018\\\u2019) except foralphanumerics, underscores, hyphens, and dollar signs.Ifno command is specified, an interactive shell is executed.Most shells behave differently when a command is specifiedas compared to an interactive session; consult the shell'smanual for details.-U user, --other-user=userUsed in conjunction with the -l option to list theprivileges for user instead of for the invoking user.Thesecurity policy may restrict listing other users'privileges.When using the sudoers policy, the -U optionis restricted to the root user and users with either the\u201clist\u201d priviege for the specified user or the ability torun any command as root or user on the current host.-T timeout, --command-timeout=timeoutUsed to set a timeout for the command.If the timeoutexpires before the command has exited, the command will beterminated.The security policy may restrict the user'sability to set timeouts.The sudoers policy requires thatuser-specified timeouts be explicitly enabled.-u user, --user=userRun the command as a user other than the default targetuser (usually root).The user may be either a user name ora numeric user-ID (UID) prefixed with the \u2018#\u2019 character(e.g., \u2018#0\u2019 for UID 0).When running commands as a UID,many shells require that the \u2018#\u2019 be escaped with abackslash (\u2018\\\u2019).Some security policies may restrict UIDsto those listed in the password database.The sudoerspolicy allows UIDs that are not in the password database aslong as the targetpw option is not set.Other securitypolicies may not support this.-V, --versionPrint the sudo version string as well as the version stringof any configured plugins.If the invoking user is alreadyroot, the -V option will display the options passed toconfigure when sudo was built; plugins may displayadditional information such as default options.-v, --validateUpdate the user's cached credentials, authenticating theuser if necessary.For the sudoers plugin, this extendsthe sudo timeout for another 5 minutes by default, but doesnot run a command.Not all security policies supportcached credentials.--The -- is used to delimit the end of the sudo options.Subsequent options are passed to the command.Options that take a value may only be specified once unlessotherwise indicated in the description.This is to help guardagainst problems caused by poorly written scripts that invoke sudowith user-controlled input.Environment variables to be set for the command may also be passedas options to sudo in the form VAR=value, for exampleLD_LIBRARY_PATH=/usr/local/pkg/lib.Environment variables may besubject to restrictions imposed by the security policy plugin.Thesudoers policy subjects environment variables passed as options tothe same restrictions as existing environment variables with oneimportant difference.If the setenv option is set in sudoers, thecommand to be run has the SETENV tag set or the command matched isALL, the user may set variables that would otherwise be forbidden.See sudoers(5) for more information.",
        "name": "sudo, sudoedit \u2014 execute a command as another user",
        "section": 8
    },
    {
        "command": "sudoreplay",
        "description": "sudoreplay plays back or lists the output logs created by sudo.When replaying, sudoreplay can play the session back in real-time,or the playback speed may be adjusted (faster or slower) based onthe command line options.The ID should either be a six character sequence of digits andupper case letters, e.g., \u201c0100A5\u201d or a path name.The ID mayinclude an optional @offset suffix which may be used to startreplaying at a specific time offset.The @offset is specified as anumber in seconds since the start of the session with an optionaldecimal fraction.Path names may be relative to the I/O log directory/var/log/sudo-io (unless overridden by the -d option) or fullyqualified, beginning with a \u2018/\u2019 character.When a command is runvia sudo with log_output enabled in the sudoers file, a \u201cTSID=ID\u201dstring is logged via syslog(3) or to the sudo log file.The ID mayalso be determined using sudoreplay's list mode.In list mode, sudoreplay can be used to find the ID of a sessionbased on a number of criteria such as the user, tty, or commandrun.In replay mode, if the standard input and output are connected to aterminal and the -n option is not specified, sudoreplay willoperate interactively.In interactive mode, sudoreplay willattempt to adjust the terminal size to match that of the sessionand write directly to the terminal (not all terminals supportthis).Additionally, it will poll the keyboard and act on thefollowing keys:\u2018\\n\u2019 or \u2018\\r\u2019Skip to the next replay event; useful for longpauses.\u2018 \u2019 (space)Pause output; press any key to resume.\u2018<\u2019Reduce the playback speed by one half.\u2018>\u2019Double the playback speed.The session can be interrupted via control-C.When the session hasfinished, the terminal is restored to its original size if it waschanged during playback.The options are as follows:-d dir, --directory=dirStore session logs in dir instead of the default,/var/log/sudo-io.-f filter, --filter=filterSelect which I/O type(s) to display.By default,sudoreplay will display the command's standard output,standard error, and tty output.The filter argument is acomma-separated list, consisting of one or more offollowing: stdin, stdout, stderr, ttyin, and ttyout.-F, --followEnable \u201cfollow mode\u201d.When replaying a session, sudoreplaywill ignore end-of-file and keep replaying until the log iscomplete.This can be used to replay a session that isstill in progress, similar to \u201ctail -f\u201d.An I/O log fileis considered to be complete when the write bits have beencleared on the session's timing file.Versions of sudoprior to 1.9.1 do not clear the write bits upon completion.-h, --helpDisplay a short help message to the standard output andexit.-l, --list [search expression]Enable \u201clist mode\u201d.In this mode, sudoreplay will listavailable sessions in a format similar to the sudo log fileformat, sorted by file name (or sequence number).Anycontrol characters present in the log data are formatted inoctal with a leading \u2018#\u2019 character.For example, ahorizontal tab is displayed as \u2018#011\u2019 and an embeddedcarriage return is displayed as \u2018#015\u2019.Space charactersin the command name and arguments are also formatted inoctal.If a search expression is specified, it will be used torestrict the IDs that are displayed.An expression iscomposed of the following predicates:command patternEvaluates to true if the command run matches thePOSIX extended regular expression pattern.cwd directoryEvaluates to true if the command was run with thespecified current working directory.fromdate dateEvaluates to true if the command was run on orafter date.See Date and time format for adescription of supported date and time formats.group runas_groupEvaluates to true if the command was run with thespecified runas_group.Unless a runas_group wasexplicitly specified when sudo was run this fieldwill be empty in the log.host hostnameEvaluates to true if the command was run on thespecified hostname.runas runas_userEvaluates to true if the command was run as thespecified runas_user.By default, sudo runscommands as the root user.todate dateEvaluates to true if the command was run on orprior to date.See Date and time format for adescription of supported date and time formats.tty tty nameEvaluates to true if the command was run on thespecified terminal device.The tty name should bespecified without the /dev/ prefix, e.g., tty01instead of /dev/tty01.user user nameEvaluates to true if the ID matches a command runby user name.Predicates may be abbreviated to the shortest uniquestring.Predicates may be combined using and, or, and ! operatorsas well as \u2018(\u2019 and \u2018)\u2019 grouping (parentheses must generallybe escaped from the shell).The and operator is optional,adjacent predicates have an implied and unless separated byan or.-m, --max-wait max_waitSpecify an upper bound on how long to wait between keypresses or output data.By default, sudoreplay willaccurately reproduce the delays between key presses orprogram output.However, this can be tedious when thesession includes long pauses.When the -m option isspecified, sudoreplay will limit these pauses to at mostmax_wait seconds.The value may be specified as a floatingpoint number, e.g., 2.5.A max_wait of zero or less willeliminate the pauses entirely.-n, --non-interactiveDo not prompt for user input or attempt to re-size theterminal.The session is written to the standard output,not directly to the user's terminal.-R, --no-resizeDo not attempt to re-size the terminal to match theterminal size of the session.-S, --suspend-waitWait while the command was suspended.By default,sudoreplay will ignore the time interval between when thecommand was suspended and when it was resumed.If the -Soption is specified, sudoreplay will wait instead.-s, --speed speed_factorThis option causes sudoreplay to adjust the number ofseconds it will wait between key presses or program output.This can be used to slow down or speed up the display.Forexample, a speed_factor of 2 would make the output twice asfast whereas a speed_factor of .5 would make the outputtwice as slow.-V, --versionPrint the sudoreplay versions version number and exit.Date and time formatThe time and date may be specified multiple ways, common formatsinclude:HH:MM:SS am MM/DD/CCYY timezone24 hour time may be used in place of am/pm.HH:MM:SS am Month, Day Year timezone24 hour time may be used in place of am/pm, and month andday names may be abbreviated.Month and day of the weeknames must be specified in English.CCYY-MM-DD HH:MM:SSISO time formatDD Month CCYY HH:MM:SSThe month name may be abbreviated.Either time or date may be omitted, the am/pm and timezone areoptional.If no date is specified, the current day is assumed; ifno time is specified, the first second of the specified date isused.The less significant parts of both time and date may also beomitted, in which case zero is assumed.The following are all valid time and date specifications:nowThe current time and date.tomorrowExactly one day from now.yesterday24 hours ago.2 hours ago2 hours ago.next FridayThe first second of the Friday in the next (upcoming) week.Not to be confused with \u201cthis Friday\u201d which would match theFriday of the current week.last weekThe current time but 7 days ago.This is equivalent to \u201caweek ago\u201d.a fortnight agoThe current time but 14 days ago.10:01 am 9/17/200910:01 am, September 17, 2009.10:01 am10:01 am on the current day.1010:00 am on the current day.9/17/200900:00 am, September 17, 2009.10:01 am Sep 17, 200910:01 am, September 17, 2009.Relative time specifications do not always work as expected.Forexample, the \u201cnext\u201d qualifier is intended to be used in conjunctionwith a day such as \u201cnext Monday\u201d.When used with units of weeks,months, years, etc the result will be one more than expected.Forexample, \u201cnext week\u201d will result in a time exactly two weeks fromnow, which is probably not what was intended.This will beaddressed in a future version of sudoreplay.Debugging sudoreplaysudoreplay versions 1.8.4 and higher support a flexible debuggingframework that is configured via Debug lines in the sudo.conf(5)file.For more information on configuring sudo.conf(5), refer to itsmanual.",
        "name": "sudoreplay \u2014 replay sudo session logs",
        "section": 8
    },
    {
        "command": "sulogin",
        "description": "sulogin is invoked by init when the system goes into single-usermode.The user is prompted:Give root password for system maintenance (or type Control-D fornormal startup):If the root account is locked and --force is specified, nopassword is required.sulogin will be connected to the current terminal, or to theoptional tty device that can be specified on the command line(typically /dev/console).When the user exits from the single-user shell, or pressescontrol-D at the prompt, the system will continue to boot.",
        "name": "sulogin - single-user login",
        "section": 8
    },
    {
        "command": "svcgssd",
        "description": "The rpcsec_gss protocol gives a means of using the gss-apigeneric security api to provide security for protocols using rpc(in particular, nfs).Before exchanging any rpc requests usingrpcsec_gss, the rpc client must first establish a securitycontext with the rpc server.The linux kernel's implementationof rpcsec_gss depends on the userspace daemon rpc.svcgssd tohandle context establishment on the rpc server.The daemon usesfiles in the proc filesystem to communicate with the kernel.",
        "name": "rpc.svcgssd - server-side rpcsec_gss daemon",
        "section": 8
    },
    {
        "command": "swaplabel",
        "description": "swaplabel will display or change the label or UUID of a swappartition located on device (or regular file).If the optional arguments -L and -U are not given, swaplabel willsimply display the current swap-area label and UUID of device.If an optional argument is present, then swaplabel will changethe appropriate value on device. These values can also be setduring swap creation using mkswap(8). The swaplabel utilityallows changing the label or UUID on an actively used swapdevice.",
        "name": "swaplabel - print or change the label or UUID of a swap area",
        "section": 8
    },
    {
        "command": "swapoff",
        "description": "swapon is used to specify devices on which paging and swappingare to take place.The device or file used is given by the specialfile parameter. Itmay be of the form -L label or -U uuid to indicate a device bylabel or uuid.Calls to swapon normally occur in the system boot scripts makingall swap devices available, so that the paging and swappingactivity is interleaved across several devices and files.swapoff disables swapping on the specified devices and files.When the -a flag is given, swapping is disabled on all known swapdevices and files (as found in /proc/swaps or /etc/fstab).",
        "name": "swapon, swapoff - enable/disable devices and files for paging andswapping",
        "section": 8
    },
    {
        "command": "swapon",
        "description": "swapon is used to specify devices on which paging and swappingare to take place.The device or file used is given by the specialfile parameter. Itmay be of the form -L label or -U uuid to indicate a device bylabel or uuid.Calls to swapon normally occur in the system boot scripts makingall swap devices available, so that the paging and swappingactivity is interleaved across several devices and files.swapoff disables swapping on the specified devices and files.When the -a flag is given, swapping is disabled on all known swapdevices and files (as found in /proc/swaps or /etc/fstab).",
        "name": "swapon, swapoff - enable/disable devices and files for paging andswapping",
        "section": 8
    },
    {
        "command": "switch_root",
        "description": "switch_root moves already mounted /proc, /dev, /sys and /run tonewroot and makes newroot the new root filesystem and starts initprocess.WARNING: switch_root removes recursively all files anddirectories on the current root filesystem.",
        "name": "switch_root - switch to another filesystem as the root of themount tree",
        "section": 8
    },
    {
        "command": "sysctl",
        "description": "sysctl is used to modify kernel parameters at runtime.Theparameters available are those listed under /proc/sys/.Procfsis required for sysctl support in Linux.You can use sysctl toboth read and write sysctl data.",
        "name": "sysctl - configure kernel parameters at runtime",
        "section": 8
    },
    {
        "command": "sysdig",
        "description": null,
        "name": null,
        "section": 8
    },
    {
        "command": "system-config-selinux",
        "description": "system-config-selinux provides a graphical interface for managingthe SELinux configuration.",
        "name": "system-config-selinux - SELinux Management tool",
        "section": 8
    },
    {
        "command": "systemd-ask-password-console.path",
        "description": "systemd-ask-password-console.service is a system service thatqueries the user for system passwords (such as hard diskencryption keys and SSL certificate passphrases) on the console.It is intended to be used during boot to ensure proper handlingof passwords necessary for boot.systemd-ask-password-wall.service is a system service thatinforms all logged in users for system passwords via wall(1). Itis intended to be used after boot to ensure that users areproperly notified.See the developer documentation[1] for more information about thesystem password logic.Note that these services invoke systemd-tty-ask-password-agent(1)with either the --watch --console or --watch --wall command lineparameters.",
        "name": "systemd-ask-password-console.service, systemd-ask-password-console.path, systemd-ask-password-wall.service, systemd-ask-password-wall.path - Query the user for system passwords on theconsole and via wall",
        "section": 8
    },
    {
        "command": "systemd-ask-password-console.service",
        "description": "systemd-ask-password-console.service is a system service thatqueries the user for system passwords (such as hard diskencryption keys and SSL certificate passphrases) on the console.It is intended to be used during boot to ensure proper handlingof passwords necessary for boot.systemd-ask-password-wall.service is a system service thatinforms all logged in users for system passwords via wall(1). Itis intended to be used after boot to ensure that users areproperly notified.See the developer documentation[1] for more information about thesystem password logic.Note that these services invoke systemd-tty-ask-password-agent(1)with either the --watch --console or --watch --wall command lineparameters.",
        "name": "systemd-ask-password-console.service, systemd-ask-password-console.path, systemd-ask-password-wall.service, systemd-ask-password-wall.path - Query the user for system passwords on theconsole and via wall",
        "section": 8
    },
    {
        "command": "systemd-ask-password-wall.path",
        "description": "systemd-ask-password-console.service is a system service thatqueries the user for system passwords (such as hard diskencryption keys and SSL certificate passphrases) on the console.It is intended to be used during boot to ensure proper handlingof passwords necessary for boot.systemd-ask-password-wall.service is a system service thatinforms all logged in users for system passwords via wall(1). Itis intended to be used after boot to ensure that users areproperly notified.See the developer documentation[1] for more information about thesystem password logic.Note that these services invoke systemd-tty-ask-password-agent(1)with either the --watch --console or --watch --wall command lineparameters.",
        "name": "systemd-ask-password-console.service, systemd-ask-password-console.path, systemd-ask-password-wall.service, systemd-ask-password-wall.path - Query the user for system passwords on theconsole and via wall",
        "section": 8
    },
    {
        "command": "systemd-ask-password-wall.service",
        "description": "systemd-ask-password-console.service is a system service thatqueries the user for system passwords (such as hard diskencryption keys and SSL certificate passphrases) on the console.It is intended to be used during boot to ensure proper handlingof passwords necessary for boot.systemd-ask-password-wall.service is a system service thatinforms all logged in users for system passwords via wall(1). Itis intended to be used after boot to ensure that users areproperly notified.See the developer documentation[1] for more information about thesystem password logic.Note that these services invoke systemd-tty-ask-password-agent(1)with either the --watch --console or --watch --wall command lineparameters.",
        "name": "systemd-ask-password-console.service, systemd-ask-password-console.path, systemd-ask-password-wall.service, systemd-ask-password-wall.path - Query the user for system passwords on theconsole and via wall",
        "section": 8
    },
    {
        "command": "systemd-backlight",
        "description": "systemd-backlight@.service is a service that restores the displaybacklight brightness at early boot and saves it at shutdown. Ondisk, the backlight brightness is stored in/var/lib/systemd/backlight/. During loading, if the udev propertyID_BACKLIGHT_CLAMP is not set to false, the brightness is clampedto a value of at least 1 or 5% of maximum brightness, whicheveris greater. This restriction will be removed when the kernelallows user space to reliably set a brightness value which doesnot turn off the display.",
        "name": "systemd-backlight@.service, systemd-backlight - Load and save thedisplay backlight brightness at boot and shutdown",
        "section": 8
    },
    {
        "command": "systemd-backlight.service",
        "description": "systemd-backlight@.service is a service that restores the displaybacklight brightness at early boot and saves it at shutdown. Ondisk, the backlight brightness is stored in/var/lib/systemd/backlight/. During loading, if the udev propertyID_BACKLIGHT_CLAMP is not set to false, the brightness is clampedto a value of at least 1 or 5% of maximum brightness, whicheveris greater. This restriction will be removed when the kernelallows user space to reliably set a brightness value which doesnot turn off the display.",
        "name": "systemd-backlight@.service, systemd-backlight - Load and save thedisplay backlight brightness at boot and shutdown",
        "section": 8
    },
    {
        "command": "systemd-backlight@.service",
        "description": "systemd-backlight@.service is a service that restores the displaybacklight brightness at early boot and saves it at shutdown. Ondisk, the backlight brightness is stored in/var/lib/systemd/backlight/. During loading, if the udev propertyID_BACKLIGHT_CLAMP is not set to false, the brightness is clampedto a value of at least 1 or 5% of maximum brightness, whicheveris greater. This restriction will be removed when the kernelallows user space to reliably set a brightness value which doesnot turn off the display.",
        "name": "systemd-backlight@.service, systemd-backlight - Load and save thedisplay backlight brightness at boot and shutdown",
        "section": 8
    },
    {
        "command": "systemd-binfmt",
        "description": "systemd-binfmt.service is an early boot service that registersadditional binary formats for executables in the kernel.See binfmt.d(5) for information about the configuration of thisservice.",
        "name": "systemd-binfmt.service, systemd-binfmt - Configure additionalbinary formats for executables at boot",
        "section": 8
    },
    {
        "command": "systemd-binfmt.service",
        "description": "systemd-binfmt.service is an early boot service that registersadditional binary formats for executables in the kernel.See binfmt.d(5) for information about the configuration of thisservice.",
        "name": "systemd-binfmt.service, systemd-binfmt - Configure additionalbinary formats for executables at boot",
        "section": 8
    },
    {
        "command": "systemd-boot-check-no-failures",
        "description": "systemd-boot-check-no-failures.service is a system service thatchecks whether the system booted up successfully. This serviceimplements a very minimal test only: whether there are any failedunits on the system. This service is disabled by default. Whenenabled, it is ordered before boot-complete.target, thus ensuringthe target cannot be reached when the system booted up withfailed services.Note that due the simple nature of this check this service isprobably not suitable for deployment in most scenarios. It isprimarily useful only as example for developing more fine-grainedchecks to order before boot-complete.target.",
        "name": "systemd-boot-check-no-failures.service, systemd-boot-check-no-failures - verify that the system booted up cleanly",
        "section": 8
    },
    {
        "command": "systemd-boot-check-no-failures.service",
        "description": "systemd-boot-check-no-failures.service is a system service thatchecks whether the system booted up successfully. This serviceimplements a very minimal test only: whether there are any failedunits on the system. This service is disabled by default. Whenenabled, it is ordered before boot-complete.target, thus ensuringthe target cannot be reached when the system booted up withfailed services.Note that due the simple nature of this check this service isprobably not suitable for deployment in most scenarios. It isprimarily useful only as example for developing more fine-grainedchecks to order before boot-complete.target.",
        "name": "systemd-boot-check-no-failures.service, systemd-boot-check-no-failures - verify that the system booted up cleanly",
        "section": 8
    },
    {
        "command": "systemd-confext",
        "description": "systemd-sysext activates/deactivates system extension images.System extension images may \u2013 dynamically at runtime \u2014 extend the/usr/ and /opt/ directory hierarchies with additional files. Thisis particularly useful on immutable system images where a /usr/and/or /opt/ hierarchy residing on a read-only file system shallbe extended temporarily at runtime without making any persistentmodifications.System extension images should contain files and directoriessimilar in fashion to regular operating system tree. When one ormore system extension images are activated, their /usr/ and /opt/hierarchies are combined via \"overlayfs\" with the samehierarchies of the host OS, and the host /usr/ and /opt/overmounted with it (\"merging\"). When they are deactivated, themount point is disassembled \u2014 again revealing the unmodifiedoriginal host version of the hierarchy (\"unmerging\"). Mergingthus makes the extension's resources suddenly appear below the/usr/ and /opt/ hierarchies as if they were included in the baseOS image itself. Unmerging makes them disappear again, leaving inplace only the files that were shipped with the base OS imageitself.Files and directories contained in the extension images outsideof the /usr/ and /opt/ hierarchies are not merged, and hence haveno effect when included in a system extension image. Inparticular, files in the /etc/ and /var/ included in a systemextension image will not appear in the respective hierarchiesafter activation.System extension images are strictly read-only, and the host/usr/ and /opt/ hierarchies become read-only too while they areactivated.System extensions are supposed to be purely additive, i.e. theyare supposed to include only files that do not exist in theunderlying basic OS image. However, the underlying mechanism(overlayfs) also allows overlaying or removing files, but it isrecommended not to make use of this.System extension images may be provided in the following formats:1. Plain directories or btrfs subvolumes containing the OS tree2. Disk images with a GPT disk label, following the DiscoverablePartitions Specification[1]3. Disk images lacking a partition table, with a naked Linuxfile system (e.g. erofs, squashfs or ext4)These image formats are the same ones that systemd-nspawn(1)supports via its --directory=/--image= switches and those thatthe service manager supports via RootDirectory=/RootImage=.Similar to them they may optionally carry Verity authenticationinformation.System extensions are searched for in the directories/etc/extensions/, /run/extensions/ and /var/lib/extensions/. Thefirst two listed directories are not suitable for carrying largebinary images, however are still useful for carrying symlinks tothem. The primary place for installing system extensions is/var/lib/extensions/. Any directories found in these searchdirectories are considered directory based extension images; anyfiles with the .raw suffix are considered disk image basedextension images. When invoked in the initrd, the additionaldirectory /.extra/sysext/ is included in the directories that aresearched for extension images. Note however, that by default atighter image policy applies to images found there, though, seebelow. This directory is populated by systemd-stub(7) withextension images found in the system's EFI System Partition.During boot OS extension images are activated automatically, ifthe systemd-sysext.service is enabled. Note that this serviceruns only after the underlying file systems where systemextensions may be located have been mounted. This means they arenot suitable for shipping resources that are processed bysubsystems running in earliest boot. Specifically, OS extensionimages are not suitable for shipping system services orsystemd-sysusers(8) definitions. See the Portable ServicesDocumentation[2] for a simple mechanism for shipping systemservices in disk images, in a similar fashion to OS extensions.Note the different isolation on these two mechanisms: whilesystem extension directly extend the underlying OS image withadditional files that appear in a way very similar to as if theywere shipped in the OS image itself and thus imply no securityisolation, portable services imply service level sandboxing inone way or another. The systemd-sysext.service service isguaranteed to finish start-up before basic.target is reached;i.e. at the time regular services initialize (those which do notuse DefaultDependencies=no), the files and directories systemextensions provide are available in /usr/ and /opt/ and may beaccessed.Note that there is no concept of enabling/disabling installedsystem extension images: all installed extension images areautomatically activated at boot. However, you can place an emptydirectory named like the extension (no .raw) in /etc/extensions/to \"mask\" an extension with the same name in a system folder withlower precedence.A simple mechanism for version compatibility is enforced: asystem extension image must carry a/usr/lib/extension-release.d/extension-release.$name file, whichmust match its image name, that is compared with the hostos-release file: the contained ID= fields have to match unless\"_any\" is set for the extension. If the extension ID= is not\"_any\", the SYSEXT_LEVEL= field (if defined) has to match. If thelatter is not defined, the VERSION_ID= field has to matchinstead. If the extension defines the ARCHITECTURE= field and thevalue is not \"_any\" it has to match the kernel's architecturereported by uname(2) but the used architecture identifiers arethe same as for ConditionArchitecture= described insystemd.unit(5). System extensions should not ship a/usr/lib/os-release file (as that would be merged into the host/usr/ tree, overriding the host OS version data, which is notdesirable). The extension-release file follows the same formatand semantics, and carries the same content, as the os-releasefile of the OS, but it describes the resources carried in theextension image.The systemd-confext concept follows the same principle as thesystemd-sysext(1) functionality but instead of working on /usrand /opt, confext will extend only /etc. Files and directoriescontained in the confext images outside of the /etc/ hierarchyare not merged, and hence have no effect when included in theimage. Formats for these images are of the same as sysext images.The merged hierarchy will be mounted with \"nosuid\" and (if notdisabled via --noexec=false) \"noexec\".Confexts are looked for in the directories /run/confexts/,/var/lib/confexts/, /usr/lib/confexts/ and/usr/local/lib/confexts/. The first listed directory is notsuitable for carrying large binary images, however is stilluseful for carrying symlinks to them. The primary place forinstalling configuration extensions is /var/lib/confexts/. Anydirectories found in these search directories are considereddirectory based confext images; any files with the .raw suffixare considered disk image based confext images.Again, just like sysext images, the confext images will contain a/etc/extension-release.d/extension-release.$name file, which mustmatch the image name (with the usual escape hatch of xattr), andagain with content being one or more of ID=, VERSION_ID=, andCONFEXT_LEVEL. Confext images will then be checked and matchedagainst the base OS layer.",
        "name": "systemd-sysext, systemd-sysext.service, systemd-confext, systemd-confext.service - Activates System Extension Images",
        "section": 8
    },
    {
        "command": "systemd-confext.service",
        "description": "systemd-sysext activates/deactivates system extension images.System extension images may \u2013 dynamically at runtime \u2014 extend the/usr/ and /opt/ directory hierarchies with additional files. Thisis particularly useful on immutable system images where a /usr/and/or /opt/ hierarchy residing on a read-only file system shallbe extended temporarily at runtime without making any persistentmodifications.System extension images should contain files and directoriessimilar in fashion to regular operating system tree. When one ormore system extension images are activated, their /usr/ and /opt/hierarchies are combined via \"overlayfs\" with the samehierarchies of the host OS, and the host /usr/ and /opt/overmounted with it (\"merging\"). When they are deactivated, themount point is disassembled \u2014 again revealing the unmodifiedoriginal host version of the hierarchy (\"unmerging\"). Mergingthus makes the extension's resources suddenly appear below the/usr/ and /opt/ hierarchies as if they were included in the baseOS image itself. Unmerging makes them disappear again, leaving inplace only the files that were shipped with the base OS imageitself.Files and directories contained in the extension images outsideof the /usr/ and /opt/ hierarchies are not merged, and hence haveno effect when included in a system extension image. Inparticular, files in the /etc/ and /var/ included in a systemextension image will not appear in the respective hierarchiesafter activation.System extension images are strictly read-only, and the host/usr/ and /opt/ hierarchies become read-only too while they areactivated.System extensions are supposed to be purely additive, i.e. theyare supposed to include only files that do not exist in theunderlying basic OS image. However, the underlying mechanism(overlayfs) also allows overlaying or removing files, but it isrecommended not to make use of this.System extension images may be provided in the following formats:1. Plain directories or btrfs subvolumes containing the OS tree2. Disk images with a GPT disk label, following the DiscoverablePartitions Specification[1]3. Disk images lacking a partition table, with a naked Linuxfile system (e.g. erofs, squashfs or ext4)These image formats are the same ones that systemd-nspawn(1)supports via its --directory=/--image= switches and those thatthe service manager supports via RootDirectory=/RootImage=.Similar to them they may optionally carry Verity authenticationinformation.System extensions are searched for in the directories/etc/extensions/, /run/extensions/ and /var/lib/extensions/. Thefirst two listed directories are not suitable for carrying largebinary images, however are still useful for carrying symlinks tothem. The primary place for installing system extensions is/var/lib/extensions/. Any directories found in these searchdirectories are considered directory based extension images; anyfiles with the .raw suffix are considered disk image basedextension images. When invoked in the initrd, the additionaldirectory /.extra/sysext/ is included in the directories that aresearched for extension images. Note however, that by default atighter image policy applies to images found there, though, seebelow. This directory is populated by systemd-stub(7) withextension images found in the system's EFI System Partition.During boot OS extension images are activated automatically, ifthe systemd-sysext.service is enabled. Note that this serviceruns only after the underlying file systems where systemextensions may be located have been mounted. This means they arenot suitable for shipping resources that are processed bysubsystems running in earliest boot. Specifically, OS extensionimages are not suitable for shipping system services orsystemd-sysusers(8) definitions. See the Portable ServicesDocumentation[2] for a simple mechanism for shipping systemservices in disk images, in a similar fashion to OS extensions.Note the different isolation on these two mechanisms: whilesystem extension directly extend the underlying OS image withadditional files that appear in a way very similar to as if theywere shipped in the OS image itself and thus imply no securityisolation, portable services imply service level sandboxing inone way or another. The systemd-sysext.service service isguaranteed to finish start-up before basic.target is reached;i.e. at the time regular services initialize (those which do notuse DefaultDependencies=no), the files and directories systemextensions provide are available in /usr/ and /opt/ and may beaccessed.Note that there is no concept of enabling/disabling installedsystem extension images: all installed extension images areautomatically activated at boot. However, you can place an emptydirectory named like the extension (no .raw) in /etc/extensions/to \"mask\" an extension with the same name in a system folder withlower precedence.A simple mechanism for version compatibility is enforced: asystem extension image must carry a/usr/lib/extension-release.d/extension-release.$name file, whichmust match its image name, that is compared with the hostos-release file: the contained ID= fields have to match unless\"_any\" is set for the extension. If the extension ID= is not\"_any\", the SYSEXT_LEVEL= field (if defined) has to match. If thelatter is not defined, the VERSION_ID= field has to matchinstead. If the extension defines the ARCHITECTURE= field and thevalue is not \"_any\" it has to match the kernel's architecturereported by uname(2) but the used architecture identifiers arethe same as for ConditionArchitecture= described insystemd.unit(5). System extensions should not ship a/usr/lib/os-release file (as that would be merged into the host/usr/ tree, overriding the host OS version data, which is notdesirable). The extension-release file follows the same formatand semantics, and carries the same content, as the os-releasefile of the OS, but it describes the resources carried in theextension image.The systemd-confext concept follows the same principle as thesystemd-sysext(1) functionality but instead of working on /usrand /opt, confext will extend only /etc. Files and directoriescontained in the confext images outside of the /etc/ hierarchyare not merged, and hence have no effect when included in theimage. Formats for these images are of the same as sysext images.The merged hierarchy will be mounted with \"nosuid\" and (if notdisabled via --noexec=false) \"noexec\".Confexts are looked for in the directories /run/confexts/,/var/lib/confexts/, /usr/lib/confexts/ and/usr/local/lib/confexts/. The first listed directory is notsuitable for carrying large binary images, however is stilluseful for carrying symlinks to them. The primary place forinstalling configuration extensions is /var/lib/confexts/. Anydirectories found in these search directories are considereddirectory based confext images; any files with the .raw suffixare considered disk image based confext images.Again, just like sysext images, the confext images will contain a/etc/extension-release.d/extension-release.$name file, which mustmatch the image name (with the usual escape hatch of xattr), andagain with content being one or more of ID=, VERSION_ID=, andCONFEXT_LEVEL. Confext images will then be checked and matchedagainst the base OS layer.",
        "name": "systemd-sysext, systemd-sysext.service, systemd-confext, systemd-confext.service - Activates System Extension Images",
        "section": 8
    },
    {
        "command": "systemd-coredump",
        "description": "systemd-coredump@.service is a system service to process coredumps. It will log a summary of the event tosystemd-journald.service(8), including information about theprocess identifier, owner, the signal that killed the process,and the stack trace if possible. It may also save the core dumpfor later processing. See the \"Information about the crashedprocess\" section below.The behavior of a specific program upon reception of a signal isgoverned by a few factors which are described in detail incore(5). In particular, the core dump will only be processed whenthe related resource limits are sufficient.Core dumps can be written to the journal or saved as a file. Inboth cases, they can be retrieved for further processing, forexample in gdb(1). See coredumpctl(1), in particular the list anddebug verbs.By default, systemd-coredump will log the core dump to thejournal, including a backtrace if possible, and store the coredump (an image of the memory contents of the process) itself inan external file in /var/lib/systemd/coredump. These core dumpsare deleted after a few days by default; see/usr/lib/tmpfiles.d/systemd.conf for details. Note that theremoval of core files from the file system and the purging ofjournal entries are independent, and the core file may be presentwithout the journal entry, and journal entries may point tosince-removed core files. Some metadata is attached to core filesin the form of extended attributes, so the core files are usefulfor some purposes even without the full metadata available in thejournal entry.For further details see systemd Coredump Handling[1].Invocation of systemd-coredumpThe systemd-coredump executable does the actual work. It isinvoked twice: once as the handler by the kernel, and the secondtime in the systemd-coredump@.service to actually write the datato the journal and process and save the core file.When the kernel invokes systemd-coredump to handle a core dump,it runs in privileged mode, and will connect to the socketcreated by the systemd-coredump.socket unit, which in turn willspawn an unprivileged systemd-coredump@.service instance toprocess the core dump. Hence systemd-coredump.socket andsystemd-coredump@.service are helper units which do the actualprocessing of core dumps and are subject to normal servicemanagement.It is also possible to invoke systemd-coredump with --backtraceoption. In this case, systemd-coredump expects a journal entry inthe journal Journal Export Format[2] on standard input. The entryshould contain a MESSAGE= field and any additional metadatafields the caller deems reasonable.systemd-coredump will appendadditional metadata fields in the same way it does for core dumpsreceived from the kernel. In this mode, no core dump is stored inthe journal.",
        "name": "systemd-coredump, systemd-coredump.socket, systemd-coredump@.service - Acquire, save and process core dumps",
        "section": 8
    },
    {
        "command": "systemd-coredump.service",
        "description": "systemd-coredump@.service is a system service to process coredumps. It will log a summary of the event tosystemd-journald.service(8), including information about theprocess identifier, owner, the signal that killed the process,and the stack trace if possible. It may also save the core dumpfor later processing. See the \"Information about the crashedprocess\" section below.The behavior of a specific program upon reception of a signal isgoverned by a few factors which are described in detail incore(5). In particular, the core dump will only be processed whenthe related resource limits are sufficient.Core dumps can be written to the journal or saved as a file. Inboth cases, they can be retrieved for further processing, forexample in gdb(1). See coredumpctl(1), in particular the list anddebug verbs.By default, systemd-coredump will log the core dump to thejournal, including a backtrace if possible, and store the coredump (an image of the memory contents of the process) itself inan external file in /var/lib/systemd/coredump. These core dumpsare deleted after a few days by default; see/usr/lib/tmpfiles.d/systemd.conf for details. Note that theremoval of core files from the file system and the purging ofjournal entries are independent, and the core file may be presentwithout the journal entry, and journal entries may point tosince-removed core files. Some metadata is attached to core filesin the form of extended attributes, so the core files are usefulfor some purposes even without the full metadata available in thejournal entry.For further details see systemd Coredump Handling[1].Invocation of systemd-coredumpThe systemd-coredump executable does the actual work. It isinvoked twice: once as the handler by the kernel, and the secondtime in the systemd-coredump@.service to actually write the datato the journal and process and save the core file.When the kernel invokes systemd-coredump to handle a core dump,it runs in privileged mode, and will connect to the socketcreated by the systemd-coredump.socket unit, which in turn willspawn an unprivileged systemd-coredump@.service instance toprocess the core dump. Hence systemd-coredump.socket andsystemd-coredump@.service are helper units which do the actualprocessing of core dumps and are subject to normal servicemanagement.It is also possible to invoke systemd-coredump with --backtraceoption. In this case, systemd-coredump expects a journal entry inthe journal Journal Export Format[2] on standard input. The entryshould contain a MESSAGE= field and any additional metadatafields the caller deems reasonable.systemd-coredump will appendadditional metadata fields in the same way it does for core dumpsreceived from the kernel. In this mode, no core dump is stored inthe journal.",
        "name": "systemd-coredump, systemd-coredump.socket, systemd-coredump@.service - Acquire, save and process core dumps",
        "section": 8
    },
    {
        "command": "systemd-coredump.socket",
        "description": "systemd-coredump@.service is a system service to process coredumps. It will log a summary of the event tosystemd-journald.service(8), including information about theprocess identifier, owner, the signal that killed the process,and the stack trace if possible. It may also save the core dumpfor later processing. See the \"Information about the crashedprocess\" section below.The behavior of a specific program upon reception of a signal isgoverned by a few factors which are described in detail incore(5). In particular, the core dump will only be processed whenthe related resource limits are sufficient.Core dumps can be written to the journal or saved as a file. Inboth cases, they can be retrieved for further processing, forexample in gdb(1). See coredumpctl(1), in particular the list anddebug verbs.By default, systemd-coredump will log the core dump to thejournal, including a backtrace if possible, and store the coredump (an image of the memory contents of the process) itself inan external file in /var/lib/systemd/coredump. These core dumpsare deleted after a few days by default; see/usr/lib/tmpfiles.d/systemd.conf for details. Note that theremoval of core files from the file system and the purging ofjournal entries are independent, and the core file may be presentwithout the journal entry, and journal entries may point tosince-removed core files. Some metadata is attached to core filesin the form of extended attributes, so the core files are usefulfor some purposes even without the full metadata available in thejournal entry.For further details see systemd Coredump Handling[1].Invocation of systemd-coredumpThe systemd-coredump executable does the actual work. It isinvoked twice: once as the handler by the kernel, and the secondtime in the systemd-coredump@.service to actually write the datato the journal and process and save the core file.When the kernel invokes systemd-coredump to handle a core dump,it runs in privileged mode, and will connect to the socketcreated by the systemd-coredump.socket unit, which in turn willspawn an unprivileged systemd-coredump@.service instance toprocess the core dump. Hence systemd-coredump.socket andsystemd-coredump@.service are helper units which do the actualprocessing of core dumps and are subject to normal servicemanagement.It is also possible to invoke systemd-coredump with --backtraceoption. In this case, systemd-coredump expects a journal entry inthe journal Journal Export Format[2] on standard input. The entryshould contain a MESSAGE= field and any additional metadatafields the caller deems reasonable.systemd-coredump will appendadditional metadata fields in the same way it does for core dumpsreceived from the kernel. In this mode, no core dump is stored inthe journal.",
        "name": "systemd-coredump, systemd-coredump.socket, systemd-coredump@.service - Acquire, save and process core dumps",
        "section": 8
    },
    {
        "command": "systemd-coredump@.service",
        "description": "systemd-coredump@.service is a system service to process coredumps. It will log a summary of the event tosystemd-journald.service(8), including information about theprocess identifier, owner, the signal that killed the process,and the stack trace if possible. It may also save the core dumpfor later processing. See the \"Information about the crashedprocess\" section below.The behavior of a specific program upon reception of a signal isgoverned by a few factors which are described in detail incore(5). In particular, the core dump will only be processed whenthe related resource limits are sufficient.Core dumps can be written to the journal or saved as a file. Inboth cases, they can be retrieved for further processing, forexample in gdb(1). See coredumpctl(1), in particular the list anddebug verbs.By default, systemd-coredump will log the core dump to thejournal, including a backtrace if possible, and store the coredump (an image of the memory contents of the process) itself inan external file in /var/lib/systemd/coredump. These core dumpsare deleted after a few days by default; see/usr/lib/tmpfiles.d/systemd.conf for details. Note that theremoval of core files from the file system and the purging ofjournal entries are independent, and the core file may be presentwithout the journal entry, and journal entries may point tosince-removed core files. Some metadata is attached to core filesin the form of extended attributes, so the core files are usefulfor some purposes even without the full metadata available in thejournal entry.For further details see systemd Coredump Handling[1].Invocation of systemd-coredumpThe systemd-coredump executable does the actual work. It isinvoked twice: once as the handler by the kernel, and the secondtime in the systemd-coredump@.service to actually write the datato the journal and process and save the core file.When the kernel invokes systemd-coredump to handle a core dump,it runs in privileged mode, and will connect to the socketcreated by the systemd-coredump.socket unit, which in turn willspawn an unprivileged systemd-coredump@.service instance toprocess the core dump. Hence systemd-coredump.socket andsystemd-coredump@.service are helper units which do the actualprocessing of core dumps and are subject to normal servicemanagement.It is also possible to invoke systemd-coredump with --backtraceoption. In this case, systemd-coredump expects a journal entry inthe journal Journal Export Format[2] on standard input. The entryshould contain a MESSAGE= field and any additional metadatafields the caller deems reasonable.systemd-coredump will appendadditional metadata fields in the same way it does for core dumpsreceived from the kernel. In this mode, no core dump is stored inthe journal.",
        "name": "systemd-coredump, systemd-coredump.socket, systemd-coredump@.service - Acquire, save and process core dumps",
        "section": 8
    },
    {
        "command": "systemd-debug-generator",
        "description": "systemd-debug-generator is a generator that reads the kernelcommand line and understands three options:If the systemd.mask= or rd.systemd.mask= option is specified andfollowed by a unit name, this unit is masked for the runtime(i.e. for this session \u2014 from boot to shutdown), similarly to theeffect of systemctl(1)'s mask command. This is useful to bootwith certain units removed from the initial boot transaction fordebugging system startup. May be specified more than once.rd.systemd.mask= is honored only by initial RAM disk (initrd)while systemd.mask= is honored only in the main system.If the systemd.wants= or rd.systemd.wants= option is specifiedand followed by a unit name, a start job for this unit is addedto the initial transaction. This is useful to start one or moreadditional units at boot. May be specified more than once.rd.systemd.wants= is honored only by initial RAM disk (initrd)while systemd.wants= is honored only in the main system.If the systemd.debug_shell or rd.systemd.debug_shell option isspecified, the debug shell service \"debug-shell.service\" ispulled into the boot transaction and a debug shell will bespawned during early boot. By default, /dev/tty9 is used, but aspecific tty can also be set, either with or without the /dev/prefix. Note that the shell may also be turned on persistently byenabling it with systemctl(1)'s enable command.rd.systemd.debug_shell= is honored only by initial RAM disk(initrd) while systemd.debug_shell is honored only in the mainsystem.systemd-debug-generator implements systemd.generator(7).",
        "name": "systemd-debug-generator - Generator for enabling a runtime debugshell and masking specific units at boot",
        "section": 8
    },
    {
        "command": "systemd-environment-d-generator",
        "description": "systemd-environment-d-generator is asystemd.environment-generator(7) that reads environmentconfiguration specified by environment.d(5) configuration filesand passes it to the systemd(1) user manager instance.",
        "name": "systemd-environment-d-generator, 30-systemd-environment-d-generator - Load variables specified by environment.d",
        "section": 8
    },
    {
        "command": "systemd-fsck",
        "description": "systemd-fsck@.service, systemd-fsck-root.service, andsystemd-fsck-usr.service are services responsible for file systemchecks. They are instantiated for each device that is configuredfor file system checking.systemd-fsck-root.service andsystemd-fsck-usr.service are responsible for file system checkson the root and /usr file system, respectively, but only if theroot filesystem was not checked in the initrd.systemd-fsck@.service is used for all other file systems and forthe root file system in the initrd.These services are started at boot if passno in /etc/fstab forthe file system is set to a value greater than zero. The filesystem check for root is performed before the other file systems.Other file systems may be checked in parallel, except when theyare on the same rotating disk.systemd-fsck does not know any details about specificfilesystems, and simply executes file system checkers specific toeach filesystem type (fsck.type). These checkers will decide ifthe filesystem should actually be checked based on the time sincelast check, number of mounts, unclean unmount, etc.systemd-fsck-root.service and systemd-fsck-usr.service willactivate reboot.target if fsck returns the \"System should reboot\"condition, or emergency.target if fsck returns the \"Filesystemerrors left uncorrected\" condition.systemd-fsck@.service will fail if fsck returns with either\"System should reboot\" or \"Filesystem errors left uncorrected\"conditions. For filesystems listed in /etc/fstab without \"nofail\"or \"noauto\" options, \"local-fs.target\" will then activateemergency.target.",
        "name": "systemd-fsck@.service, systemd-fsck-root.service, systemd-fsck-usr.service, systemd-fsck - File system checker logic",
        "section": 8
    },
    {
        "command": "systemd-fsck-root.service",
        "description": "systemd-fsck@.service, systemd-fsck-root.service, andsystemd-fsck-usr.service are services responsible for file systemchecks. They are instantiated for each device that is configuredfor file system checking.systemd-fsck-root.service andsystemd-fsck-usr.service are responsible for file system checkson the root and /usr file system, respectively, but only if theroot filesystem was not checked in the initrd.systemd-fsck@.service is used for all other file systems and forthe root file system in the initrd.These services are started at boot if passno in /etc/fstab forthe file system is set to a value greater than zero. The filesystem check for root is performed before the other file systems.Other file systems may be checked in parallel, except when theyare on the same rotating disk.systemd-fsck does not know any details about specificfilesystems, and simply executes file system checkers specific toeach filesystem type (fsck.type). These checkers will decide ifthe filesystem should actually be checked based on the time sincelast check, number of mounts, unclean unmount, etc.systemd-fsck-root.service and systemd-fsck-usr.service willactivate reboot.target if fsck returns the \"System should reboot\"condition, or emergency.target if fsck returns the \"Filesystemerrors left uncorrected\" condition.systemd-fsck@.service will fail if fsck returns with either\"System should reboot\" or \"Filesystem errors left uncorrected\"conditions. For filesystems listed in /etc/fstab without \"nofail\"or \"noauto\" options, \"local-fs.target\" will then activateemergency.target.",
        "name": "systemd-fsck@.service, systemd-fsck-root.service, systemd-fsck-usr.service, systemd-fsck - File system checker logic",
        "section": 8
    },
    {
        "command": "systemd-fsck-usr.service",
        "description": "systemd-fsck@.service, systemd-fsck-root.service, andsystemd-fsck-usr.service are services responsible for file systemchecks. They are instantiated for each device that is configuredfor file system checking.systemd-fsck-root.service andsystemd-fsck-usr.service are responsible for file system checkson the root and /usr file system, respectively, but only if theroot filesystem was not checked in the initrd.systemd-fsck@.service is used for all other file systems and forthe root file system in the initrd.These services are started at boot if passno in /etc/fstab forthe file system is set to a value greater than zero. The filesystem check for root is performed before the other file systems.Other file systems may be checked in parallel, except when theyare on the same rotating disk.systemd-fsck does not know any details about specificfilesystems, and simply executes file system checkers specific toeach filesystem type (fsck.type). These checkers will decide ifthe filesystem should actually be checked based on the time sincelast check, number of mounts, unclean unmount, etc.systemd-fsck-root.service and systemd-fsck-usr.service willactivate reboot.target if fsck returns the \"System should reboot\"condition, or emergency.target if fsck returns the \"Filesystemerrors left uncorrected\" condition.systemd-fsck@.service will fail if fsck returns with either\"System should reboot\" or \"Filesystem errors left uncorrected\"conditions. For filesystems listed in /etc/fstab without \"nofail\"or \"noauto\" options, \"local-fs.target\" will then activateemergency.target.",
        "name": "systemd-fsck@.service, systemd-fsck-root.service, systemd-fsck-usr.service, systemd-fsck - File system checker logic",
        "section": 8
    },
    {
        "command": "systemd-fsck.service",
        "description": "systemd-fsck@.service, systemd-fsck-root.service, andsystemd-fsck-usr.service are services responsible for file systemchecks. They are instantiated for each device that is configuredfor file system checking.systemd-fsck-root.service andsystemd-fsck-usr.service are responsible for file system checkson the root and /usr file system, respectively, but only if theroot filesystem was not checked in the initrd.systemd-fsck@.service is used for all other file systems and forthe root file system in the initrd.These services are started at boot if passno in /etc/fstab forthe file system is set to a value greater than zero. The filesystem check for root is performed before the other file systems.Other file systems may be checked in parallel, except when theyare on the same rotating disk.systemd-fsck does not know any details about specificfilesystems, and simply executes file system checkers specific toeach filesystem type (fsck.type). These checkers will decide ifthe filesystem should actually be checked based on the time sincelast check, number of mounts, unclean unmount, etc.systemd-fsck-root.service and systemd-fsck-usr.service willactivate reboot.target if fsck returns the \"System should reboot\"condition, or emergency.target if fsck returns the \"Filesystemerrors left uncorrected\" condition.systemd-fsck@.service will fail if fsck returns with either\"System should reboot\" or \"Filesystem errors left uncorrected\"conditions. For filesystems listed in /etc/fstab without \"nofail\"or \"noauto\" options, \"local-fs.target\" will then activateemergency.target.",
        "name": "systemd-fsck@.service, systemd-fsck-root.service, systemd-fsck-usr.service, systemd-fsck - File system checker logic",
        "section": 8
    },
    {
        "command": "systemd-fsck@.service",
        "description": "systemd-fsck@.service, systemd-fsck-root.service, andsystemd-fsck-usr.service are services responsible for file systemchecks. They are instantiated for each device that is configuredfor file system checking.systemd-fsck-root.service andsystemd-fsck-usr.service are responsible for file system checkson the root and /usr file system, respectively, but only if theroot filesystem was not checked in the initrd.systemd-fsck@.service is used for all other file systems and forthe root file system in the initrd.These services are started at boot if passno in /etc/fstab forthe file system is set to a value greater than zero. The filesystem check for root is performed before the other file systems.Other file systems may be checked in parallel, except when theyare on the same rotating disk.systemd-fsck does not know any details about specificfilesystems, and simply executes file system checkers specific toeach filesystem type (fsck.type). These checkers will decide ifthe filesystem should actually be checked based on the time sincelast check, number of mounts, unclean unmount, etc.systemd-fsck-root.service and systemd-fsck-usr.service willactivate reboot.target if fsck returns the \"System should reboot\"condition, or emergency.target if fsck returns the \"Filesystemerrors left uncorrected\" condition.systemd-fsck@.service will fail if fsck returns with either\"System should reboot\" or \"Filesystem errors left uncorrected\"conditions. For filesystems listed in /etc/fstab without \"nofail\"or \"noauto\" options, \"local-fs.target\" will then activateemergency.target.",
        "name": "systemd-fsck@.service, systemd-fsck-root.service, systemd-fsck-usr.service, systemd-fsck - File system checker logic",
        "section": 8
    },
    {
        "command": "systemd-fstab-generator",
        "description": "systemd-fstab-generator is a generator that translates /etc/fstab(see fstab(5) for details) into native systemd units early atboot and when configuration of the system manager is reloaded.This will instantiate mount and swap units as necessary.The passno field is treated like a simple boolean, and theordering information is discarded. However, if the root filesystem is checked, it is checked before all the other filesystems.See systemd.mount(5) and systemd.swap(5) for more informationabout special /etc/fstab mount options this generatorunderstands.One special topic is handling of symbolic links. Historical initimplementations supported symlinks in /etc/fstab. Because mountunits will refuse mounts where the target is a symbolic link,this generator will resolve any symlinks as far as possible whenprocessing /etc/fstab in order to enhance backwardscompatibility. If a symlink target does not exist at the timethat this generator runs, it is assumed that the symlink targetis the final target of the mount.systemd-fstab-generator implements systemd.generator(7).",
        "name": "systemd-fstab-generator - Unit generator for /etc/fstab",
        "section": 8
    },
    {
        "command": "systemd-getty-generator",
        "description": "systemd-getty-generator is a generator that automaticallyinstantiates serial-getty@.service on the kernel consoles, ifthey can function as ttys and are not provided by the virtualconsole subsystem. It will also instantiate serial-getty@.serviceinstances for virtualizer consoles, if execution in a virtualizedenvironment is detected. If execution in a container environmentis detected, it will instead enable console-getty.service for/dev/console, and container-getty@.service instances foradditional container pseudo TTYs as requested by the containermanager (see Container Interface[1]). This should ensure that theuser is shown a login prompt at the right place, regardless ofwhich environment the system is started in. For example, it issufficient to redirect the kernel console with a kernel commandline argument such as console= to get both kernel messages and agetty prompt on a serial TTY. See The kernel's command-lineparameters[2] for more information on the console= kernelparameter.systemd-getty-generator implements systemd.generator(7).Further information about configuration of gettys can be found insystemd for Administrators, Part XVI: Gettys on Serial Consoles(and Elsewhere)[3].",
        "name": "systemd-getty-generator - Generator for enabling getty instanceson the console",
        "section": 8
    },
    {
        "command": "systemd-gpt-auto-generator",
        "description": "systemd-gpt-auto-generator is a unit generator that automaticallydiscovers the root partition, /home/, /srv/, /var/, /var/tmp/,the EFI System Partition, the Extended Boot Loader Partition, andswap partitions and creates mount and swap units for them, basedon the partition type GUIDs of GUID partition tables (GPT). SeeUEFI Specification[1], chapter 5 for more details. It implementsthe Discoverable Partitions Specification[2].Note that this generator has no effect on non-GPT systems. Itwill also not create mount point configuration for directorieswhich already contain files or if the mount point is explicitlyconfigured in fstab(5). If the units this generator creates areoverridden, for example by units in directories with higherprecedence, drop-ins and additional dependencies created by thisgenerator might still be used.This generator will only look for the root partition on the samephysical disk where the EFI System Partition (ESP) is located.Note that support from the boot loader is required: the EFIvariable LoaderDevicePartUUID of the4a67b082-0a4c-41cf-b6c7-440b29bb8c4f vendor UUID is used todetermine from which partition, and hence the disk, from whichthe system was booted. If the boot loader does not set thisvariable, this generator will not be able to detect the rootpartition. See the Boot Loader Interface[3] for details.Similarly, this generator will only look for the other partitionson the same physical disk as the root partition. In this case,boot loader support is not required. These partitions will not besearched for on systems where the root file system is distributedon multiple disks, for example via btrfs RAID.systemd-gpt-auto-generator is useful for centralizing file systemconfiguration in the partition table and making configuration in/etc/fstab or on the kernel command line unnecessary.This generator looks for the partitions based on their partitiontype GUID. The following partition type GUIDs are identified:Table 1. Partition Type GUIDs\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502Partition Type\u2502 Name\u2502 Mount Point \u2502 Explanation\u2502\u2502GUID\u2502\u2502\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_ROOT_X86_64\u2502 Root Partition \u2502 /\u2502 The first\u2502\u25024f68bce3-e8cd-4db1-96e7-fbcaf984b709 \u2502 (x86-64)\u2502\u2502 partition with\u2502\u2502\u2502\u2502\u2502 this type\u2502\u2502\u2502\u2502\u2502 UUID, located\u2502\u2502\u2502\u2502\u2502 on the same\u2502\u2502\u2502\u2502\u2502 disk as the\u2502\u2502\u2502\u2502\u2502 ESP, is used\u2502\u2502\u2502\u2502\u2502 as the root\u2502\u2502\u2502\u2502\u2502 file system /\u2502\u2502\u2502\u2502\u2502 on AMD64 /\u2502\u2502\u2502\u2502\u2502 64-bit x86\u2502\u2502\u2502\u2502\u2502 systems.\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_ROOT_ARM64\u2502 Root Partition \u2502 /\u2502 The first\u2502\u2502b921b045-1df0-41c3-af44-4c6f280d3fae \u2502 (64-bit ARM)\u2502\u2502 partition with\u2502\u2502\u2502\u2502\u2502 this type\u2502\u2502\u2502\u2502\u2502 UUID, located\u2502\u2502\u2502\u2502\u2502 on the same\u2502\u2502\u2502\u2502\u2502 disk as the\u2502\u2502\u2502\u2502\u2502 ESP, is used\u2502\u2502\u2502\u2502\u2502 as the root\u2502\u2502\u2502\u2502\u2502 file system /\u2502\u2502\u2502\u2502\u2502 on AArch64 /\u2502\u2502\u2502\u2502\u2502 64-bit ARM\u2502\u2502\u2502\u2502\u2502 systems.\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_ROOT_ALPHA SD_GPT_ROOT_ARC\u2502 root\u2502 /\u2502 The first\u2502\u2502SD_GPT_ROOT_ARM SD_GPT_ROOT_ARM64\u2502 partitions for \u2502\u2502 partition with\u2502\u2502SD_GPT_ROOT_IA64\u2502 other\u2502\u2502 the type UUID\u2502\u2502SD_GPT_ROOT_LOONGARCH64\u2502 architectures\u2502\u2502 matching the\u2502\u2502SD_GPT_ROOT_MIPS_LE\u2502\u2502\u2502 architecture,\u2502\u2502SD_GPT_ROOT_MIPS64_LE\u2502\u2502\u2502 located on the\u2502\u2502SD_GPT_ROOT_PARISC SD_GPT_ROOT_PPC\u2502\u2502\u2502 same disk as\u2502\u2502SD_GPT_ROOT_PPC64\u2502\u2502\u2502 the ESP, is\u2502\u2502SD_GPT_ROOT_PPC64_LE\u2502\u2502\u2502 used as the\u2502\u2502SD_GPT_ROOT_RISCV32\u2502\u2502\u2502 root file\u2502\u2502SD_GPT_ROOT_RISCV64 SD_GPT_ROOT_S390 \u2502\u2502\u2502 system /. For\u2502\u2502SD_GPT_ROOT_S390X SD_GPT_ROOT_TILEGX \u2502\u2502\u2502 the full list\u2502\u2502SD_GPT_ROOT_X86 SD_GPT_ROOT_X86_64\u2502\u2502\u2502 and constant\u2502\u2502SD_GPT_USR_ALPHA SD_GPT_USR_ARC\u2502\u2502\u2502 values, see\u2502\u2502SD_GPT_USR_ARM SD_GPT_USR_IA64\u2502\u2502\u2502 Discoverable\u2502\u2502SD_GPT_USR_LOONGARCH64\u2502\u2502\u2502 Partitions\u2502\u2502SD_GPT_USR_MIPS_LE\u2502\u2502\u2502 Specification[2]. \u2502\u2502SD_GPT_USR_MIPS64_LE\u2502\u2502\u2502\u2502\u2502SD_GPT_USR_PARISC SD_GPT_USR_PPC\u2502\u2502\u2502\u2502\u2502SD_GPT_USR_PPC64 SD_GPT_USR_PPC64_LE \u2502\u2502\u2502\u2502\u2502SD_GPT_USR_RISCV32\u2502\u2502\u2502\u2502\u2502SD_GPT_USR_RISCV64 SD_GPT_USR_S390\u2502\u2502\u2502\u2502\u2502SD_GPT_USR_S390X SD_GPT_USR_TILEGX\u2502\u2502\u2502\u2502\u2502SD_GPT_USR_X86\u2502\u2502\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_HOME\u2502 Home Partition \u2502 /home/\u2502 The first\u2502\u2502933ac7e1-2eb4-4f13-b844-0e14e2aef915 \u2502\u2502\u2502 partition with\u2502\u2502\u2502\u2502\u2502 this type UUID on \u2502\u2502\u2502\u2502\u2502 the same disk as\u2502\u2502\u2502\u2502\u2502 the ESP is\u2502\u2502\u2502\u2502\u2502 mounted to\u2502\u2502\u2502\u2502\u2502 /home/.\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_SRV\u2502 Server Data\u2502 /srv/\u2502 The first\u2502\u25023b8f8425-20e0-4f3b-907f-1a25a76f98e8 \u2502 Partition\u2502\u2502 partition with\u2502\u2502\u2502\u2502\u2502 this type UUID on \u2502\u2502\u2502\u2502\u2502 the same disk as\u2502\u2502\u2502\u2502\u2502 the ESP is\u2502\u2502\u2502\u2502\u2502 mounted to /srv/. \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_VAR\u2502 Variable Data\u2502 /var/\u2502 The first\u2502\u25024d21b016-b534-45c2-a9fb-5c16e091fd2d \u2502 Partition\u2502\u2502 partition with\u2502\u2502\u2502\u2502\u2502 this type UUID on \u2502\u2502\u2502\u2502\u2502 the same disk as\u2502\u2502\u2502\u2502\u2502 the ESP is\u2502\u2502\u2502\u2502\u2502 mounted to /var/\u2502\u2502\u2502\u2502\u2502 \u2014 under the\u2502\u2502\u2502\u2502\u2502 condition its\u2502\u2502\u2502\u2502\u2502 partition UUID\u2502\u2502\u2502\u2502\u2502 matches the first \u2502\u2502\u2502\u2502\u2502 128 bit of the\u2502\u2502\u2502\u2502\u2502 HMAC-SHA256 of\u2502\u2502\u2502\u2502\u2502 the GPT type uuid \u2502\u2502\u2502\u2502\u2502 of this partition \u2502\u2502\u2502\u2502\u2502 keyed by the\u2502\u2502\u2502\u2502\u2502 machine ID of the \u2502\u2502\u2502\u2502\u2502 installation\u2502\u2502\u2502\u2502\u2502 stored in\u2502\u2502\u2502\u2502\u2502 machine-id(5).\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_TMP\u2502 Temporary Data \u2502 /var/tmp/\u2502 The first\u2502\u25027ec6f557-3bc5-4aca-b293-16ef5df639d1 \u2502 Partition\u2502\u2502 partition with\u2502\u2502\u2502\u2502\u2502 this type UUID on \u2502\u2502\u2502\u2502\u2502 the same disk as\u2502\u2502\u2502\u2502\u2502 the ESP is\u2502\u2502\u2502\u2502\u2502 mounted to\u2502\u2502\u2502\u2502\u2502 /var/tmp/.\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_SWAP\u2502 Swap\u2502 n/a\u2502 All partitions\u2502\u25020657fd6d-a4ab-43c4-84e5-0933c84b4f4f \u2502\u2502\u2502 with this type\u2502\u2502\u2502\u2502\u2502 UUID on the same\u2502\u2502\u2502\u2502\u2502 disk as the ESP\u2502\u2502\u2502\u2502\u2502 are used as swap. \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_ESP\u2502 EFI System\u2502 /efi/ or\u2502 The first\u2502\u2502c12a7328-f81f-11d2-ba4b-00a0c93ec93b \u2502 Partition\u2502 /boot/\u2502 partition with\u2502\u2502\u2502 (ESP)\u2502\u2502 this type UUID\u2502\u2502\u2502\u2502\u2502 located on the\u2502\u2502\u2502\u2502\u2502 same disk as the\u2502\u2502\u2502\u2502\u2502 root partition is \u2502\u2502\u2502\u2502\u2502 mounted to /boot/ \u2502\u2502\u2502\u2502\u2502 or /efi/, see\u2502\u2502\u2502\u2502\u2502 below.\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_XBOOTLDR\u2502 Extended Boot\u2502 /boot/\u2502 The first\u2502\u2502bc13c2ff-59e6-4262-a352-b275fd6f7172 \u2502 Loader\u2502\u2502 partition with\u2502\u2502\u2502 Partition\u2502\u2502 this type UUID\u2502\u2502\u2502\u2502\u2502 located on the\u2502\u2502\u2502\u2502\u2502 same disk as the\u2502\u2502\u2502\u2502\u2502 root partition is \u2502\u2502\u2502\u2502\u2502 mounted to\u2502\u2502\u2502\u2502\u2502 /boot/, see\u2502\u2502\u2502\u2502\u2502 below.\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518This generator understands the following attribute flags forpartitions:Table 2. Partition Attribute Flags\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502Flag\u2502 Applicable to\u2502 Explanation\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_FLAG_READ_ONLY\u2502 /, /home/, /srv/, \u2502 Partition is\u2502\u25020x1000000000000000\u2502 /var/, /var/tmp/, \u2502 mounted read-only \u2502\u2502\u2502 Extended Boot\u2502\u2502\u2502\u2502 Loader Partition\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_FLAG_NO_AUTO\u2502 /, /home/, /srv/, \u2502 Partition is not\u2502\u25020x8000000000000000\u2502 /var/, /var/tmp/, \u2502 mounted\u2502\u2502\u2502 Extended Boot\u2502 automatically\u2502\u2502\u2502 Loader Partition\u2502\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502SD_GPT_FLAG_NO_BLOCK_IO_PROTOCOL \u2502 EFI System\u2502 Partition is not\u2502\u25020x0000000000000002\u2502 Partition (ESP)\u2502 mounted\u2502\u2502\u2502\u2502 automatically\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518The /home/, /srv/, /var/, /var/tmp/ and swap partitions may beencrypted in LUKS format. In this case, a device mapper device isset up under the names /dev/mapper/home, /dev/mapper/srv,/dev/mapper/var, /dev/mapper/tmp or /dev/mapper/swap. Note thatthis might create conflicts if the same partition is listed in/etc/crypttab with a different device mapper device name.When systemd is running in the initrd the / partition may beencrypted with LUKS as well. In this case, a device mapper deviceis set up under the name /dev/mapper/root, and a sysroot.mount isset up that mounts the device under /sysroot. For moreinformation, see bootup(7).The root partition can be specified by symlinking/run/systemd/volatile-root to /dev/block/$major:$minor. This isespecially useful if the root mount has been replaced by someform of volatile file system (overlayfs).Mount and automount units for the EFI System Partition (ESP) andExtended Boot Loader Partition (XBOOTLDR) are generated on EFIsystems. If the disk contains an XBOOTLDR partition, as definedin the Boot Loader Specification[4], it is made available at/boot/. This generator creates an automount unit; the mount willonly be activated on-demand when accessed. The mount point willbe created if necessary.The ESP is mounted to /boot/ if that directory exists and is notused for XBOOTLDR, and otherwise to /efi/. Same as for /boot/, anautomount unit is used. The mount point will be created ifnecessary.No configuration is created for mount points that are configuredin fstab(5) or when the target directory contains files.When using this generator in conjunction with btrfs file systems,make sure to set the correct default subvolumes on them, usingbtrfs subvolume set-default.If the system was booted via systemd-stub(7) and the stubreported to userspace that the kernel image was measured to aTPM2 PCR, then any discovered root and /var/ volume identifiers(and volume encryption key in case it is encrypted) will beautomatically measured into PCR 15 on activation, viasystemd-pcrfs@.service(8).systemd-gpt-auto-generator implements systemd.generator(7).",
        "name": "systemd-gpt-auto-generator - Generator for automaticallydiscovering and mounting root, /home/, /srv/, /var/ and /var/tmp/partitions, as well as discovering and enabling swap partitions,based on GPT partition type GUIDs",
        "section": 8
    },
    {
        "command": "systemd-growfs",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-growfs-root.service",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-growfs.service",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-growfs@.service",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-halt.service",
        "description": "systemd-poweroff.service is a system service that is pulled in bypoweroff.target and is responsible for the actual systempower-off operation. Similarly, systemd-halt.service is pulled inby halt.target, systemd-reboot.service by reboot.target andsystemd-kexec.service by kexec.target to execute the respectiveactions.When these services are run, they ensure that PID 1 is replacedby the /usr/lib/systemd/systemd-shutdown tool which is thenresponsible for the actual shutdown. Before shutting down, thisbinary will try to unmount all remaining file systems (or atleast remount them read-only), disable all remaining swapdevices, detach all remaining storage devices and kill allremaining processes.It is necessary to have this code in a separate binary becauseotherwise rebooting after an upgrade might be broken \u2014 therunning PID 1 could still depend on libraries which are notavailable any more, thus keeping the file system busy, which thencannot be re-mounted read-only.Shortly before executing the actual systempower-off/halt/reboot/kexec systemd-shutdown will run allexecutables in /usr/lib/systemd/system-shutdown/ and pass onearguments to them: either \"poweroff\", \"halt\", \"reboot\", or\"kexec\", depending on the chosen action. All executables in thisdirectory are executed in parallel, and execution of the actionis not continued before all executables finished. Note that theseexecutables are run after all services have been shut down, andafter most mounts have been detached (the root file system aswell as /run/ and various API file systems are still aroundthough). This means any programs dropped into this directory mustbe prepared to run in such a limited execution environment andnot rely on external services or hierarchies such as /var/ to bearound (or writable).Note that systemd-poweroff.service (and the related units) shouldnever be executed directly. Instead, trigger system shutdown witha command such as \"systemctl poweroff\".Another form of shutdown is provided by thesystemd-soft-reboot.service(8) functionality. It reboots only theOS userspace, leaving the kernel, firmware, and hardware as itis.",
        "name": "systemd-poweroff.service, systemd-halt.service, systemd-reboot.service, systemd-kexec.service, systemd-shutdown - Systemshutdown logic",
        "section": 8
    },
    {
        "command": "systemd-hibernate-resume",
        "description": "systemd-hibernate-resume@.service initiates the resume fromhibernation. It is instantiated with the device to resume from asthe template argument.systemd-hibernate-resume only supports the in-kernel hibernationimplementation, see Swap suspend[1]. Internally, it works bywriting the major:minor of specified device node to/sys/power/resume.Failing to initiate a resume is not an error condition. It maymean that there was no resume image (e. g. if the system has beensimply powered off and not hibernated). In such case, the boot isordinarily continued.",
        "name": "systemd-hibernate-resume@.service, systemd-hibernate-resume -Resume from hibernation",
        "section": 8
    },
    {
        "command": "systemd-hibernate-resume-generator",
        "description": "systemd-hibernate-resume-generator is a generator that initiatesthe procedure to resume the system from hibernation. Itinstantiates the systemd-hibernate-resume@.service(8) unitaccording to the value of resume= parameter specified on thekernel command line, which will instruct the kernel to resume thesystem from the hibernation image on that device.",
        "name": "systemd-hibernate-resume-generator - Unit generator for resume=kernel parameter",
        "section": 8
    },
    {
        "command": "systemd-hibernate-resume.service",
        "description": "systemd-hibernate-resume@.service initiates the resume fromhibernation. It is instantiated with the device to resume from asthe template argument.systemd-hibernate-resume only supports the in-kernel hibernationimplementation, see Swap suspend[1]. Internally, it works bywriting the major:minor of specified device node to/sys/power/resume.Failing to initiate a resume is not an error condition. It maymean that there was no resume image (e. g. if the system has beensimply powered off and not hibernated). In such case, the boot isordinarily continued.",
        "name": "systemd-hibernate-resume@.service, systemd-hibernate-resume -Resume from hibernation",
        "section": 8
    },
    {
        "command": "systemd-hibernate-resume@.service",
        "description": "systemd-hibernate-resume@.service initiates the resume fromhibernation. It is instantiated with the device to resume from asthe template argument.systemd-hibernate-resume only supports the in-kernel hibernationimplementation, see Swap suspend[1]. Internally, it works bywriting the major:minor of specified device node to/sys/power/resume.Failing to initiate a resume is not an error condition. It maymean that there was no resume image (e. g. if the system has beensimply powered off and not hibernated). In such case, the boot isordinarily continued.",
        "name": "systemd-hibernate-resume@.service, systemd-hibernate-resume -Resume from hibernation",
        "section": 8
    },
    {
        "command": "systemd-hibernate.service",
        "description": "systemd-suspend.service is a system service that is pulled in bysuspend.target and is responsible for the actual system suspend.Similarly, systemd-hibernate.service is pulled in byhibernate.target to execute the actual hibernation. Finally,systemd-hybrid-sleep.service is pulled in by hybrid-sleep.targetto execute hybrid hibernation with system suspend and pulled inby suspend-then-hibernate.target to execute system suspend with atimeout that will activate hibernate later.Immediately before entering system suspend and/or hibernationsystemd-suspend.service (and the other mentioned units,respectively) will run all executables in/usr/lib/systemd/system-sleep/ and pass two arguments to them.The first argument will be \"pre\", the second either \"suspend\",\"hibernate\", \"hybrid-sleep\", or \"suspend-then-hibernate\"depending on the chosen action. An environment variable called\"SYSTEMD_SLEEP_ACTION\" will be set and contain the sleep actionthat is processing. This is primarily helpful for\"suspend-then-hibernate\" where the value of the variable will be\"suspend\", \"hibernate\", or \"suspend-after-failed-hibernate\" incases where hibernation has failed. Immediately after leavingsystem suspend and/or hibernation the same executables are run,but the first argument is now \"post\". All executables in thisdirectory are executed in parallel, and execution of the actionis not continued until all executables have finished.Note that scripts or binaries dropped in/usr/lib/systemd/system-sleep/ are intended for local use onlyand should be considered hacks. If applications want to react tosystem suspend/hibernation and resume, they should rather use theInhibitor interface[1].Note that systemd-suspend.service, systemd-hibernate.service,systemd-hybrid-sleep.service, andsystemd-suspend-then-hibernate.service should never be executeddirectly. Instead, trigger system sleep with a command such assystemctl suspend or systemctl hibernate.Internally, this service will echo a string like \"mem\" into/sys/power/state, to trigger the actual system suspend. Whatexactly is written where can be configured in the [Sleep] sectionof /etc/systemd/sleep.conf or a sleep.conf.d file. Seesystemd-sleep.conf(5).",
        "name": "systemd-suspend.service, systemd-hibernate.service, systemd-hybrid-sleep.service, systemd-suspend-then-hibernate.service,systemd-sleep - System sleep state logic",
        "section": 8
    },
    {
        "command": "systemd-hostnamed",
        "description": "systemd-hostnamed.service is a system service that may be used tochange the system's hostname and related machine metadata fromuser programs. It is automatically activated on request andterminates itself when unused.It currently offers access to five variables:\u2022The current hostname (Example: \"dhcp-192-168-47-11\")\u2022The static (configured) hostname (Example:\"lennarts-computer\")\u2022The pretty hostname (Example: \"Lennart's Computer\")\u2022A suitable icon name for the local host (Example:\"computer-laptop\")\u2022A chassis type (Example: \"tablet\")The static hostname is stored in /etc/hostname, see hostname(5)for more information. The pretty hostname, chassis type, and iconname are stored in /etc/machine-info, see machine-info(5).The tool hostnamectl(1) is a command line client to this service.See org.freedesktop.hostname1(5) andorg.freedesktop.LogControl1(5) for a description of the D-BusAPI.",
        "name": "systemd-hostnamed.service, systemd-hostnamed - Daemon to controlsystem hostname from programs",
        "section": 8
    },
    {
        "command": "systemd-hostnamed.service",
        "description": "systemd-hostnamed.service is a system service that may be used tochange the system's hostname and related machine metadata fromuser programs. It is automatically activated on request andterminates itself when unused.It currently offers access to five variables:\u2022The current hostname (Example: \"dhcp-192-168-47-11\")\u2022The static (configured) hostname (Example:\"lennarts-computer\")\u2022The pretty hostname (Example: \"Lennart's Computer\")\u2022A suitable icon name for the local host (Example:\"computer-laptop\")\u2022A chassis type (Example: \"tablet\")The static hostname is stored in /etc/hostname, see hostname(5)for more information. The pretty hostname, chassis type, and iconname are stored in /etc/machine-info, see machine-info(5).The tool hostnamectl(1) is a command line client to this service.See org.freedesktop.hostname1(5) andorg.freedesktop.LogControl1(5) for a description of the D-BusAPI.",
        "name": "systemd-hostnamed.service, systemd-hostnamed - Daemon to controlsystem hostname from programs",
        "section": 8
    },
    {
        "command": "systemd-hwdb",
        "description": "systemd-hwdb expects a command and command specific arguments. Itmanages the binary hardware database.",
        "name": "systemd-hwdb - hardware database management tool",
        "section": 8
    },
    {
        "command": "systemd-hybrid-sleep.service",
        "description": "systemd-suspend.service is a system service that is pulled in bysuspend.target and is responsible for the actual system suspend.Similarly, systemd-hibernate.service is pulled in byhibernate.target to execute the actual hibernation. Finally,systemd-hybrid-sleep.service is pulled in by hybrid-sleep.targetto execute hybrid hibernation with system suspend and pulled inby suspend-then-hibernate.target to execute system suspend with atimeout that will activate hibernate later.Immediately before entering system suspend and/or hibernationsystemd-suspend.service (and the other mentioned units,respectively) will run all executables in/usr/lib/systemd/system-sleep/ and pass two arguments to them.The first argument will be \"pre\", the second either \"suspend\",\"hibernate\", \"hybrid-sleep\", or \"suspend-then-hibernate\"depending on the chosen action. An environment variable called\"SYSTEMD_SLEEP_ACTION\" will be set and contain the sleep actionthat is processing. This is primarily helpful for\"suspend-then-hibernate\" where the value of the variable will be\"suspend\", \"hibernate\", or \"suspend-after-failed-hibernate\" incases where hibernation has failed. Immediately after leavingsystem suspend and/or hibernation the same executables are run,but the first argument is now \"post\". All executables in thisdirectory are executed in parallel, and execution of the actionis not continued until all executables have finished.Note that scripts or binaries dropped in/usr/lib/systemd/system-sleep/ are intended for local use onlyand should be considered hacks. If applications want to react tosystem suspend/hibernation and resume, they should rather use theInhibitor interface[1].Note that systemd-suspend.service, systemd-hibernate.service,systemd-hybrid-sleep.service, andsystemd-suspend-then-hibernate.service should never be executeddirectly. Instead, trigger system sleep with a command such assystemctl suspend or systemctl hibernate.Internally, this service will echo a string like \"mem\" into/sys/power/state, to trigger the actual system suspend. Whatexactly is written where can be configured in the [Sleep] sectionof /etc/systemd/sleep.conf or a sleep.conf.d file. Seesystemd-sleep.conf(5).",
        "name": "systemd-suspend.service, systemd-hibernate.service, systemd-hybrid-sleep.service, systemd-suspend-then-hibernate.service,systemd-sleep - System sleep state logic",
        "section": 8
    },
    {
        "command": "systemd-importd",
        "description": "systemd-importd is a system service that allows importing,exporting and downloading of system images suitable for runningas VM or containers. It is a companion service forsystemd-machined.service(8), and provides the implementation formachinectl(1)'s pull-raw, pull-tar, import-raw, import-tar,import-fs, export-raw, and export-tar commands.See org.freedesktop.import1(5) and org.freedesktop.LogControl1(5)for a description of the D-Bus API.",
        "name": "systemd-importd.service, systemd-importd - VM and container imageimport and export service",
        "section": 8
    },
    {
        "command": "systemd-importd.service",
        "description": "systemd-importd is a system service that allows importing,exporting and downloading of system images suitable for runningas VM or containers. It is a companion service forsystemd-machined.service(8), and provides the implementation formachinectl(1)'s pull-raw, pull-tar, import-raw, import-tar,import-fs, export-raw, and export-tar commands.See org.freedesktop.import1(5) and org.freedesktop.LogControl1(5)for a description of the D-Bus API.",
        "name": "systemd-importd.service, systemd-importd - VM and container imageimport and export service",
        "section": 8
    },
    {
        "command": "systemd-initctl",
        "description": "systemd-initctl is a system service that implements compatibilitywith the /dev/initctl FIFO file system object, as implemented bythe SysV init system.systemd-initctl is automatically activatedon request and terminates itself when it is unused.",
        "name": "systemd-initctl.service, systemd-initctl.socket, systemd-initctl- /dev/initctl compatibility",
        "section": 8
    },
    {
        "command": "systemd-initctl.service",
        "description": "systemd-initctl is a system service that implements compatibilitywith the /dev/initctl FIFO file system object, as implemented bythe SysV init system.systemd-initctl is automatically activatedon request and terminates itself when it is unused.",
        "name": "systemd-initctl.service, systemd-initctl.socket, systemd-initctl- /dev/initctl compatibility",
        "section": 8
    },
    {
        "command": "systemd-initctl.socket",
        "description": "systemd-initctl is a system service that implements compatibilitywith the /dev/initctl FIFO file system object, as implemented bythe SysV init system.systemd-initctl is automatically activatedon request and terminates itself when it is unused.",
        "name": "systemd-initctl.service, systemd-initctl.socket, systemd-initctl- /dev/initctl compatibility",
        "section": 8
    },
    {
        "command": "systemd-journald",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-journald-audit.socket",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-journald-dev-log.socket",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-journald-varlink.socket",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-journald-varlink@.socket",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-journald.service",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-journald.socket",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-journald@.service",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-journald@.socket",
        "description": "systemd-journald is a system service that collects and storeslogging data. It creates and maintains structured, indexedjournals based on logging information that is received from avariety of sources:\u2022Kernel log messages, via kmsg\u2022Simple system log messages, via the libc syslog(3) call\u2022Structured system log messages via the native Journal API,see sd_journal_print(3) and Native Journal Protocol[1]\u2022Standard output and standard error of service units. Forfurther details see below.\u2022Audit records, originating from the kernel audit subsystemThe daemon will implicitly collect numerous metadata fields foreach log messages in a secure and unfakeable way. Seesystemd.journal-fields(7) for more information about thecollected metadata.Log data collected by the journal is primarily text-based but canalso include binary data where necessary. Individual fieldsmaking up a log record stored in the journal may be up to 2\u2076\u2074-1bytes in size.The journal service stores log data either persistently below/var/log/journal or in a volatile way below /run/log/journal/ (inthe latter case it is lost at reboot). By default, log data isstored persistently if /var/log/journal/ exists during boot, withan implicit fallback to volatile storage otherwise. Use Storage=in journald.conf(5) to configure where log data is placed,independently of the existence of /var/log/journal/.Note that journald will initially use volatile storage, until acall to journalctl --flush (or sending SIGUSR1 to journald) willcause it to switch to persistent logging (under the conditionsmentioned above). This is done automatically on boot via\"systemd-journal-flush.service\".On systems where /var/log/journal/ does not exist yet but wherepersistent logging is desired (and the default journald.conf isused), it is sufficient to create the directory, and ensure ithas the correct access modes and ownership:mkdir -p /var/log/journalsystemd-tmpfiles --create --prefix /var/log/journalSee journald.conf(5) for information about the configuration ofthis service.",
        "name": "systemd-journald.service, systemd-journald.socket, systemd-journald-dev-log.socket, systemd-journald-audit.socket, systemd-journald@.service, systemd-journald@.socket, systemd-journald-varlink@.socket, systemd-journald - Journal service",
        "section": 8
    },
    {
        "command": "systemd-kexec.service",
        "description": "systemd-poweroff.service is a system service that is pulled in bypoweroff.target and is responsible for the actual systempower-off operation. Similarly, systemd-halt.service is pulled inby halt.target, systemd-reboot.service by reboot.target andsystemd-kexec.service by kexec.target to execute the respectiveactions.When these services are run, they ensure that PID 1 is replacedby the /usr/lib/systemd/systemd-shutdown tool which is thenresponsible for the actual shutdown. Before shutting down, thisbinary will try to unmount all remaining file systems (or atleast remount them read-only), disable all remaining swapdevices, detach all remaining storage devices and kill allremaining processes.It is necessary to have this code in a separate binary becauseotherwise rebooting after an upgrade might be broken \u2014 therunning PID 1 could still depend on libraries which are notavailable any more, thus keeping the file system busy, which thencannot be re-mounted read-only.Shortly before executing the actual systempower-off/halt/reboot/kexec systemd-shutdown will run allexecutables in /usr/lib/systemd/system-shutdown/ and pass onearguments to them: either \"poweroff\", \"halt\", \"reboot\", or\"kexec\", depending on the chosen action. All executables in thisdirectory are executed in parallel, and execution of the actionis not continued before all executables finished. Note that theseexecutables are run after all services have been shut down, andafter most mounts have been detached (the root file system aswell as /run/ and various API file systems are still aroundthough). This means any programs dropped into this directory mustbe prepared to run in such a limited execution environment andnot rely on external services or hierarchies such as /var/ to bearound (or writable).Note that systemd-poweroff.service (and the related units) shouldnever be executed directly. Instead, trigger system shutdown witha command such as \"systemctl poweroff\".Another form of shutdown is provided by thesystemd-soft-reboot.service(8) functionality. It reboots only theOS userspace, leaving the kernel, firmware, and hardware as itis.",
        "name": "systemd-poweroff.service, systemd-halt.service, systemd-reboot.service, systemd-kexec.service, systemd-shutdown - Systemshutdown logic",
        "section": 8
    },
    {
        "command": "systemd-localed",
        "description": "systemd-localed.service is a system service that may be used asmechanism to change the system locale settings, as well as theconsole key mapping and default X11 key mapping.systemd-localedis automatically activated on request and terminates itself whenit is unused.The tool localectl(1) is a command line client to this service.See org.freedesktop.locale1(5) and org.freedesktop.LogControl1(5)for a description of the D-Bus API.",
        "name": "systemd-localed.service, systemd-localed - Locale bus mechanism",
        "section": 8
    },
    {
        "command": "systemd-localed.service",
        "description": "systemd-localed.service is a system service that may be used asmechanism to change the system locale settings, as well as theconsole key mapping and default X11 key mapping.systemd-localedis automatically activated on request and terminates itself whenit is unused.The tool localectl(1) is a command line client to this service.See org.freedesktop.locale1(5) and org.freedesktop.LogControl1(5)for a description of the D-Bus API.",
        "name": "systemd-localed.service, systemd-localed - Locale bus mechanism",
        "section": 8
    },
    {
        "command": "systemd-logind",
        "description": "systemd-logind is a system service that manages user logins. Itis responsible for:\u2022Keeping track of users and sessions, their processes andtheir idle state. This is implemented by allocating a systemdslice unit for each user below user.slice, and a scope unitbelow it for each concurrent session of a user. Also, aper-user service manager is started as system serviceinstance of user@.service for each logged in user.\u2022Generating and managing session IDs. If auditing is availableand an audit session ID is already set for a session, thenthis ID is reused as the session ID. Otherwise, anindependent session counter is used.\u2022Providing polkit[1]-based access for users for operationssuch as system shutdown or sleep\u2022Implementing a shutdown/sleep inhibition logic forapplications\u2022Handling of power/sleep hardware keys\u2022Multi-seat management\u2022Session switch management\u2022Device access management for users\u2022Automatic spawning of text logins (gettys) on virtual consoleactivation and user runtime directory managementUser sessions are registered with logind via the pam_systemd(8)PAM module.See logind.conf(5) for information about the configuration ofthis service.See sd-login(3) for information about the basic concepts oflogind such as users, sessions and seats.See org.freedesktop.login1(5) and org.freedesktop.LogControl1(5)for information about the D-Bus APIs systemd-logind provides.For more information on the inhibition logic see the InhibitorLock Developer Documentation[2].If you are interested in writing a display manager that makes useof logind, please have look at Writing Display Managers[3]. Ifyou are interested in writing a desktop environment that makesuse of logind, please have look at Writing DesktopEnvironments[4].",
        "name": "systemd-logind.service, systemd-logind - Login manager",
        "section": 8
    },
    {
        "command": "systemd-logind.service",
        "description": "systemd-logind is a system service that manages user logins. Itis responsible for:\u2022Keeping track of users and sessions, their processes andtheir idle state. This is implemented by allocating a systemdslice unit for each user below user.slice, and a scope unitbelow it for each concurrent session of a user. Also, aper-user service manager is started as system serviceinstance of user@.service for each logged in user.\u2022Generating and managing session IDs. If auditing is availableand an audit session ID is already set for a session, thenthis ID is reused as the session ID. Otherwise, anindependent session counter is used.\u2022Providing polkit[1]-based access for users for operationssuch as system shutdown or sleep\u2022Implementing a shutdown/sleep inhibition logic forapplications\u2022Handling of power/sleep hardware keys\u2022Multi-seat management\u2022Session switch management\u2022Device access management for users\u2022Automatic spawning of text logins (gettys) on virtual consoleactivation and user runtime directory managementUser sessions are registered with logind via the pam_systemd(8)PAM module.See logind.conf(5) for information about the configuration ofthis service.See sd-login(3) for information about the basic concepts oflogind such as users, sessions and seats.See org.freedesktop.login1(5) and org.freedesktop.LogControl1(5)for information about the D-Bus APIs systemd-logind provides.For more information on the inhibition logic see the InhibitorLock Developer Documentation[2].If you are interested in writing a display manager that makes useof logind, please have look at Writing Display Managers[3]. Ifyou are interested in writing a desktop environment that makesuse of logind, please have look at Writing DesktopEnvironments[4].",
        "name": "systemd-logind.service, systemd-logind - Login manager",
        "section": 8
    },
    {
        "command": "systemd-machine-id-commit.service",
        "description": "systemd-machine-id-commit.service is an early boot serviceresponsible for committing transient /etc/machine-id files to awritable disk file system. See machine-id(5) for more informationabout machine IDs.This service is started after local-fs.target in case/etc/machine-id is a mount point of its own (usually from amemory file system such as \"tmpfs\") and /etc is writable. Theservice will invoke systemd-machine-id-setup --commit, whichwrites the current transient machine ID to disk and unmount the/etc/machine-id file in a race-free manner to ensure that file isalways valid and accessible for other processes. Seesystemd-machine-id-setup(1) for details.The main use case of this service are systems where/etc/machine-id is read-only and initially not initialized. Inthis case, the system manager will generate a transient machineID file on a memory file system, and mount it over/etc/machine-id, during the early boot phase. This service isthen invoked in a later boot phase, as soon as /etc/ has beenremounted writable and the ID may thus be committed to disk tomake it permanent.",
        "name": "systemd-machine-id-commit.service - Commit a transient machine IDto disk",
        "section": 8
    },
    {
        "command": "systemd-machined",
        "description": "systemd-machined is a system service that keeps track of locallyrunning virtual machines and containers.systemd-machined is useful for registering and keeping track ofboth OS containers (containers that share the host kernel but runa full init system of their own and behave in most regards like afull virtual operating system rather than just one virtualizedapp) and full virtual machines (virtualized hardware runningnormal operating systems and possibly different kernels).systemd-machined should not be used for registering/keeping trackof application sandbox containers. A machine in the context ofsystemd-machined is supposed to be an abstract term covering bothOS containers and full virtual machines, but not applicationsandboxes.Machines registered with machined are exposed in various ways inthe system. For example:\u2022Tools like ps(1) will show to which machine a specificprocess belongs in a column of its own, and so willgnome-system-monitor[1] or systemd-cgls(1).\u2022systemd's various tools (systemctl(1), journalctl(1),loginctl(1), hostnamectl(1), timedatectl(1), localectl(1),machinectl(1), ...) support the -M switch to operate on localcontainers instead of the host system.\u2022systemctl list-machines will show the system state of alllocal containers, connecting to the container's init systemfor that.\u2022systemctl's --recursive switch has the effect of not onlyshowing the locally running services, but recursively showingthe services of all registered containers.\u2022The machinectl command provides access to a number of usefuloperations on registered containers, such as introspectingthem, rebooting, shutting them down, and getting a loginprompt on them.\u2022The sd-bus(3) library exposes thesd_bus_open_system_machine(3) call to connect to the systembus of any registered container.\u2022The nss-mymachines(8) module makes sure all registeredcontainers can be resolved via normal glibc gethostbyname(3)or getaddrinfo(3) calls.See systemd-nspawn(1) for some examples on how to run containerswith OS tools.If you are interested in writing a VM or container manager thatmakes use of machined, please have look at Writing VirtualMachine or Container Managers[2]. Also see the New Control GroupInterfaces[3].The daemon provides both a C library interface (which is sharedwith systemd-logind.service(8)) as well as a D-Bus interface. Thelibrary interface may be used to introspect and watch the stateof virtual machines/containers. The bus interface provides thesame but in addition may also be used to register or terminatemachines. For more information please consult sd-login(3) andorg.freedesktop.machine1(5) and org.freedesktop.LogControl1(5).A small companion daemon systemd-importd.service(8) is alsoavailable, which implements importing, exporting, and downloadingof container and VM images.For each container registered with systemd-machined.service thatemploys user namespacing, users/groups are synthesized for theused UIDs/GIDs. These are made available to the system using theUser/Group Record Lookup API via Varlink[4], and thus may beresolved with userdbctl(1) or the usual glibc NSS calls.",
        "name": "systemd-machined.service, systemd-machined - Virtual machine andcontainer registration manager",
        "section": 8
    },
    {
        "command": "systemd-machined.service",
        "description": "systemd-machined is a system service that keeps track of locallyrunning virtual machines and containers.systemd-machined is useful for registering and keeping track ofboth OS containers (containers that share the host kernel but runa full init system of their own and behave in most regards like afull virtual operating system rather than just one virtualizedapp) and full virtual machines (virtualized hardware runningnormal operating systems and possibly different kernels).systemd-machined should not be used for registering/keeping trackof application sandbox containers. A machine in the context ofsystemd-machined is supposed to be an abstract term covering bothOS containers and full virtual machines, but not applicationsandboxes.Machines registered with machined are exposed in various ways inthe system. For example:\u2022Tools like ps(1) will show to which machine a specificprocess belongs in a column of its own, and so willgnome-system-monitor[1] or systemd-cgls(1).\u2022systemd's various tools (systemctl(1), journalctl(1),loginctl(1), hostnamectl(1), timedatectl(1), localectl(1),machinectl(1), ...) support the -M switch to operate on localcontainers instead of the host system.\u2022systemctl list-machines will show the system state of alllocal containers, connecting to the container's init systemfor that.\u2022systemctl's --recursive switch has the effect of not onlyshowing the locally running services, but recursively showingthe services of all registered containers.\u2022The machinectl command provides access to a number of usefuloperations on registered containers, such as introspectingthem, rebooting, shutting them down, and getting a loginprompt on them.\u2022The sd-bus(3) library exposes thesd_bus_open_system_machine(3) call to connect to the systembus of any registered container.\u2022The nss-mymachines(8) module makes sure all registeredcontainers can be resolved via normal glibc gethostbyname(3)or getaddrinfo(3) calls.See systemd-nspawn(1) for some examples on how to run containerswith OS tools.If you are interested in writing a VM or container manager thatmakes use of machined, please have look at Writing VirtualMachine or Container Managers[2]. Also see the New Control GroupInterfaces[3].The daemon provides both a C library interface (which is sharedwith systemd-logind.service(8)) as well as a D-Bus interface. Thelibrary interface may be used to introspect and watch the stateof virtual machines/containers. The bus interface provides thesame but in addition may also be used to register or terminatemachines. For more information please consult sd-login(3) andorg.freedesktop.machine1(5) and org.freedesktop.LogControl1(5).A small companion daemon systemd-importd.service(8) is alsoavailable, which implements importing, exporting, and downloadingof container and VM images.For each container registered with systemd-machined.service thatemploys user namespacing, users/groups are synthesized for theused UIDs/GIDs. These are made available to the system using theUser/Group Record Lookup API via Varlink[4], and thus may beresolved with userdbctl(1) or the usual glibc NSS calls.",
        "name": "systemd-machined.service, systemd-machined - Virtual machine andcontainer registration manager",
        "section": 8
    },
    {
        "command": "systemd-makefs",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-makefs.service",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-makefs@.service",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-mkswap.service",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-mkswap@.service",
        "description": "systemd-makefs@.service, systemd-mkswap@.service,systemd-growfs@.service, and systemd-growfs-root.service are usedto implement the x-systemd.makefs and x-systemd.growfs options infstab(5), see systemd.mount(5). They are instantiated for eachdevice for which the file system or swap structure needs to beinitialized, and for each mount point where the file system needsto be grown.These services are started at boot, either right before or rightafter the mount point or swap device are used.systemd-makefs knows very little about specific file systems andswap devices, and after checking that the block device does notalready contain a file system or other content, it will executebinaries specific to each filesystem type (/sbin/mkfs.type or/sbin/mkswap). For certain file system types (currentlyext2/ext3/ext4(5), btrfs(5), xfs(5), f2fs, vfat) and for swapdevices, it will configure reasonable defaults and set the filesystem label and UUID based on the device name.systemd-growfs knows very little about specific file systems andswap devices, and will instruct the kernel to grow the mountedfilesystem to full size of the underlying block device.Nevertheless, it needs to know the ioctl(2) number specific toeach file system, so only certain types are supported. Currently:ext4(5), btrfs(5), xfs(5), and dm-crypt partitions (seecryptsetup(8)).If the creation of a file system or swap device fails, the mountpoint or swap is failed too. If the growing of a file systemfails, a warning is emitted.",
        "name": "systemd-makefs@.service, systemd-mkswap@.service, systemd-growfs@.service, systemd-growfs-root.service, systemd-makefs,systemd-growfs - Creating and growing file systems on demand",
        "section": 8
    },
    {
        "command": "systemd-modules-load",
        "description": "systemd-modules-load.service is an early boot service that loadskernel modules. It reads static configuration from files in /usr/and /etc/, but also runtime configuration from /run/ and thekernel command line (see below).See modules-load.d(5) for information about the configurationformat of this service and paths where configuration files can becreated.",
        "name": "systemd-modules-load.service, systemd-modules-load - Load kernelmodules at boot",
        "section": 8
    },
    {
        "command": "systemd-modules-load.service",
        "description": "systemd-modules-load.service is an early boot service that loadskernel modules. It reads static configuration from files in /usr/and /etc/, but also runtime configuration from /run/ and thekernel command line (see below).See modules-load.d(5) for information about the configurationformat of this service and paths where configuration files can becreated.",
        "name": "systemd-modules-load.service, systemd-modules-load - Load kernelmodules at boot",
        "section": 8
    },
    {
        "command": "systemd-network-generator",
        "description": "systemd-network-generator.service is a system service thattranslates ip= and the related settings on the kernel commandline (see below) into systemd.network(5), systemd.netdev(5), andsystemd.link(5) configuration files understood bysystemd-networkd.service(8) and systemd-udevd.service(8).Files are generated in /run/systemd/network/.Note: despite the name, this generator executes as a normalsystemd service and is not an implementation of thesystemd.generator(7) concept.",
        "name": "systemd-network-generator.service, systemd-network-generator -Generate network configuration from the kernel command line",
        "section": 8
    },
    {
        "command": "systemd-network-generator.service",
        "description": "systemd-network-generator.service is a system service thattranslates ip= and the related settings on the kernel commandline (see below) into systemd.network(5), systemd.netdev(5), andsystemd.link(5) configuration files understood bysystemd-networkd.service(8) and systemd-udevd.service(8).Files are generated in /run/systemd/network/.Note: despite the name, this generator executes as a normalsystemd service and is not an implementation of thesystemd.generator(7) concept.",
        "name": "systemd-network-generator.service, systemd-network-generator -Generate network configuration from the kernel command line",
        "section": 8
    },
    {
        "command": "systemd-networkd",
        "description": "systemd-networkd is a system service that manages networks. Itdetects and configures network devices as they appear, as well ascreating virtual network devices.To configure low-level link settings independently of networks,see systemd.link(5).systemd-networkd will create network devices based on theconfiguration in systemd.netdev(5) files, respecting the [Match]sections in those files.systemd-networkd will manage network addresses and routes for anylink for which it finds a .network file with an appropriate[Match] section, see systemd.network(5). For those links, it willflush existing network addresses and routes when bringing up thedevice. Any links not matched by one of the .network files willbe ignored. It is also possible to explicitly tellsystemd-networkd to ignore a link by using Unmanaged=yes option,see systemd.network(5).When systemd-networkd exits, it generally leaves existing networkdevices and configuration intact. This makes it possible totransition from the initrd and to restart the service withoutbreaking connectivity. This also means that when configuration isupdated and systemd-networkd is restarted, netdev interfaces forwhich configuration was removed will not be dropped, and may needto be cleaned up manually.systemd-networkd may be introspected and controlled at runtimeusing networkctl(1).",
        "name": "systemd-networkd.service, systemd-networkd - Network manager",
        "section": 8
    },
    {
        "command": "systemd-networkd-wait-online",
        "description": "systemd-networkd-wait-online is a oneshot system service (seesystemd.service(5)), that waits for the network to be configured.By default, it will wait for all links it is aware of and whichare managed by systemd-networkd.service(8) to be fully configuredor failed, and for at least one link to be online. Here, onlinemeans that the link's operational state is equal or higher than\"degraded\". The threshold can be configured by--operational-state= option.The service systemd-networkd-wait-online.service invokessystemd-networkd-wait-online without any options. Thus, it waitsfor all managed interfaces to be configured or failed, and for atleast one to be online.The service systemd-networkd-wait-online@.service takes aninterface name, and invokes systemd-networkd-wait-online with -iand the specified interface name. Thus, wait for the specifiedinterface to be configured and online. For example,systemd-networkd-wait-online@eth0.service waits for eth0 to beconfigured by systemd-networkd and online.",
        "name": "systemd-networkd-wait-online.service, systemd-networkd-wait-online@.service, systemd-networkd-wait-online - Wait for networkto come online",
        "section": 8
    },
    {
        "command": "systemd-networkd-wait-online.service",
        "description": "systemd-networkd-wait-online is a oneshot system service (seesystemd.service(5)), that waits for the network to be configured.By default, it will wait for all links it is aware of and whichare managed by systemd-networkd.service(8) to be fully configuredor failed, and for at least one link to be online. Here, onlinemeans that the link's operational state is equal or higher than\"degraded\". The threshold can be configured by--operational-state= option.The service systemd-networkd-wait-online.service invokessystemd-networkd-wait-online without any options. Thus, it waitsfor all managed interfaces to be configured or failed, and for atleast one to be online.The service systemd-networkd-wait-online@.service takes aninterface name, and invokes systemd-networkd-wait-online with -iand the specified interface name. Thus, wait for the specifiedinterface to be configured and online. For example,systemd-networkd-wait-online@eth0.service waits for eth0 to beconfigured by systemd-networkd and online.",
        "name": "systemd-networkd-wait-online.service, systemd-networkd-wait-online@.service, systemd-networkd-wait-online - Wait for networkto come online",
        "section": 8
    },
    {
        "command": "systemd-networkd-wait-online@.service",
        "description": "systemd-networkd-wait-online is a oneshot system service (seesystemd.service(5)), that waits for the network to be configured.By default, it will wait for all links it is aware of and whichare managed by systemd-networkd.service(8) to be fully configuredor failed, and for at least one link to be online. Here, onlinemeans that the link's operational state is equal or higher than\"degraded\". The threshold can be configured by--operational-state= option.The service systemd-networkd-wait-online.service invokessystemd-networkd-wait-online without any options. Thus, it waitsfor all managed interfaces to be configured or failed, and for atleast one to be online.The service systemd-networkd-wait-online@.service takes aninterface name, and invokes systemd-networkd-wait-online with -iand the specified interface name. Thus, wait for the specifiedinterface to be configured and online. For example,systemd-networkd-wait-online@eth0.service waits for eth0 to beconfigured by systemd-networkd and online.",
        "name": "systemd-networkd-wait-online.service, systemd-networkd-wait-online@.service, systemd-networkd-wait-online - Wait for networkto come online",
        "section": 8
    },
    {
        "command": "systemd-networkd.service",
        "description": "systemd-networkd is a system service that manages networks. Itdetects and configures network devices as they appear, as well ascreating virtual network devices.To configure low-level link settings independently of networks,see systemd.link(5).systemd-networkd will create network devices based on theconfiguration in systemd.netdev(5) files, respecting the [Match]sections in those files.systemd-networkd will manage network addresses and routes for anylink for which it finds a .network file with an appropriate[Match] section, see systemd.network(5). For those links, it willflush existing network addresses and routes when bringing up thedevice. Any links not matched by one of the .network files willbe ignored. It is also possible to explicitly tellsystemd-networkd to ignore a link by using Unmanaged=yes option,see systemd.network(5).When systemd-networkd exits, it generally leaves existing networkdevices and configuration intact. This makes it possible totransition from the initrd and to restart the service withoutbreaking connectivity. This also means that when configuration isupdated and systemd-networkd is restarted, netdev interfaces forwhich configuration was removed will not be dropped, and may needto be cleaned up manually.systemd-networkd may be introspected and controlled at runtimeusing networkctl(1).",
        "name": "systemd-networkd.service, systemd-networkd - Network manager",
        "section": 8
    },
    {
        "command": "systemd-oomd",
        "description": "systemd-oomd is a system service that uses cgroups-v2 andpressure stall information (PSI) to monitor and take correctiveaction before an OOM occurs in the kernel space.You can enable monitoring and actions on units by settingManagedOOMSwap= and ManagedOOMMemoryPressure= in the unitconfiguration, see systemd.resource-control(5).systemd-oomdretrieves information about such units from systemd(1) when itstarts and watches for subsequent changes.Cgroups of units with ManagedOOMSwap= orManagedOOMMemoryPressure= set to kill will be monitored.systemd-oomd periodically polls PSI statistics for the system andthose cgroups to decide when to take action. If the configuredlimits are exceeded, systemd-oomd will select a cgroup toterminate, and send SIGKILL to all processes in it. Note thatonly descendant cgroups are eligible candidates for killing; theunit with its property set to kill is not a candidate (unless oneof its ancestors set their property to kill). Also only leafcgroups and cgroups with memory.oom.group set to 1 are eligiblecandidates; see OOMPolicy= in systemd.service(5).oomctl(1) can be used to list monitored cgroups and pressureinformation.See oomd.conf(5) for more information about the configuration ofthis service.",
        "name": "systemd-oomd.service, systemd-oomd - A userspace out-of-memory(OOM) killer",
        "section": 8
    },
    {
        "command": "systemd-oomd.service",
        "description": "systemd-oomd is a system service that uses cgroups-v2 andpressure stall information (PSI) to monitor and take correctiveaction before an OOM occurs in the kernel space.You can enable monitoring and actions on units by settingManagedOOMSwap= and ManagedOOMMemoryPressure= in the unitconfiguration, see systemd.resource-control(5).systemd-oomdretrieves information about such units from systemd(1) when itstarts and watches for subsequent changes.Cgroups of units with ManagedOOMSwap= orManagedOOMMemoryPressure= set to kill will be monitored.systemd-oomd periodically polls PSI statistics for the system andthose cgroups to decide when to take action. If the configuredlimits are exceeded, systemd-oomd will select a cgroup toterminate, and send SIGKILL to all processes in it. Note thatonly descendant cgroups are eligible candidates for killing; theunit with its property set to kill is not a candidate (unless oneof its ancestors set their property to kill). Also only leafcgroups and cgroups with memory.oom.group set to 1 are eligiblecandidates; see OOMPolicy= in systemd.service(5).oomctl(1) can be used to list monitored cgroups and pressureinformation.See oomd.conf(5) for more information about the configuration ofthis service.",
        "name": "systemd-oomd.service, systemd-oomd - A userspace out-of-memory(OOM) killer",
        "section": 8
    },
    {
        "command": "systemd-portabled",
        "description": "systemd-portabled is a system service that may be used to attach,detach and inspect portable service images.Most of systemd-portabled's functionality is accessible throughthe portablectl(1) command.See the Portable Services Documentation[1] for details about theconcepts this service implements.",
        "name": "systemd-portabled.service, systemd-portabled - Portable servicemanager",
        "section": 8
    },
    {
        "command": "systemd-portabled.service",
        "description": "systemd-portabled is a system service that may be used to attach,detach and inspect portable service images.Most of systemd-portabled's functionality is accessible throughthe portablectl(1) command.See the Portable Services Documentation[1] for details about theconcepts this service implements.",
        "name": "systemd-portabled.service, systemd-portabled - Portable servicemanager",
        "section": 8
    },
    {
        "command": "systemd-poweroff.service",
        "description": "systemd-poweroff.service is a system service that is pulled in bypoweroff.target and is responsible for the actual systempower-off operation. Similarly, systemd-halt.service is pulled inby halt.target, systemd-reboot.service by reboot.target andsystemd-kexec.service by kexec.target to execute the respectiveactions.When these services are run, they ensure that PID 1 is replacedby the /usr/lib/systemd/systemd-shutdown tool which is thenresponsible for the actual shutdown. Before shutting down, thisbinary will try to unmount all remaining file systems (or atleast remount them read-only), disable all remaining swapdevices, detach all remaining storage devices and kill allremaining processes.It is necessary to have this code in a separate binary becauseotherwise rebooting after an upgrade might be broken \u2014 therunning PID 1 could still depend on libraries which are notavailable any more, thus keeping the file system busy, which thencannot be re-mounted read-only.Shortly before executing the actual systempower-off/halt/reboot/kexec systemd-shutdown will run allexecutables in /usr/lib/systemd/system-shutdown/ and pass onearguments to them: either \"poweroff\", \"halt\", \"reboot\", or\"kexec\", depending on the chosen action. All executables in thisdirectory are executed in parallel, and execution of the actionis not continued before all executables finished. Note that theseexecutables are run after all services have been shut down, andafter most mounts have been detached (the root file system aswell as /run/ and various API file systems are still aroundthough). This means any programs dropped into this directory mustbe prepared to run in such a limited execution environment andnot rely on external services or hierarchies such as /var/ to bearound (or writable).Note that systemd-poweroff.service (and the related units) shouldnever be executed directly. Instead, trigger system shutdown witha command such as \"systemctl poweroff\".Another form of shutdown is provided by thesystemd-soft-reboot.service(8) functionality. It reboots only theOS userspace, leaving the kernel, firmware, and hardware as itis.",
        "name": "systemd-poweroff.service, systemd-halt.service, systemd-reboot.service, systemd-kexec.service, systemd-shutdown - Systemshutdown logic",
        "section": 8
    },
    {
        "command": "systemd-pstore",
        "description": "systemd-pstore.service is a system service that archives thecontents of the Linux persistent storage filesystem, pstore, toother storage, thus preserving the existing information containedin the pstore, and clearing pstore storage for future errorevents.Linux provides a persistent storage file system, pstore, that canstore error records when the kernel dies (or reboots orpowers-off). These records in turn can be referenced to debugkernel problems (currently the kernel stores the tail of thekernel log, which also contains a stack backtrace, into pstore).The pstore file system supports a variety of backends that maponto persistent storage, such as the ACPI ERST and UEFIvariables. The pstore backends typically offer a relatively smallamount of persistent storage, e.g. 64KiB, which can quickly fillup and thus prevent subsequent kernel crashes from recordingerrors. Thus there is a need to monitor and extract the pstorecontents so that future kernel problems can also recordinformation in the pstore.The pstore service is independent of the kdump service. In cloudenvironments specifically, host and guest filesystems are onremote filesystems (e.g. iSCSI or NFS), thus kdump relies(implicitly and/or explicitly) upon proper operation ofnetworking software *and* hardware *and* infrastructure. Thus itmay not be possible to capture a kernel coredump to a file sincewrites over the network may not be possible.The pstore backend, on the other hand, is completely local andprovides a path to store error records which will survive areboot and aid in post-mortem debugging.The systemd-pstore executable does the actual work. Uponstarting, the pstore.conf file is read and the /sys/fs/pstore/directory contents are processed according to the options. Pstorefiles are written to the journal, and optionally saved into/var/lib/systemd/pstore/.",
        "name": "systemd-pstore.service, systemd-pstore - A service to archivecontents of pstore",
        "section": 8
    },
    {
        "command": "systemd-pstore.service",
        "description": "systemd-pstore.service is a system service that archives thecontents of the Linux persistent storage filesystem, pstore, toother storage, thus preserving the existing information containedin the pstore, and clearing pstore storage for future errorevents.Linux provides a persistent storage file system, pstore, that canstore error records when the kernel dies (or reboots orpowers-off). These records in turn can be referenced to debugkernel problems (currently the kernel stores the tail of thekernel log, which also contains a stack backtrace, into pstore).The pstore file system supports a variety of backends that maponto persistent storage, such as the ACPI ERST and UEFIvariables. The pstore backends typically offer a relatively smallamount of persistent storage, e.g. 64KiB, which can quickly fillup and thus prevent subsequent kernel crashes from recordingerrors. Thus there is a need to monitor and extract the pstorecontents so that future kernel problems can also recordinformation in the pstore.The pstore service is independent of the kdump service. In cloudenvironments specifically, host and guest filesystems are onremote filesystems (e.g. iSCSI or NFS), thus kdump relies(implicitly and/or explicitly) upon proper operation ofnetworking software *and* hardware *and* infrastructure. Thus itmay not be possible to capture a kernel coredump to a file sincewrites over the network may not be possible.The pstore backend, on the other hand, is completely local andprovides a path to store error records which will survive areboot and aid in post-mortem debugging.The systemd-pstore executable does the actual work. Uponstarting, the pstore.conf file is read and the /sys/fs/pstore/directory contents are processed according to the options. Pstorefiles are written to the journal, and optionally saved into/var/lib/systemd/pstore/.",
        "name": "systemd-pstore.service, systemd-pstore - A service to archivecontents of pstore",
        "section": 8
    },
    {
        "command": "systemd-quotacheck",
        "description": "systemd-quotacheck.service is a service responsible for filesystem quota checks. It is run once at boot after all necessaryfile systems are mounted. It is pulled in only if at least onefile system has quotas enabled.",
        "name": "systemd-quotacheck.service, systemd-quotacheck - File systemquota checker logic",
        "section": 8
    },
    {
        "command": "systemd-quotacheck.service",
        "description": "systemd-quotacheck.service is a service responsible for filesystem quota checks. It is run once at boot after all necessaryfile systems are mounted. It is pulled in only if at least onefile system has quotas enabled.",
        "name": "systemd-quotacheck.service, systemd-quotacheck - File systemquota checker logic",
        "section": 8
    },
    {
        "command": "systemd-random-seed",
        "description": "systemd-random-seed.service is a service that loads an on-diskrandom seed into the kernel entropy pool during boot and saves itat shutdown. See random(4) for details. By default, no entropy iscredited when the random seed is written into the kernel entropypool, but this may be changed with $SYSTEMD_RANDOM_SEED_CREDIT,see below. On disk the random seed is stored in/var/lib/systemd/random-seed.Note that this service runs relatively late during the early bootphase, i.e. generally after the initrd phase has finished and the/var/ file system has been mounted. Many system services requireentropy much earlier than this \u2014 this service is hence of limiteduse for complex system. It is recommended to use a boot loaderthat can pass an initial random seed to the kernel to ensure thatentropy is available from earliest boot on, for examplesystemd-boot(7), with its bootctl random-seed functionality.When loading the random seed from disk, the file is immediatelyupdated with a new seed retrieved from the kernel, in order toensure no two boots operate with the same random seed. This newseed is retrieved synchronously from the kernel, which means theservice will not complete start-up until the random pool is fullyinitialized. On entropy-starved systems this may take a while.This functionality is intended to be used as synchronizationpoint for ordering services that require an initialized entropypool to function securely (i.e. services that access /dev/urandomwithout any further precautions).Care should be taken when creating OS images that are replicatedto multiple systems: if the random seed file is includedunmodified each system will initialize its entropy pool with thesame data, and thus \u2014 if otherwise entropy-starved \u2014 generate thesame or at least guessable random seed streams. As a safetyprecaution crediting entropy is thus disabled by default. It isrecommended to remove the random seed from OS images intended forreplication on multiple systems, in which case it is safe toenable entropy crediting, see below. Also see Safely BuildingImages[1].See Random Seeds[2] for further information.",
        "name": "systemd-random-seed.service, systemd-random-seed - Load and savethe OS system random seed at boot and shutdown",
        "section": 8
    },
    {
        "command": "systemd-random-seed.service",
        "description": "systemd-random-seed.service is a service that loads an on-diskrandom seed into the kernel entropy pool during boot and saves itat shutdown. See random(4) for details. By default, no entropy iscredited when the random seed is written into the kernel entropypool, but this may be changed with $SYSTEMD_RANDOM_SEED_CREDIT,see below. On disk the random seed is stored in/var/lib/systemd/random-seed.Note that this service runs relatively late during the early bootphase, i.e. generally after the initrd phase has finished and the/var/ file system has been mounted. Many system services requireentropy much earlier than this \u2014 this service is hence of limiteduse for complex system. It is recommended to use a boot loaderthat can pass an initial random seed to the kernel to ensure thatentropy is available from earliest boot on, for examplesystemd-boot(7), with its bootctl random-seed functionality.When loading the random seed from disk, the file is immediatelyupdated with a new seed retrieved from the kernel, in order toensure no two boots operate with the same random seed. This newseed is retrieved synchronously from the kernel, which means theservice will not complete start-up until the random pool is fullyinitialized. On entropy-starved systems this may take a while.This functionality is intended to be used as synchronizationpoint for ordering services that require an initialized entropypool to function securely (i.e. services that access /dev/urandomwithout any further precautions).Care should be taken when creating OS images that are replicatedto multiple systems: if the random seed file is includedunmodified each system will initialize its entropy pool with thesame data, and thus \u2014 if otherwise entropy-starved \u2014 generate thesame or at least guessable random seed streams. As a safetyprecaution crediting entropy is thus disabled by default. It isrecommended to remove the random seed from OS images intended forreplication on multiple systems, in which case it is safe toenable entropy crediting, see below. Also see Safely BuildingImages[1].See Random Seeds[2] for further information.",
        "name": "systemd-random-seed.service, systemd-random-seed - Load and savethe OS system random seed at boot and shutdown",
        "section": 8
    },
    {
        "command": "systemd-rc-local-generator",
        "description": "systemd-rc-local-generator is a generator that checks whether/etc/rc.local exists and is executable, and if it is, pulls therc-local.service unit into the boot process. This unit isresponsible for running this script during late boot. The scriptis run after network.target, but in parallel with most otherregular system services.Note that rc-local.service runs with slightly different semanticsthan the original System V version, which was executed \"last\" inthe boot process, which is a concept that does not translate tosystemd.Also note that rc-local.service is ordered after network.target,which does not mean that the network is functional, seesystemd.special(7). If the script requires a configured networkconnection, it may be desirable to pull in and order it afternetwork-online.target with a drop-in:# /etc/systemd/system/rc-local.service.d/network.conf[Unit]Wants=network-online.targetAfter=network-online.targetSupport for /etc/rc.local is provided for compatibility withspecific System V systems only. However, it is stronglyrecommended to avoid making use of this script today, and insteadprovide proper unit files with appropriate dependencies for anyscripts to run during the boot process. Note that the path to thescript is set at compile time and varies between distributions.systemd-rc-local-generator implements systemd.generator(7).",
        "name": "systemd-rc-local-generator, rc-local.service - Compatibilitygenerator and service to start /etc/rc.local during boot",
        "section": 8
    },
    {
        "command": "systemd-reboot.service",
        "description": "systemd-poweroff.service is a system service that is pulled in bypoweroff.target and is responsible for the actual systempower-off operation. Similarly, systemd-halt.service is pulled inby halt.target, systemd-reboot.service by reboot.target andsystemd-kexec.service by kexec.target to execute the respectiveactions.When these services are run, they ensure that PID 1 is replacedby the /usr/lib/systemd/systemd-shutdown tool which is thenresponsible for the actual shutdown. Before shutting down, thisbinary will try to unmount all remaining file systems (or atleast remount them read-only), disable all remaining swapdevices, detach all remaining storage devices and kill allremaining processes.It is necessary to have this code in a separate binary becauseotherwise rebooting after an upgrade might be broken \u2014 therunning PID 1 could still depend on libraries which are notavailable any more, thus keeping the file system busy, which thencannot be re-mounted read-only.Shortly before executing the actual systempower-off/halt/reboot/kexec systemd-shutdown will run allexecutables in /usr/lib/systemd/system-shutdown/ and pass onearguments to them: either \"poweroff\", \"halt\", \"reboot\", or\"kexec\", depending on the chosen action. All executables in thisdirectory are executed in parallel, and execution of the actionis not continued before all executables finished. Note that theseexecutables are run after all services have been shut down, andafter most mounts have been detached (the root file system aswell as /run/ and various API file systems are still aroundthough). This means any programs dropped into this directory mustbe prepared to run in such a limited execution environment andnot rely on external services or hierarchies such as /var/ to bearound (or writable).Note that systemd-poweroff.service (and the related units) shouldnever be executed directly. Instead, trigger system shutdown witha command such as \"systemctl poweroff\".Another form of shutdown is provided by thesystemd-soft-reboot.service(8) functionality. It reboots only theOS userspace, leaving the kernel, firmware, and hardware as itis.",
        "name": "systemd-poweroff.service, systemd-halt.service, systemd-reboot.service, systemd-kexec.service, systemd-shutdown - Systemshutdown logic",
        "section": 8
    },
    {
        "command": "systemd-remount-fs",
        "description": "systemd-remount-fs.service is an early boot service that appliesmount options listed in fstab(5), or gathered from the partitiontable (when systemd-gpt-auto-generator(8) is active) to the rootfile system, the /usr/ file system, and the kernel API filesystems. This is required so that the mount options of these filesystems \u2014 which are pre-mounted by the kernel, the initrd,container environments or system manager code \u2014 are updated tothose configured in /etc/fstab and the other sources. Thisservice ignores normal file systems and only changes the rootfile system (i.e./), /usr/, and the virtual kernel API filesystems such as /proc/, /sys/ or /dev/. This service executes nooperation if no configuration is found (/etc/fstab does not existor lists no entries for the mentioned file systems, or thepartition table does not contain relevant entries).For a longer discussion of kernel API file systems see API FileSystems[1].Note: systemd-remount-fs.service is usually pulled in bysystemd-fstab-generator(8), hence it is also affected by thekernel command line option fstab=, which may be used to disablethe generator. It may also pulled in bysystemd-gpt-auto-generator(8), which is affected bysystemd.gpt_auto and other options.",
        "name": "systemd-remount-fs.service, systemd-remount-fs - Remount root andkernel file systems",
        "section": 8
    },
    {
        "command": "systemd-remount-fs.service",
        "description": "systemd-remount-fs.service is an early boot service that appliesmount options listed in fstab(5), or gathered from the partitiontable (when systemd-gpt-auto-generator(8) is active) to the rootfile system, the /usr/ file system, and the kernel API filesystems. This is required so that the mount options of these filesystems \u2014 which are pre-mounted by the kernel, the initrd,container environments or system manager code \u2014 are updated tothose configured in /etc/fstab and the other sources. Thisservice ignores normal file systems and only changes the rootfile system (i.e./), /usr/, and the virtual kernel API filesystems such as /proc/, /sys/ or /dev/. This service executes nooperation if no configuration is found (/etc/fstab does not existor lists no entries for the mentioned file systems, or thepartition table does not contain relevant entries).For a longer discussion of kernel API file systems see API FileSystems[1].Note: systemd-remount-fs.service is usually pulled in bysystemd-fstab-generator(8), hence it is also affected by thekernel command line option fstab=, which may be used to disablethe generator. It may also pulled in bysystemd-gpt-auto-generator(8), which is affected bysystemd.gpt_auto and other options.",
        "name": "systemd-remount-fs.service, systemd-remount-fs - Remount root andkernel file systems",
        "section": 8
    },
    {
        "command": "systemd-resolved",
        "description": "systemd-resolved is a system service that provides network nameresolution to local applications. It implements a caching andvalidating DNS/DNSSEC stub resolver, as well as an LLMNR andMulticastDNS resolver and responder. Local applications maysubmit network name resolution requests via three interfaces:\u2022The native, fully-featured API systemd-resolved exposes onthe bus, see org.freedesktop.resolve1(5) andorg.freedesktop.LogControl1(5) for details. Usage of this APIis generally recommended to clients as it is asynchronous andfully featured (for example, properly returns DNSSECvalidation status and interface scope for addresses asnecessary for supporting link-local networking).\u2022The glibc getaddrinfo(3) API as defined by RFC3493[1] and itsrelated resolver functions, including gethostbyname(3). ThisAPI is widely supported, including beyond the Linux platform.In its current form it does not expose DNSSEC validationstatus information however, and is synchronous only. This APIis backed by the glibc Name Service Switch (nss(5)). Usage ofthe glibc NSS module nss-resolve(8) is required in order toallow glibc's NSS resolver functions to resolve hostnames viasystemd-resolved.\u2022Additionally, systemd-resolved provides a local DNS stublistener on the IP addresses 127.0.0.53 and 127.0.0.54 on thelocal loopback interface. Programs issuing DNS requestsdirectly, bypassing any local API may be directed to thisstub, in order to connect them to systemd-resolved. Notehowever that it is strongly recommended that local programsuse the glibc NSS or bus APIs instead (as described above),as various network resolution concepts (such as link-localaddressing, or LLMNR Unicode domains) cannot be mapped to theunicast DNS protocol.The DNS stub resolver on 127.0.0.53 provides the full featureset of the local resolver, which includes offeringLLMNR/MulticastDNS resolution. The DNS stub resolver on127.0.0.54 provides a more limited resolver, that operates in\"proxy\" mode only, i.e. it will pass most DNS messagesrelatively unmodified to the current upstream DNS servers andback, but not try to process the messages locally, and hencedoes not validate DNSSEC, or offer up LLMNR/MulticastDNS. (Itwill translate to DNS-over-TLS communication if neededhowever.)The DNS servers contacted are determined from the global settingsin /etc/systemd/resolved.conf, the per-link static settings in/etc/systemd/network/*.network files (in casesystemd-networkd.service(8) is used), the per-link dynamicsettings received over DHCP, information provided viaresolvectl(1), and any DNS server information made available byother system services. See resolved.conf(5) andsystemd.network(5) for details about systemd's own configurationfiles for DNS servers. To improve compatibility, /etc/resolv.confis read in order to discover configured system DNS servers, butonly if it is not a symlink to/run/systemd/resolve/stub-resolv.conf,/usr/lib/systemd/resolv.conf or /run/systemd/resolve/resolv.conf(see below).",
        "name": "systemd-resolved.service, systemd-resolved - Network NameResolution manager",
        "section": 8
    },
    {
        "command": "systemd-resolved.service",
        "description": "systemd-resolved is a system service that provides network nameresolution to local applications. It implements a caching andvalidating DNS/DNSSEC stub resolver, as well as an LLMNR andMulticastDNS resolver and responder. Local applications maysubmit network name resolution requests via three interfaces:\u2022The native, fully-featured API systemd-resolved exposes onthe bus, see org.freedesktop.resolve1(5) andorg.freedesktop.LogControl1(5) for details. Usage of this APIis generally recommended to clients as it is asynchronous andfully featured (for example, properly returns DNSSECvalidation status and interface scope for addresses asnecessary for supporting link-local networking).\u2022The glibc getaddrinfo(3) API as defined by RFC3493[1] and itsrelated resolver functions, including gethostbyname(3). ThisAPI is widely supported, including beyond the Linux platform.In its current form it does not expose DNSSEC validationstatus information however, and is synchronous only. This APIis backed by the glibc Name Service Switch (nss(5)). Usage ofthe glibc NSS module nss-resolve(8) is required in order toallow glibc's NSS resolver functions to resolve hostnames viasystemd-resolved.\u2022Additionally, systemd-resolved provides a local DNS stublistener on the IP addresses 127.0.0.53 and 127.0.0.54 on thelocal loopback interface. Programs issuing DNS requestsdirectly, bypassing any local API may be directed to thisstub, in order to connect them to systemd-resolved. Notehowever that it is strongly recommended that local programsuse the glibc NSS or bus APIs instead (as described above),as various network resolution concepts (such as link-localaddressing, or LLMNR Unicode domains) cannot be mapped to theunicast DNS protocol.The DNS stub resolver on 127.0.0.53 provides the full featureset of the local resolver, which includes offeringLLMNR/MulticastDNS resolution. The DNS stub resolver on127.0.0.54 provides a more limited resolver, that operates in\"proxy\" mode only, i.e. it will pass most DNS messagesrelatively unmodified to the current upstream DNS servers andback, but not try to process the messages locally, and hencedoes not validate DNSSEC, or offer up LLMNR/MulticastDNS. (Itwill translate to DNS-over-TLS communication if neededhowever.)The DNS servers contacted are determined from the global settingsin /etc/systemd/resolved.conf, the per-link static settings in/etc/systemd/network/*.network files (in casesystemd-networkd.service(8) is used), the per-link dynamicsettings received over DHCP, information provided viaresolvectl(1), and any DNS server information made available byother system services. See resolved.conf(5) andsystemd.network(5) for details about systemd's own configurationfiles for DNS servers. To improve compatibility, /etc/resolv.confis read in order to discover configured system DNS servers, butonly if it is not a symlink to/run/systemd/resolve/stub-resolv.conf,/usr/lib/systemd/resolv.conf or /run/systemd/resolve/resolv.conf(see below).",
        "name": "systemd-resolved.service, systemd-resolved - Network NameResolution manager",
        "section": 8
    },
    {
        "command": "systemd-rfkill",
        "description": "systemd-rfkill.service is a service that restores the RF killswitch state at early boot and saves it on each change. On disk,the RF kill switch state is stored in /var/lib/systemd/rfkill/.",
        "name": "systemd-rfkill.service, systemd-rfkill.socket, systemd-rfkill -Load and save the RF kill switch state at boot and change",
        "section": 8
    },
    {
        "command": "systemd-rfkill.service",
        "description": "systemd-rfkill.service is a service that restores the RF killswitch state at early boot and saves it on each change. On disk,the RF kill switch state is stored in /var/lib/systemd/rfkill/.",
        "name": "systemd-rfkill.service, systemd-rfkill.socket, systemd-rfkill -Load and save the RF kill switch state at boot and change",
        "section": 8
    },
    {
        "command": "systemd-rfkill.socket",
        "description": "systemd-rfkill.service is a service that restores the RF killswitch state at early boot and saves it on each change. On disk,the RF kill switch state is stored in /var/lib/systemd/rfkill/.",
        "name": "systemd-rfkill.service, systemd-rfkill.socket, systemd-rfkill -Load and save the RF kill switch state at boot and change",
        "section": 8
    },
    {
        "command": "systemd-run-generator",
        "description": "systemd-run-generator is a generator that reads the kernelcommand line and understands three options:If the systemd.run= option is specified and followed by a commandline, a unit named kernel-command-line.service is generated forit and booted into. The service has Type=oneshot set, and hasSuccessAction=exit and FailureAction=exit configured by default,thus ensuring that the system is shut down as soon as the commandcompletes. The exit status of the command line is propagated tothe invoking container manager, if this applies (which mightpropagate this further, to the calling shell \u2014 e.g.systemd-nspawn(7) does this). If this option is used multipletimes the unit file will contain multiple ExecStart= lines, toexecute all commands in order. The command is started as regularservice, i.e. with DefaultDependencies= on.Use systemd.run_success_action= and systemd.run_failure_action=to tweak how to react to the process completing. In particularassigning \"none\" will leave the system running after the commandcompletes. For further details on supported arguments, seesystemd.unit(5).systemd-run-generator implements systemd.generator(7).",
        "name": "systemd-run-generator - Generator for invoking commands specifiedon the kernel command line as system service",
        "section": 8
    },
    {
        "command": "systemd-shutdown",
        "description": "systemd-poweroff.service is a system service that is pulled in bypoweroff.target and is responsible for the actual systempower-off operation. Similarly, systemd-halt.service is pulled inby halt.target, systemd-reboot.service by reboot.target andsystemd-kexec.service by kexec.target to execute the respectiveactions.When these services are run, they ensure that PID 1 is replacedby the /usr/lib/systemd/systemd-shutdown tool which is thenresponsible for the actual shutdown. Before shutting down, thisbinary will try to unmount all remaining file systems (or atleast remount them read-only), disable all remaining swapdevices, detach all remaining storage devices and kill allremaining processes.It is necessary to have this code in a separate binary becauseotherwise rebooting after an upgrade might be broken \u2014 therunning PID 1 could still depend on libraries which are notavailable any more, thus keeping the file system busy, which thencannot be re-mounted read-only.Shortly before executing the actual systempower-off/halt/reboot/kexec systemd-shutdown will run allexecutables in /usr/lib/systemd/system-shutdown/ and pass onearguments to them: either \"poweroff\", \"halt\", \"reboot\", or\"kexec\", depending on the chosen action. All executables in thisdirectory are executed in parallel, and execution of the actionis not continued before all executables finished. Note that theseexecutables are run after all services have been shut down, andafter most mounts have been detached (the root file system aswell as /run/ and various API file systems are still aroundthough). This means any programs dropped into this directory mustbe prepared to run in such a limited execution environment andnot rely on external services or hierarchies such as /var/ to bearound (or writable).Note that systemd-poweroff.service (and the related units) shouldnever be executed directly. Instead, trigger system shutdown witha command such as \"systemctl poweroff\".Another form of shutdown is provided by thesystemd-soft-reboot.service(8) functionality. It reboots only theOS userspace, leaving the kernel, firmware, and hardware as itis.",
        "name": "systemd-poweroff.service, systemd-halt.service, systemd-reboot.service, systemd-kexec.service, systemd-shutdown - Systemshutdown logic",
        "section": 8
    },
    {
        "command": "systemd-sleep",
        "description": "systemd-suspend.service is a system service that is pulled in bysuspend.target and is responsible for the actual system suspend.Similarly, systemd-hibernate.service is pulled in byhibernate.target to execute the actual hibernation. Finally,systemd-hybrid-sleep.service is pulled in by hybrid-sleep.targetto execute hybrid hibernation with system suspend and pulled inby suspend-then-hibernate.target to execute system suspend with atimeout that will activate hibernate later.Immediately before entering system suspend and/or hibernationsystemd-suspend.service (and the other mentioned units,respectively) will run all executables in/usr/lib/systemd/system-sleep/ and pass two arguments to them.The first argument will be \"pre\", the second either \"suspend\",\"hibernate\", \"hybrid-sleep\", or \"suspend-then-hibernate\"depending on the chosen action. An environment variable called\"SYSTEMD_SLEEP_ACTION\" will be set and contain the sleep actionthat is processing. This is primarily helpful for\"suspend-then-hibernate\" where the value of the variable will be\"suspend\", \"hibernate\", or \"suspend-after-failed-hibernate\" incases where hibernation has failed. Immediately after leavingsystem suspend and/or hibernation the same executables are run,but the first argument is now \"post\". All executables in thisdirectory are executed in parallel, and execution of the actionis not continued until all executables have finished.Note that scripts or binaries dropped in/usr/lib/systemd/system-sleep/ are intended for local use onlyand should be considered hacks. If applications want to react tosystem suspend/hibernation and resume, they should rather use theInhibitor interface[1].Note that systemd-suspend.service, systemd-hibernate.service,systemd-hybrid-sleep.service, andsystemd-suspend-then-hibernate.service should never be executeddirectly. Instead, trigger system sleep with a command such assystemctl suspend or systemctl hibernate.Internally, this service will echo a string like \"mem\" into/sys/power/state, to trigger the actual system suspend. Whatexactly is written where can be configured in the [Sleep] sectionof /etc/systemd/sleep.conf or a sleep.conf.d file. Seesystemd-sleep.conf(5).",
        "name": "systemd-suspend.service, systemd-hibernate.service, systemd-hybrid-sleep.service, systemd-suspend-then-hibernate.service,systemd-sleep - System sleep state logic",
        "section": 8
    },
    {
        "command": "systemd-socket-proxyd",
        "description": "systemd-socket-proxyd is a generic socket-activated networksocket forwarder proxy daemon for IPv4, IPv6 and UNIX streamsockets. It may be used to bi-directionally forward traffic froma local listening socket to a local or remote destination socket.One use of this tool is to provide socket activation support forservices that do not natively support socket activation. Onbehalf of the service to activate, the proxy inherits the socketfrom systemd, accepts each client connection, opens a connectionto a configured server for each client, and then bidirectionallyforwards data between the two.This utility's behavior is similar to socat(1). The maindifferences for systemd-socket-proxyd are support for socketactivation with \"Accept=no\" and an event-driven design thatscales better with the number of connections.",
        "name": "systemd-socket-proxyd - Bidirectionally proxy local sockets toanother (possibly remote) socket",
        "section": 8
    },
    {
        "command": "systemd-soft-reboot.service",
        "description": "systemd-soft-reboot.service is a system service that is pulled inby soft-reboot.target and is responsible for performing auserspace-only reboot operation. When invoked, it will send theSIGTERM signal to any processes left running (but does not followup with SIGKILL, and does not wait for the processes to exit). Ifthe /run/nextroot/ directory exists (which may be a regulardirectory, a directory mount point or a symlink to either) thenit will switch the file system root to it. It then reexecutes theservice manager off the (possibly now new) root file system,which will enqueue a new boot transaction as in a normal reboot.Such a userspace-only reboot operation permits updating orresetting the entirety of userspace with minimal downtime, as thereboot operation does not transition through:\u2022The second phase of regular shutdown, as implemented bysystemd-shutdown(8).\u2022The third phase of regular shutdown, i.e. the return to theinitrd context\u2022The hardware reboot operation\u2022The firmware initialization\u2022The boot loader initialization\u2022The kernel initialization\u2022The initrd initializationHowever this form of reboot comes with drawbacks as well:\u2022The OS update remains incomplete, as the kernel is not resetand continues running.\u2022Kernel settings (such as /proc/sys/ settings, a.k.a.\"sysctl\", or /sys/ settings) are not reset.These limitations may be addressed by various means, which areoutside of the scope of this documentation, such as kernellive-patching and sufficiently comprehensive /etc/sysctl.d/files.",
        "name": "systemd-soft-reboot.service - Userspace reboot operation",
        "section": 8
    },
    {
        "command": "systemd-suspend-then-hibernate.service",
        "description": "systemd-suspend.service is a system service that is pulled in bysuspend.target and is responsible for the actual system suspend.Similarly, systemd-hibernate.service is pulled in byhibernate.target to execute the actual hibernation. Finally,systemd-hybrid-sleep.service is pulled in by hybrid-sleep.targetto execute hybrid hibernation with system suspend and pulled inby suspend-then-hibernate.target to execute system suspend with atimeout that will activate hibernate later.Immediately before entering system suspend and/or hibernationsystemd-suspend.service (and the other mentioned units,respectively) will run all executables in/usr/lib/systemd/system-sleep/ and pass two arguments to them.The first argument will be \"pre\", the second either \"suspend\",\"hibernate\", \"hybrid-sleep\", or \"suspend-then-hibernate\"depending on the chosen action. An environment variable called\"SYSTEMD_SLEEP_ACTION\" will be set and contain the sleep actionthat is processing. This is primarily helpful for\"suspend-then-hibernate\" where the value of the variable will be\"suspend\", \"hibernate\", or \"suspend-after-failed-hibernate\" incases where hibernation has failed. Immediately after leavingsystem suspend and/or hibernation the same executables are run,but the first argument is now \"post\". All executables in thisdirectory are executed in parallel, and execution of the actionis not continued until all executables have finished.Note that scripts or binaries dropped in/usr/lib/systemd/system-sleep/ are intended for local use onlyand should be considered hacks. If applications want to react tosystem suspend/hibernation and resume, they should rather use theInhibitor interface[1].Note that systemd-suspend.service, systemd-hibernate.service,systemd-hybrid-sleep.service, andsystemd-suspend-then-hibernate.service should never be executeddirectly. Instead, trigger system sleep with a command such assystemctl suspend or systemctl hibernate.Internally, this service will echo a string like \"mem\" into/sys/power/state, to trigger the actual system suspend. Whatexactly is written where can be configured in the [Sleep] sectionof /etc/systemd/sleep.conf or a sleep.conf.d file. Seesystemd-sleep.conf(5).",
        "name": "systemd-suspend.service, systemd-hibernate.service, systemd-hybrid-sleep.service, systemd-suspend-then-hibernate.service,systemd-sleep - System sleep state logic",
        "section": 8
    },
    {
        "command": "systemd-suspend.service",
        "description": "systemd-suspend.service is a system service that is pulled in bysuspend.target and is responsible for the actual system suspend.Similarly, systemd-hibernate.service is pulled in byhibernate.target to execute the actual hibernation. Finally,systemd-hybrid-sleep.service is pulled in by hybrid-sleep.targetto execute hybrid hibernation with system suspend and pulled inby suspend-then-hibernate.target to execute system suspend with atimeout that will activate hibernate later.Immediately before entering system suspend and/or hibernationsystemd-suspend.service (and the other mentioned units,respectively) will run all executables in/usr/lib/systemd/system-sleep/ and pass two arguments to them.The first argument will be \"pre\", the second either \"suspend\",\"hibernate\", \"hybrid-sleep\", or \"suspend-then-hibernate\"depending on the chosen action. An environment variable called\"SYSTEMD_SLEEP_ACTION\" will be set and contain the sleep actionthat is processing. This is primarily helpful for\"suspend-then-hibernate\" where the value of the variable will be\"suspend\", \"hibernate\", or \"suspend-after-failed-hibernate\" incases where hibernation has failed. Immediately after leavingsystem suspend and/or hibernation the same executables are run,but the first argument is now \"post\". All executables in thisdirectory are executed in parallel, and execution of the actionis not continued until all executables have finished.Note that scripts or binaries dropped in/usr/lib/systemd/system-sleep/ are intended for local use onlyand should be considered hacks. If applications want to react tosystem suspend/hibernation and resume, they should rather use theInhibitor interface[1].Note that systemd-suspend.service, systemd-hibernate.service,systemd-hybrid-sleep.service, andsystemd-suspend-then-hibernate.service should never be executeddirectly. Instead, trigger system sleep with a command such assystemctl suspend or systemctl hibernate.Internally, this service will echo a string like \"mem\" into/sys/power/state, to trigger the actual system suspend. Whatexactly is written where can be configured in the [Sleep] sectionof /etc/systemd/sleep.conf or a sleep.conf.d file. Seesystemd-sleep.conf(5).",
        "name": "systemd-suspend.service, systemd-hibernate.service, systemd-hybrid-sleep.service, systemd-suspend-then-hibernate.service,systemd-sleep - System sleep state logic",
        "section": 8
    },
    {
        "command": "systemd-sysctl",
        "description": "systemd-sysctl.service is an early boot service that configuressysctl(8) kernel parameters by invoking/usr/lib/systemd/systemd-sysctl.When invoked with no arguments, /usr/lib/systemd/systemd-sysctlapplies all directives from configuration files listed insysctl.d(5). If one or more filenames are passed on the commandline, only the directives in these files are applied.In addition, --prefix= option may be used to limit which sysctlsettings are applied.See sysctl.d(5) for information about the configuration of sysctlsettings. After sysctl configuration is changed on disk, it mustbe written to the files in /proc/sys/ before it takes effect. Itis possible to update specific settings, or simply to reload allconfiguration, see Examples below.",
        "name": "systemd-sysctl.service, systemd-sysctl - Configure kernelparameters at boot",
        "section": 8
    },
    {
        "command": "systemd-sysctl.service",
        "description": "systemd-sysctl.service is an early boot service that configuressysctl(8) kernel parameters by invoking/usr/lib/systemd/systemd-sysctl.When invoked with no arguments, /usr/lib/systemd/systemd-sysctlapplies all directives from configuration files listed insysctl.d(5). If one or more filenames are passed on the commandline, only the directives in these files are applied.In addition, --prefix= option may be used to limit which sysctlsettings are applied.See sysctl.d(5) for information about the configuration of sysctlsettings. After sysctl configuration is changed on disk, it mustbe written to the files in /proc/sys/ before it takes effect. Itis possible to update specific settings, or simply to reload allconfiguration, see Examples below.",
        "name": "systemd-sysctl.service, systemd-sysctl - Configure kernelparameters at boot",
        "section": 8
    },
    {
        "command": "systemd-sysext",
        "description": "systemd-sysext activates/deactivates system extension images.System extension images may \u2013 dynamically at runtime \u2014 extend the/usr/ and /opt/ directory hierarchies with additional files. Thisis particularly useful on immutable system images where a /usr/and/or /opt/ hierarchy residing on a read-only file system shallbe extended temporarily at runtime without making any persistentmodifications.System extension images should contain files and directoriessimilar in fashion to regular operating system tree. When one ormore system extension images are activated, their /usr/ and /opt/hierarchies are combined via \"overlayfs\" with the samehierarchies of the host OS, and the host /usr/ and /opt/overmounted with it (\"merging\"). When they are deactivated, themount point is disassembled \u2014 again revealing the unmodifiedoriginal host version of the hierarchy (\"unmerging\"). Mergingthus makes the extension's resources suddenly appear below the/usr/ and /opt/ hierarchies as if they were included in the baseOS image itself. Unmerging makes them disappear again, leaving inplace only the files that were shipped with the base OS imageitself.Files and directories contained in the extension images outsideof the /usr/ and /opt/ hierarchies are not merged, and hence haveno effect when included in a system extension image. Inparticular, files in the /etc/ and /var/ included in a systemextension image will not appear in the respective hierarchiesafter activation.System extension images are strictly read-only, and the host/usr/ and /opt/ hierarchies become read-only too while they areactivated.System extensions are supposed to be purely additive, i.e. theyare supposed to include only files that do not exist in theunderlying basic OS image. However, the underlying mechanism(overlayfs) also allows overlaying or removing files, but it isrecommended not to make use of this.System extension images may be provided in the following formats:1. Plain directories or btrfs subvolumes containing the OS tree2. Disk images with a GPT disk label, following the DiscoverablePartitions Specification[1]3. Disk images lacking a partition table, with a naked Linuxfile system (e.g. erofs, squashfs or ext4)These image formats are the same ones that systemd-nspawn(1)supports via its --directory=/--image= switches and those thatthe service manager supports via RootDirectory=/RootImage=.Similar to them they may optionally carry Verity authenticationinformation.System extensions are searched for in the directories/etc/extensions/, /run/extensions/ and /var/lib/extensions/. Thefirst two listed directories are not suitable for carrying largebinary images, however are still useful for carrying symlinks tothem. The primary place for installing system extensions is/var/lib/extensions/. Any directories found in these searchdirectories are considered directory based extension images; anyfiles with the .raw suffix are considered disk image basedextension images. When invoked in the initrd, the additionaldirectory /.extra/sysext/ is included in the directories that aresearched for extension images. Note however, that by default atighter image policy applies to images found there, though, seebelow. This directory is populated by systemd-stub(7) withextension images found in the system's EFI System Partition.During boot OS extension images are activated automatically, ifthe systemd-sysext.service is enabled. Note that this serviceruns only after the underlying file systems where systemextensions may be located have been mounted. This means they arenot suitable for shipping resources that are processed bysubsystems running in earliest boot. Specifically, OS extensionimages are not suitable for shipping system services orsystemd-sysusers(8) definitions. See the Portable ServicesDocumentation[2] for a simple mechanism for shipping systemservices in disk images, in a similar fashion to OS extensions.Note the different isolation on these two mechanisms: whilesystem extension directly extend the underlying OS image withadditional files that appear in a way very similar to as if theywere shipped in the OS image itself and thus imply no securityisolation, portable services imply service level sandboxing inone way or another. The systemd-sysext.service service isguaranteed to finish start-up before basic.target is reached;i.e. at the time regular services initialize (those which do notuse DefaultDependencies=no), the files and directories systemextensions provide are available in /usr/ and /opt/ and may beaccessed.Note that there is no concept of enabling/disabling installedsystem extension images: all installed extension images areautomatically activated at boot. However, you can place an emptydirectory named like the extension (no .raw) in /etc/extensions/to \"mask\" an extension with the same name in a system folder withlower precedence.A simple mechanism for version compatibility is enforced: asystem extension image must carry a/usr/lib/extension-release.d/extension-release.$name file, whichmust match its image name, that is compared with the hostos-release file: the contained ID= fields have to match unless\"_any\" is set for the extension. If the extension ID= is not\"_any\", the SYSEXT_LEVEL= field (if defined) has to match. If thelatter is not defined, the VERSION_ID= field has to matchinstead. If the extension defines the ARCHITECTURE= field and thevalue is not \"_any\" it has to match the kernel's architecturereported by uname(2) but the used architecture identifiers arethe same as for ConditionArchitecture= described insystemd.unit(5). System extensions should not ship a/usr/lib/os-release file (as that would be merged into the host/usr/ tree, overriding the host OS version data, which is notdesirable). The extension-release file follows the same formatand semantics, and carries the same content, as the os-releasefile of the OS, but it describes the resources carried in theextension image.The systemd-confext concept follows the same principle as thesystemd-sysext(1) functionality but instead of working on /usrand /opt, confext will extend only /etc. Files and directoriescontained in the confext images outside of the /etc/ hierarchyare not merged, and hence have no effect when included in theimage. Formats for these images are of the same as sysext images.The merged hierarchy will be mounted with \"nosuid\" and (if notdisabled via --noexec=false) \"noexec\".Confexts are looked for in the directories /run/confexts/,/var/lib/confexts/, /usr/lib/confexts/ and/usr/local/lib/confexts/. The first listed directory is notsuitable for carrying large binary images, however is stilluseful for carrying symlinks to them. The primary place forinstalling configuration extensions is /var/lib/confexts/. Anydirectories found in these search directories are considereddirectory based confext images; any files with the .raw suffixare considered disk image based confext images.Again, just like sysext images, the confext images will contain a/etc/extension-release.d/extension-release.$name file, which mustmatch the image name (with the usual escape hatch of xattr), andagain with content being one or more of ID=, VERSION_ID=, andCONFEXT_LEVEL. Confext images will then be checked and matchedagainst the base OS layer.",
        "name": "systemd-sysext, systemd-sysext.service, systemd-confext, systemd-confext.service - Activates System Extension Images",
        "section": 8
    },
    {
        "command": "systemd-sysext.service",
        "description": "systemd-sysext activates/deactivates system extension images.System extension images may \u2013 dynamically at runtime \u2014 extend the/usr/ and /opt/ directory hierarchies with additional files. Thisis particularly useful on immutable system images where a /usr/and/or /opt/ hierarchy residing on a read-only file system shallbe extended temporarily at runtime without making any persistentmodifications.System extension images should contain files and directoriessimilar in fashion to regular operating system tree. When one ormore system extension images are activated, their /usr/ and /opt/hierarchies are combined via \"overlayfs\" with the samehierarchies of the host OS, and the host /usr/ and /opt/overmounted with it (\"merging\"). When they are deactivated, themount point is disassembled \u2014 again revealing the unmodifiedoriginal host version of the hierarchy (\"unmerging\"). Mergingthus makes the extension's resources suddenly appear below the/usr/ and /opt/ hierarchies as if they were included in the baseOS image itself. Unmerging makes them disappear again, leaving inplace only the files that were shipped with the base OS imageitself.Files and directories contained in the extension images outsideof the /usr/ and /opt/ hierarchies are not merged, and hence haveno effect when included in a system extension image. Inparticular, files in the /etc/ and /var/ included in a systemextension image will not appear in the respective hierarchiesafter activation.System extension images are strictly read-only, and the host/usr/ and /opt/ hierarchies become read-only too while they areactivated.System extensions are supposed to be purely additive, i.e. theyare supposed to include only files that do not exist in theunderlying basic OS image. However, the underlying mechanism(overlayfs) also allows overlaying or removing files, but it isrecommended not to make use of this.System extension images may be provided in the following formats:1. Plain directories or btrfs subvolumes containing the OS tree2. Disk images with a GPT disk label, following the DiscoverablePartitions Specification[1]3. Disk images lacking a partition table, with a naked Linuxfile system (e.g. erofs, squashfs or ext4)These image formats are the same ones that systemd-nspawn(1)supports via its --directory=/--image= switches and those thatthe service manager supports via RootDirectory=/RootImage=.Similar to them they may optionally carry Verity authenticationinformation.System extensions are searched for in the directories/etc/extensions/, /run/extensions/ and /var/lib/extensions/. Thefirst two listed directories are not suitable for carrying largebinary images, however are still useful for carrying symlinks tothem. The primary place for installing system extensions is/var/lib/extensions/. Any directories found in these searchdirectories are considered directory based extension images; anyfiles with the .raw suffix are considered disk image basedextension images. When invoked in the initrd, the additionaldirectory /.extra/sysext/ is included in the directories that aresearched for extension images. Note however, that by default atighter image policy applies to images found there, though, seebelow. This directory is populated by systemd-stub(7) withextension images found in the system's EFI System Partition.During boot OS extension images are activated automatically, ifthe systemd-sysext.service is enabled. Note that this serviceruns only after the underlying file systems where systemextensions may be located have been mounted. This means they arenot suitable for shipping resources that are processed bysubsystems running in earliest boot. Specifically, OS extensionimages are not suitable for shipping system services orsystemd-sysusers(8) definitions. See the Portable ServicesDocumentation[2] for a simple mechanism for shipping systemservices in disk images, in a similar fashion to OS extensions.Note the different isolation on these two mechanisms: whilesystem extension directly extend the underlying OS image withadditional files that appear in a way very similar to as if theywere shipped in the OS image itself and thus imply no securityisolation, portable services imply service level sandboxing inone way or another. The systemd-sysext.service service isguaranteed to finish start-up before basic.target is reached;i.e. at the time regular services initialize (those which do notuse DefaultDependencies=no), the files and directories systemextensions provide are available in /usr/ and /opt/ and may beaccessed.Note that there is no concept of enabling/disabling installedsystem extension images: all installed extension images areautomatically activated at boot. However, you can place an emptydirectory named like the extension (no .raw) in /etc/extensions/to \"mask\" an extension with the same name in a system folder withlower precedence.A simple mechanism for version compatibility is enforced: asystem extension image must carry a/usr/lib/extension-release.d/extension-release.$name file, whichmust match its image name, that is compared with the hostos-release file: the contained ID= fields have to match unless\"_any\" is set for the extension. If the extension ID= is not\"_any\", the SYSEXT_LEVEL= field (if defined) has to match. If thelatter is not defined, the VERSION_ID= field has to matchinstead. If the extension defines the ARCHITECTURE= field and thevalue is not \"_any\" it has to match the kernel's architecturereported by uname(2) but the used architecture identifiers arethe same as for ConditionArchitecture= described insystemd.unit(5). System extensions should not ship a/usr/lib/os-release file (as that would be merged into the host/usr/ tree, overriding the host OS version data, which is notdesirable). The extension-release file follows the same formatand semantics, and carries the same content, as the os-releasefile of the OS, but it describes the resources carried in theextension image.The systemd-confext concept follows the same principle as thesystemd-sysext(1) functionality but instead of working on /usrand /opt, confext will extend only /etc. Files and directoriescontained in the confext images outside of the /etc/ hierarchyare not merged, and hence have no effect when included in theimage. Formats for these images are of the same as sysext images.The merged hierarchy will be mounted with \"nosuid\" and (if notdisabled via --noexec=false) \"noexec\".Confexts are looked for in the directories /run/confexts/,/var/lib/confexts/, /usr/lib/confexts/ and/usr/local/lib/confexts/. The first listed directory is notsuitable for carrying large binary images, however is stilluseful for carrying symlinks to them. The primary place forinstalling configuration extensions is /var/lib/confexts/. Anydirectories found in these search directories are considereddirectory based confext images; any files with the .raw suffixare considered disk image based confext images.Again, just like sysext images, the confext images will contain a/etc/extension-release.d/extension-release.$name file, which mustmatch the image name (with the usual escape hatch of xattr), andagain with content being one or more of ID=, VERSION_ID=, andCONFEXT_LEVEL. Confext images will then be checked and matchedagainst the base OS layer.",
        "name": "systemd-sysext, systemd-sysext.service, systemd-confext, systemd-confext.service - Activates System Extension Images",
        "section": 8
    },
    {
        "command": "systemd-system-update-generator",
        "description": "systemd-system-update-generator is a generator that automaticallyredirects the boot process to system-update.target, if/system-update or /etc/system-update exists. This is required toimplement the logic explained in the systemd.offline-updates(7).systemd-system-update-generator implements systemd.generator(7).",
        "name": "systemd-system-update-generator - Generator for redirecting bootto offline update mode",
        "section": 8
    },
    {
        "command": "systemd-sysusers",
        "description": "systemd-sysusers creates system users and groups, based on filesin the format described in sysusers.d(5).If invoked with no arguments, it applies all directives from allfiles found in the directories specified by sysusers.d(5). Wheninvoked with positional arguments, if option --replace=PATH isspecified, arguments specified on the command line are usedinstead of the configuration file PATH. Otherwise, just theconfiguration specified by the command line arguments isexecuted. The string \"-\" may be specified instead of a filenameto instruct systemd-sysusers to read the configuration fromstandard input. If the argument is a relative path, allconfiguration directories are searched for a matching file andthe file found that has the highest priority is executed. If theargument is an absolute path, that file is used directly withoutsearching of the configuration directories.",
        "name": "systemd-sysusers, systemd-sysusers.service - Allocate systemusers and groups",
        "section": 8
    },
    {
        "command": "systemd-sysusers.service",
        "description": "systemd-sysusers creates system users and groups, based on filesin the format described in sysusers.d(5).If invoked with no arguments, it applies all directives from allfiles found in the directories specified by sysusers.d(5). Wheninvoked with positional arguments, if option --replace=PATH isspecified, arguments specified on the command line are usedinstead of the configuration file PATH. Otherwise, just theconfiguration specified by the command line arguments isexecuted. The string \"-\" may be specified instead of a filenameto instruct systemd-sysusers to read the configuration fromstandard input. If the argument is a relative path, allconfiguration directories are searched for a matching file andthe file found that has the highest priority is executed. If theargument is an absolute path, that file is used directly withoutsearching of the configuration directories.",
        "name": "systemd-sysusers, systemd-sysusers.service - Allocate systemusers and groups",
        "section": 8
    },
    {
        "command": "systemd-sysv-generator",
        "description": "systemd-sysv-generator is a generator that creates wrapper.service units for SysV init[1] scripts in /etc/init.d/* at bootand when configuration of the system manager is reloaded. Thisallows systemd(1) to support them similarly to native units.LSB headers[2] in SysV init scripts are interpreted, and theordering specified in the header is turned into dependenciesbetween the generated unit and other units. The LSB facilities\"$remote_fs\", \"$network\", \"$named\", \"$portmap\", \"$time\" aresupported and will be turned into dependencies on specific nativesystemd targets. See systemd.special(7) for more details.Note that compatibility is quite comprehensive but not 100%, formore details see Incompatibilities with SysV[3].SysV runlevels have corresponding systemd targets(runlevelX.target). The wrapper unit that is generated will bewanted by those targets which correspond to runlevels for whichthe script is enabled.systemd does not support SysV scripts as part of early boot, soall wrapper units are ordered after basic.target.systemd-sysv-generator implements systemd.generator(7).",
        "name": "systemd-sysv-generator - Unit generator for SysV init scripts",
        "section": 8
    },
    {
        "command": "systemd-time-wait-sync",
        "description": "systemd-time-wait-sync is a system service that delays the startof units that are ordered after time-sync.target (seesystemd.special(7) for details) until the system time has beensynchronized with an accurate remote reference time source bysystemd-timesyncd.service.systemd-timesyncd.service notifies systemd-time-wait-sync aboutsuccessful synchronization.systemd-time-wait-sync also tries todetect when the kernel marks the system clock as synchronized,but this detection is not reliable and is intended only as afallback for compatibility with alternative NTP services that canbe used to synchronize time (e.g., ntpd, chronyd).",
        "name": "systemd-time-wait-sync.service, systemd-time-wait-sync - Waituntil kernel time is synchronized",
        "section": 8
    },
    {
        "command": "systemd-time-wait-sync.service",
        "description": "systemd-time-wait-sync is a system service that delays the startof units that are ordered after time-sync.target (seesystemd.special(7) for details) until the system time has beensynchronized with an accurate remote reference time source bysystemd-timesyncd.service.systemd-timesyncd.service notifies systemd-time-wait-sync aboutsuccessful synchronization.systemd-time-wait-sync also tries todetect when the kernel marks the system clock as synchronized,but this detection is not reliable and is intended only as afallback for compatibility with alternative NTP services that canbe used to synchronize time (e.g., ntpd, chronyd).",
        "name": "systemd-time-wait-sync.service, systemd-time-wait-sync - Waituntil kernel time is synchronized",
        "section": 8
    },
    {
        "command": "systemd-timedated",
        "description": "systemd-timedated.service is a system service that may be used asa mechanism to change the system clock and timezone, as well asto enable/disable network time synchronization.systemd-timedated is automatically activated on request andterminates itself when it is unused.The tool timedatectl(1) is a command line client to this service.systemd-timedated currently offers access to the following foursettings:\u2022The system time\u2022The system timezone\u2022A boolean controlling whether the system RTC is in local orUTC timezone\u2022Whether the time synchronization service is enabled/startedor disabled/stopped, see next section.See org.freedesktop.timedate1(5) andorg.freedesktop.LogControl1(5) for information about the D-BusAPI.",
        "name": "systemd-timedated.service, systemd-timedated - Time and date busmechanism",
        "section": 8
    },
    {
        "command": "systemd-timedated.service",
        "description": "systemd-timedated.service is a system service that may be used asa mechanism to change the system clock and timezone, as well asto enable/disable network time synchronization.systemd-timedated is automatically activated on request andterminates itself when it is unused.The tool timedatectl(1) is a command line client to this service.systemd-timedated currently offers access to the following foursettings:\u2022The system time\u2022The system timezone\u2022A boolean controlling whether the system RTC is in local orUTC timezone\u2022Whether the time synchronization service is enabled/startedor disabled/stopped, see next section.See org.freedesktop.timedate1(5) andorg.freedesktop.LogControl1(5) for information about the D-BusAPI.",
        "name": "systemd-timedated.service, systemd-timedated - Time and date busmechanism",
        "section": 8
    },
    {
        "command": "systemd-timesyncd",
        "description": "systemd-timesyncd is a system service that may be used tosynchronize the local system clock with a remote Network TimeProtocol (NTP) server. It also saves the local time to disk everytime the clock has been synchronized and uses this to possiblyadvance the system realtime clock on subsequent reboots to ensureit (roughly) monotonically advances even if the system lacks abattery-buffered RTC chip.The systemd-timesyncd service implements SNTP only. Thisminimalistic service will step the system clock for large offsetsor slowly adjust it for smaller deltas. Complex use cases thatrequire full NTP support (and where SNTP is not sufficient) arenot covered by systemd-timesyncd.The NTP servers contacted are determined from the global settingsin timesyncd.conf(5), the per-link static settings in .networkfiles, and the per-link dynamic settings received over DHCP. Seesystemd.network(5) for further details.timedatectl(1)'s set-ntp command may be used to enable and start,or disable and stop this service.timedatectl(1)'s timesync-status or show-timesync command can beused to show the current status of this service.systemd-timesyncd initialization delays the start of units thatare ordered after time-set.target (see systemd.special(7) fordetails) until the local time has been updated from/var/lib/systemd/timesync/clock (see below) in order to make itroughly monotonic. It does not delay other units untilsynchronization with an accurate reference time sources has beenreached. Use systemd-time-wait-sync.service(8) to achieve that,which will delay start of units that are ordered aftertime-sync.target until synchronization to an accurate referenceclock is reached.",
        "name": "systemd-timesyncd.service, systemd-timesyncd - Network TimeSynchronization",
        "section": 8
    },
    {
        "command": "systemd-timesyncd.service",
        "description": "systemd-timesyncd is a system service that may be used tosynchronize the local system clock with a remote Network TimeProtocol (NTP) server. It also saves the local time to disk everytime the clock has been synchronized and uses this to possiblyadvance the system realtime clock on subsequent reboots to ensureit (roughly) monotonically advances even if the system lacks abattery-buffered RTC chip.The systemd-timesyncd service implements SNTP only. Thisminimalistic service will step the system clock for large offsetsor slowly adjust it for smaller deltas. Complex use cases thatrequire full NTP support (and where SNTP is not sufficient) arenot covered by systemd-timesyncd.The NTP servers contacted are determined from the global settingsin timesyncd.conf(5), the per-link static settings in .networkfiles, and the per-link dynamic settings received over DHCP. Seesystemd.network(5) for further details.timedatectl(1)'s set-ntp command may be used to enable and start,or disable and stop this service.timedatectl(1)'s timesync-status or show-timesync command can beused to show the current status of this service.systemd-timesyncd initialization delays the start of units thatare ordered after time-set.target (see systemd.special(7) fordetails) until the local time has been updated from/var/lib/systemd/timesync/clock (see below) in order to make itroughly monotonic. It does not delay other units untilsynchronization with an accurate reference time sources has beenreached. Use systemd-time-wait-sync.service(8) to achieve that,which will delay start of units that are ordered aftertime-sync.target until synchronization to an accurate referenceclock is reached.",
        "name": "systemd-timesyncd.service, systemd-timesyncd - Network TimeSynchronization",
        "section": 8
    },
    {
        "command": "systemd-tmpfiles",
        "description": "systemd-tmpfiles creates, deletes, and cleans up volatile andtemporary files and directories, using the configuration fileformat and location specified in tmpfiles.d(5). It must beinvoked with one or more options --create, --remove, and --clean,to select the respective subset of operations.By default, directives from all configuration files are applied.When invoked with --replace=PATH, arguments specified on thecommand line are used instead of the configuration file PATH.Otherwise, if one or more absolute filenames are passed on thecommand line, only the directives in these files are applied. If\"-\" is specified instead of a filename, directives are read fromstandard input. If only the basename of a configuration file isspecified, all configuration directories as specified intmpfiles.d(5) are searched for a matching file and the file foundthat has the highest priority is executed.System services (systemd-tmpfiles-setup.service,systemd-tmpfiles-setup-dev.service,systemd-tmpfiles-clean.service) invoke systemd-tmpfiles to createsystem files and to perform system wide cleanup. Those servicesread administrator-controlled configuration files in tmpfiles.d/directories. User services (systemd-tmpfiles-setup.service,systemd-tmpfiles-clean.service) also invoke systemd-tmpfiles, butit reads a separate set of files, which includes user-controlledfiles under ~/.config/user-tmpfiles.d/ and~/.local/share/user-tmpfiles.d/, and administrator-controlledfiles under /usr/share/user-tmpfiles.d/. Users may use this tocreate and clean up files under their control, but the systeminstance performs global cleanup and is not influenced by userconfiguration. Note that this means a time-based cleanupconfigured in the system instance, such as the one typicallyconfigured for /tmp/, will thus also affect files created by theuser instance if they are placed in /tmp/, even if the userinstance's time-based cleanup is turned off.To re-apply settings after configuration has been modified,simply restart systemd-tmpfiles-clean.service, which will applyany settings which can be safely executed at runtime. To debugsystemd-tmpfiles, it may be useful to invoke it directly from thecommand line with increased log level (see $SYSTEMD_LOG_LEVELbelow).",
        "name": "systemd-tmpfiles, systemd-tmpfiles-setup.service, systemd-tmpfiles-setup-dev.service, systemd-tmpfiles-clean.service,systemd-tmpfiles-clean.timer - Creates, deletes and cleans upvolatile and temporary files and directories",
        "section": 8
    },
    {
        "command": "systemd-tmpfiles-clean.service",
        "description": "systemd-tmpfiles creates, deletes, and cleans up volatile andtemporary files and directories, using the configuration fileformat and location specified in tmpfiles.d(5). It must beinvoked with one or more options --create, --remove, and --clean,to select the respective subset of operations.By default, directives from all configuration files are applied.When invoked with --replace=PATH, arguments specified on thecommand line are used instead of the configuration file PATH.Otherwise, if one or more absolute filenames are passed on thecommand line, only the directives in these files are applied. If\"-\" is specified instead of a filename, directives are read fromstandard input. If only the basename of a configuration file isspecified, all configuration directories as specified intmpfiles.d(5) are searched for a matching file and the file foundthat has the highest priority is executed.System services (systemd-tmpfiles-setup.service,systemd-tmpfiles-setup-dev.service,systemd-tmpfiles-clean.service) invoke systemd-tmpfiles to createsystem files and to perform system wide cleanup. Those servicesread administrator-controlled configuration files in tmpfiles.d/directories. User services (systemd-tmpfiles-setup.service,systemd-tmpfiles-clean.service) also invoke systemd-tmpfiles, butit reads a separate set of files, which includes user-controlledfiles under ~/.config/user-tmpfiles.d/ and~/.local/share/user-tmpfiles.d/, and administrator-controlledfiles under /usr/share/user-tmpfiles.d/. Users may use this tocreate and clean up files under their control, but the systeminstance performs global cleanup and is not influenced by userconfiguration. Note that this means a time-based cleanupconfigured in the system instance, such as the one typicallyconfigured for /tmp/, will thus also affect files created by theuser instance if they are placed in /tmp/, even if the userinstance's time-based cleanup is turned off.To re-apply settings after configuration has been modified,simply restart systemd-tmpfiles-clean.service, which will applyany settings which can be safely executed at runtime. To debugsystemd-tmpfiles, it may be useful to invoke it directly from thecommand line with increased log level (see $SYSTEMD_LOG_LEVELbelow).",
        "name": "systemd-tmpfiles, systemd-tmpfiles-setup.service, systemd-tmpfiles-setup-dev.service, systemd-tmpfiles-clean.service,systemd-tmpfiles-clean.timer - Creates, deletes and cleans upvolatile and temporary files and directories",
        "section": 8
    },
    {
        "command": "systemd-tmpfiles-clean.timer",
        "description": "systemd-tmpfiles creates, deletes, and cleans up volatile andtemporary files and directories, using the configuration fileformat and location specified in tmpfiles.d(5). It must beinvoked with one or more options --create, --remove, and --clean,to select the respective subset of operations.By default, directives from all configuration files are applied.When invoked with --replace=PATH, arguments specified on thecommand line are used instead of the configuration file PATH.Otherwise, if one or more absolute filenames are passed on thecommand line, only the directives in these files are applied. If\"-\" is specified instead of a filename, directives are read fromstandard input. If only the basename of a configuration file isspecified, all configuration directories as specified intmpfiles.d(5) are searched for a matching file and the file foundthat has the highest priority is executed.System services (systemd-tmpfiles-setup.service,systemd-tmpfiles-setup-dev.service,systemd-tmpfiles-clean.service) invoke systemd-tmpfiles to createsystem files and to perform system wide cleanup. Those servicesread administrator-controlled configuration files in tmpfiles.d/directories. User services (systemd-tmpfiles-setup.service,systemd-tmpfiles-clean.service) also invoke systemd-tmpfiles, butit reads a separate set of files, which includes user-controlledfiles under ~/.config/user-tmpfiles.d/ and~/.local/share/user-tmpfiles.d/, and administrator-controlledfiles under /usr/share/user-tmpfiles.d/. Users may use this tocreate and clean up files under their control, but the systeminstance performs global cleanup and is not influenced by userconfiguration. Note that this means a time-based cleanupconfigured in the system instance, such as the one typicallyconfigured for /tmp/, will thus also affect files created by theuser instance if they are placed in /tmp/, even if the userinstance's time-based cleanup is turned off.To re-apply settings after configuration has been modified,simply restart systemd-tmpfiles-clean.service, which will applyany settings which can be safely executed at runtime. To debugsystemd-tmpfiles, it may be useful to invoke it directly from thecommand line with increased log level (see $SYSTEMD_LOG_LEVELbelow).",
        "name": "systemd-tmpfiles, systemd-tmpfiles-setup.service, systemd-tmpfiles-setup-dev.service, systemd-tmpfiles-clean.service,systemd-tmpfiles-clean.timer - Creates, deletes and cleans upvolatile and temporary files and directories",
        "section": 8
    },
    {
        "command": "systemd-tmpfiles-setup-dev.service",
        "description": "systemd-tmpfiles creates, deletes, and cleans up volatile andtemporary files and directories, using the configuration fileformat and location specified in tmpfiles.d(5). It must beinvoked with one or more options --create, --remove, and --clean,to select the respective subset of operations.By default, directives from all configuration files are applied.When invoked with --replace=PATH, arguments specified on thecommand line are used instead of the configuration file PATH.Otherwise, if one or more absolute filenames are passed on thecommand line, only the directives in these files are applied. If\"-\" is specified instead of a filename, directives are read fromstandard input. If only the basename of a configuration file isspecified, all configuration directories as specified intmpfiles.d(5) are searched for a matching file and the file foundthat has the highest priority is executed.System services (systemd-tmpfiles-setup.service,systemd-tmpfiles-setup-dev.service,systemd-tmpfiles-clean.service) invoke systemd-tmpfiles to createsystem files and to perform system wide cleanup. Those servicesread administrator-controlled configuration files in tmpfiles.d/directories. User services (systemd-tmpfiles-setup.service,systemd-tmpfiles-clean.service) also invoke systemd-tmpfiles, butit reads a separate set of files, which includes user-controlledfiles under ~/.config/user-tmpfiles.d/ and~/.local/share/user-tmpfiles.d/, and administrator-controlledfiles under /usr/share/user-tmpfiles.d/. Users may use this tocreate and clean up files under their control, but the systeminstance performs global cleanup and is not influenced by userconfiguration. Note that this means a time-based cleanupconfigured in the system instance, such as the one typicallyconfigured for /tmp/, will thus also affect files created by theuser instance if they are placed in /tmp/, even if the userinstance's time-based cleanup is turned off.To re-apply settings after configuration has been modified,simply restart systemd-tmpfiles-clean.service, which will applyany settings which can be safely executed at runtime. To debugsystemd-tmpfiles, it may be useful to invoke it directly from thecommand line with increased log level (see $SYSTEMD_LOG_LEVELbelow).",
        "name": "systemd-tmpfiles, systemd-tmpfiles-setup.service, systemd-tmpfiles-setup-dev.service, systemd-tmpfiles-clean.service,systemd-tmpfiles-clean.timer - Creates, deletes and cleans upvolatile and temporary files and directories",
        "section": 8
    },
    {
        "command": "systemd-tmpfiles-setup.service",
        "description": "systemd-tmpfiles creates, deletes, and cleans up volatile andtemporary files and directories, using the configuration fileformat and location specified in tmpfiles.d(5). It must beinvoked with one or more options --create, --remove, and --clean,to select the respective subset of operations.By default, directives from all configuration files are applied.When invoked with --replace=PATH, arguments specified on thecommand line are used instead of the configuration file PATH.Otherwise, if one or more absolute filenames are passed on thecommand line, only the directives in these files are applied. If\"-\" is specified instead of a filename, directives are read fromstandard input. If only the basename of a configuration file isspecified, all configuration directories as specified intmpfiles.d(5) are searched for a matching file and the file foundthat has the highest priority is executed.System services (systemd-tmpfiles-setup.service,systemd-tmpfiles-setup-dev.service,systemd-tmpfiles-clean.service) invoke systemd-tmpfiles to createsystem files and to perform system wide cleanup. Those servicesread administrator-controlled configuration files in tmpfiles.d/directories. User services (systemd-tmpfiles-setup.service,systemd-tmpfiles-clean.service) also invoke systemd-tmpfiles, butit reads a separate set of files, which includes user-controlledfiles under ~/.config/user-tmpfiles.d/ and~/.local/share/user-tmpfiles.d/, and administrator-controlledfiles under /usr/share/user-tmpfiles.d/. Users may use this tocreate and clean up files under their control, but the systeminstance performs global cleanup and is not influenced by userconfiguration. Note that this means a time-based cleanupconfigured in the system instance, such as the one typicallyconfigured for /tmp/, will thus also affect files created by theuser instance if they are placed in /tmp/, even if the userinstance's time-based cleanup is turned off.To re-apply settings after configuration has been modified,simply restart systemd-tmpfiles-clean.service, which will applyany settings which can be safely executed at runtime. To debugsystemd-tmpfiles, it may be useful to invoke it directly from thecommand line with increased log level (see $SYSTEMD_LOG_LEVELbelow).",
        "name": "systemd-tmpfiles, systemd-tmpfiles-setup.service, systemd-tmpfiles-setup-dev.service, systemd-tmpfiles-clean.service,systemd-tmpfiles-clean.timer - Creates, deletes and cleans upvolatile and temporary files and directories",
        "section": 8
    },
    {
        "command": "systemd-udev-settle.service",
        "description": "This service calls udevadm settle to wait until all events thathave been queued by udev(7) have been processed. It is a crudeway to wait until \"all\" hardware has been discovered. Servicesmay pull in this service and order themselves after it to waitfor the udev queue to be empty.Using this service is not recommended.There can be no guaranteethat hardware is fully discovered at any specific time, becausethe kernel does hardware detection asynchronously, and certainbuses and devices take a very long time to become ready, and alsoadditional hardware may be plugged in at any time. Instead,services should subscribe to udev events and react to any newhardware as it is discovered. Services that, based onconfiguration, expect certain devices to appear, may warn orreport failure after a timeout. This timeout should be tailoredto the hardware type. Waiting for systemd-udev-settle.serviceusually slows boot significantly, because it means waiting forall unrelated events too.",
        "name": "systemd-udev-settle.service - Wait for all pending udev events tobe handled",
        "section": 8
    },
    {
        "command": "systemd-udevd",
        "description": "systemd-udevd listens to kernel uevents. For every event,systemd-udevd executes matching instructions specified in udevrules. See udev(7).The behavior of the daemon can be configured using udev.conf(5),its command line options, environment variables, and on thekernel command line, or changed dynamically with udevadm control.",
        "name": "systemd-udevd.service, systemd-udevd-control.socket, systemd-udevd-kernel.socket, systemd-udevd - Device event managing daemon",
        "section": 8
    },
    {
        "command": "systemd-udevd-control.socket",
        "description": "systemd-udevd listens to kernel uevents. For every event,systemd-udevd executes matching instructions specified in udevrules. See udev(7).The behavior of the daemon can be configured using udev.conf(5),its command line options, environment variables, and on thekernel command line, or changed dynamically with udevadm control.",
        "name": "systemd-udevd.service, systemd-udevd-control.socket, systemd-udevd-kernel.socket, systemd-udevd - Device event managing daemon",
        "section": 8
    },
    {
        "command": "systemd-udevd-kernel.socket",
        "description": "systemd-udevd listens to kernel uevents. For every event,systemd-udevd executes matching instructions specified in udevrules. See udev(7).The behavior of the daemon can be configured using udev.conf(5),its command line options, environment variables, and on thekernel command line, or changed dynamically with udevadm control.",
        "name": "systemd-udevd.service, systemd-udevd-control.socket, systemd-udevd-kernel.socket, systemd-udevd - Device event managing daemon",
        "section": 8
    },
    {
        "command": "systemd-udevd.service",
        "description": "systemd-udevd listens to kernel uevents. For every event,systemd-udevd executes matching instructions specified in udevrules. See udev(7).The behavior of the daemon can be configured using udev.conf(5),its command line options, environment variables, and on thekernel command line, or changed dynamically with udevadm control.",
        "name": "systemd-udevd.service, systemd-udevd-control.socket, systemd-udevd-kernel.socket, systemd-udevd - Device event managing daemon",
        "section": 8
    },
    {
        "command": "systemd-update-done",
        "description": "systemd-update-done.service is a service that is invoked as partof the first boot after the vendor operating system resources in/usr/ have been updated. This is useful to implement offlineupdates of /usr/ which might require updates to /etc/ or /var/ onthe following boot.systemd-update-done.service updates the file modification time(mtime) of the stamp files /etc/.updated and /var/.updated to themodification time of the /usr/ directory, unless the stamp filesare already newer.Services that shall run after offline upgrades of /usr/ shouldorder themselves before systemd-update-done.service, and use theConditionNeedsUpdate= (see systemd.unit(5)) condition to makesure to run when /etc/ or /var/ are older than /usr/ according tothe modification times of the files described above. Thisrequires that updates to /usr/ are always followed by an updateof the modification time of /usr/, for example by invokingtouch(1) on it.Note that if the systemd.condition-needs-update= kernel commandline option is used it overrides the ConditionNeedsUpdate= unitcondition checks. In that case systemd-update-done.service willnot reset the condition state until a follow-up reboot where thekernel switch is not specified anymore.",
        "name": "systemd-update-done.service, systemd-update-done - Mark /etc/ and/var/ fully updated",
        "section": 8
    },
    {
        "command": "systemd-update-done.service",
        "description": "systemd-update-done.service is a service that is invoked as partof the first boot after the vendor operating system resources in/usr/ have been updated. This is useful to implement offlineupdates of /usr/ which might require updates to /etc/ or /var/ onthe following boot.systemd-update-done.service updates the file modification time(mtime) of the stamp files /etc/.updated and /var/.updated to themodification time of the /usr/ directory, unless the stamp filesare already newer.Services that shall run after offline upgrades of /usr/ shouldorder themselves before systemd-update-done.service, and use theConditionNeedsUpdate= (see systemd.unit(5)) condition to makesure to run when /etc/ or /var/ are older than /usr/ according tothe modification times of the files described above. Thisrequires that updates to /usr/ are always followed by an updateof the modification time of /usr/, for example by invokingtouch(1) on it.Note that if the systemd.condition-needs-update= kernel commandline option is used it overrides the ConditionNeedsUpdate= unitcondition checks. In that case systemd-update-done.service willnot reset the condition state until a follow-up reboot where thekernel switch is not specified anymore.",
        "name": "systemd-update-done.service, systemd-update-done - Mark /etc/ and/var/ fully updated",
        "section": 8
    },
    {
        "command": "systemd-update-utmp",
        "description": "systemd-update-utmp-runlevel.service is a service that writesSysV runlevel changes to utmp and wtmp, as well as the auditlogs, as they occur.systemd-update-utmp.service does the samefor system reboots and shutdown requests.",
        "name": "systemd-update-utmp.service, systemd-update-utmp-runlevel.service, systemd-update-utmp - Write audit and utmpupdates at bootup, runlevel changes and shutdown",
        "section": 8
    },
    {
        "command": "systemd-update-utmp-runlevel.service",
        "description": "systemd-update-utmp-runlevel.service is a service that writesSysV runlevel changes to utmp and wtmp, as well as the auditlogs, as they occur.systemd-update-utmp.service does the samefor system reboots and shutdown requests.",
        "name": "systemd-update-utmp.service, systemd-update-utmp-runlevel.service, systemd-update-utmp - Write audit and utmpupdates at bootup, runlevel changes and shutdown",
        "section": 8
    },
    {
        "command": "systemd-update-utmp.service",
        "description": "systemd-update-utmp-runlevel.service is a service that writesSysV runlevel changes to utmp and wtmp, as well as the auditlogs, as they occur.systemd-update-utmp.service does the samefor system reboots and shutdown requests.",
        "name": "systemd-update-utmp.service, systemd-update-utmp-runlevel.service, systemd-update-utmp - Write audit and utmpupdates at bootup, runlevel changes and shutdown",
        "section": 8
    },
    {
        "command": "systemd-user-sessions",
        "description": "systemd-user-sessions.service is a service that controls userlogins through pam_nologin(8). After basic system initializationis complete, it removes /run/nologin, thus permitting logins.Before system shutdown, it creates /run/nologin, thus prohibitingfurther logins.",
        "name": "systemd-user-sessions.service, systemd-user-sessions - Permituser logins after boot, prohibit user logins at shutdown",
        "section": 8
    },
    {
        "command": "systemd-user-sessions.service",
        "description": "systemd-user-sessions.service is a service that controls userlogins through pam_nologin(8). After basic system initializationis complete, it removes /run/nologin, thus permitting logins.Before system shutdown, it creates /run/nologin, thus prohibitingfurther logins.",
        "name": "systemd-user-sessions.service, systemd-user-sessions - Permituser logins after boot, prohibit user logins at shutdown",
        "section": 8
    },
    {
        "command": "systemd-userdbd",
        "description": "systemd-userdbd is a system service that multiplexes user/grouplookups to all local services that provide JSON user/group recorddefinitions to the system. In addition it synthesizes JSONuser/group records from classic UNIX/glibc NSS user/group recordsin order to provide full backwards compatibility. It may alsopick up statically defined JSON user/group records from drop-infiles in /etc/userdb/, /run/userdb/, /run/host/userdb/ and/usr/lib/userdb/.Most of systemd-userdbd's functionality is accessible through theuserdbctl(1) command.The user and group records this service provides access to followthe JSON User Records[1] and JSON Group Record[2] definitions.This service implements the User/Group Record Lookup API viaVarlink[3], and multiplexes access other services implementingthis API, too. It is thus both server and client of this API.This service provides three distinct Varlink[4] services:io.systemd.Multiplexer provides a single, unified API forquerying JSON user and group records. Internally it talks to allother user/group record services running on the system inparallel and forwards any information discovered. This simplifiesclients substantially since they need to talk to a single serviceonly instead of all of them in parallel.io.systemd.NameServiceSwitch provides compatibility with classicUNIX/glibc NSS user records, i.e. converts struct passwd andstruct group records as acquired with APIs such as getpwnam(1) toJSON user/group records, thus hiding the differences between theservices as much as possible.io.systemd.DropIn makes JSONuser/group records from the aforementioned drop-in directoriesavailable.",
        "name": "systemd-userdbd.service, systemd-userdbd - JSON User/Group RecordQuery Multiplexer/NSS Compatibility",
        "section": 8
    },
    {
        "command": "systemd-userdbd.service",
        "description": "systemd-userdbd is a system service that multiplexes user/grouplookups to all local services that provide JSON user/group recorddefinitions to the system. In addition it synthesizes JSONuser/group records from classic UNIX/glibc NSS user/group recordsin order to provide full backwards compatibility. It may alsopick up statically defined JSON user/group records from drop-infiles in /etc/userdb/, /run/userdb/, /run/host/userdb/ and/usr/lib/userdb/.Most of systemd-userdbd's functionality is accessible through theuserdbctl(1) command.The user and group records this service provides access to followthe JSON User Records[1] and JSON Group Record[2] definitions.This service implements the User/Group Record Lookup API viaVarlink[3], and multiplexes access other services implementingthis API, too. It is thus both server and client of this API.This service provides three distinct Varlink[4] services:io.systemd.Multiplexer provides a single, unified API forquerying JSON user and group records. Internally it talks to allother user/group record services running on the system inparallel and forwards any information discovered. This simplifiesclients substantially since they need to talk to a single serviceonly instead of all of them in parallel.io.systemd.NameServiceSwitch provides compatibility with classicUNIX/glibc NSS user records, i.e. converts struct passwd andstruct group records as acquired with APIs such as getpwnam(1) toJSON user/group records, thus hiding the differences between theservices as much as possible.io.systemd.DropIn makes JSONuser/group records from the aforementioned drop-in directoriesavailable.",
        "name": "systemd-userdbd.service, systemd-userdbd - JSON User/Group RecordQuery Multiplexer/NSS Compatibility",
        "section": 8
    },
    {
        "command": "systemd-vconsole-setup",
        "description": "systemd-vconsole-setup sets up and configures either all virtualconsoles, or \u2014 if the optional TTY parameter is provided \u2014 aspecific one. When the system is booting up, it's called bysystemd-udevd(8) during VT console subsystem initialization.Also, systemd-localed.service(8) invokes it as needed whenlanguage or console changes are made. Internally, this programcalls loadkeys(1) and setfont(8).Execute systemctl restart systemd-vconsole-setup.service in orderto apply any manual changes made to /etc/vconsole.conf.See vconsole.conf(5) for information about the configurationfiles and kernel command line options understood by this program.",
        "name": "systemd-vconsole-setup.service, systemd-vconsole-setup -Configure the virtual consoles",
        "section": 8
    },
    {
        "command": "systemd-vconsole-setup.service",
        "description": "systemd-vconsole-setup sets up and configures either all virtualconsoles, or \u2014 if the optional TTY parameter is provided \u2014 aspecific one. When the system is booting up, it's called bysystemd-udevd(8) during VT console subsystem initialization.Also, systemd-localed.service(8) invokes it as needed whenlanguage or console changes are made. Internally, this programcalls loadkeys(1) and setfont(8).Execute systemctl restart systemd-vconsole-setup.service in orderto apply any manual changes made to /etc/vconsole.conf.See vconsole.conf(5) for information about the configurationfiles and kernel command line options understood by this program.",
        "name": "systemd-vconsole-setup.service, systemd-vconsole-setup -Configure the virtual consoles",
        "section": 8
    },
    {
        "command": "systemd-volatile-root",
        "description": "systemd-volatile-root.service is a service that replaces the rootdirectory with a volatile memory file system (\"tmpfs\"), mountingthe original (non-volatile) /usr/ inside it read-only. This way,vendor data from /usr/ is available as usual, but allconfiguration data in /etc/, all state data in /var/ and allother resources stored directly under the root directory arereset on boot and lost at shutdown, enabling fully statelesssystems.This service is only enabled if full volatile mode is selected,for example by specifying \"systemd.volatile=yes\" on the kernelcommand line. This service runs only in the initrd, before thesystem transitions to the host's root directory. Note that thisservice is not used if \"systemd.volatile=state\" is used, as inthat mode the root directory is non-volatile.",
        "name": "systemd-volatile-root.service, systemd-volatile-root - Make theroot file system volatile",
        "section": 8
    },
    {
        "command": "systemd-volatile-root.service",
        "description": "systemd-volatile-root.service is a service that replaces the rootdirectory with a volatile memory file system (\"tmpfs\"), mountingthe original (non-volatile) /usr/ inside it read-only. This way,vendor data from /usr/ is available as usual, but allconfiguration data in /etc/, all state data in /var/ and allother resources stored directly under the root directory arereset on boot and lost at shutdown, enabling fully statelesssystems.This service is only enabled if full volatile mode is selected,for example by specifying \"systemd.volatile=yes\" on the kernelcommand line. This service runs only in the initrd, before thesystem transitions to the host's root directory. Note that thisservice is not used if \"systemd.volatile=state\" is used, as inthat mode the root directory is non-volatile.",
        "name": "systemd-volatile-root.service, systemd-volatile-root - Make theroot file system volatile",
        "section": 8
    },
    {
        "command": "systemd-xdg-autostart-generator",
        "description": "systemd-xdg-autostart-generator is a generator that creates.service units for XDG autostart[1] files. This permits desktopenvironments to delegate startup of these applications tosystemd(1) .Units created by systemd-xdg-autostart-generator can be startedby the desktop environment using \"xdg-desktop-autostart.target\".See systemd.special(7) for more details.XDG autostart may be conditionalized using both standardized andnon-standardized keys. In order to handle these, the generatormay create one or more ExecCondition= entries. Fornon-standardized keys, well-known helper binaries provided byDesktop Environments are used. All external helpers must detecttheir corresponding desktop environment and must return successwhen run in a different environment. This is important as allExecCondition= directives must succeed for an application to bestarted.Table 1.Special XDG desktop file entries that are processed\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502Entry\u2502 Handling\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502Hidden=, X-systemd-skip=\u2502 No service will be\u2502\u2502\u2502 generated if set to true\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502OnlyShowIn=, NotShowIn=\u2502 ExecCondition= using\u2502\u2502\u2502 systemd-xdg-autostart-condition\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502TryExec=\u2502 No service will be generated if\u2502\u2502\u2502 the binary does not exist or\u2502\u2502\u2502 cannot be executed\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502AutostartCondition=\u2502 ExecCondition= using\u2502\u2502(GNOME extension)\u2502 gnome-systemd-autostart-condition \u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502X-GNOME-Autostart-Phase=\u2502 No service will be generated if\u2502\u2502\u2502 set to any value\u2502\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u2502X-KDE-autostart-condition= \u2502 ExecCondition= using\u2502\u2502\u2502 kde-systemd-start-condition\u2502\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518systemd-xdg-autostart-generator implements systemd.generator(7).",
        "name": "systemd-xdg-autostart-generator - User unit generator for XDGautostart files",
        "section": 8
    },
    {
        "command": "systemtap-service",
        "description": "The SystemTap initscript aims to provide a way to run scripts asa service and easily control them individually. Scripts can beconfigured to start upon manual request, or during systemstartup. On dracut-based systems, it is also possible tointegrate scripts in the initramfs and have them start duringearly-boot.The SystemTap initscript can be invoked manually using thesystemtap-service command. On systemd-based systems, theinitscript is controlled by systemctl with the service filesystemtap.service.There are various parameters and options available to modifyglobal behaviour, as well as script behaviour. Dependenciesbetween scripts can be established so that starting one startsothers.The configuration file of the initscript is located at${prefix}/etc/systemtap/config. Acceptable parameters aredetailed in the GLOBAL PARAMETERS section.Scripts must be placed in the ${prefix}/etc/systemtap/script.ddirectory and must have a .stp extension. When referring to themon the command-line however, there in no need to include the .stpextension.Script names can only contain alphanumeric characters(and '_') and must not start with a number. The scripts directorymay be changed by setting the SCRIPT_PATH parameter in theconfiguration file.",
        "name": "systemtap-service - SystemTap initscript and systemd service",
        "section": 8
    },
    {
        "command": "tbf",
        "description": "The Token Bucket Filter is a classful queueing disciplineavailable for traffic control with the tc(8) command.TBF is a pure shaper and never schedules traffic. It is non-work-conserving and may throttle itself, although packets areavailable, to ensure that the configured rate is not exceeded.It is able to shape up to 1mbit/s of normal traffic with idealminimal burstiness, sending out data exactly at the configuredrates.Much higher rates are possible but at the cost of losing theminimal burstiness. In that case, data is on average dequeued atthe configured rate but may be sent much faster at millisecondtimescales. Because of further queues living in network adaptors,this is often not a problem.",
        "name": "tbf - Token Bucket Filter",
        "section": 8
    },
    {
        "command": "tc",
        "description": "Tc is used to configure Traffic Control in the Linux kernel.Traffic Control consists of the following:SHAPINGWhen traffic is shaped, its rate of transmission is undercontrol. Shaping may be more than lowering the availablebandwidth - it is also used to smooth out bursts intraffic for better network behaviour. Shaping occurs onegress.SCHEDULINGBy scheduling the transmission of packets it is possibleto improve interactivity for traffic that needs it whilestill guaranteeing bandwidth to bulk transfers. Reorderingis also called prioritizing, and happens only on egress.POLICINGWhereas shaping deals with transmission of traffic,policing pertains to traffic arriving. Policing thusoccurs on ingress.DROPPINGTraffic exceeding a set bandwidth may also be droppedforthwith, both on ingress and on egress.Processing of traffic is controlled by three kinds of objects:qdiscs, classes and filters.",
        "name": "tc - show / manipulate traffic control settings",
        "section": 8
    },
    {
        "command": "tc-actions",
        "description": "The actions object in tc allows a user to define actionsindependently of a classifier (filter). These actions can then beassigned to one or more filters, with any packets matching theclassifier's criteria having that action performed on them.Each action type (mirred, police, etc.) will have its own tableto store all created actions.",
        "name": "actions - independently defined actions in tc",
        "section": 8
    },
    {
        "command": "tc-basic",
        "description": "The basic filter allows one to classify packets using theextended match infrastructure.",
        "name": "basic - basic traffic control filter",
        "section": 8
    },
    {
        "command": "tc-bfifo",
        "description": "The pfifo and bfifo qdiscs are unadorned First In, First Outqueues. They are the simplest queues possible and therefore haveno overhead.pfifo constrains the queue size as measured inpackets.bfifo does so as measured in bytes.Like all non-default qdiscs, they maintain statistics. This mightbe a reason to prefer pfifo or bfifo over the default.",
        "name": "pfifo - Packet limited First In, First Out queuebfifo - Byte limited First In, First Out queue",
        "section": 8
    },
    {
        "command": "tc-bpf",
        "description": "Extended Berkeley Packet Filter ( eBPF ) and classic BerkeleyPacket Filter (originally known as BPF, for better distinctionreferred to as cBPF here) are both available as a fullyprogrammable and highly efficient classifier and actions. Theyboth offer a minimal instruction set for implementing smallprograms which can safely be loaded into the kernel and thusexecuted in a tiny virtual machine from kernel space. An in-kernel verifier guarantees that a specified program alwaysterminates and neither crashes nor leaks data from the kernel.In Linux, it's generally considered that eBPF is the successor ofcBPF.The kernel internally transforms cBPF expressions intoeBPF expressions and executes the latter. Execution of them canbe performed in an interpreter or at setup time, they can bejust-in-time compiled (JIT'ed) to run as native machine code.Currently, the eBPF JIT compiler is available for the followingarchitectures:*x86_64 (since Linux 3.18)*arm64 (since Linux 3.18)*s390 (since Linux 4.1)*ppc64 (since Linux 4.8)*sparc64 (since Linux 4.12)*mips64 (since Linux 4.13)*arm32 (since Linux 4.14)*x86_32 (since Linux 4.18)Whereas the following architectures have cBPF, but did not (yet)switch to eBPF JIT support:*ppc32*sparc32*mips32eBPF's instruction set has similar underlying principles as thecBPF instruction set, it however is modelled closer to theunderlying architecture to better mimic native instruction setswith the aim to achieve a better run-time performance. It isdesigned to be JIT'ed with a one to one mapping, which can alsoopen up the possibility for compilers to generate optimized eBPFcode through an eBPF backend that performs almost as fast asnatively compiled code. Given that LLVM provides such an eBPFbackend, eBPF programs can therefore easily be programmed in asubset of the C language. Other than that, eBPF infrastructurealso comes with a construct called \"maps\". eBPF maps arekey/value stores that are shared between multiple eBPF programs,but also between eBPF programs and user space applications.For the traffic control subsystem, classifier and actions thatcan be attached to ingress and egress qdiscs can be written ineBPF or cBPF. The advantage over other classifier and actions isthat eBPF/cBPF provides the generic framework, while users canimplement their highly specialized use cases efficiently. Thismeans that the classifier or action written that way will notsuffer from feature bloat, and can therefore execute its taskhighly efficient. It allows for non-linear classification andeven merging the action part into the classification. Combinedwith efficient eBPF map data structures, user space can push newpolicies like classids into the kernel without reloading aclassifier, or it can gather statistics that are pushed into onemap and use another one for dynamically load balancing trafficbased on the determined load, just to provide a few examples.",
        "name": "BPF - BPF programmable classifier and actions for ingress/egressqueueing disciplines",
        "section": 8
    },
    {
        "command": "tc-cake",
        "description": "CAKE (Common Applications Kept Enhanced) is a shaping-capablequeue discipline which uses both AQM and FQ.It combines COBALT,which is an AQM algorithm combining Codel and BLUE, a shaperwhich operates in deficit mode, and a variant of DRR++ for flowisolation.8-way set-associative hashing is used to virtuallyeliminate hash collisions.Priority queuing is available througha simplified diffserv implementation.Overhead compensation forvarious encapsulation schemes is tightly integrated.All settings are optional; the default settings are chosen to besensible in most common deployments.Most people will only needto set the bandwidth parameter to get useful results, but readingthe Overhead Compensation and Round Trip Time sections isstrongly encouraged.",
        "name": "CAKE - Common Applications Kept Enhanced (CAKE)",
        "section": 8
    },
    {
        "command": "tc-cbq",
        "description": "Class Based Queueing is a classful qdisc that implements a richlinksharing hierarchy of classes. It contains shaping elements aswell as prioritizing capabilities. Shaping is performed usinglink idle time calculations based on the timing of dequeue eventsand underlying link bandwidth.",
        "name": "CBQ - Class Based Queueing",
        "section": 8
    },
    {
        "command": "tc-cbq-details",
        "description": "Class Based Queueing is a classful qdisc that implements a richlinksharing hierarchy of classes. It contains shaping elements aswell as prioritizing capabilities. Shaping is performed usinglink idle time calculations based on the timing of dequeue eventsand underlying link bandwidth.",
        "name": "CBQ - Class Based Queueing",
        "section": 8
    },
    {
        "command": "tc-cbs",
        "description": "The CBS (Credit Based Shaper) qdisc implements the shapingalgorithm defined by the IEEE 802.1Q-2014 Section 8.6.8.2, whichapplies a well defined rate limiting method to the traffic.This queueing discipline is intended to be used by TSN (TimeSensitive Networking) applications, the CBS parameters arederived directly by what is described by the Annex L of the IEEE802.1Q-2014 Specification. The algorithm and how it affects thelatency are detailed there.CBS is meant to be installed under another qdisc that maps packetflows to traffic classes, one example is mqprio(8).",
        "name": "CBS - Credit Based Shaper (CBS) Qdisc",
        "section": 8
    },
    {
        "command": "tc-cgroup",
        "description": "This filter serves as a hint to tc that the assigned class ID ofthe net_cls control group the process the packet originates frombelongs to should be used for classification. Obviously, it isuseful for locally generated packets only.",
        "name": "cgroup - control group based traffic control filter",
        "section": 8
    },
    {
        "command": "tc-choke",
        "description": "CHOKe (CHOose and Keep for responsive flows, CHOose and Kill forunresponsive flows) is a classless qdisc designed to bothidentify and penalize flows that monopolize the queue. CHOKe is avariation of RED, and the configuration is similar to RED.",
        "name": "choke - choose and keep scheduler",
        "section": 8
    },
    {
        "command": "tc-codel",
        "description": "CoDel (pronounced \"coddle\") is an adaptive \"no-knobs\" activequeue management algorithm (AQM) scheme that was developed toaddress the shortcomings of RED and its variants. It wasdeveloped with the following goals in mind:o It should be parameterless.o It should keep delays low while permitting bursts of traffic.o It should control delay.o It should adapt dynamically to changing link rates with noimpact on utilization.o It should be simple and efficient and should scale from simpleto complex routers.",
        "name": "CoDel - Controlled-Delay Active Queue Management algorithm",
        "section": 8
    },
    {
        "command": "tc-connmark",
        "description": "The connmark action is used to restore the connection's markvalue into the packet's fwmark.",
        "name": "connmark - netfilter connmark retriever action",
        "section": 8
    },
    {
        "command": "tc-csum",
        "description": "The csum action triggers checksum recalculation of specifiedpacket headers. It is commonly used to fix incorrect checksumsafter the pedit action has modified the packet content.",
        "name": "csum - checksum update action",
        "section": 8
    },
    {
        "command": "tc-ct",
        "description": "The ct action is a tc action for sending packets and interactingwith the netfilter conntrack module.It can (as shown in the synopsis, in order):Send the packet to conntrack, and commit the connection, whileconfiguring a 32bit mark, 128bit label, and src/dst nat.Send the packet to conntrack, which will mark the packet with theconnection's state and configured metadata (mark/label), andexecute previous configured nat.Clear the packet's of previous connection tracking state.",
        "name": "ct - tc connection tracking action",
        "section": 8
    },
    {
        "command": "tc-ctinfo",
        "description": "CTINFO (Conntrack Information) is a tc action for retrieving datafrom conntrack marks into various fields.At present it has twoindependent processing modes which may be viewed as sub-functions.DSCP mode copies a DSCP stored in conntrack's connmark into theIPv4/v6 diffserv field.The copying may conditionally occurbased on a flag also stored in the connmark.DSCP mode wasdesigned to assist in restoring packet classifications oningress, classifications which may then be used by qdiscs such asCAKE.It may be used in any circumstance where ingressclassification needs to be maintained across links that otherwisebleach or remap according to their own policies.CPMARK (copymark) mode copies the conntrack connmark into thepacket's mark field.Without additional parameters it isfunctionally completely equivalent to the existing connmarkaction.An optional mask may be specified to mask which bits ofthe connmark are restored.This may be useful when DSCP andCPMARK modes are combined.Simple statistics (tc -s) on DSCP restores and CPMARK copies aremaintained where values for set indicate a count of packetsaltered for that mode.DSCP includes an error count where thedestination packet's diffserv field was unwriteable.",
        "name": "ctinfo - tc connmark processing action",
        "section": 8
    },
    {
        "command": "tc-drr",
        "description": "The Deficit Round Robin Scheduler is a classful queuingdiscipline as a more flexible replacement for Stochastic FairnessQueuing.Unlike SFQ, there are no built-in queues -- you need to addclasses and then set up filters to classify packets accordingly.This can be useful e.g. for using RED qdiscs with differentsettings for particular traffic. There is no default class -- ifa packet cannot be classified, it is dropped.",
        "name": "drr - deficit round robin scheduler",
        "section": 8
    },
    {
        "command": "tc-ematch",
        "description": null,
        "name": "ematch - extended matches for use with \"basic\", \"cgroup\"or\"flow\" filters",
        "section": 8
    },
    {
        "command": "tc-etf",
        "description": "The ETF (Earliest TxTime First) qdisc allows applications tocontrol the instant when a packet should be dequeued from thetraffic control layer into the netdevice. If offload isconfigured and supported by the network interface card, the itwill also control when packets leave the network controller.ETF achieves that by buffering packets until a configurable timebefore their transmission time (i.e. txtime, or deadline), whichcan be configured through the delta option.The qdisc uses a rb-tree internally so packets are always'ordered' by their txtime and will be dequeued following the(next) earliest txtime first.It relies on the SO_TXTIME socket option and the SCM_TXTIME CMSGin each packet field to configure the behavior of time dependentsockets: the clockid to be used as a reference, if the expectedmode of txtime for that socket is deadline or strict mode, and ifpacket drops should be reported on the socket's error queue. Seesocket(7) for more information.The etf qdisc will drop any packets with a txtime in the past, orif a packet expires while waiting for being dequeued.This queueing discipline is intended to be used by TSN (TimeSensitive Networking) applications, and it exposes a trafficshaping functionality that is commonly documented as \"LaunchTime\" or \"Time-Based Scheduling\" by vendors and the documentationof network interface controllers.ETF is meant to be installed under another qdisc that maps packetflows to traffic classes, one example is mqprio(8).",
        "name": "ETF - Earliest TxTime First (ETF) Qdisc",
        "section": 8
    },
    {
        "command": "tc-ets",
        "description": "The Enhanced Transmission Selection scheduler is a classfulqueuing discipline that merges functionality of PRIO and DRRqdiscs in one scheduler. ETS makes it easy to configure a set ofstrict and bandwidth-sharing bands to implement the transmissionselection described in 802.1Qaz.On creation with 'tc qdisc add', a fixed number of bands iscreated. Each band is a class, although it is not possible todirectly add and remove bands with 'tc class' commands. Thenumber of bands to be created must instead be specified on thecommand line as the qdisc is added.The minor number of classid to use when referring to a band isthe band number increased by one. Thus band 0 will have classidof major:1, band 1 that of major:2, etc.ETS bands are of two types: some number may be in strict mode,the remaining ones are in bandwidth-sharing mode.",
        "name": "ETS - Enhanced Transmission Selection scheduler",
        "section": 8
    },
    {
        "command": "tc-flow",
        "description": "The flow classifier is meant to extend the SFQ hashingcapabilities without hard-coding new hash functions. It alsoallows deterministic mappings of keys to classes.",
        "name": "flow - flow based traffic control filter",
        "section": 8
    },
    {
        "command": "tc-flower",
        "description": "The flower filter matches flows to the set of keys specified andassigns an arbitrarily chosen class ID to packets belonging tothem. Additionally (or alternatively) an action from the genericaction framework may be called.",
        "name": "flower - flow based traffic control filter",
        "section": 8
    },
    {
        "command": "tc-fq",
        "description": "FQ (Fair Queue) is a classless packet scheduler meant to bemostly used for locally generated traffic.It is designed toachieve per flow pacing.FQ does flow separation, and is able torespect pacing requirements set by TCP stack.All packetsbelonging to a socket are considered as a 'flow'.For non localpackets (router workload), packet hash is used as fallback.An application can specify a maximum pacing rate using theSO_MAX_PACING_RATE setsockopt call.This packet scheduler addsdelay between packets to respect rate limitation set on eachsocket. Note that after linux-4.20, linux adopted EDT (EarliestDeparture Time) and TCP directly sets the appropriate DepartureTime for each skb.Dequeueing happens in a round-robin fashion.A special FIFOqueue is reserved for high priority packets ( TC_PRIO_CONTROLpriority), such packets are always dequeued first.FQ is non-work-conserving.TCP pacing is good for flows having idle times, as the congestionwindow permits TCP stack to queue a possibly large number ofpackets.This removes the 'slow start after idle' choice, badlyhitting large BDP flows and applications delivering chunks ofdata such as video streams.",
        "name": "FQ - Fair Queue traffic policing",
        "section": 8
    },
    {
        "command": "tc-fq_codel",
        "description": "FQ_Codel (Fair Queuing Controlled Delay) is queuing disciplinethat combines Fair Queuing with the CoDel AQM scheme. FQ_Codeluses a stochastic model to classify incoming packets intodifferent flows and is used to provide a fair share of thebandwidth to all the flows using the queue. Each such flow ismanaged by the CoDel queuing discipline. Reordering within a flowis avoided since Codel internally uses a FIFO queue.",
        "name": "CoDel - Fair Queuing (FQ) with Controlled Delay (CoDel)",
        "section": 8
    },
    {
        "command": "tc-fq_pie",
        "description": "FQ-PIE (Flow Queuing with Proportional Integral controllerEnhanced) is a queuing discipline that combines Flow Queuing withthe PIE AQM scheme. FQ-PIE uses a Jenkins hash function toclassify incoming packets into different flows and is used toprovide a fair share of the bandwidth to all the flows using theqdisc. Each such flow is managed by the PIE algorithm.",
        "name": "FQ-PIE - Flow Queue Proportional Integral controller Enhanced",
        "section": 8
    },
    {
        "command": "tc-fw",
        "description": "the fw filter allows one to classify packets based on apreviously set fwmark by iptables.If the masked value of thefwmark matches the filter's masked handle, the filter matches. Bydefault, all 32 bits of the handle and the fwmark are masked.iptables allows one to mark single packets with the MARK target,or whole connections using CONNMARK.The benefit of using thisfilter instead of doing the heavy-lifting with tc itself is thaton one hand it might be convenient to keep packet filtering andclassification in one place, possibly having to match a packetjust once, and on the other users familiar with iptables but nottc will have a less hard time adding QoS to their setups.",
        "name": "fw - fwmark traffic control filter",
        "section": 8
    },
    {
        "command": "tc-gate",
        "description": "GATE action allows specified ingress frames can be passed atspecific time slot, or be dropped at specific time slot. Tcfilter filters the ingress frames, then tc gate action wouldspecify which time slot and how many bytes these frames can bepassed to device and which time slot frames would be dropped.Gate action also assign a base-time to tell when the entry liststart.Then gate action would start to repeat the gate entrylist cyclically at the start base-time.For the softwaresimulation, gate action requires the user assign reference timeclock type.",
        "name": "gate - Stream Gate Action",
        "section": 8
    },
    {
        "command": "tc-hfsc",
        "description": null,
        "name": "HFSC - Hierarchical Fair Service Curve's control under linux",
        "section": 8
    },
    {
        "command": "tc-htb",
        "description": "HTB is meant as a more understandable and intuitive replacementfor the CBQ qdisc in Linux. Both CBQ and HTB help you to controlthe use of the outbound bandwidth on a given link. Both allow youto use one physical link to simulate several slower links and tosend different kinds of traffic on different simulated links. Inboth cases, you have to specify how to divide the physical linkinto simulated links and how to decide which simulated link touse for a given packet to be sent.Unlike CBQ, HTB shapes traffic based on the Token Bucket Filteralgorithm which does not depend on interface characteristics andso does not need to know the underlying bandwidth of the outgoinginterface.",
        "name": "HTB - Hierarchy Token Bucket",
        "section": 8
    },
    {
        "command": "tc-ife",
        "description": "The ife action allows for a sending side to encapsulate arbitrarymetadata, which is then decapsulated by the receiving end. Thesender runs in encoding mode and the receiver in decode mode.Both sender and receiver must specify the same ethertype. In thefuture, a registered ethertype may be available as a default.",
        "name": "IFE - encapsulate/decapsulate metadata",
        "section": 8
    },
    {
        "command": "tc-matchall",
        "description": "The matchall filter allows one to classify every packet thatflows on the port and run a action on it.",
        "name": "matchall - traffic control filter that matches every packet",
        "section": 8
    },
    {
        "command": "tc-mirred",
        "description": "The mirred action allows packet mirroring (copying) orredirecting (stealing) the packet it receives. Mirroring is whatis sometimes referred to as Switch Port Analyzer (SPAN) and iscommonly used to analyze and/or debug flows.",
        "name": "mirred - mirror/redirect action",
        "section": 8
    },
    {
        "command": "tc-mpls",
        "description": "The mpls action performs mpls encapsulation or decapsulation on apacket, reflected by the operation modes POP, PUSH, MODIFY andDEC_TTL.The POP mode requires the ethertype of the header thatfollows the MPLS header (e.g.IPv4 or another MPLS). It willremove the outer MPLS header and replace the ethertype in the MACheader with that passed. The PUSH and MODIFY modes update thecurrent MPLS header information or add a new header.PUSHrequires at least an MPLS_LABEL.DEC_TTL requires no argumentsand simply subtracts 1 from the MPLS header TTL field.",
        "name": "mpls - mpls manipulation module",
        "section": 8
    },
    {
        "command": "tc-mqprio",
        "description": "The MQPRIO qdisc is a simple queuing discipline that allowsmapping traffic flows to hardware queue ranges using prioritiesand a configurable priority to traffic class mapping. A trafficclass in this context is a set of contiguous qdisc classes whichmap 1:1 to a set of hardware exposed queues.By default the qdisc allocates a pfifo qdisc (packet limitedfirst in, first out queue) per TX queue exposed by the lowerlayer device. Other queuing disciplines may be addedsubsequently. Packets are enqueued using the map parameter andhashed across the indicated queues in the offset and count.Bydefault these parameters are configured by the hardware driver tomatch the hardware QOS structures.Channel mode supports full offload of the mqprio options, thetraffic classes, the queue configurations and QOS attributes tothe hardware. Enabled hardware can provide hardware QOS with theability to steer traffic flows to designated traffic classesprovided by this qdisc. Hardware based QOS is configured usingthe shaper parameter.bw_rlimit with minimum and maximumbandwidth rates can be used for setting transmission rates oneach traffic class. Also further qdiscs may be added to theclasses of MQPRIO to create more complex configurations.",
        "name": "MQPRIO - Multiqueue Priority Qdisc (Offloaded Hardware QOS)",
        "section": 8
    },
    {
        "command": "tc-nat",
        "description": "The nat action allows one to perform NAT without the overhead ofconntrack, which is desirable if the number of flows or addressesto perform NAT on is large. This action is best used incombination with the u32 filter to allow for efficient lookups ofa large number of stateless NAT rules in constant time.",
        "name": "nat - stateless native address translation action",
        "section": 8
    },
    {
        "command": "tc-netem",
        "description": "The netem queue discipline provides Network Emulationfunctionality for testing protocols by emulating the propertiesof real-world networks.The queue discipline provides one or more network impairments topackets such as: delay, loss, duplication, and packet corruption.",
        "name": "netem - Network Emulator",
        "section": 8
    },
    {
        "command": "tc-pedit",
        "description": "The pedit action can be used to change arbitrary packet data. Thelocation of data to change can either be specified by giving anoffset and size as in RAW_OP, or for header values by naming theheader and field to edit the size is then chosen automaticallybased on the header field size.",
        "name": "pedit - generic packet editor action",
        "section": 8
    },
    {
        "command": "tc-pfifo",
        "description": "The pfifo and bfifo qdiscs are unadorned First In, First Outqueues. They are the simplest queues possible and therefore haveno overhead.pfifo constrains the queue size as measured inpackets.bfifo does so as measured in bytes.Like all non-default qdiscs, they maintain statistics. This mightbe a reason to prefer pfifo or bfifo over the default.",
        "name": "pfifo - Packet limited First In, First Out queuebfifo - Byte limited First In, First Out queue",
        "section": 8
    },
    {
        "command": "tc-pfifo_fast",
        "description": "pfifo_fast is the default qdisc of each interface.Whenever an interface is created, the pfifo_fast qdisc isautomatically used as a queue. If another qdisc is attached, itpreempts the default pfifo_fast, which automatically returns tofunction when an existing qdisc is detached.In this sense this qdisc is magic, and unlike other qdiscs.",
        "name": "pfifo_fast - three-band first in, first out queue",
        "section": 8
    },
    {
        "command": "tc-pie",
        "description": "Proportional Integral controller-Enhanced (PIE) is a controltheoretic active queue management scheme. It is based on theproportional integral controller but aims to control delay. Themain design goals areo Low latency controlo High link utilizationo Simple implementationo Guaranteed stability and fast responsiveness",
        "name": "PIE - Proportional Integral controller-Enhanced AQM algorithm",
        "section": 8
    },
    {
        "command": "tc-police",
        "description": "The police action allows limiting of the byte or packet rate oftraffic matched by the filter it is attached to.There are two different algorithms available to measure the byterate: The first one uses an internal dual token bucket and isconfigured using the rate, burst, mtu, peakrate, overhead andlinklayer parameters. The second one uses an in-kernel samplingmechanism. It can be fine-tuned using the estimator filterparameter.There is one algorithm available to measure packet rate and it issimilar to the first algorithm described for byte rate. It isconfigured using the pkt_rate and pkt_burst parameters.At least one of the rate and pkt_rate parameters must beconfigured.",
        "name": "police - policing action",
        "section": 8
    },
    {
        "command": "tc-prio",
        "description": "The PRIO qdisc is a simple classful queueing discipline thatcontains an arbitrary number of classes of differing priority.The classes are dequeued in numerical descending order ofpriority. PRIO is a scheduler and never delays packets - it is awork-conserving qdisc, though the qdiscs contained in the classesmay not be.Very useful for lowering latency when there is no need forslowing down traffic.",
        "name": "PRIO - Priority qdisc",
        "section": 8
    },
    {
        "command": "tc-red",
        "description": "Random Early Detection is a classless qdisc which manages itsqueue size smartly. Regular queues simply drop packets from thetail when they are full, which may not be the optimal behaviour.RED also performs tail drop, but does so in a more gradual way.Once the queue hits a certain average length, packets enqueuedhave a configurable chance of being marked (which may meandropped). This chance increases linearly up to a point called themax average queue length, although the queue might get bigger.This has a host of benefits over simple taildrop, while not beingprocessor intensive. It prevents synchronous retransmits after aburst in traffic, which cause further retransmits, etc.The goal is to have a small queue size, which is good forinteractivity while not disturbing TCP/IP traffic with too manysudden drops after a burst of traffic.Depending on if ECN is configured, marking either means droppingor purely marking a packet as overlimit.",
        "name": "red - Random Early Detection",
        "section": 8
    },
    {
        "command": "tc-route",
        "description": "Match packets based on routing table entries. This filter centersaround the possibility to assign a realm to routing tableentries. For any packet to be classified by this filter, arouting table lookup is performed and the returned realm is usedto decide on whether the packet is a match or not.",
        "name": "route - route traffic control filter",
        "section": 8
    },
    {
        "command": "tc-sample",
        "description": "The sample action allows sampling packets matching classifier.The packets are chosen randomly according to the rate parameter,and are sampled using the psample generic netlink channel. Theuser can also specify packet truncation to save user-kerneltraffic. Each sample includes some informative metadata about theoriginal packet, which is sent using netlink attributes,alongside the original packet data.The user can either specify the sample action parameters aspresented in the first form above, or use an existing sampleaction using its index, as presented in the second form.",
        "name": "sample - packet sampling tc action",
        "section": 8
    },
    {
        "command": "tc-sfb",
        "description": "Stochastic Fair Blue is a classless qdisc to manage congestionbased on packet loss and link utilization history while trying toprevent non-responsive flows (i.e. flows that do not react tocongestion marking or dropped packets) from impacting performanceof responsive flows.Unlike RED, where the marking probabilityhas to be configured, BLUE tries to determine the ideal markingprobability automatically.",
        "name": "sfb - Stochastic Fair Blue",
        "section": 8
    },
    {
        "command": "tc-sfq",
        "description": "Stochastic Fairness Queueing is a classless queueing disciplineavailable for traffic control with the tc(8) command.SFQ does not shape traffic but only schedules the transmission ofpackets, based on 'flows'.The goal is to ensure fairness sothat each flow is able to send data in turn, thus preventing anysingle flow from drowning out the rest.This may in fact have some effect in mitigating a Denial ofService attempt.SFQ is work-conserving and therefore always delivers a packet ifit has one available.",
        "name": "sfq - Stochastic Fairness Queueing",
        "section": 8
    },
    {
        "command": "tc-simple",
        "description": "This is a pedagogical example rather than an actually usefulaction. Upon every access, it prints the given STRING which maybe of arbitrary length.",
        "name": "simple - basic example action",
        "section": 8
    },
    {
        "command": "tc-skbedit",
        "description": "The skbedit action allows one to change a packet's associatedmeta data. It complements the pedit action, which in turn allowsone to change parts of the packet data itself.The most unique feature of skbedit is its ability to decide overwhich queue of an interface with multiple transmit queues thepacket is to be sent out. The number of available transmit queuesis reflected by sysfs entries within/sys/class/net/<interface>/queues with name tx-N (where N is theactual queue number).",
        "name": "skbedit - SKB editing action",
        "section": 8
    },
    {
        "command": "tc-skbmod",
        "description": "The skbmod action is intended as a usability upgrade to theexisting pedit action. Instead of having to manually edit 8-,16-, or 32-bit chunks of an ethernet header, skbmod allowscomplete substitution of supported elements.Action must be oneof set, swap and ecn.set and swap only affect Ethernet packets,while ecn only affects IP packets.",
        "name": "skbmod - user-friendly packet editor action",
        "section": 8
    },
    {
        "command": "tc-skbprio",
        "description": "SKB Priority Queue is a queueing discipline intended toprioritize the most important packets during a denial-of-service( DoS ) attack. The priority of a packet is given byskb->priority , where a higher value places the packet closer tothe exit of the queue. When the queue is full, the lowestpriority packet in the queue is dropped to make room for thepacket to be added if it has higher priority. If the packet to beadded has lower priority than all packets in the queue, it isdropped.Without SKB priority queue, queue length limits must be imposedon individual sub-queues, and there is no straightforward way toenforce a global queue length limit across all priorities.SKBprio queue enforces a global queue length limit while notrestricting the lengths of individual sub-queues.While SKB Priority Queue is agnostic to how skb->priority isassigned. A typical use case is to copy the 6-bit DS field ofIPv4 and IPv6 packets using tc-skbedit(8).If skb->priority isgreater or equal to 64, the priority is assumed to be 63.Priorities less than 64 are taken at face value.SKB Priority Queue enables routers to locally decide whichpackets to drop under a DoS attack.Priorities should beassigned to packets such that the higher the priority, the moreexpected behavior a source shows.So sources have an incentiveto play by the rules.",
        "name": "skbprio - SKB Priority Queue",
        "section": 8
    },
    {
        "command": "tc-stab",
        "description": "Size tables allow manipulation of packet sizes, as seen by thewhole scheduler framework (of course, the actual packet sizeremains the same). Adjusted packet size is calculated only once -when a qdisc enqueues the packet. Initial root enqueueinitializes it to the real packet's size.Each qdisc can use a different size table, but the adjusted sizeis stored in an area shared by whole qdisc hierarchy attached tothe interface. The effect is that if you have such a setup, thelast qdisc with a stab in a chain \"wins\". For example, considerHFSC with simple pfifo attached to one of its leaf classes.Ifthat pfifo qdisc has stab defined, it will override lengthscalculated during HFSC's enqueue; and in turn, whenever HFSCtries to dequeue a packet, it will use a potentially invalid sizein its calculations. Normal setups will usually include stabdefined only on root qdisc, but further overriding gives extraflexibility for less usual setups.The initial size table is calculated by tc tool using mtu andtsize parameters. The algorithm sets each slot's size to thesmallest power of 2 value, so the whole mtu is covered by thesize table. Neither tsize, nor mtu have to be power of 2 value,so the size table will usually support more than is required bymtu.For example, with mtu = 1500 and tsize = 128, a table with 128slots will be created, where slot 0 will correspond to sizes0-16, slot 1 to 17 - 32, ..., slot 127 to 2033 - 2048. Sizesassigned to each slot depend on linklayer parameter.Stab calculation is also safe for an unusual case, when a sizeassigned to a slot would be larger than 2^16-1 (you will lose theaccuracy though).During the kernel part of packet size adjustment, overhead willbe added to original size, and then slot will be calculated. Ifthe size would cause overflow, more than 1 slot will be used toget the final size. This of course will affect accuracy, but it'sonly a guard against unusual situations.Currently there are two methods of creating values stored in thesize table - ethernet and atm (adsl):ethernetThis is basically 1-1 mapping, so following our example fromabove (disregarding mpu for a moment) slot 0 would have 8,slot 1 would have 16 and so on, up to slot 127 with 2048.Note, that mpu > 0 must be specified, and slots that wouldget less than specified by mpu will get mpu instead. If youdon't specify mpu, the size table will not be created at all(it wouldn't make any difference), although any overheadvalue will be respected during calculations.atm, adslATM linklayer consists of 53 byte cells, where each of themprovides 48 bytes for payload. Also all the cells must befully utilized, thus the last one is padded if/as necessary.When the size table is calculated, adjusted size that fitsproperly into lowest amount of cells is assigned to a slot.For example, a 100 byte long packet requires three 48-bytepayloads, so the final size would require 3 ATM cells - 159bytes.For ATM size tables, 16 bytes sized slots are perfectlyenough. The default values of mtu and tsize create 4 bytessized slots.",
        "name": "tc-stab - Generic size table manipulations",
        "section": 8
    },
    {
        "command": "tc-taprio",
        "description": "The TAPRIO qdisc implements a simplified version of thescheduling state machine defined by IEEE 802.1Q-2018 Section8.6.9, which allows configuration of a sequence of gate states,where each gate state allows outgoing traffic for a subset(potentially empty) of traffic classes.How traffic is mapped to different hardware queues is similar tomqprio(8) and so the map and queues parameters have the samemeaning.The other parameters specify the schedule, and at what point intime it should start (it can behave as the schedule started inthe past).",
        "name": "TAPRIO - Time Aware Priority Shaper",
        "section": 8
    },
    {
        "command": "tc-tbf",
        "description": "The Token Bucket Filter is a classful queueing disciplineavailable for traffic control with the tc(8) command.TBF is a pure shaper and never schedules traffic. It is non-work-conserving and may throttle itself, although packets areavailable, to ensure that the configured rate is not exceeded.It is able to shape up to 1mbit/s of normal traffic with idealminimal burstiness, sending out data exactly at the configuredrates.Much higher rates are possible but at the cost of losing theminimal burstiness. In that case, data is on average dequeued atthe configured rate but may be sent much faster at millisecondtimescales. Because of further queues living in network adaptors,this is often not a problem.",
        "name": "tbf - Token Bucket Filter",
        "section": 8
    },
    {
        "command": "tc-tcindex",
        "description": "This filter allows one to match packets based on their tcindexfield value, i.e. the combination of the DSCP and ECN fields aspresent in IPv4 and IPv6 headers.",
        "name": "tcindex - traffic control index filter",
        "section": 8
    },
    {
        "command": "tc-tunnel_key",
        "description": "The tunnel_key action combined with a shared IP tunnel device,allows one to perform IP tunnel en- or decapsulation on a packet,reflected by the operation modes UNSET and SET.The UNSET modeis optional - even without using it, the metadata informationwill be released automatically when packet processing will befinished.UNSET function could be used in cases when traffic isforwarded between two tunnels, where the metadata from the firsttunnel will be used for encapsulation done by the second tunnel.SET mode requires the source and destination ip ADDRESS and thetunnel key id KEY_ID which will be used by the ip tunnel shareddevice to create the tunnel header. The tunnel_key action isuseful only in combination with a mirred redirect action to ashared IP tunnel device which will use the metadata (for SET )and unset the metadata created by it (for UNSET ).",
        "name": "tunnel_key - Tunnel metadata manipulation",
        "section": 8
    },
    {
        "command": "tc-u32",
        "description": "The Universal/Ugly 32bit filter allows one to match arbitrarybitfields in the packet. Due to breaking everything down tovalues, masks and offsets, It is equally powerful and hard touse. Luckily many abstracting directives are present which allowdefining rules on a higher level and therefore free the user fromhaving to fiddle with bits and masks in many cases.There are two general modes of invocation: The first mode createsa new filter to delegate packets to different destinations. Apartfrom the obvious ones, namely classifying the packet byspecifying a CLASSID or calling an action, one may link onefilter to another one (or even a list of them), effectivelyorganizing filters into a tree-like hierarchy.Typically filter delegation is done by means of a hash table,which leads to the second mode of invocation: it merely serves toset up these hash tables. Filters can select a hash table andprovide a key selector from which a hash is to be computed andused as key to lookup the table's bucket which contains filtersfor further processing. This is useful if a high number offilters is in use, as the overhead of performing the hashoperation and table lookup becomes negligible in that case. Usinghashtables with u32 basically involves the following pattern:(1) Creating a new hash table, specifying it's size using thedivisor parameter and ideally a handle by which the table canbe identified. If the latter is not given, the kernel choosesone on it's own, which has to be guessed later.(2) Creating filters which link to the created table in (1) usingthe link parameter and defining the packet data which thekernel will use to calculate the hashkey.(3) Adding filters to buckets in the hash table from (1).Inorder to avoid having to know how exactly the kernel createsthe hash key, there is the sample parameter, which givessample data to hash and thereby define the table bucket thefilter should be added to.In fact, even if not explicitly requested u32 creates a hashtable for every priority a filter is being added with. Thetable's size is 1 though, so it is in fact merely a linked list.",
        "name": "u32 - universal 32bit traffic control filter",
        "section": 8
    },
    {
        "command": "tc-vlan",
        "description": "The vlan action allows one to perform 802.1Q en- or decapsulationon a packet, reflected by the operation modes POP, PUSH andMODIFY.The POP mode is simple, as no further information isrequired to just drop the outer-most VLAN encapsulation. The PUSHand MODIFY modes require at least a VLANID and allow one tooptionally choose the VLANPROTO to use.The vlan action can also be used to add or remove the baseEthernet header. The pop_eth mode, which takes no argument, isused to remove the base Ethernet header. All existing VLANs musthave been previously dropped. The opposite operation, adding abase Ethernet header, is done with the push_eth mode. In thatcase, the packet must have no MAC header (stacking MAC headers isnot permitted). This mode is mostly useful when a previous actionhas encapsulated the whole original frame behind a network headerand one needs to prepend an Ethernet header before forwarding theresulting packet.",
        "name": "vlan - vlan manipulation module",
        "section": 8
    },
    {
        "command": "tc-xt",
        "description": "The xt action allows one to call arbitrary iptables targets forpackets matching the filter this action is attached to.",
        "name": "xt - tc iptables action",
        "section": 8
    },
    {
        "command": "tcindex",
        "description": "This filter allows one to match packets based on their tcindexfield value, i.e. the combination of the DSCP and ECN fields aspresent in IPv4 and IPv6 headers.",
        "name": "tcindex - traffic control index filter",
        "section": 8
    },
    {
        "command": "telinit",
        "description": "telinit may be used to change the SysV system runlevel. Since theconcept of SysV runlevels is obsolete the runlevel requests willbe transparently translated into systemd unit activationrequests.",
        "name": "telinit - Change SysV runlevel",
        "section": 8
    },
    {
        "command": "tipc",
        "description": "The Transparent Inter-Process Communication (TIPC) protocoloffers total address transparency between processes which allowsapplications in a clustered computer environment to communicatequickly and reliably with each other, regardless of theirlocation within the cluster.TIPC originated at the telecommunications manufacturer Ericsson.The first open source version of TIPC was created in 2000 whenEricsson released its first Linux version of TIPC. TIPC wasintroduced in the mainline Linux kernel in 2006 and is now widelyused both within and outside of Ericsson.",
        "name": "tipc - a TIPC configuration and management tool",
        "section": 8
    },
    {
        "command": "tipc-bearer",
        "description": "Bearer identificationmedia MEDIASpecifies the TIPC media type for a particular bearer tooperate on.Different media types have different ways ofidentifying a unique bearer.For example, ib and ethidentify a bearer with a DEVICE while udp identify abearer with a LOCALIP and a NAMEib - Infinibandeth - Ethernetudp - User Datagram Protocol (UDP)name NAMELogical bearer identifier valid for bearers on udp media.device DEVICEPhysical bearer device valid for bearers on eth and ibmedia.Bearer propertiesdomainThe addressing domain (region) in which a bearer willestablish links and accept link establish requests.priorityDefault link priority inherited by all links subsequentlyestablished over a bearer. A single bearer can only hostone link to a particular node. This means the default linkpriority for a bearer typically affects which bearer touse when communicating with a particular node in an multibearer setup. For more info about link priority seetipc-link(8)toleranceDefault link tolerance inherited by all links subsequentlyestablished over a bearer. For more info about linktolerance see tipc-link(8)windowDefault link window inherited by all links subsequentlyestablished over a bearer. For more info about the linkwindow size see tipc-link(8)UDP bearer optionslocalip LOCALIPSpecify a local IP v4/v6 address for a udp bearer.localport LOCALPORTSpecify the local port for a udp bearer. The default port6118 is used if no port is specified.remoteip REMOTEIPSpecify a remote IP for a udp bearer. If no remote IP isspecified a udp bearer runs in multicast mode and tries toauto-discover its neighbours.The multicast IP address isgenerated based on the TIPC network ID. If a remote IP isspecified the udp bearer runs in point-to-point mode.Multiple remoteip addresses can be added via the beareradd command. Adding one or more unicast remoteip addressesto an existing udp bearer puts the bearer in replicastmode where IP multicast is emulated by sending multipleunicast messages to each configured remoteip.When a peersees a TIPC discovery message from an unknown peer thepeer address is automatically added to the remoteip(replicast) list, thus only one side of a link needs to bemanually configured. A remoteip address cannot be added toa multicast bearer.remoteport REMOTEPORTSpecify the remote port for a udp bearer. The default port6118 is used if no port is specified.",
        "name": "tipc-bearer - show or modify TIPC bearers",
        "section": 8
    },
    {
        "command": "tipc-link",
        "description": "Link statisticsACTIVE link stateAn ACTIVE link is serving traffic. Two links to the samenode can become ACTIVE if they have the same linkpriority.If there is more than two links with the samepriority the additional links will be put in STANDBYstate.STANDBY link stateA STANDBY link has lower link priority than an ACTIVElink. A STANDBY link has control traffic flowing and isready to take over should the ACTIVE link(s) go down.MTUThe Maximum Transmission Unit. The two endpoints advertisetheir default or configured MTU at initial link setup andwill agree to use the lower of the two values should theydiffer.PacketsThe total amount of transmitted or received TIPC packetson a link. Including fragmented and bundled packets.FragmentsRepresented in the form fragments/fragmented.Wherefragmented is the amount of data messages which have beenbroken into fragments.Subsequently the fragments are thetotal amount of packets that the fragmented messages hasbeen broken into.BundlesRepresented in the form bundles/bundled.If a linkbecomes congested the link will attempt to bundle datafrom small bundled packets into bundles of full MTU sizepackets before they are transmitted.ProfileShows the average packet size in octets/bytes for a sampleof packets. It also shows the packet size distribution ofthe sampled packets in the intervals0-64 bytes64-256 bytes256-1024 bytes1024-4096 bytes4096-16384 bytes16384-32768 bytes32768-66000 bytesMessage countersstates - Number of link state messagesprobes - Link state messages with probe flag set.Typically sent when a link is idlenacks - Number of negative acknowledgement (NACK) packetssent and received by the linkdefs - Number of packets received out of orderdups - Number of duplicate packets receivedCongestion linkThe number of times an application has tried to send datawhen the TIPC link was congestedSend queueMax is the maximum amount of messages that has resided inthe out queue during the statistics collection period of alink.Avg is the average outqueue size during the lifetime of alink.Link propertiespriorityThe priority between logical TIPC links to a particularnode. Link priority can range from 0 (lowest) to 31(highest).toleranceLink tolerance specifies the maximum time in millisecondsthat TIPC will allow a communication problem to existbefore taking the link down. The default value is 1500milliseconds.windowThe link window controls how many unacknowledged messagesa link endpoint can have in its transmit queue beforeTIPC's congestion control mechanism is activated.Monitor propertiesthresholdThe threshold specifies the cluster size exceeding whichthe link monitoring algorithm will switch from \"full-mesh\"to \"overlapping-ring\".If set of 0 the overlapping-ringmonitoring is always on and if set to a value larger thananticipated cluster size the overlapping-ring is disabled.The default value is 32.Monitor informationtable_generationRepresents the event count in a node's local monitoringlist. It steps every time something changes in the localmonitor list, including changes in the local domain.cluster_sizeRepresents the current count of cluster members.algorithmThe current supervision algorithm used for neighbourmonitoring for the bearer.Possible values are full-meshor overlapping-ring.statusThe node status derived by the local node.Possiblestatus are up or down.monitoredRepresent the type of monitoring chosen by the local node.Possible values are direct or indirect.generationRepresents the domain generation which is the event countin a node's local domain. Every time something changes(peer add/remove/up/down) the domain generation is steppedand a new version of node record is sent to inform theneighbors about this change. The domain generation helpsthe receiver of a domain record to know if it shouldignore or process the record.applied_node_statusThe node status reported by the peer node for thesucceeding peers in the node list. The Node list is acircular list of ascending addresses starting with thelocal node.Possible status are: U or D. The status Uimplies up and D down.[non_applied_node:status]Represents the nodes and their status as reported by thepeer node.These nodes were not applied to the monitoringlist for this peer node.They are usually transient andoccur during the cluster startup phase or networkreconfiguration.Possible status are: U or D. The statusU implies up and D down.Broadcast propertiesBROADCASTForces all multicast traffic to be transmitted viabroadcast only, irrespective of cluster size and number ofdestinations.REPLICASTForces all multicast traffic to be transmitted viareplicast only, irrespective of cluster size and number ofdestinations.AUTOSELECTAuto switching to broadcast or replicast depending oncluster size and destination node number.ratio SIZESet the AUTOSELECT criteria, percentage of destinationnodes vs cluster size.",
        "name": "tipc-link - show links or modify link properties",
        "section": 8
    },
    {
        "command": "tipc-media",
        "description": "Media propertiespriorityDefault link priority inherited by all bearerssubsequently enabled on a media. For more info about linkpriority see tipc-link(8)toleranceDefault link tolerance inherited by all bearerssubsequently enabled on a media. For more info about linktolerance see tipc-link(8)windowDefault link window inherited by all bearers subsequentlyenabled on a media. For more info about link window seetipc-link(8)",
        "name": "tipc-media - list or modify media properties",
        "section": 8
    },
    {
        "command": "tipc-nametable",
        "description": "The nametable shows TIPC publication information.Nametable formatTypeThe 32-bit type field of the port name. The type fieldoften indicates the class of service provided by a port.LowerThe lower bound of the 32-bit instance field of the portname.The instance field is often used as as a sub-classindicator.UpperThe upper bound of the 32-bit instance field of the portname.The instance field is often used as as a sub-classindicator.A difference in lower and upper means thesocket is bound to the port name range [lower,upper]Port IdentityThe unique socket (port) identifier within the TIPCcluster. The port identity consists of a node identityfollowed by a socket reference number.PublicationThe publication ID is a random number used internally torepresent a publication.ScopeThe publication scope specifies the visibility of a boundport name.The scope can be specified to comprise threedifferent domains: node, cluster and zone.Applicationsresiding within the specified scope can see and access theport using the displayed port name.",
        "name": "tipc-nametable - show TIPC nametable",
        "section": 8
    },
    {
        "command": "tipc-node",
        "description": "Node parametersaddressThe TIPC logical address. On the form x.y.z where x, y andz are unsigned integers.netidNetwork identity. Can by used to create individual TIPCclusters on the same media.",
        "name": "tipc-node - modify and show local node parameters or list peernodes",
        "section": 8
    },
    {
        "command": "tipc-peer",
        "description": "Peer removeRemove an offline peer node from the local data structures. Thepeer is identified by its address",
        "name": "tipc-peer - modify peer information",
        "section": 8
    },
    {
        "command": "tipc-socket",
        "description": "A TIPC socket is represented by an unsigned integer.Bound stateA bound socket has a logical TIPC port name associatedwith it.Connected stateA connected socket is directly connected to another socketcreating a point to point connection between TIPC sockets.If the connection to X was made using a logical port nameY that name will show up as connected to X via Y",
        "name": "tipc-socket - show TIPC socket (port) information",
        "section": 8
    },
    {
        "command": "togglesebool",
        "description": "togglesebool flips the current value of a list of booleans. Ifthe value is currently a 1, then it will be changed to a 0 andvice versa. Only the \"in memory\" values are changed; the boot-time settings are unaffected.",
        "name": "togglesebool - flip the current value of a SELinux boolean",
        "section": 8
    },
    {
        "command": "tracepath",
        "description": "It traces the network path to destination discovering MTU alongthis path. It uses UDP port port or some random port. It issimilar to traceroute. However, it does not require superuserprivileges and has no fancy options.tracepath -6 is a good replacement for traceroute6 and classicexample of application of Linux error queues. The situation withIPv4 is worse, because commercial IP routers do not return enoughinformation in ICMP error messages. Probably, it will change,when they are updated. For now it uses Van Jacobson's trick,sweeping a range of UDP ports to maintain trace history.",
        "name": "tracepath - traces path to a network host discovering MTU alongthis path",
        "section": 8
    },
    {
        "command": "traceroute",
        "description": "traceroute tracks the route packets taken from an IP network ontheir way to a given host. It utilizes the IP protocol's time tolive (TTL) field and attempts to elicit an ICMP TIME_EXCEEDEDresponse from each gateway along the path to the host.traceroute6 is equivalent to traceroute -6The only required parameter is the name or IP address of thedestination host .The optional packet_len`gth is the total sizeof the probing packet (default 60 bytes for IPv4 and 80 forIPv6). The specified size can be ignored in some situations orincreased up to a minimal value.This program attempts to trace the route an IP packet wouldfollow to some internet host by launching probe packets with asmall ttl (time to live) then listening for an ICMP \"timeexceeded\" reply from a gateway.We start our probes with a ttlof one and increase by one until we get an ICMP \"portunreachable\" (or TCP reset), which means we got to the \"host\", orhit a max (which defaults to 30 hops). Three probes (by default)are sent at each ttl setting and a line is printed showing thettl, address of the gateway and round trip time of each probe.The address can be followed by additional information whenrequested. If the probe answers come from different gateways, theaddress of each responding system will be printed.If there isno response within a certain timeout, an \"*\" (asterisk) isprinted for that probe.After the trip time, some additional annotation can be printed:!H, !N, or !P (host, network or protocol unreachable), !S (sourceroute failed), !F (fragmentation needed), !X (communicationadministratively prohibited), !V (host precedence violation), !C(precedence cutoff in effect), or !<num> (ICMP unreachable code<num>).If almost all the probes result in some kind ofunreachable, traceroute will give up and exit.We don't want the destination host to process the UDP probepackets, so the destination port is set to an unlikely value (youcan change it with the -p flag). There is no such a problem forICMP or TCP tracerouting (for TCP we use half-open technique,which prevents our probes to be seen by applications on thedestination host).In the modern network environment the traditional traceroutemethods can not be always applicable, because of widespread useof firewalls.Such firewalls filter the \"unlikely\" UDP ports, oreven ICMP echoes.To solve this, some additional traceroutingmethods are implemented (including tcp), see LIST OF AVAILABLEMETHODS below. Such methods try to use particular protocol andsource/destination port, in order to bypass firewalls (to be seenby firewalls just as a start of allowed type of a networksession).",
        "name": "traceroute - print the route packets trace to network host",
        "section": 8
    },
    {
        "command": "trafgen",
        "description": "trafgen is a fast, zero-copy network traffic generator fordebugging, performance evaluation, and fuzz-testing. trafgenutilizes the packet(7) socket interface of Linux which postponescomplete control over packet data and packet headers into theuser space. It has a powerful packet configuration language,which is rather low-level and not limited to particularprotocols.Thus, trafgen can be used for many purposes. Its onlylimitation is that it cannot mimic full streams resp. sessions.However, it is very useful for various kinds of load testing inorder to analyze and subsequently improve systems behaviour underDoS attack scenarios, for instance.trafgen is Linux specific, meaning there is no support for otheroperating systems, same as netsniff-ng(8), thus we can keep thecode footprint quite minimal and to the point. trafgen makes useof packet(7) socket's TX_RING interface of the Linux kernel,which is a mmap(2)'ed ring buffer shared between user and kernelspace.By default, trafgen starts as many processes as available CPUs,pins each of them to their respective CPU and sets up the ringbuffer each in their own process space after having compiled alist of packets to transmit. Thus, this is likely the fastest onecan get out of the box in terms of transmission performance fromuser space, without having to load unsupported or non-mainlinethird-party kernel modules. On Gigabit Ethernet, trafgen has acomparable performance to pktgen, the built-in Linux kerneltraffic generator, except that trafgen is more flexible in termsof packet configuration possibilities. On 10-Gigabit-per-secondEthernet, trafgen might be slower than pktgen due to theuser/kernel space overhead but still has a fairly highperformance for out of the box kernels.trafgen has the potential to do fuzz testing, meaning a packetconfiguration can be built with random numbers on all or certainpacket offsets that are freshly generated each time a packet issent out. With a built-in IPv4 ping, trafgen can send out an ICMPprobe after each packet injection to the remote host in order totest if it is still responsive/alive. Assuming there is no answerfrom the remote host after a certain threshold of probes, themachine is considered dead and the last sent packet is printedtogether with the random seed that was used by trafgen. You mightnot really get lucky fuzz-testing the Linux kernel, butpresumably there are buggy closed-source embedded systems ornetwork driver's firmware files that are prone to bugs, wheretrafgen could help in finding them.trafgen's configuration language is quite powerful, also due tothe fact, that it supports C preprocessor macros. A stddef.h isbeing shipped with trafgen for this purpose, so that well knowndefines from Linux kernel or network programming can be reused.After a configuration file has passed the C preprocessor stage,it is processed by the trafgen packet compiler. The languageitself supports a couple of features that are useful whenassembling packets, such as built-in runtime checksum support forIP, UDP and TCP. Also it has an expression evaluator wherearithmetic (basic operations, bit operations, bit shifting, ...)on constant expressions is being reduced to a single constant oncompile time. Other features are ''fill'' macros, where a packetcan be filled with n bytes by a constant, a compile-time randomnumber or run-time random number (as mentioned with fuzztesting). Also, netsniff-ng(8) is able to convert a pcap fileinto a trafgen configuration file, thus such a configuration canbe further tweaked for a given scenario.",
        "name": "trafgen - a fast, multithreaded network packet generator",
        "section": 8
    },
    {
        "command": "tune2fs",
        "description": "tune2fs allows the system administrator to adjust various tunablefile system parameters on Linux ext2, ext3, or ext4 file systems.The current values of these options can be displayed by using the-l option to tune2fs(8) program, or by using the dumpe2fs(8)program.The device specifier can either be a filename (i.e., /dev/sda1),or a LABEL or UUID specifier: \"LABEL=volume-label\" or\"UUID=uuid\".(i.e., LABEL=home or UUID=e40486c6-84d5-4f2f-b99c-032281799c9d).",
        "name": "tune2fs - adjust tunable file system parameters on ext2/ext3/ext4file systems",
        "section": 8
    },
    {
        "command": "tunnel_key",
        "description": "The tunnel_key action combined with a shared IP tunnel device,allows one to perform IP tunnel en- or decapsulation on a packet,reflected by the operation modes UNSET and SET.The UNSET modeis optional - even without using it, the metadata informationwill be released automatically when packet processing will befinished.UNSET function could be used in cases when traffic isforwarded between two tunnels, where the metadata from the firsttunnel will be used for encapsulation done by the second tunnel.SET mode requires the source and destination ip ADDRESS and thetunnel key id KEY_ID which will be used by the ip tunnel shareddevice to create the tunnel header. The tunnel_key action isuseful only in combination with a mirred redirect action to ashared IP tunnel device which will use the metadata (for SET )and unset the metadata created by it (for UNSET ).",
        "name": "tunnel_key - Tunnel metadata manipulation",
        "section": 8
    },
    {
        "command": "tzselect",
        "description": "The tzselect program asks the user for information about thecurrent location, and outputs the resulting timezone to standardoutput.The output is suitable as a value for the TZ environmentvariable.All interaction with the user is done via standard input andstandard error.",
        "name": "tzselect - select a timezone",
        "section": 8
    },
    {
        "command": "u32",
        "description": "The Universal/Ugly 32bit filter allows one to match arbitrarybitfields in the packet. Due to breaking everything down tovalues, masks and offsets, It is equally powerful and hard touse. Luckily many abstracting directives are present which allowdefining rules on a higher level and therefore free the user fromhaving to fiddle with bits and masks in many cases.There are two general modes of invocation: The first mode createsa new filter to delegate packets to different destinations. Apartfrom the obvious ones, namely classifying the packet byspecifying a CLASSID or calling an action, one may link onefilter to another one (or even a list of them), effectivelyorganizing filters into a tree-like hierarchy.Typically filter delegation is done by means of a hash table,which leads to the second mode of invocation: it merely serves toset up these hash tables. Filters can select a hash table andprovide a key selector from which a hash is to be computed andused as key to lookup the table's bucket which contains filtersfor further processing. This is useful if a high number offilters is in use, as the overhead of performing the hashoperation and table lookup becomes negligible in that case. Usinghashtables with u32 basically involves the following pattern:(1) Creating a new hash table, specifying it's size using thedivisor parameter and ideally a handle by which the table canbe identified. If the latter is not given, the kernel choosesone on it's own, which has to be guessed later.(2) Creating filters which link to the created table in (1) usingthe link parameter and defining the packet data which thekernel will use to calculate the hashkey.(3) Adding filters to buckets in the hash table from (1).Inorder to avoid having to know how exactly the kernel createsthe hash key, there is the sample parameter, which givessample data to hash and thereby define the table bucket thefilter should be added to.In fact, even if not explicitly requested u32 creates a hashtable for every priority a filter is being added with. Thetable's size is 1 though, so it is in fact merely a linked list.",
        "name": "u32 - universal 32bit traffic control filter",
        "section": 8
    },
    {
        "command": "udevadm",
        "description": "udevadm expects a command and command specific options. Itcontrols the runtime behavior of systemd-udevd, requests kernelevents, manages the event queue, and provides simple debuggingmechanisms.",
        "name": "udevadm - udev management tool",
        "section": 8
    },
    {
        "command": "udflabel",
        "description": "When udflabel is invoked without identifier-options and withoutspecifying new-label then it shows current label of UDFfilesystem on device to standard output terminated by new line.Otherwise it updates UDF filesystem (up to the revision 2.60) ondevice with new specified identifiers from identifier-options.Specifying new-label is synonym for both --lvid and --vid, seesection UDF LABEL AND UUID.",
        "name": "udflabel \u2014 show or change UDF filesystem label",
        "section": 8
    },
    {
        "command": "umount",
        "description": "The umount command detaches the mentioned filesystem(s) from thefile hierarchy. A filesystem is specified by giving the directorywhere it has been mounted. Giving the special device on which thefilesystem lives may also work, but is obsolete, mainly becauseit will fail in case this device was mounted on more than onedirectory.Note that a filesystem cannot be unmounted when it is 'busy' -for example, when there are open files on it, or when someprocess has its working directory there, or when a swap file onit is in use. The offending process could even be umount itself -it opens libc, and libc in its turn may open for example localefiles. A lazy unmount avoids this problem, but it may introduceother issues. See --lazy description below.",
        "name": "umount - unmount filesystems",
        "section": 8
    },
    {
        "command": "umount.nfs",
        "description": "umount.nfs and umount.nfs4 are a part of nfs(5) utilitiespackage, which provides NFS client functionality.umount.nfs4 and umount.nfs are meant to be used by the umount(8)command for unmounting NFS shares. This subcommand, however, canalso be used as a standalone command with limited functionality.dir is the directory on which the file system is mounted.",
        "name": "umount.nfs, umount.nfs4 - unmount a Network File System",
        "section": 8
    },
    {
        "command": "umount.nfs4",
        "description": "umount.nfs and umount.nfs4 are a part of nfs(5) utilitiespackage, which provides NFS client functionality.umount.nfs4 and umount.nfs are meant to be used by the umount(8)command for unmounting NFS shares. This subcommand, however, canalso be used as a standalone command with limited functionality.dir is the directory on which the file system is mounted.",
        "name": "umount.nfs, umount.nfs4 - unmount a Network File System",
        "section": 8
    },
    {
        "command": "uname26",
        "description": "setarch modifies execution domains and process personality flags.The execution domains currently only affects the output of uname-m. For example, on an AMD64 system, running setarch i386 programwill cause program to see i686 instead of x86_64 as the machinetype. It can also be used to set various personality options. Thedefault program is /bin/sh.Since version 2.33 the arch command line argument is optional andsetarch may be used to change personality flags (ADDR_LIMIT_*,SHORT_INODE, etc) without modification of the execution domain.",
        "name": "setarch - change reported architecture in new program environmentand/or set personality flags",
        "section": 8
    },
    {
        "command": "unix_chkpwd",
        "description": "unix_chkpwd is a helper program for the pam_unix module thatverifies the password of the current user. It also checkspassword and account expiration dates in shadow. It is notintended to be run directly from the command line and logs asecurity violation if done so.It is typically installed setuid root or setgid shadow.The interface of the helper - command line options, andinput/output data format are internal to the pam_unix module andit should not be called directly from applications.",
        "name": "unix_chkpwd - Helper binary that verifies the password of thecurrent user",
        "section": 8
    },
    {
        "command": "unix_update",
        "description": "unix_update is a helper program for the pam_unix module thatupdates the password of a given user. It is not intended to berun directly from the command line and logs a security violationif done so.The purpose of the helper is to enable tighter confinement oflogin and password changing services. The helper is thus calledonly when SELinux is enabled on the system.The interface of the helper - command line options, andinput/output data format are internal to the pam_unix module andit should not be called directly from applications.",
        "name": "unix_update - Helper binary that updates the password of a givenuser",
        "section": 8
    },
    {
        "command": "update-pciids",
        "description": "update-pciids fetches the current version of the pci.ids filefrom the primary distribution site and installs it.This utility requires curl, wget or lynx to be installed. If gzipor bzip2 are available, it automatically downloads the compressedversion of the list.",
        "name": "update-pciids - download new version of the PCI ID list",
        "section": 8
    },
    {
        "command": "useradd",
        "description": "When invoked without the -D option, the useradd command creates anew user account using the values specified on the command lineplus the default values from the system. Depending on commandline options, the useradd command will update system files andmay also create the new user's home directory and copy initialfiles.By default, a group will also be created for the new user (see-g, -N, -U, and USERGROUPS_ENAB).",
        "name": "useradd - create a new user or update default new userinformation",
        "section": 8
    },
    {
        "command": "userdel",
        "description": "The userdel command modifies the system account files, deletingall entries that refer to the user name LOGIN. The named usermust exist.",
        "name": "userdel - delete a user account and related files",
        "section": 8
    },
    {
        "command": "usermod",
        "description": "The usermod command modifies the system account files.",
        "name": "usermod - modify a user account",
        "section": 8
    },
    {
        "command": "uuidd",
        "description": "The uuidd daemon is used by the UUID library to generateuniversally unique identifiers (UUIDs), especially time-basedUUIDs, in a secure and guaranteed-unique fashion, even in theface of large numbers of threads running on different CPUs tryingto grab UUIDs.",
        "name": "uuidd - UUID generation daemon",
        "section": 8
    },
    {
        "command": "vdpa",
        "description": null,
        "name": "vdpa - vdpa management tool",
        "section": 8
    },
    {
        "command": "vdpa-dev",
        "description": "vdpa dev show - display vdpa device attributesDEV - specifies the vdpa device to show.If this argument isomitted all devices are listed.Format is:VDPA_DEVICE_NAMEvdpa dev add - add a new vdpa device.name NAMEName of the new vdpa device to add.mgmtdev MGMTDEVName of the management device to use for device addition.device_features DEVICE_FEATURES Specifies the virtio devicefeatures bit-mask that is provisioned for the new vdpa device.The bits can be found under include/uapi/linux/virtio*h.see macros such as VIRTIO_F_ and VIRTIO_XXX(e.g NET)_F_ forspecific bit values.This is optional.mac MACADDR - specifies the mac address for the new vdpa device.This is applicable only for the network type of vdpa device. Thisis optional.mtu MTU - specifies the mtu for the new vdpa device.This isapplicable only for the network type of vdpa device. This isoptional.vdpa dev del - Delete the vdpa device.DEV - specifies the vdpa device to delete.vdpa dev config show - Show configuration of specific device or alldevices.DEV - specifies the vdpa device to show its configuration.Ifthis argument is omitted all devices configuration is listed.Format is:VDPA_DEVICE_NAMEvdpa dev vstats show - shows vendor specific statistics for the givendevice and virtqueue index. The information is presented as name-value pairs where name is the name of the field and value is anumeric value for it.DEV- specifies the vdpa device to queryqidx QUEUE_INDEX- specifies the virtqueue index to query",
        "name": "vdpa-dev - vdpa device configuration",
        "section": 8
    },
    {
        "command": "vdpa-mgmtdev",
        "description": "vdpa mgmtdev show - display vdpa management device attributesMGMTDEV - specifies the vdpa management device to show.If thisargument is omitted all management devices are listed.",
        "name": "vdpa-dev - vdpa management device view",
        "section": 8
    },
    {
        "command": "veritysetup",
        "description": "Veritysetup is used to configure dm-verity managed device-mappermappings.Device-mapper verity target provides read-only transparentintegrity checking of block devices using kernel crypto API.The dm-verity devices are always read-only.",
        "name": "veritysetup - manage dm-verity (block level verification) volumes",
        "section": 8
    },
    {
        "command": "vgcfgbackup",
        "description": "vgcfgbackup creates back up files containing metadata of VGs.Ifno VGs are named, back up files are created for all VGs.Seevgcfgrestore for information on using the back up files.In a default installation, each VG is backed up into a separatefile bearing the name of the VG in the directory /etc/lvm/backup.To use an alternative back up file, use -f. In this case, whenbacking up multiple VGs, the file name is treated as a template,with %s replaced by the VG name.NB. This DOES NOT back up the data content of LVs.It may also be useful to regularly back up the files in /etc/lvm.",
        "name": "vgcfgbackup \u2014 Backup volume group configuration(s)",
        "section": 8
    },
    {
        "command": "vgcfgrestore",
        "description": "vgcfgrestore restores the metadata of a VG from a text back upfile produced by vgcfgbackup. This writes VG metadata onto thedevices specified in back up file.A back up file can be specified with --file.If no backup fileis specified, the most recent one is used. Use --list for a listof the available back up and archive files of a VG.WARNING: When a VG contains thin pools, changes to thin metadatacannot be reverted, and data loss may occur if thin metadata haschanged. The force option is required to restore in this case.",
        "name": "vgcfgrestore \u2014 Restore volume group configuration",
        "section": 8
    },
    {
        "command": "vgchange",
        "description": "vgchange changes VG attributes, changes LV activation in thekernel, and includes other utilities for VG maintenance.",
        "name": "vgchange \u2014 Change volume group attributes",
        "section": 8
    },
    {
        "command": "vgck",
        "description": "vgck checks LVM metadata for consistency.",
        "name": "vgck \u2014 Check the consistency of volume group(s)",
        "section": 8
    },
    {
        "command": "vgconvert",
        "description": "vgconvert is no longer a part of LVM.It was removed along withsupport for the LVM1 format.Use an older version of LVM toconvert VGs from the LVM1 format to LVM2.",
        "name": "vgconvert \u2014 Change volume group metadata format",
        "section": 8
    },
    {
        "command": "vgcreate",
        "description": "vgcreate creates a new VG on block devices. If the devices werenot previously initialized as PVs with pvcreate(8), vgcreate willinititialize them, making them PVs. The pvcreate options forinitializing devices are also available with vgcreate.When vgcreate uses an existing PV, that PV's existing values formetadata size, PE start, etc, are used, even if different valuesare specified in the vgcreate command.To change these values,first use pvremove on the device.",
        "name": "vgcreate \u2014 Create a volume group",
        "section": 8
    },
    {
        "command": "vgdisplay",
        "description": "vgdisplay shows the attributes of VGs, and the associated PVs andLVs.vgs(8) is a preferred alternative that shows the same informationand more, using a more compact and configurable output format.",
        "name": "vgdisplay \u2014 Display volume group information",
        "section": 8
    },
    {
        "command": "vgexport",
        "description": "vgexport changes a VG into the exported state, which ensures thatthe VG and its disks are not being used, and cannot be used untilthe VG is imported by vgimport(8).Putting a VG into anunusable, offline state can be useful when doing things likemoving a VG's disks to another system.Exporting a VG providessome protection from its LVs being accidentally used, or beingused by an automated system before it's ready.A VG cannot be exported until all of its LVs are inactive.LVM commands will ignore an exported VG or report an error if acommand tries to use it.For an exported VG, the vgs command will display attribute, andthe pvs command will display attribute.Both vgs and pvs willdisplay report field.vgexport clears the VG system ID, and vgimport sets the VG systemID to match the host running vgimport (if the host has a systemID).",
        "name": "vgexport \u2014 Unregister volume group(s) from the system",
        "section": 8
    },
    {
        "command": "vgextend",
        "description": "vgextend adds one or more PVs to a VG. This increases the spaceavailable for LVs in the VG.Also, PVs that have gone missing and then returned, e.g. due to atransient device failure, can be added back to the VG without re-initializing them (see --restoremissing).If the specified PVs have not yet been initialized with pvcreate,vgextend will initialize them. In this case pvcreate options canbe used, e.g.--labelsector, --metadatasize, --metadataignore,--pvmetadatacopies, --dataalignment, --dataalignmentoffset.",
        "name": "vgextend \u2014 Add physical volumes to a volume group",
        "section": 8
    },
    {
        "command": "vgimport",
        "description": "vgimport makes exported VGs known to the system again, perhapsafter moving the PVs from a different system.vgexport clears the VG system ID, and vgimport sets the VG systemID to match the host running vgimport (if the host has a systemID).",
        "name": "vgimport \u2014 Register exported volume group with system",
        "section": 8
    },
    {
        "command": "vgimportclone",
        "description": "vgimportclone imports a VG from duplicated PVs, e.g. created by ahardware snapshot of existing PVs.A duplicated VG cannot used until it is made to coexist with theoriginal VG. vgimportclone renames the VG associated with thespecified PVs and changes the associated VG and PV UUIDs.",
        "name": "vgimportclone \u2014 Import a VG from cloned PVs",
        "section": 8
    },
    {
        "command": "vgimportdevices",
        "description": "vgimportdevices adds PVs from a VG to the devices file.This issimilar to using using lvmdevices --adddev to add each PV to thedevices file individually.vgimportdevices will also update theVG metadata to include the device IDs of each PV.vgimportdevices will create a new devices file if none exists.When a devices file is used, the regex filter is ignored, exceptin the case of vgimportdevices which will apply the regex filterwhen looking for the VGs to import to the devices file.Usevgimportdevices -a to import all VGs on a system to the devicesfile.",
        "name": "vgimportdevices \u2014 Add devices for a VG to the devices file.",
        "section": 8
    },
    {
        "command": "vgmerge",
        "description": "vgmerge merges two existing VGs. The inactive source VG is mergedinto the destination VG if physical extent sizes are equal and PVand LV summaries of both VGs fit into the destination VG'slimits.",
        "name": "vgmerge \u2014 Merge volume groups",
        "section": 8
    },
    {
        "command": "vgmknodes",
        "description": "vgmknodes checks the LVM device nodes in /dev that are needed foractive LVs and creates any that are missing and removes unusedones.This command should not usually be needed if all the systemcomponents are interoperating correctly.",
        "name": "vgmknodes \u2014 Create the special files for volume group devices in/dev",
        "section": 8
    },
    {
        "command": "vgreduce",
        "description": "vgreduce removes one or more unused PVs from a VG.",
        "name": "vgreduce \u2014 Remove physical volume(s) from a volume group",
        "section": 8
    },
    {
        "command": "vgremove",
        "description": "vgremove removes one or more VGs. If LVs exist in the VG, aprompt is used to confirm LV removal.If one or more PVs in the VG are lost, consider vgreduce--removemissing to make the VG metadata consistent again.Repeat the force option (-ff) to forcibly remove LVs in the VGwithout confirmation.",
        "name": "vgremove \u2014 Remove volume group(s)",
        "section": 8
    },
    {
        "command": "vgrename",
        "description": "vgrename renames a VG.All VGs visible to a system need to have different names,otherwise many LVM commands will refuse to run or give warningmessages. VGs with the same name can occur when disks are movedbetween machines, or filters are changed. If a newly connecteddisk has a VG with the same name as the VG containing the rootfilesystem, the machine may not boot correctly. When two VGs havethe same name, the VG UUID can be used in place of the source VGname.",
        "name": "vgrename \u2014 Rename a volume group",
        "section": 8
    },
    {
        "command": "vgs",
        "description": "vgs produces formatted output about VGs.",
        "name": "vgs \u2014 Display information about volume groups",
        "section": 8
    },
    {
        "command": "vgscan",
        "description": "vgscan scans all supported LVM block devices in the system forVGs.",
        "name": "vgscan \u2014 Search for all volume groups",
        "section": 8
    },
    {
        "command": "vgsplit",
        "description": "vgsplit moves one or more PVs from a source VG (the first VG arg)to a destination VG (the second VG arg).The PV(s) to move arenamed after the source and destination VGs, or an LV is named, inwhich case the PVs underlying the LV are moved.If the destination VG does not exist, a new VG is created(command options can be used to specify properties of the new VG,also see vgcreate(8)).LVs cannot be split between VGs; each LV must be entirely on thePVs in the source or destination VG.vgsplit can only move complete PVs. (See pvmove(8) for movingpart of a PV.)",
        "name": "vgsplit \u2014 Move physical volumes into a new or existing volumegroup",
        "section": 8
    },
    {
        "command": "vigr",
        "description": "The vipw and vigr commands edit the files /etc/passwd and/etc/group, respectively. With the -s flag, they will edit theshadow versions of those files, /etc/shadow and /etc/gshadow,respectively. The programs will set the appropriate locks toprevent file corruption. When looking for an editor, the programswill first try the environment variable $VISUAL, then theenvironment variable $EDITOR, and finally the default editor,vi(1).",
        "name": "vipw, vigr - edit the password, group, shadow-password orshadow-group file",
        "section": 8
    },
    {
        "command": "vipw",
        "description": "The vipw and vigr commands edit the files /etc/passwd and/etc/group, respectively. With the -s flag, they will edit theshadow versions of those files, /etc/shadow and /etc/gshadow,respectively. The programs will set the appropriate locks toprevent file corruption. When looking for an editor, the programswill first try the environment variable $VISUAL, then theenvironment variable $EDITOR, and finally the default editor,vi(1).",
        "name": "vipw, vigr - edit the password, group, shadow-password orshadow-group file",
        "section": 8
    },
    {
        "command": "visudo",
        "description": "visudo edits the sudoers file in a safe fashion, analogous tovipw(8).visudo locks the sudoers file against multiplesimultaneous edits, performs basic validity checks, and checks forsyntax errors before installing the edited file.If the sudoersfile is currently being edited you will receive a message to tryagain later.visudo parses the sudoers file after editing and will not save thechanges if there is a syntax error.Upon finding an error, visudowill print a message stating the line number(s) where the erroroccurred and the user will receive the \u201cWhat now?\u201d prompt.At thispoint the user may enter \u2018e\u2019 to re-edit the sudoers file, \u2018x\u2019 toexit without saving the changes, or \u2018Q\u2019 to quit and save changes.The \u2018Q\u2019 option should be used with extreme caution because ifvisudo believes there to be a syntax error, so will sudo.If \u2018e\u2019is typed to edit the sudoers file after a syntax error has beendetected, the cursor will be placed on the line where the erroroccurred (if the editor supports this feature).There are two sudoers settings that determine which editor visudowill run.editorA colon (\u2018:\u2019) separated list of editors allowed to beused with visudo.visudo will choose the editor thatmatches the user's SUDO_EDITOR, VISUAL, or EDITORenvironment variable if possible, or the first editorin the list that exists and is executable.sudo doesnot preserve the SUDO_EDITOR, VISUAL, or EDITORenvironment variables unless they are present in theenv_keep list or the env_reset option is disabled inthe sudoers file.The default editor path is/usr/bin/vi which can be set at compile time via the--with-editor configure option.env_editorIf set, visudo will use the value of the SUDO_EDITOR,VISUAL, or EDITOR environment variables before fallingback on the default editor list.visudo is typicallyrun as root so this option may allow a user with visudoprivileges to run arbitrary commands as root withoutlogging.An alternative is to place a colon-separatedlist of \u201csafe\u201d editors in the editor variable.visudowill then only use SUDO_EDITOR, VISUAL, or EDITOR ifthey match a value specified in editor.If theenv_reset flag is enabled, the SUDO_EDITOR, VISUAL,and/or EDITOR environment variables must be present inthe env_keep list for the env_editor flag to functionwhen visudo is invoked via sudo.The default value ison, which can be set at compile time via the--with-env-editor configure option.The options are as follows:-c, --checkEnable check-only mode.The existing sudoers file (and anyother files it includes) will be checked for syntax errors.If the path to the sudoers file was not specified, visudowill also check the file ownership and permissions (see the-O and -P options).A message will be printed to thestandard output describing the status of sudoers unless the-q option was specified.If the check completessuccessfully, visudo will exit with a value of 0.If anerror is encountered, visudo will exit with a value of 1.-f sudoers, --file=sudoersSpecify an alternate sudoers file location, see below.Asof version 1.8.27, the sudoers path can be specifiedwithout using the -f option.-h, --helpDisplay a short help message to the standard output andexit.-I, --no-includesDisable the editing of include files unless there is a pre-existing syntax error.By default, visudo will edit themain sudoers file and any files included via @include or#include directives.Files included via @includedir or#includedir are never edited unless they contain a syntaxerror.-O, --ownerEnforce the default ownership (user and group) of thesudoers file.In edit mode, the owner of the edited filewill be set to the default.In check mode (-c), an errorwill be reported if the owner is incorrect.This option isenabled by default if the sudoers file was not specified.-P, --permsEnforce the default permissions (mode) of the sudoers file.In edit mode, the permissions of the edited file will beset to the default.In check mode (-c), an error will bereported if the file permissions are incorrect.Thisoption is enabled by default if the sudoers file was notspecified.-q, --quietEnable quiet mode.In this mode details about syntaxerrors are not printed.This option is only useful whencombined with the -c option.-s, --strictEnable strict checking of the sudoers file.If an alias isreferenced but not actually defined or if there is a cyclein an alias, visudo will consider this a syntax error.Itis not possible to differentiate between an alias and ahost name or user name that consists solely of uppercaseletters, digits, and the underscore (\u2018_\u2019) character.-V, --versionPrint the visudo and sudoers grammar versions and exit.A sudoers file may be specified instead of the default,/etc/sudoers.The temporary file used is the specified sudoersfile with \u201c.tmp\u201d appended to it.In check-only mode only, \u2018-\u2019 maybe used to indicate that sudoers will be read from the standardinput.Because the policy is evaluated in its entirety, it is notsufficient to check an individual sudoers include file for syntaxerrors.Debugging and sudoers plugin argumentsvisudo versions 1.8.4 and higher support a flexible debuggingframework that is configured via Debug lines in the sudo.conf(5)file.Starting with sudo 1.8.12, visudo will also parse the arguments tothe sudoers plugin to override the default sudoers path name, user-ID, group-ID, and file mode.These arguments, if present, shouldbe listed after the path to the plugin (i.e., after sudoers.so).Multiple arguments may be specified, separated by white space.Forexample:Plugin sudoers_policy sudoers.so sudoers_mode=0400The following arguments are supported:sudoers_file=pathnameThe sudoers_file argument can be used to override the defaultpath to the sudoers file.sudoers_uid=user-IDThe sudoers_uid argument can be used to override the defaultowner of the sudoers file.It should be specified as anumeric user-ID.sudoers_gid=group-IDThe sudoers_gid argument can be used to override the defaultgroup of the sudoers file.It must be specified as a numericgroup-ID (not a group name).sudoers_mode=modeThe sudoers_mode argument can be used to override the defaultfile mode for the sudoers file.It should be specified as anoctal value.For more information on configuring sudo.conf(5), refer to itsmanual.",
        "name": "visudo \u2014 edit the sudoers file",
        "section": 8
    },
    {
        "command": "vlan",
        "description": "The vlan action allows one to perform 802.1Q en- or decapsulationon a packet, reflected by the operation modes POP, PUSH andMODIFY.The POP mode is simple, as no further information isrequired to just drop the outer-most VLAN encapsulation. The PUSHand MODIFY modes require at least a VLANID and allow one tooptionally choose the VLANPROTO to use.The vlan action can also be used to add or remove the baseEthernet header. The pop_eth mode, which takes no argument, isused to remove the base Ethernet header. All existing VLANs musthave been previously dropped. The opposite operation, adding abase Ethernet header, is done with the push_eth mode. In thatcase, the packet must have no MAC header (stacking MAC headers isnot permitted). This mode is mostly useful when a previous actionhas encapsulated the whole original frame behind a network headerand one needs to prepend an Ethernet header before forwarding theresulting packet.",
        "name": "vlan - vlan manipulation module",
        "section": 8
    },
    {
        "command": "vmstat",
        "description": "vmstat reports information about processes, memory, paging, blockIO, traps, disks and cpu activity.The first report produced gives averages since the last reboot.Additional reports give information on a sampling period oflength delay.The process and memory reports are instantaneousin either case.",
        "name": "vmstat - Report virtual memory statistics",
        "section": 8
    },
    {
        "command": "vtep-ctl",
        "description": "The vtep-ctl program configures a VTEP database.See vtep(5) forcomprehensive documentation of the database schema.vtep-ctl connects to an ovsdb-server process that maintains aVTEP configuration database.Using this connection, it queriesand possibly applies changes to the database, depending on thesupplied commands.vtep-ctl can perform any number of commands in a single run,implemented as a single atomic transaction against the database.The vtep-ctl command line begins with global options (see OPTIONSbelow for details).The global options are followed by one ormore commands.Each command should begin with -- by itself as acommand-line argument, to separate it from the followingcommands.(The -- before the first command is optional.)Thecommand itself starts with command-specific options, if any,followed by the command name and any arguments.See EXAMPLESbelow for syntax examples.",
        "name": "vtep-ctl - utility for querying and configuring a VTEP database",
        "section": 8
    },
    {
        "command": "warnquota",
        "description": "warnquota checks the disk quota for specified local filesystems(or for each local filesystem if none specified) and mails awarning message to those users who have reached their softlimit.It is typically run via cron(8).-F, --format=quotaformatPerform setting for specified format (ie. don't performformat autodetection).Possible format names are: vfsoldOriginal quota format with 16-bit UIDs / GIDs, vfsv0 Quotaformat with 32-bit UIDs / GIDs, 64-bit space usage, 32-bitinode usage and limits, vfsv1 Quota format with 64-bitquota limits and usage, xfs Quota on XFS filesystem.-q, --quota-tab=quotatabUse quotatab instead of /etc/quotatab as file with devicedescription strings (see quotatab(5) for syntax).-c, --config=configfileUse configfile instead of /etc/warnquota.conf asconfiguration file (see warnquota.conf(5) for syntax).-a, --admins-file=adminsfileUse adminsfile instead of /etc/quotagrpadmins as a filewith administrators of the groups (see quotagrpadmins(5)for syntax).-u, --usercheck whether users are not exceeding quotas (default).-g, --groupcheck whether groups are not exceeding quotas. If group isexceeding quota a mail is sent to the user specified in/etc/quotagrpadmins.-s, --human-readable[=units]Try to report used space, number of used inodes and limitsin more appropriate units than the default ones. Units canbe also specified explicitely by an optional argument informat [ kgt ],[ kgt ] where the first character specifiesspace units and the second character specifies inodeunits.-i, --no-autofsignore mountpoints mounted by automounter.-d, --no-detailsdo not attach quota report in email.",
        "name": "warnquota - send mail to users over quota",
        "section": 8
    },
    {
        "command": "wdctl",
        "description": "Show hardware watchdog status. The default device is/dev/watchdog. If more than one device is specified then theoutput is separated by one blank line.If the device is already used or user has no permissions to readfrom the device, then wdctl reads data from sysfs. In this caseinformation about supported features (flags) might be missing.Note that the number of supported watchdog features is hardwarespecific.",
        "name": "wdctl - show hardware watchdog status",
        "section": 8
    },
    {
        "command": "wg",
        "description": "wg is the configuration utility for getting and setting theconfiguration of WireGuard tunnel interfaces. The interfacesthemselves can be added and removed using ip-link(8) and their IPaddresses and routing tables can be set using ip-address(8) andip-route(8).The wg utility provides a series of sub-commandsfor changing WireGuard-specific aspects of WireGuard interfaces.If no COMMAND is specified, COMMAND defaults to show.Sub-commands that take an INTERFACE must be passed a WireGuardinterface.",
        "name": "wg - set and retrieve configuration of WireGuard interfaces",
        "section": 8
    },
    {
        "command": "wg-quick",
        "description": "This is an extremely simple script for easily bringing up aWireGuard interface, suitable for a few common use cases.Use up to add and set up an interface, and use down to tear downand remove an interface. Running up adds a WireGuard interface,brings up the interface with the supplied IP addresses, sets upmtu and routes, and optionally runs pre/post up scripts. Runningdown optionally saves the current configuration, removes theWireGuard interface, and optionally runs pre/post down scripts.Running save saves the configuration of an existing interfacewithout bringing the interface down. Use strip to output aconfiguration file with all wg-quick(8)-specific options removed,suitable for use with wg(8).CONFIG_FILE is a configuration file, whose filename is theinterface name followed by `.conf'. Otherwise, INTERFACE is aninterface name, with configuration found at`/etc/wireguard/INTERFACE.conf', searched first, followed bydistro-specific search paths.Generally speaking, this utility is just a simple script thatwraps invocations to wg(8) and ip(8) in order to set up aWireGuard interface. It is designed for users with simple needs,and users with more advanced needs are highly encouraged to use amore specific tool, a more complete network manager, or otherwisejust use wg(8) and ip(8), as usual.",
        "name": "wg-quick - set up a WireGuard interface simply",
        "section": 8
    },
    {
        "command": "wipefs",
        "description": "wipefs can erase filesystem, raid or partition-table signatures(magic strings) from the specified device to make the signaturesinvisible for libblkid. wipefs does not erase the filesystemitself nor any other data from the device.When used without any options, wipefs lists all visiblefilesystems and the offsets of their basic signatures. Thedefault output is subject to change. So whenever possible, youshould avoid using default outputs in your scripts. Alwaysexplicitly define expected columns by using --output columns-listin environments where a stable output is required.wipefs calls the BLKRRPART ioctl when it has erased apartition-table signature to inform the kernel about the change.The ioctl is called as the last step and when all specifiedsignatures from all specified devices are already erased. Thisfeature can be used to wipe content on partitions devices as wellas partition table on a disk device, for example by wipefs -a/dev/sdc1 /dev/sdc2 /dev/sdc.Note that some filesystems and some partition tables store moremagic strings on the device (e.g., FAT, ZFS, GPT). The wipefscommand (since v2.31) lists all the offset where a magic stringshave been detected.When option -a is used, all magic strings that are visible forlibblkid(3) are erased. In this case the wipefs scans the deviceagain after each modification (erase) until no magic string isfound.Note that by default wipefs does not erase nested partitiontables on non-whole disk devices. For this the option --force isrequired.",
        "name": "wipefs - wipe a signature from a device",
        "section": 8
    },
    {
        "command": "x86_64",
        "description": "setarch modifies execution domains and process personality flags.The execution domains currently only affects the output of uname-m. For example, on an AMD64 system, running setarch i386 programwill cause program to see i686 instead of x86_64 as the machinetype. It can also be used to set various personality options. Thedefault program is /bin/sh.Since version 2.33 the arch command line argument is optional andsetarch may be used to change personality flags (ADDR_LIMIT_*,SHORT_INODE, etc) without modification of the execution domain.",
        "name": "setarch - change reported architecture in new program environmentand/or set personality flags",
        "section": 8
    },
    {
        "command": "xfs_admin",
        "description": "xfs_admin uses the xfs_db(8) command to modify various parametersof a filesystem.Devices that are mounted cannot be modified.Administrators mustunmount filesystems before xfs_admin or xfs_db(8) can convertparameters.A number of parameters of a mounted filesystem canbe examined and modified using the xfs_growfs(8) command.The optional logdev parameter specifies the device special filewhere the filesystem's external log resides.This is requiredonly for filesystems that use an external log.See the mkfs.xfs-l option, and refer to xfs(5) for a detailed description of theXFS log.",
        "name": "xfs_admin - change parameters of an XFS filesystem",
        "section": 8
    },
    {
        "command": "xfs_bmap",
        "description": "xfs_bmap prints the map of disk blocks used by files in an XFSfilesystem.The map lists each extent used by the file, as wellas regions in the file that do not have any corresponding blocks(holes).Each line of the listings takes the following form:extent: [startoffset..endoffset]: startblock..endblockHoles are marked by replacing the startblock..endblock with hole.All the file offsets and disk blocks are in units of 512-byteblocks, no matter what the filesystem's block size is.",
        "name": "xfs_bmap - print block mapping for an XFS file",
        "section": 8
    },
    {
        "command": "xfs_copy",
        "description": "xfs_copy copies an XFS filesystem to one or more targets inparallel (see xfs(5)).The first (source) argument must be thepathname of the device or file containing the XFS filesystem. Theremaining arguments specify one or more target devices or filenames. If the pathnames specify devices, a copy of the source XFSfilesystem is created on each device. The target can also be thename of a regular file, in which case an image of the source XFSfilesystem is created in that file. If the file does not exist,xfs_copy creates the file. The length of the resulting file isequal to the size of the source filesystem. However, if the fileis created on an XFS filesystem, the file consumes roughly theamount of space actually used in the source filesystem by thefilesystem and the XFS log.The space saving is because xfs_copyseeks over free blocks instead of copying them and the XFSfilesystem supports sparse files efficiently.xfs_copy should only be used to copy unmounted filesystems, read-only mounted filesystems, or frozen filesystems (seexfs_freeze(8)).Otherwise, the generated filesystem(s) would beinconsistent or corrupt.xfs_copy does not alter the source filesystem in any way. Eachnew (target) filesystem is identical to the original filesystemexcept that new filesystems each have a new unique filesystemidentifier (UUID).Therefore, if both the old and newfilesystems will be used as separate distinct filesystems,xfs_copy or xfsdump(8)/xfsrestore(8) should be used to generatethe new filesystem(s) instead of dd(1) or other programs that doblock-by-block disk copying.xfs_copy uses synchronous writes to ensure that write errors aredetected.xfs_copy uses pthreads(7) to perform simultaneous parallelwrites.xfs_copy creates one additional thread for each targetto be written.All threads die if xfs_copy terminates or aborts.",
        "name": "xfs_copy - copy the contents of an XFS filesystem",
        "section": 8
    },
    {
        "command": "xfs_db",
        "description": "xfs_db is used to examine an XFS filesystem. Under rarecircumstances it can also be used to modify an XFS filesystem,but that task is normally left to xfs_repair(8) or to scriptssuch as xfs_admin(8) that run xfs_db.",
        "name": "xfs_db - debug an XFS filesystem",
        "section": 8
    },
    {
        "command": "xfs_estimate",
        "description": "For each directory argument, xfs_estimate estimates the spacethat directory would take if it were copied to an XFS filesystem.xfs_estimate does not cross mount points.The followingdefinitions are used:KB = *1024MB = *1024*1024GB = *1024*1024*1024The xfs_estimate options are:-b blocksizeUse blocksize instead of the default blocksize of 4096bytes.The modifier k can be used after the number toindicate multiplication by 1024.For example,xfs_estimate -b 64k /requests an estimate of the space required by thedirectory / on an XFS filesystem using a blocksize of 64K(65536) bytes.-vDisplay more information, formatted.-hDisplay usage message.-i, -e logsizeUse logsize instead of the default log size of 1000blocks.-i refers to an internal log, while -e refers toan external log.The modifiers k or m can be used afterthe number to indicate multiplication by 1024 or 1048576,respectively.For example,xfs_estimate -i 1m /requests an estimate of the space required by thedirectory / on an XFS filesystem using an internal log of1 megabyte.-VPrint the version number and exits.",
        "name": "xfs_estimate - estimate the space that an XFS filesystem willtake",
        "section": 8
    },
    {
        "command": "xfs_freeze",
        "description": "xfs_freeze suspends and resumes access to an XFS filesystem (seexfs(5)).xfs_freeze halts new access to the filesystem and creates astable image on disk.xfs_freeze is intended to be used withvolume managers and hardware RAID devices that support thecreation of snapshots.The mount-point argument is the pathname of the directory wherethe filesystem is mounted.The filesystem must be mounted to befrozen (see mount(8)).The -f flag requests the specified XFS filesystem to be frozenfrom new modifications.When this is selected, all ongoingtransactions in the filesystem are allowed to complete, new writesystem calls are halted, other calls which modify the filesystemare halted, and all dirty data, metadata, and log information arewritten to disk.Any process attempting to write to the frozenfilesystem will block waiting for the filesystem to be unfrozen.Note that even after freezing, the on-disk filesystem can containinformation on files that are still in the process of unlinking.These files will not be unlinked until the filesystem is unfrozenor a clean mount of the snapshot is complete.The -u flag is used to un-freeze the filesystem and allowoperations to continue.Any filesystem modifications that wereblocked by the freeze are unblocked and allowed to complete.The -V flag prints the version number and exits.Unless -V is specified, one of -f or -u must be supplied toxfs_freeze.",
        "name": "xfs_freeze - suspend access to an XFS filesystem",
        "section": 8
    },
    {
        "command": "xfs_fsr",
        "description": "xfs_fsr is applicable only to XFS filesystems.xfs_fsr improves the organization of mounted filesystems.Thereorganization algorithm operates on one file at a time,compacting or otherwise improving the layout of the file extents(contiguous blocks of file data).The following options are accepted by xfs_fsr.The -m, -t, and-f options have no meaning if any filesystems or files arespecified on the command line.-m mtabUse this file for the list of filesystems to reorganize.The default is to use /etc/mtab.-t secondsHow long to reorganize.The default is 7200 seconds (2hours).-p passesNumber of passes before terminating global re-org.Thedefault is 10 passes.-f leftoffUse this file instead of /var/tmp/.fsrlast to read thestate of where to start and as the file to store the stateof where reorganization left off.-vVerbose.Print cryptic information about each file beingreorganized.-dDebug.Print even more cryptic information.-gPrint to syslog (default if stdout not a tty).-VPrints the version number and exits.When invoked with no arguments xfs_fsr reorganizes all regularfiles in all mounted filesystems.xfs_fsr makes many cycles over/etc/mtab each time making a single pass over each XFSfilesystem.Each pass goes through and selects files that havethe largest number of extents.It attempts to defragment the top10% of these files on each pass.It runs for up to two hours after which it records the filesystemwhere it left off, so it can start there the next time.Thisinformation is stored in the file /var/tmp/.fsrlast_xfs.If theinformation found here is somehow inconsistent or out of date itis ignored and reorganization starts at the beginning of thefirst filesystem found in /etc/mtab.xfs_fsr can be called with one or more arguments namingfilesystems (block device name), and files to reorganize.Inthis mode xfs_fsr does not read or write /var/tmp/.fsrlast_xfsnor does it run for a fixed time interval.It makes one passthrough each specified regular file and all regular files in eachspecified filesystem.A command line name referring to asymbolic link (except to a file system device), FIFO, or UNIXdomain socket generates a warning message, but is otherwiseignored.While traversing the filesystem these types of filesare silently skipped.",
        "name": "xfs_fsr - filesystem reorganizer for XFS",
        "section": 8
    },
    {
        "command": "xfs_growfs",
        "description": "xfs_growfs expands an existing XFS filesystem (see xfs(5)).Themount-point argument is the pathname of the directory where thefilesystem is mounted. The block-device argument is the devicename of a mounted XFS filesystem.The filesystem must be mountedto be grown (see mount(8)).The existing contents of thefilesystem are undisturbed, and the added space becomes availablefor additional file storage.",
        "name": "xfs_growfs - expand an XFS filesystem",
        "section": 8
    },
    {
        "command": "xfs_info",
        "description": "xfs_info displays geometry information about an existing XFSfilesystem.The mount-point argument is the pathname of adirectory where the filesystem is mounted.The block-device orfile-image contain a raw XFS filesystem.The existing contentsof the filesystem are undisturbed.",
        "name": "xfs_info - display XFS filesystem geometry information",
        "section": 8
    },
    {
        "command": "xfs_io",
        "description": "xfs_io is a debugging tool like xfs_db(8), but is aimed atexamining the regular file I/O paths rather than the raw XFSvolume itself.These code paths include not only the obviousread/write/mmap interfaces for manipulating files, but also coverall of the XFS extensions (such as space preallocation,additional inode flags, etc).",
        "name": "xfs_io - debug the I/O path of an XFS filesystem",
        "section": 8
    },
    {
        "command": "xfs_logprint",
        "description": "xfs_logprint prints the log of an XFS filesystem (see xfs(5)).The device argument is the pathname of the partition or logicalvolume containing the filesystem. The device can be a regularfile if the -f option is used. The contents of the filesystemremain undisturbed.There are two major modes of operation inxfs_logprint.One mode is better for filesystem operation debugging.It iscalled the transactional view and is enabled through the -toption. The transactional view prints only the portion of the logthat pertains to recovery. In other words, it prints out completetransactions between the tail and the head. This view tries todisplay each transaction without regard to how they are splitacross log records.The second mode starts printing out information from thebeginning of the log.Some error blocks might print out in thebeginning because the last log record usually overlaps the oldestlog record. A message is printed when the physical end of the logis reached and when the logical end of the log is reached. A logrecord view is displayed one record at a time. Transactions thatspan log records may not be decoded fully.",
        "name": "xfs_logprint - print the log of an XFS filesystem",
        "section": 8
    },
    {
        "command": "xfs_mdrestore",
        "description": "xfs_mdrestore is a debugging tool that restores a metadata imagegenerated by xfs_metadump(8) to a filesystem. The source argumentspecifies the location of the metadump image and the targetargument specifies the destination for the filesystem image.Ifthe source is -, then the metadata image is read from stdin. Thisallows the output of be another program such as a compressionapplication to be redirected to xfs_mdrestore.The target can beeither a file or a device.xfs_mdrestore should not be used to restore metadata onto anexisting filesystem unless you are completely certain the targetcan be destroyed.",
        "name": "xfs_mdrestore - restores an XFS metadump image to a filesystemimage",
        "section": 8
    },
    {
        "command": "xfs_metadump",
        "description": "xfs_metadump is a debugging tool that copies the metadata from anXFS filesystem to a file.The source argument must be thepathname of the device or file containing the XFS filesystem andthe target argument specifies the destination file name.Iftarget is -, then the output is sent to stdout. This allows theoutput to be redirected to another program such as a compressionapplication.xfs_metadump may only be used to copy unmounted filesystems, orread-only mounted filesystems.xfs_metadump does not alter the source filesystem in any way. Thetarget image is a contiguous (non-sparse) file containing all thefilesystem's metadata and indexes to where the blocks were copiedfrom.By default, xfs_metadump obfuscates most file (regular file,directory and symbolic link) names and extended attribute namesto allow the dumps to be sent without revealing confidentialinformation. Extended attribute values are zeroed and no data iscopied. The only exceptions are file or attribute names that are4 or less characters in length. Also file names that span extents(this can only occur with the mkfs.xfs(8) options where -n size >-b size) are not obfuscated. Names between 5 and 8 characters inlength inclusively are partially obfuscated.xfs_metadump cannot obfuscate metadata in the filesystem log.Log recovery of an obfuscated metadump image may expose clear-text metadata and/or cause filesystem corruption in the restoredimage.It is recommended that the source filesystem first bemounted and unmounted, if possible, to ensure that the log isclean.A subsequent invocation of xfs_metadump will capture aclean log and obfuscate all metadata correctly.If a metadump must be produced from a filesystem with a dirtylog, it is recommended that obfuscation be turned off with -ooption, if metadata such as filenames is not consideredsensitive.If obfuscation is required on a metadump with a dirtylog, please inform the recipient of the metadump image about thissituation.xfs_metadump should not be used for any purposes other than fordebugging and reporting filesystem problems. The most commonusage scenario for this tool is when xfs_repair(8) fails torepair a filesystem and a metadump image can be sent foranalysis.The file generated by xfs_metadump can be restored to filesystemimage (minus the data) using the xfs_mdrestore(8) tool.",
        "name": "xfs_metadump - copy XFS filesystem metadata to a file",
        "section": 8
    },
    {
        "command": "xfs_mkfile",
        "description": "xfs_mkfile creates one or more files. The file is padded withzeroes by default.The default size is in bytes, but it can beflagged as kilobytes, blocks, megabytes, or gigabytes with the k,b, m, or g suffixes, respectively.",
        "name": "xfs_mkfile - create an XFS file",
        "section": 8
    },
    {
        "command": "xfs_ncheck",
        "description": "xfs_ncheck with no -i arguments generates an inode number andpathname list of all files on the given filesystem. Names ofdirectory files are followed by /..The output is not sorted inany particular order.The filesystem to be examined is specifiedby the device argument, which should be the disk or volume devicefor the filesystem.Filesystems stored in files can also bechecked, using the -f flag.",
        "name": "xfs_ncheck - generate pathnames from i-numbers for XFS",
        "section": 8
    },
    {
        "command": "xfs_quota",
        "description": "xfs_quota is a utility for reporting and editing various aspectsof filesystem quota.The options to xfs_quota are:-c cmd xfs_quota commands may be run interactively (the default)or as arguments on the command line. Multiple -c argumentsmay be given.The commands are run in the sequence given,then the program exits.-p progSet the program name for prompts and some error messages,the default value is xfs_quota.-xEnable expert mode.All of the administrative commands(see the ADMINISTRATOR COMMANDS section below) which allowmodifications to the quota system are available only inexpert mode.-fEnable foreign filesystem mode.A limited number of userand administrative commands are available for use on someforeign (non-XFS) filesystems.-d projectProject names or numeric identifiers may be specified withthis option, which restricts the output of the individualxfs_quota commands to the set of projects specified.Multiple -d arguments may be given.-D projects_fileSpecify a file containing the mapping of numeric projectidentifiers to directory trees./etc/projects as default,if this option is none.-P projid_fileSpecify a file containing the mapping of numeric projectidentifiers to project names./etc/projid as default, ifthis option is none.-VPrints the version number and exits.The optional path argument(s) can be used to specify mount pointsor device files which identify XFS filesystems. The output of theindividual xfs_quota commands will then be restricted to the setof filesystems specified.This manual page is divided into two sections - firstly,information for users of filesystems with quota enabled, and thexfs_quota commands of interest to such users; and theninformation which is useful only to administrators of XFSfilesystems using quota and the quota commands which allowmodifications to the quota system.Note that common to almost all of the individual commandsdescribed below are the options for specifying which quota typesare of interest - user quota (-u), group quota (-g), and/orproject quota (-p).Also, several commands provide options tooperate on \"blocks used\" (-b), \"inodes used\" (-i), and/or\"realtime blocks used\" (-r).Many commands also have extensive online help. Use the helpcommand for more details on any command.",
        "name": "xfs_quota - manage use of quota on XFS filesystems",
        "section": 8
    },
    {
        "command": "xfs_repair",
        "description": "xfs_repair repairs corrupt or damaged XFS filesystems (seexfs(5)).The filesystem is specified using the device argumentwhich should be the device name of the disk partition or volumecontaining the filesystem. If given the name of a block device,xfs_repair will attempt to find the raw device associated withthe specified block device and will use the raw device instead.Regardless, the filesystem to be repaired must be unmounted,otherwise, the resulting filesystem may be inconsistent orcorrupt.",
        "name": "xfs_repair - repair an XFS filesystem",
        "section": 8
    },
    {
        "command": "xfs_rtcp",
        "description": "xfs_rtcp copies a file to the realtime partition on an XFSfilesystem.If there is more than one source and target, thefinal argument (the target) must be a directory which alreadyexists.",
        "name": "xfs_rtcp - XFS realtime copy command",
        "section": 8
    },
    {
        "command": "xfs_scrub",
        "description": "xfs_scrub attempts to check and repair all metadata in a mountedXFS filesystem.WARNING!This program is EXPERIMENTAL, which means that itsbehavior and interface could change at any time!xfs_scrub asks the kernel to scrub all metadata objects in thefilesystem.Metadata records are scanned for obviously badvalues and then cross-referenced against other metadata.Thegoal is to establish a reasonable confidence about theconsistency of the overall filesystem by examining theconsistency of individual metadata records against the othermetadata in the filesystem.Damaged metadata can be rebuilt fromother metadata if there exists redundant data structures whichare intact.Filesystem corruption and optimization opportunities will belogged to the standard error stream.Enabling verbose mode willincrease the amount of status information sent to the output.If the kernel scrub reports that metadata needs repairs oroptimizations and the user does not pass -n on the command line,this program will ask the kernel to make the repairs and toperform the optimizations.See the sections about optimizationsand repairs for a list of optimizations and repairs known to thisprogram.The kernel may not support repairing or optimizing thefilesystem.If this is the case, the filesystem must beunmounted and xfs_repair(8) run on the filesystem to fix theproblems.",
        "name": "xfs_scrub - check and repair the contents of a mounted XFSfilesystem",
        "section": 8
    },
    {
        "command": "xfs_scrub_all",
        "description": "xfs_scrub_all attempts to read and check all the metadata on allmounted XFS filesystems.The online scrub is performed via thexfs_scrub tool, either by running it directly or by using systemdto start it in a restricted fashion.Mounted filesystems aremapped to physical storage devices so that scrub operations canbe run in parallel so long as no two scrubbers access the samedevice simultaneously.",
        "name": "xfs_scrub_all - scrub all mounted XFS filesystems",
        "section": 8
    },
    {
        "command": "xfs_spaceman",
        "description": "xfs_spaceman reports and controls free space usage in an XFSfilesystem.",
        "name": "xfs_spaceman - show free space information about an XFSfilesystem",
        "section": 8
    },
    {
        "command": "xfsdump",
        "description": "xfsdump backs up files and their attributes in a filesystem.Thefiles are dumped to storage media, a regular file, or standardoutput.Options allow the operator to have all files dumped,just files that have changed since a previous dump, or just filescontained in a list of pathnames.The xfsrestore(8) utility re-populates a filesystem with thecontents of the dump.Each invocation of xfsdump dumps just one filesystem.Thatinvocation is termed a dump session.The dump session splits thefilesystem into one or more dump streams, one per destination.The split is done in filesystem inode number (ino) order, atboundaries selected to equalize the size of each stream.Furthermore, the breakpoints between streams may be in the middleof very large files (at extent boundaries) if necessary toachieve reasonable stream size equalization.Each dump streamcan span several media objects, and a single media object cancontain several dump streams.The typical media object is a tapecartridge.The media object records the dump stream as one ormore media files.A media file is a self-contained partial dump,intended to minimize the impact of media dropouts on the entiredump stream at the expense of increasing the time required tocomplete the dump. By default only one media file is writtenunless a media file size is specified using the -d option. Othertechniques, such as making a second copy of the dump image,provide more protection against media failures than multiplemedia files will.xfsdump maintains an online dump inventory in/var/lib/xfsdump/inventory.The -I option displays the inventorycontents hierarchically.The levels of the hierarchy are:filesystem, dump session, stream, and media file.The options to xfsdump are:-aSpecifies that files for which the Data Migration Facility(DMF) has complete offline copies (dual-state files) betreated as if they were offline (OFL).This means that thefile data will not be dumped by xfsdump, resulting in asmaller dump file.If the file is later restored the filedata is still accessible through DMF.If both '-a option'and '-z option' are specified, the '-a option' takesprecedence (see '-z option' below).-b blocksizeSpecifies the blocksize, in bytes, to be used for the dump.The same blocksize must be specified to restore the tape.If the -m option is not used, then -b does not need to bespecified. Instead, a default blocksize of 1Mb will be used.-c prognameUse the specified program to alert the operator when a mediachange is required. The alert program is typically a scriptto send a mail or flash a window to draw the operator'sattention.-d filesizeSpecifies the size, in megabytes, of dump media files.Ifnot specified, xfsdump will dump data to tape using a singlemedia file per media object.The specified media file sizemay need to be adjusted if, for example, xfsdump cannot fita media file onto a single tape.-eAllow files to be excluded from the dump.This will causexfsdump to skip files which have the \"no dump\" fileattribute set. See the \"Excluding individual files\" sectionbelow for details on setting this file attribute.-f dest [ -f dest ... ]Specifies a dump destination.A dump destination can be thepathname of a device (such as a tape drive), a regular fileor a remote tape drive (see rmt(8)).This option must beomitted if the standard output option (a lone - precedingthe source filesystem specification) is specified.-l levelSpecifies a dump level of 0 to 9.The dump level determinesthe base dump to which this dump is relative.The base dumpis the most recent dump at a lesser level.A level 0 dumpis absolute - all files are dumped.A dump level where 1 <=level <= 9 is referred to as an incremental dump.Onlyfiles that have been changed since the base dump are dumped.Subtree dumps (see the -s option below) cannot be used asthe base for incremental dumps.-mUse the minimal tape protocol for non-scsi tape destinationsor remote tape destinations which are not scsi Linux tapedrives nor IRIX tape drives.This option cannot be usedwithout specifying a blocksize to be used (see -b optionabove).-oOverwrite the tape. With this option, xfsdump does not readthe tape first to check the contents. This option may beused if xfsdump is unable to determine the block size of atape .-p intervalCauses progress reports to be printed at the specifiedinterval.interval is given in seconds.The progressreport indicates how many files have been dumped, the totalnumber of files to dump, the percentage of data dumped, andthe elapsed time.-qDestination tape drive is a QIC tape.QIC tapes only use a512 byte blocksize, for which xfsdump must make specialallowances.-s pathname [ -s pathname ... ]Restricts the dump to files contained in the specifiedpathnames (subtrees).A pathname must be relative to themount point of the filesystem.For example, if a filesystemis mounted at /d2, the pathname argument for the directory/d2/users is ``users''.A pathname can be a file or adirectory; if it is a directory, the entire hierarchy offiles and subdirectories rooted at that directory is dumped.Subtree dumps cannot be used as the base for incrementaldumps (see the -l option above).-t fileSets the dump time to the modification time of file ratherthan using the current time.xfsdump uses the dump time todetermine what files need to be backed up during anincremental dump. This option should be used when dumpingsnapshots so that the dump time matches the time thesnapshot was taken. Otherwise files modified after asnapshot is taken may be skipped in the next incrementaldump.-v verbosity-v subsys=verbosity[,subsys=verbosity,...]Specifies the level of detail used for messages displayedduring the course of the dump. The verbosity argument can bepassed as either a string or an integer. If passed as astring the following values may be used: silent, verbose,trace, debug, or nitty.If passed as an integer, valuesfrom 0-5 may be used. The values 0-4 correspond to thestrings already listed. The value 5 can be used to produceeven more verbose debug output.The first form of this option activates message loggingacross all dump subsystems. The second form allows themessage logging level to be controlled on a per-subsystembasis. The two forms can be combined (see the examplebelow). The argument subsys can take one of the followingvalues: general, proc, drive, media, inventory, inomap andexcluded_files.For example, to dump the root filesystem with tracingactivated for all subsystems:# xfsdump -v trace -f /dev/tape /To enable debug-level tracing for drive and mediaoperations:# xfsdump -v drive=debug,media=debug -f /dev/tape /To enable tracing for all subsystems, and debug leveltracing for drive operations only:# xfsdump -v trace,drive=debug -f /dev/tape /To list files that will be excluded from the dump:# xfsdump -e -v excluded_files=debug -f /dev/tape /-z sizeSpecifies the maximum size, in kilobytes, of files to beincluded in the dump.Files over this size, will beexcluded from the dump, except for DMF dual-state files when'-a option' is specified (see '-a option' above).Whenspecified, '-a option' takes precedence over '-z option'.The size is an estimate based on the number of disk blocksactually used by the file, and so does not include holes.In other words, size refers to the amount of space the filewould take in the resulting dump.On an interactiverestore, the skipped file is visible with xfsrestore's 'ls'and while you can use the 'add' and 'extract' commands,nothing will be restored.-ADo not dump extended file attributes.When dumping afilesystem managed within a DMF environment this optionshould not be used. DMF stores file migration status withinextended attributes associated with each file. If theseattributes are not preserved when the filesystem isrestored, files that had been in migrated state will not berecallable by DMF. Note that dumps containing extended fileattributes cannot be restored with older versions ofxfsrestore(8).-B session_idSpecifies the ID of the dump session upon which this dumpsession is to be based.If this option is specified, the -l(level) and -R (resume) options are not allowed.Instead,xfsdump determines if the current dump session should beincremental and/or resumed, by looking at the base session'slevel and interrupted attributes.If the base session wasinterrupted, the current dump session is a resumption ofthat base at the same level.Otherwise, the current dumpsession is an incremental dump with a level one greater thanthat of the base session.This option allows incrementaland resumed dumps to be based on any previous dump, ratherthan just the most recent.-DControls which directories are backed up during anincremental dump. By default unchanged directories aredumped if files or directories beneath them have changed.This results in a self-contained dump -- if a base dump islost, or you know the file(s) you wish to restore is in anincremental dump, you can restore just that dump withoutloading the base dump(s) first. However, this methodrequires a potentially expensive traversal through thefilesystem.When -D is specified, unchanged directories are not dumped.This results in a faster dump, but files will end up in thexfsrestore(8) orphanage directory unless the base dump(s) isloaded first.-EPre-erase media.If this option is specified, media iserased prior to use.The operator is prompted forconfirmation, unless the -F option is also specified.-FDon't prompt the operator.When xfsdump encounters a mediaobject containing non-xfsdump data, xfsdump normally asksthe operator for permission to overwrite.With this optionthe overwrite is performed, no questions asked.Whenxfsdump encounters end-of-media during a dump, xfsdumpnormally asks the operator if another media object will beprovided.With this option the dump is instead interrupted.-IDisplays the xfsdump inventory (no dump is performed).xfsdump records each dump session in an online inventory in/var/lib/xfsdump/inventory.xfsdump uses this inventory todetermine the base for incremental dumps.It is also usefulfor manually identifying a dump session to be restored.Suboptions to filter the inventory display are describedlater.-JInhibits the normal update of the inventory.This is usefulwhen the media being dumped to will be discarded oroverwritten.-KGenerate a format 2 dump instead of the current format. Thisis useful if the dump will be restored on a system with anolder xfsrestore which does not understand the current dumpformat. Use of this option is otherwise not recommended.-L session_labelSpecifies a label for the dump session.It can be anyarbitrary string up to 255 characters long.-M label [ -M label ... ]Specifies a label for the first media object (for example,tape cartridge) written on the corresponding destinationduring the session.It can be any arbitrary string up to255 characters long.Multiple media object labels can bespecified, one for each destination.-O options_fileInsert the options contained in options_file into thebeginning of the command line.The options are specifiedjust as they would appear if typed into the command line.In addition, newline characters (\\n) can be used aswhitespace.The options are placed before all optionsactually given on the command line, just after the commandname.Only one -O option can be used.Recursive use isignored.The source filesystem cannot be specified inoptions_file.-RResumes a previously interrupted dump session.If the mostrecent dump at this dump's level (-l option) wasinterrupted, this dump contains only files not in theinterrupted dump and consistent with the incremental level.However, files contained in the interrupted dump that havebeen subsequently modified are re-dumped.-TInhibits interactive dialogue timeouts.When the -F optionis not specified, xfsdump prompts the operator for labelsand media changes.Each dialogue normally times out if noresponse is supplied.This option prevents the timeout.-Y lengthSpecify I/O buffer ring length.xfsdump uses a ring ofoutput buffers to achieve maximum throughput when dumping totape drives.The default ring length is 3.However, thisis not currently enabled on Linux yet, making this optionbenign.-A lone - causes the dump stream to be sent to the standardoutput, where it can be piped to another utility such asxfsrestore(8) or redirected to a file.This option cannotbe used with the -f option.The - must follow all otheroptions and precede the filesystem specification.The filesystem, filesystem, can be specified either as a mountpoint or as a special device file (for example,/dev/dsk/dks0d1s0).The filesystem must be mounted to be dumped.",
        "name": "xfsdump - XFS filesystem incremental dump utility",
        "section": 8
    },
    {
        "command": "xfsinvutil",
        "description": "xfsdump maintains an online dump inventory in/var/lib/xfsdump/inventory.The -I option of xfsdump displaysthe inventory contents hierarchically.The levels of thehierarchy are: filesystem, dump session, stream, and media file.xfsinvutil is a utility to check this inventory database forconsistency, to remove entries of dump sessions which may nolonger be of relevance, and to browse the contents of theinventory.The following command line options are available:-FDon't prompt the operator.When xfsinvutil encounters amatching dump session, xfsinvutil will normally ask theoperator for permission to delete the entry. With thisoption the deletion is performed, no questions asked.-iInteractive mode.Causes xfsinvutil to run in a mode thatwill allow the operator to browse and modify the contents ofthe inventory. Please refer to the Interactive Mode sectionbelow for more information.-M mount_point mm/dd/yyyyPrunes dump sessions identified by the given mount pointwhich were created prior to the specified date. Optionally-m may be be specified to further limit the matching dumpsessions by media label.xfsinvutil will prompt theoperator prior to pruning a dump session unless the -F or -ioptions are given.-u UUID mm/dd/yyyyLike -M, except the matching filesystem is specified usingits universally unique identifier (UUID) instead of itsmount point.-m media_labelIf specified, only sessions with at least one media filewhose label matches this value will be eligible for pruning.This restriction is in addition to those imposed by the dateand the -M or -u options.This option allows the pruning ofall inventory references to media which may have beenoverwritten or lost. Note that this option does not apply tosessions with no media files.-s SESSION_IDPrunes the dump session identified by the given session id.xfsinvutil will prompt the operator prior to pruning a dumpsession unless the -F option is given.-CWith this option, xfsinvutil performs consistency checks forall entries in the inventory database.It fixes anyproblems found. If no consistent entries are found , thecorresponding inventory database file is removed.Interactive ModeWhen run with -i, xfsinvutil will present the operator with ahierarchical representation of the xfsdump inventory.In thismode, the operator can use the arrow keys (or the h j k l keys)to navigate the inventory and also use the following keys andfunctionality:+Expand a branch of the tree.-Collapse a branch of the tree.*Fully expand a branch of the tree.%Fully collapse a branch of the tree.hMove selection to the parent entry.lMove selection to the child entry.jMove selection to next entry.kMove selection to previous entry.dMark the current record, and all sub-records, to be deleted.uClear the current record and all parent records from beingdeleted.iImport another xfsdump inventory.?Show help window.xCommit any changes made to the inventory, and exit theprogram.qExit the program without committing any changes to theinventory.Inventories from other hosts can be imported to create a largercommon inventory.It is recommended that the contents of/var/lib/xfsdump/inventory on the remote host be copied to atemporary local directory prior to running xfsinvutil.Whenxfsinvutil is run in interactive mode, and the operator hits i,they will be prompted for the path to the inventory to beimported.xfsinvutil will then add the contents of the importedinventory to the main window, and all entries will be marked asimported.The operator can then delete any of these entries thatare not to be imported.When the operator commits the changes,xfsinvutil will import any entries which were not marked asdeleted.Deleted entries are not actually deleted from theoriginal imported inventory.",
        "name": "xfsinvutil - xfsdump inventory database checking and pruningutility",
        "section": 8
    },
    {
        "command": "xfsrestore",
        "description": "xfsrestore restores filesystems from dumps produced byxfsdump(8).Two modes of operation are available: simple andcumulative.The default is simple mode.xfsrestore populates the specifieddestination directory, dest, with the files contained in the dumpmedia.The -r option specifies the cumulative mode.Successiveinvocations of xfsrestore are used to apply a chronologicallyordered sequence of delta dumps to a base (level 0) dump.Thecontents of the filesystem at the time each dump was produced isreproduced.This can involve adding, deleting, renaming,linking, and unlinking files and directories.A delta dump is defined as either an incremental dump (xfsdump -loption with level > 0) or a resumed dump (xfsdump -R option).The deltas must be applied in the order they were produced.Eachdelta applied must have been produced with the previously applieddelta as its base.xfsrestore keeps state information in thexfsrestorehousekeepingdir, to inform subsequent invocations whenused in cumulative mode, or in the event a restore isinterrupted.To ensure that the state information can beprocessed, a compatible version of xfsrestore must be used foreach subsequent invocation. Additionally, each invocation mustrun on a system of the same endianness and page size.The options to xfsrestore are:-a housekeepingEach invocation of xfsrestore creates a directory calledxfsrestorehousekeepingdir.This directory is normallycreated directly under the dest directory.The -a optionallows the operator to specify an alternate directory,housekeeping, in which xfsrestore creates thexfsrestorehousekeepingdir directory.When performing acumulative (-r option) restore or resuming (-R option) arestore, each successive invocation must specify the samealternate directory.-b blocksizeSpecifies the blocksize, in bytes, to be used for therestore.For other drives such as DAT or 8 mm , the sameblocksize used for the xfsdump operation must be specifiedto restore the tape.The default block size is 1Mb.-c prognameUse the specified program to alert the operator when a mediachange is required. The alert program is typically a scriptto send a mail or flash a window to draw the operator'sattention.-ePrevents xfsrestore from overwriting existing files in thedest directory.-f source [ -f source ... ]Specifies a source of the dump to be restored.This can bethe pathname of a device (such as a tape drive), a regularfile or a remote tape drive (see rmt(8)).This option mustbe omitted if the standard input option (a lone - precedingthe dest specification) is specified.-iSelects interactive operation.Once the on-media directoryhierarchy has been read, an interactive dialogue is begun.The operator uses a small set of commands to peruse thedirectory hierarchy, selecting files and subtrees forextraction.The available commands are given below.Initially nothing is selected, except for those subtreesspecified with -s command line options.ls [arg]List the entries in the current directory or thespecified directory, or the specified non-directoryfile entry.Both the entry's original inode numberand name are displayed.Entries that are directoriesare appended with a `/'.Entries that have beenselected for extraction are prepended with a `*'.cd [arg]Change the current working directory to the specifiedargument, or to the filesystem root directory if noargument is specified.pwdPrint the pathname of the current directory, relativeto the filesystem root.add [arg]The current directory or specified file or directorywithin the current directory is selected forextraction.If a directory is specified, then it andall its descendents are selected.Entries that areselected for extraction are prepended with a `*' whenthey are listed by ls.delete [arg]The current directory or specified file or directorywithin the current directory is deselected forextraction.If a directory is specified, then it andall its descendents are deselected.The mostexpedient way to extract most of the files from adirectory is to select the directory and thendeselect those files that are not needed.extractEnds the interactive dialogue, and causes allselected subtrees to be restored.quitxfsrestore ends the interactive dialogue andimmediately exits, even if there are files orsubtrees selected for extraction.helpList a summary of the available commands.-mUse the minimal tape protocol.This option cannot be usedwithout specifying a blocksize to be used (see -b optionabove).-n fileAllows xfsrestore to restore only files newer than file.The modification time of file (i.e., as displayed with thels -l command) is compared to the inode modification time ofeach file on the source media (i.e., as displayed with thels -lc command).A file is restored from media only if itsinode modification time is greater than or equal to themodification time of file.-oRestore file and directory owner/group even if not root.When run with an effective user id of root, xfsrestorerestores owner and group of each file and directory.Whenrun with any other effective user id it does not, unlessthis option is specified.-p intervalCauses progress reports to be printed at intervals ofinterval seconds.The interval value is approximate,xfsrestore will delay progress reports to avoid undueprocessing overhead.-qSource tape drive is a QIC tape.QIC tapes only use a 512byte blocksize, for which xfsrestore must make specialallowances.-rSelects the cumulative mode of operation. The -a anddestination options must be the same for each invocation.-s subtreeSpecifies a subtree to restore.Any number of -s optionsare allowed.The restore is constrained to the union of allsubtrees specified.Each subtree is specified as a pathnamerelative to the restore dest.If a directory is specified,the directory and all files beneath that directory arerestored.-tDisplays the contents of the dump, but does not create ormodify any files or directories.It may be desirable to setthe verbosity level to silent when using this option.-v verbosity-v subsys=verbosity[,subsys=verbosity,...]Specifies the level of detail used for messages displayedduring the course of the restore. The verbosity argument canbe passed as either a string or an integer. If passed as astring the following values may be used: silent, verbose,trace, debug, or nitty.If passed as an integer, valuesfrom 0-5 may be used. The values 0-4 correspond to thestrings already listed. The value 5 can be used to produceeven more verbose debug output.The first form of this option activates message loggingacross all restore subsystems. The second form allows themessage logging level to be controlled on a per-subsystembasis. The two forms can be combined (see the examplebelow). The argument subsys can take one of the followingvalues: general, proc, drive, media, inventory, and tree.For example, to restore the root filesystem with tracingactivated for all subsystems:# xfsrestore -v trace -f /dev/tape /To enable debug-level tracing for drive and mediaoperations:# xfsrestore -v drive=debug,media=debug -f /dev/tape /To enable tracing for all subsystems, and debug leveltracing for drive operations only:# xfsrestore -v trace,drive=debug -f /dev/tape /-ADo not restore extended file attributes.When restoring afilesystem managed within a DMF environment this optionshould not be used. DMF stores file migration status withinextended attributes associated with each file. If theseattributes are not preserved when the filesystem isrestored, files that had been in migrated state will not berecallable by DMF. Note that dumping of extended fileattributes is also optional.-BChange the ownership and permissions of the destinationdirectory to match those of the root directory of the dump.-DRestore DMAPI (Data Management Application ProgrammingInterface) event settings. If the restored filesystem willbe managed within the same DMF environment as the originaldump it is essential that the -D option be used. Otherwiseit is not usually desirable to restore these settings.-EPrevents xfsrestore from overwriting newer versions offiles.The inode modification time of the on-media file iscompared to the inode modification time of correspondingfile in the dest directory.The file is restored only ifthe on-media version is newer than the version in the destdirectory.The inode modification time of a file can bedisplayed with the ls -lc command.-FInhibit interactive operator prompts.This option inhibitsxfsrestore from prompting the operator for verification ofthe selected dump as the restore target and from promptingfor any media change.-ICauses the xfsdump inventory to be displayed (no restore isperformed).Each time xfsdump is used, an online inventoryin /var/lib/xfsdump/inventory is updated.This is used todetermine the base for incremental dumps.It is also usefulfor manually identifying a dump session to be restored (seethe -L and -S options).Suboptions to filter the inventorydisplay are described later.-JInhibits inventory update when on-media session inventoryencountered during restore.xfsrestore opportunisticallyupdates the online inventory when it encounters an on-mediasession inventory, but only if run with an effective user idof root and only if this option is not given.-KForce xfsrestore to use dump format 2 generation numbers.Normally the need for this is determined automatically, butthis option is required on the first xfsrestore invocationin the rare case that a cumulative restore begins with aformat 3 (or newer) dump and will be followed by a format 2dump.-L session_labelSpecifies the label of the dump session to be restored.Thesource media is searched for this label.It is anyarbitrary string up to 255 characters long.The label ofthe desired dump session can be copied from the inventorydisplay produced by the -I option.-O options_fileInsert the options contained in options_file into thebeginning of the command line.The options are specifiedjust as they would appear if typed into the command line.In addition, newline characters (\\n) can be used aswhitespace.The options are placed before all optionsactually given on the command line, just after the commandname.Only one -O option can be used.Recursive use isignored.The destination directory cannot be specified inoptions_file.-QForce completion of an interrupted restore session.Thisoption is required to work around one specific pathologicalscenario.When restoring a dump session which wasinterrupted due to an EOM condition and no online sessioninventory is available, xfsrestore cannot know when therestore of that dump session is complete.The operator isforced to interrupt the restore session.In that case, ifthe operator tries to subsequently apply a resumed dump(using the -r option), xfsrestore refuses to do so.Theoperator must tell xfsrestore to consider the base restorecomplete by using this option when applying the resumeddump.-RResume a previously interrupted restore.xfsrestore can beinterrupted at any time by pressing the terminal interruptcharacter (see stty(1)).Use this option to resume therestore.The -a and destination options must be the same.-S session_idSpecifies the session UUID of the dump session to berestored.The source media is searched for this UUID.TheUUID of the desired dump session can be copied from theinventory display produced by the -I option.-TInhibits interactive dialogue timeouts.xfsrestore promptsthe operator for media changes.This dialogue normallytimes out if no response is supplied.This option preventsthe timeout.-X subtreeSpecifies a subtree to exclude.This is the converse of the-s option.Any number of -X options are allowed.Eachsubtree is specified as a pathname relative to the restoredest.If a directory is specified, the directory and allfiles beneath that directory are excluded.-Y io_ring_lengthSpecify I/O buffer ring length.xfsrestore uses a ring ofinput buffers to achieve maximum throughput when restoringfrom tape drives.The default ring length is 3.However,this is not currently enabled on Linux yet, making thisoption benign.-A lone - causes the standard input to be read as the sourceof the dump to be restored.Standard input can be a pipefrom another utility (such as xfsdump(8)) or a redirectedfile.This option cannot be used with the -f option.The -must follow all other options, and precede the destspecification.The dumped filesystem is restored into the dest directory.Thereis no default; the dest must be specified.",
        "name": "xfsrestore - XFS filesystem incremental restore utility",
        "section": 8
    },
    {
        "command": "xqmstats",
        "description": "xqmstat queries the kernel for the XFS Quota Manager dquotstatistics.It displays:\u2022Reclaims\u2022Missed reclaims\u2022Dquot dups\u2022Cache misses\u2022Cache hits\u2022Dquot wants\u2022Shake reclaims\u2022Inact reclaims",
        "name": "xqmstats - Display XFS quota manager statistics from /proc",
        "section": 8
    },
    {
        "command": "xt",
        "description": "The xt action allows one to call arbitrary iptables targets forpackets matching the filter this action is attached to.",
        "name": "xt - tc iptables action",
        "section": 8
    },
    {
        "command": "xtables-legacy",
        "description": "xtables-legacy are the original versions of iptables that use oldgetsockopt/setsockopt-based kernel interface.This kernelinterface has some limitations, therefore iptables can also beused with the newer nf_tables based API.See xtables-nft(8) forinformation about the xtables-nft variants of iptables.",
        "name": "xtables-legacy \u2014 iptables using old getsockopt/setsockopt-basedkernel api",
        "section": 8
    },
    {
        "command": "xtables-monitor",
        "description": "xtables-monitor is used to monitor changes to the ruleset or toshow rule evaluation events for packets tagged using the TRACEtarget.xtables-monitor will run until the user abortsexecution, typically by using CTRL-C.",
        "name": "xtables-monitor \u2014 show changes to rule set and trace-events",
        "section": 8
    },
    {
        "command": "xtables-nft",
        "description": "xtables-nft are versions of iptables that use the nftables API.This is a set of tools to help the system administrator migratethe ruleset from iptables(8), ip6tables(8), arptables(8), andebtables(8) to nftables(8).The xtables-nft set is composed of several commands:\u2022 iptables-nft\u2022 iptables-nft-save\u2022 iptables-nft-restore\u2022 ip6tables-nft\u2022 ip6tables-nft-save\u2022 ip6tables-nft-restore\u2022 arptables-nft\u2022 ebtables-nftThese tools use the libxtables framework extensions and hook tothe nf_tables kernel subsystem using the nft_compat module.",
        "name": "xtables-nft \u2014 iptables using nftables kernel api",
        "section": 8
    },
    {
        "command": "xtables-translate",
        "description": "There is a set of tools to help the system administratortranslate a given ruleset from iptables(8), ip6tables(8) andebtables(8) to nftables(8).The available commands are:\u2022 iptables-translate\u2022 iptables-restore-translate\u2022 ip6tables-translate\u2022 ip6tables-restore-translate\u2022 ebtables-translate",
        "name": "iptables-translate \u2014 translation tool to migrate from iptables tonftablesip6tables-translate \u2014 translation tool to migrate from ip6tablesto nftablesebtables-translate \u2014 translation tool to migrate from ebtables tonftables",
        "section": 8
    },
    {
        "command": "yum",
        "description": "DNF is the next upcoming major version of YUM, a package managerfor RPM-based Linux distributions. It roughly maintains CLIcompatibility with YUM and defines a strict API for extensionsand plugins.Plugins can modify or extend features of DNF or provideadditional CLI commands on top of those mentioned below. If youknow the name of such a command (including commands mentionedbelow), you may find/install the package which provides it usingthe appropriate virtual provide in the form ofdnf-command(<alias>), where <alias> is the name of the command;e.g.``dnf install 'dnf-command(versionlock)'`` installs aversionlock plugin. This approach also applies to specifyingdependencies of packages that require a particular DNF command.Return values:\u2022 0: Operation was successful.\u2022 1: An error occurred, which was handled by dnf.\u2022 3: An unknown unhandled error occurred during operation.\u2022 100: See check-update\u2022 200: There was a problem with acquiring or releasing of locks.Available commands:\u2022 alias\u2022 autoremove\u2022 check\u2022 check-update\u2022 clean\u2022 deplist\u2022 distro-sync\u2022 downgrade\u2022 group\u2022 help\u2022 history\u2022 info\u2022 install\u2022 list\u2022 makecache\u2022 mark\u2022 module\u2022 provides\u2022 reinstall\u2022 remove\u2022 repoinfo\u2022 repolist\u2022 repoquery\u2022 repository-packages\u2022 search\u2022 shell\u2022 swap\u2022 updateinfo\u2022 upgrade\u2022 upgrade-minimalAdditional information:\u2022 Options\u2022 Specifying Packages\u2022 Specifying Provides\u2022 Specifying File Provides\u2022 Specifying Groups\u2022 Specifying Transactions\u2022 Metadata Synchronization\u2022 Configuration Files Replacement Policy\u2022 Files\u2022 See Also",
        "name": "yum - redirecting to DNF Command Reference",
        "section": 8
    },
    {
        "command": "yum-complete-transaction",
        "description": "yum-complete-transaction is a program which finds incomplete oraborted yum transactions on a system and attempts to completethem. It looks at the transaction-all* and transaction-done*files which can normally be found in /var/lib/yum if a yumtransaction aborted in the middle of execution.If it finds more than one unfinished transaction it will attemptto complete the most recent one first. You can run it more thanonce to clean up all unfinished transactions.",
        "name": "yum-complete-transaction - attempt to complete failed or abortedYum transactions",
        "section": 8
    },
    {
        "command": "yum-copr",
        "description": null,
        "name": "yum-plugin-copr - YUM copr PluginWork with Copr & Playground repositories on the local system.\u2022 The copr command is used to add or remove Copr repositories tothe local system\u2022 The playground is used to enable or disable the Playgroundrepository",
        "section": 8
    },
    {
        "command": "yum-cron",
        "description": "yum-cron is an alternate interface to yum that is optimised to beconvenient to call from cron.It provides methods to keeprepository metadata up to date, and to check for, download, andapply updates.Rather than accepting many different command linearguments, the different functions of yum-cron can be accessedthrough config files.config-file is used to optionally specify the path to theconfiguration file to use.If it is not given, the defaultconfiguration file will be used.It is useful to be able tospecify different configuration files for different use cases.For example, one configuration file might be set to update therepository metadata, and a line could be added to the crontab torun yum-cron frequently using this file.Then, anotherconfiguration file might be set to install updates, and yum-croncould be run from cron using this file just once each day.",
        "name": "yum-cron - an interface to conveniently call yum from cron",
        "section": 8
    },
    {
        "command": "yum-plugin-copr",
        "description": null,
        "name": "yum-plugin-copr - YUM copr PluginWork with Copr & Playground repositories on the local system.\u2022 The copr command is used to add or remove Copr repositories tothe local system\u2022 The playground is used to enable or disable the Playgroundrepository",
        "section": 8
    },
    {
        "command": "yum-shell",
        "description": "DNF is the next upcoming major version of YUM, a package managerfor RPM-based Linux distributions. It roughly maintains CLIcompatibility with YUM and defines a strict API for extensionsand plugins.Plugins can modify or extend features of DNF or provideadditional CLI commands on top of those mentioned below. If youknow the name of such a command (including commands mentionedbelow), you may find/install the package which provides it usingthe appropriate virtual provide in the form ofdnf-command(<alias>), where <alias> is the name of the command;e.g.``dnf install 'dnf-command(versionlock)'`` installs aversionlock plugin. This approach also applies to specifyingdependencies of packages that require a particular DNF command.Return values:\u2022 0: Operation was successful.\u2022 1: An error occurred, which was handled by dnf.\u2022 3: An unknown unhandled error occurred during operation.\u2022 100: See check-update\u2022 200: There was a problem with acquiring or releasing of locks.Available commands:\u2022 alias\u2022 autoremove\u2022 check\u2022 check-update\u2022 clean\u2022 deplist\u2022 distro-sync\u2022 downgrade\u2022 group\u2022 help\u2022 history\u2022 info\u2022 install\u2022 list\u2022 makecache\u2022 mark\u2022 module\u2022 provides\u2022 reinstall\u2022 remove\u2022 repoinfo\u2022 repolist\u2022 repoquery\u2022 repository-packages\u2022 search\u2022 shell\u2022 swap\u2022 updateinfo\u2022 upgrade\u2022 upgrade-minimalAdditional information:\u2022 Options\u2022 Specifying Packages\u2022 Specifying Provides\u2022 Specifying File Provides\u2022 Specifying Groups\u2022 Specifying Transactions\u2022 Metadata Synchronization\u2022 Configuration Files Replacement Policy\u2022 Files\u2022 See Also",
        "name": "yum-shell - redirecting to DNF Command Reference",
        "section": 8
    },
    {
        "command": "yum-updatesd",
        "description": "yum-updatesd provides notification of updates which are availableto be applied to your system.This notification can be doneeither via syslog, email or over dbus.Configuration is done viathe yum-updatesd.conf(5)file.",
        "name": "yum-updatesd - Update notifier daemon",
        "section": 8
    },
    {
        "command": "yum2dnf",
        "description": null,
        "name": "yum2dnf - Changes in DNF compared to YUM",
        "section": 8
    },
    {
        "command": "yumdb",
        "description": "This command is used to query and alter the yum database, whichis a simple key value store used in conjunction with the rpmdatabase. Any installed package can have arbitrary data in theyum database, however the main use case is to store extra dataabout packages as they are installed.yumdb commands are:yumdb get <key> [pkg-wildcard]...This command will get the value for the given key, limiting toany specified packages.yumdb set <key> <value> [pkg-wildcard]...This command will set the value for the given key, to the givenvalue, limiting to any specified packages.yumdb del <key> [pkg-wildcard]...This command will delete the given key, limiting to any specifiedpackages.yumdb rename <old-key> <new-key> [pkg-wildcard]...This command will rename the given old-key, to the given new-key,limiting to any specified packages. If the old-key does notexist, nothing happens.yumdb rename-force <old-key> <new-key> [pkg-wildcard]...This command will rename the given old-key, to the given new-key,limiting to any specified packages. If the old-key does notexist, new-key is deleted.yumdb copy <old-key> <new-key> [pkg-wildcard]...This command will copy the given old-key, to the given new-key,limiting to any specified packages. If the old-key does notexist, nothing happens.yumdb copy-force <old-key> <new-key> [pkg-wildcard]...This command will copy the given old-key, to the given new-key,limiting to any specified packages. If the old-key does notexist, new-key is deleted.yumdb search <key> <wildcard>...This command will search all packages for the given key, againstany of the given wildcard values.yumdb exist <key> [pkg-wildcard]...This command will print any packages which have the given key,limiting to any specified packages.yumdb unset <key> [pkg-wildcard]...This command will print any packages which do not have the givenkey, limiting to any specified packages.yumdb info [pkg-wildcard]...This command will display all the data stored in the yumdb,limiting to any specified packages.yumdb sync [pkg-wildcard]...This command will add any missing data to the yumdb from therepositories, limiting to any specified packages. This is usefulto run if you have had any aborted transactions (and thus.missing yumdb data).Note that \"yumdb sync\" cannot know all theinformation that would have been put into the yumdb at the time.yumdb sync-force [pkg-wildcard]...This command will replace any data in the yumdb from therepositories, limiting to any specified packages.",
        "name": "yumdb - query and alter the Yum database",
        "section": 8
    },
    {
        "command": "zdump",
        "description": "The zdump program prints the current time in each timezone namedon the command line.",
        "name": "zdump - timezone dumper",
        "section": 8
    },
    {
        "command": "zic",
        "description": "The zic program reads text from the file(s) named on the commandline and creates the timezone information format (TZif) filesspecified in this input.If a filename is \u201c-\u201d, standard input isread.",
        "name": "zic - timezone compiler",
        "section": 8
    },
    {
        "command": "zramctl",
        "description": "zramctl is used to quickly set up zram device parameters, toreset zram devices, and to query the status of used zram devices.If no option is given, all non-zero size zram devices are shown.Note that zramdev node specified on command line has to alreadyexist. The command zramctl creates a new /dev/zram<N> nodes onlywhen --find option specified. It\u2019s possible (and common) thatafter system boot /dev/zram<N> nodes are not created yet.",
        "name": "zramctl - set up and control zram devices",
        "section": 8
    }
]